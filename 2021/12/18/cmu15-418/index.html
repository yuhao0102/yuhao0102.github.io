<!DOCTYPE html>
<html lang="zn-ch">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="lecture 1指令级并行（ILP）  事实上，处理器确实利用并行执行使程序运行得更快，这对程序员来说是不可见的 想法：指令必须看起来是按程序顺序执行的。但处理器可以同时执行独立的指令，而不会影响程序的正确性 超标量执行：处理器在指令序列中动态查找独立指令并并行执行  下图是ILP的原理，第一行是三个可以并行的指令，之后只能串行。 ILP和处理器频率的提升已经很缓慢，所以并不能持续用这两种方法实">
<meta property="og:type" content="article">
<meta property="og:title" content="CMU 15-418 笔记">
<meta property="og:url" content="http://yoursite.com/2021/12/18/cmu15-418/index.html">
<meta property="og:site_name" content="Hao Yu&#39;s blog">
<meta property="og:description" content="lecture 1指令级并行（ILP）  事实上，处理器确实利用并行执行使程序运行得更快，这对程序员来说是不可见的 想法：指令必须看起来是按程序顺序执行的。但处理器可以同时执行独立的指令，而不会影响程序的正确性 超标量执行：处理器在指令序列中动态查找独立指令并并行执行  下图是ILP的原理，第一行是三个可以并行的指令，之后只能串行。 ILP和处理器频率的提升已经很缓慢，所以并不能持续用这两种方法实">
<meta property="og:locale" content="zn_CH">
<meta property="og:image" content="http://yoursite.com/img/1639802194.png">
<meta property="og:image" content="http://yoursite.com/img/1639807892.png">
<meta property="og:image" content="http://yoursite.com/img/1639808193.png">
<meta property="og:image" content="http://yoursite.com/img/1639808590.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639808828.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639809521.png">
<meta property="og:image" content="http://yoursite.com/img/1639809905.png">
<meta property="og:image" content="http://yoursite.com/img/1639810240.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639810613.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639816043.png">
<meta property="og:image" content="http://yoursite.com/img/1639816915.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639817132.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639817978.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639819364.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639823726.png">
<meta property="og:image" content="http://yoursite.com/img/1639824277.png">
<meta property="og:image" content="http://yoursite.com/img/1639825906.png">
<meta property="og:image" content="http://yoursite.com/img/1639826639.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639827312.png">
<meta property="og:image" content="http://yoursite.com/img/1639828816.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639828860.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639829805.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639830087.png">
<meta property="og:image" content="http://yoursite.com/img/1639830216.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639830245.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639835717.png">
<meta property="og:image" content="http://yoursite.com/img/1639839915.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639840798.png">
<meta property="og:image" content="http://yoursite.com/img/1639844349.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639844935.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639845602.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639845791.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639845945.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639846029.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639846114.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639846224.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639880273.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639880425.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639880672.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639882716.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639883901.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639884148.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639884533.png">
<meta property="og:image" content="http://yoursite.com/img/1639886382.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639886607.png">
<meta property="og:image" content="http://yoursite.com/img/1639886794.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639888794.png">
<meta property="og:image" content="http://yoursite.com/img/1639888976.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639889381.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639891385.png">
<meta property="og:image" content="http://yoursite.com/img/1639892682.jpg">
<meta property="og:image" content="http://yoursite.com/img/1639893108.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640008094.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640008248.png">
<meta property="og:image" content="http://yoursite.com/img/1640008407.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640009631.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640010466.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640011038.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640011267.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640088818.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640093948.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640097950.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640098118.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640098677.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640099335.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640099367.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640099630.png">
<meta property="og:image" content="http://yoursite.com/img/1640101577.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640101756.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640102215.png">
<meta property="og:image" content="http://yoursite.com/img/1640102456.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640103269.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640103585.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640104101.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640104259.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640162740.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640162808.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640163285.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640163929.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640164303.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640164900.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640164968.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640165011.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640165048.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640165079.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640165411.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640166452.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640167121.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640167162.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640167231.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640167369.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640167392.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640167455.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640167654.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640173839.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640174084.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640175249.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640176806.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640178658.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640178802.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640178886.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640179330.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640179399.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640179544.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640267417.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640267736.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640268240.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640268841.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640269398.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640269521.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640269774.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640269837.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640270829.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640270958.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640271045.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640271195.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640271246.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640271268.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640271351.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640271455.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640271502.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640271547.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640271944.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640272000.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640418533.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640418901.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640419258.png">
<meta property="og:image" content="http://yoursite.com/img/1640419307.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640420384.png">
<meta property="og:image" content="http://yoursite.com/img/1640421813.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640420386.png">
<meta property="og:image" content="http://yoursite.com/img/1640422233.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640424065.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640491407.png">
<meta property="og:image" content="http://yoursite.com/img/1640492001.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640497184.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640497265.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640499708.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640499819.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640500730.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640500898.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640501501.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640502017.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640504311.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640504396.png">
<meta property="og:image" content="http://yoursite.com/img/1640505321.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640505590.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640508026.jpg">
<meta property="og:image" content="http://yoursite.com/img/1640508284.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641113089.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641113220.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641114333.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641114413.png">
<meta property="og:image" content="http://yoursite.com/img/1641114552.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641114971.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641115105.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641115168.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641115292.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641115359.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641115849.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641115940.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641116056.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641116258.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641116650.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641118923.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641118955.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641118987.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641121643.jpg">
<meta property="og:image" content="http://yoursite.com/img/1641122330.jpg">
<meta property="article:published_time" content="2021-12-18T04:00:00.000Z">
<meta property="article:modified_time" content="2022-01-02T11:31:45.000Z">
<meta property="article:author" content="Hao Yu">
<meta property="article:tag" content="积累">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/img/1639802194.png">

<link rel="canonical" href="http://yoursite.com/2021/12/18/cmu15-418/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zn-ch'
  };
</script>

  <title>CMU 15-418 笔记 | Hao Yu's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hao Yu's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">The program monkey was eaten by the siege lion.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zn-ch">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2021/12/18/cmu15-418/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Hao Yu">
      <meta itemprop="description" content="Introduce something interesting and recode learning process, some articles are written by others, the original link has been given as much as possible, thanks to the original author">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hao Yu's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CMU 15-418 笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-18 12:00:00" itemprop="dateCreated datePublished" datetime="2021-12-18T12:00:00+08:00">2021-12-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-02 19:31:45" itemprop="dateModified" datetime="2022-01-02T19:31:45+08:00">2022-01-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="lecture-1"><a href="#lecture-1" class="headerlink" title="lecture 1"></a>lecture 1</h1><p>指令级并行（ILP）</p>
<ul>
<li>事实上，处理器确实利用并行执行使程序运行得更快，这对程序员来说是不可见的</li>
<li>想法：指令必须看起来是按程序顺序执行的。但处理器可以同时执行独立的指令，而不会影响程序的正确性</li>
<li>超标量执行：处理器在指令序列中动态查找独立指令并并行执行</li>
</ul>
<p>下图是ILP的原理，第一行是三个可以并行的指令，之后只能串行。<br><img src="/img/1639802194.png" alt=""></p>
<p>ILP和处理器频率的提升已经很缓慢，所以并不能持续用这两种方法实现并行加速。单指令流性能扩展率已降低（几乎为零）</p>
<h1 id="lecture-2"><a href="#lecture-2" class="headerlink" title="lecture 2"></a>lecture 2</h1><p>使用泰勒展开式计算<code>sin(x): sin(x)=x - x^3/3! + x^5/5! - x^7/7!+ ...</code>，对于N个浮点数数组的每个元素</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">sinx</span><span class="params">(<span class="type">int</span> N, <span class="type">int</span> terms, <span class="type">float</span>* x, <span class="type">float</span>* result)</span> </span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;N; i++) &#123;</span><br><span class="line">        <span class="type">float</span> value = x[i];</span><br><span class="line">        <span class="type">float</span> numer = x[i] * x[i] * x[i];</span><br><span class="line">        <span class="type">int</span> denom = <span class="number">6</span>; <span class="comment">// 3!</span></span><br><span class="line">        <span class="type">int</span> sign = ‐<span class="number">1</span>;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">1</span>; j&lt;=terms; j++) </span><br><span class="line">        &#123;</span><br><span class="line">            value += sign * numer / denom;</span><br><span class="line">            numer *= x[i] * x[i]; </span><br><span class="line">            denom *= (<span class="number">2</span>*j+<span class="number">2</span>) * (<span class="number">2</span>*j+<span class="number">3</span>); </span><br><span class="line">            sign *= ­‐<span class="number">1</span>; </span><br><span class="line">        &#125; </span><br><span class="line">        result[i] = value; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对中间的for循环中的每个<code>x[i]</code>，如果没有并行的话，每个指令都单步执行，在前三条指令中没有ILP。如果可能的话可以每个时钟解码/执行两个指令。<br><img src="/img/1639807892.png" alt=""></p>
<p>下图是Pentium 4的图，可以看到有两个简单指令解码器，就可以同时解码。<br><img src="/img/1639808193.png" alt=""></p>
<p>前多核处理器时代：大多数芯片晶体管用于执行有助于单个指令流快速运行的操作。</p>
<p>更多的晶体管=更大的缓存，更智能的无序逻辑，更智能的分支预测器，等等。（还有：更多晶体管→更小晶体管→更高的时钟频率）<br><img src="/img/1639808590.jpg" alt=""></p>
<p>在多核时代，有几个想法</p>
<ul>
<li>使用增加晶体管数向处理器添加更多内核</li>
<li>而不是使用晶体管来提高处理器逻辑的复杂性，从而加速单个指令流（例如，无序和推测性操作）</li>
</ul>
<p>如果有两个核，可以并行计算两个元素。可以使用更简单的内核：每个内核只有解码器、运算器、上下文等，没有cache和分支预测逻辑之类的，在运行单个指令流时都比我们原来的内核慢（例如，慢25%）。但是现在有两个核心：2×0.75=1.5（加速潜力！）<br><img src="/img/1639808828.jpg" alt=""></p>
<p>上边的计算程序没啥并行性，只能有一个线程执行，如果每个简单的核比正常的核慢25%，我们的程序在这样的核上只能有之前75%的性能。</p>
<p>可以使用pthreads实现并行性。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">void sinx(int N, int terms, float* x, float* result) </span><br><span class="line">&#123; </span><br><span class="line">    for (int i=0; i&lt;N; i++) </span><br><span class="line">    &#123; </span><br><span class="line">        float value = x[i]; </span><br><span class="line">        float numer = x[i] * x[i] * x[i]; </span><br><span class="line">        int denom = 6; // 3! </span><br><span class="line">        int sign = ‐1; </span><br><span class="line">        for (int j=1; j&lt;=terms; j++) </span><br><span class="line">        &#123;   </span><br><span class="line">            value += sign * numer / denom;</span><br><span class="line">            numer *= x[i] * x[i]; </span><br><span class="line">            denom *= (2*j+2) * (2*j+3); </span><br><span class="line">            sign *= -1; </span><br><span class="line">        &#125; </span><br><span class="line">        result[i] = value; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">typedef struct &#123; </span><br><span class="line">    int N; </span><br><span class="line">    int terms; </span><br><span class="line">    float* x; </span><br><span class="line">    float* result; </span><br><span class="line">&#125; my_args;</span><br><span class="line"></span><br><span class="line">void parallel_sinx(int N, int terms, float* x, float* result) </span><br><span class="line">&#123; </span><br><span class="line">     pthread_t thread_id; </span><br><span class="line">     my_args args; </span><br><span class="line">     args.N = N/2; </span><br><span class="line">     args.terms = terms; </span><br><span class="line">     args.x = x; </span><br><span class="line">     args.result = result; </span><br><span class="line">     pthread_create(&amp;thread_id, NULL, my_thread_start, &amp;args); // launch thread</span><br><span class="line">     sinx(N - args.N, terms, x + args.N, result + args.N); // do work</span><br><span class="line">    pthread_join(thread_id, NULL);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void my_thread_start(void* thread_arg) </span><br><span class="line">&#123; </span><br><span class="line">    my_args* thread_args = (my_args*)thread_arg; </span><br><span class="line">    sinx(args­‐&gt;N, args­‐&gt;terms, args‐&gt;x, args­‐&gt;result); // do work </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>如果有四个核，可以并行计算四个元素。<br><img src="/img/1639809521.png" alt=""></p>
<p>增加ALU以提高计算能力：分摊跨多个ALU管理指令流的成本/复杂性，改为SIMD单指令、多数据流，向所有ALU广播的相同指令，在所有ALU上并行执行指令。<br><img src="/img/1639809905.png" alt=""></p>
<p>矢量程序（使用AVX内部函数）使用256位向量寄存器上的向量指令同时处理八个数组元素。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;immintrin.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">sinx</span><span class="params">(<span class="type">int</span> N, <span class="type">int</span> terms, <span class="type">float</span>* x, <span class="type">float</span>* result)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">float</span> three_fact = <span class="number">6</span>;  <span class="comment">// 3!</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;N; i+=<span class="number">8</span>) </span><br><span class="line">    &#123; </span><br><span class="line">        __m256 origx = _mm256_load_ps(&amp;x[i]); </span><br><span class="line">        __m256 value = origx; </span><br><span class="line">        __m256 numer = _mm256_mul_ps(origx, _mm256_mul_ps(origx, origx)); </span><br><span class="line">        __m256 denom = _mm256_broadcast_ss(&amp;three_fact); </span><br><span class="line">        <span class="type">int</span> sign = <span class="number">-1</span>; </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">1</span>; j&lt;=terms; j++) </span><br><span class="line">        &#123;  </span><br><span class="line">            <span class="comment">// value += sign * numer / denom</span></span><br><span class="line">            __m256 tmp = _mm256_div_ps(_mm256_mul_ps(_mm256_set1ps(sign), numer), denom); </span><br><span class="line">            value = _mm256_add_ps(value, tmp); </span><br><span class="line">            numer = _mm256_mul_ps(numer, _mm256_mul_ps(origx, origx)); </span><br><span class="line">            denom = _mm256_mul_ps(denom, _mm256_broadcast_ss((<span class="number">2</span>*j<span class="number">+2</span>) * (<span class="number">2</span>*j<span class="number">+3</span>))); </span><br><span class="line">            sign *= ­‐<span class="number">1</span>; </span><br><span class="line">        &#125; </span><br><span class="line">        _mm256_store_ps(&amp;result[i], value); </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/1639810240.jpg" alt=""></p>
<p>如果是有条件跳转的执行呢？不是所有的ALU执行相同的指令，会降低性能。经过了这一段if之后才会重新全速执行。<br><img src="/img/1639810613.jpg" alt=""></p>
<p>术语</p>
<ul>
<li>指令流一致性（“一致执行”）<ul>
<li>相同的指令序列适用于同时操作的所有元件</li>
<li>一致执行对于有效利用SIMD处理资源是必要的</li>
<li>由于每个内核都具有获取/解码不同指令流的能力，因此一致执行对于跨内核的高效并行不是必需的</li>
</ul>
</li>
<li>“发散”执行<ul>
<li>缺乏指令流的连贯性</li>
</ul>
</li>
<li>注意：不要将指令流一致性与“缓存一致性”（本课程后面的一个主要主题）混淆</li>
</ul>
<p>在现代CPU上执行SIMD</p>
<ul>
<li>SSE指令：128位操作：4x32位或2x64位（4宽浮点向量）</li>
<li>AVX指令：256位操作：8x32位或4x64位（8宽浮点向量）</li>
<li>指令由编译器生成<ul>
<li>程序员使用内部函数明确请求的并行性</li>
<li>使用并行语言语义传达的并行性（例如，forall）</li>
<li>通过循环依赖性分析推断出的并行性（困难的问题是，即使是最好的编译器也无法处理任意C/C++代码）</li>
</ul>
</li>
<li>术语：“显式SIMD”：SIMD并行化在编译时执行<ul>
<li>可以检查程序二进制文件并查看指令（vstoreps、vmulps等）</li>
</ul>
</li>
</ul>
<p>在许多现代GPU上执行SIMD</p>
<ul>
<li>“隐式SIMD”<ul>
<li>编译器生成标量二进制（标量指令）</li>
<li>但N个程序实例在处理器上“始终”一起运行</li>
<li>换句话说，硬件本身的接口是数据并行的</li>
<li>硬件（不是编译器）负责在SIMD ALU上的不同数据上同时执行来自多个实例的同一指令</li>
</ul>
</li>
<li>大多数现代GPU的SIMD宽度范围为8到32<ul>
<li>分支可能是一个大问题</li>
</ul>
</li>
</ul>
<p><img src="/img/1639816043.png" alt=""></p>
<p>摘要：并行执行</p>
<ul>
<li>现代处理器中几种形式的并行执行<ul>
<li>多核：使用多个处理核<ul>
<li>提供线程级并行：在每个内核上同时执行完全不同的指令流</li>
<li>软件决定何时创建线程（例如，通过pthreadsapi）</li>
</ul>
</li>
<li>SIMD：使用由同一指令流控制的多个ALU（在一个内核内）<ul>
<li>数据并行工作负载的高效设计：在多个ALU上摊销控制</li>
<li>矢量化可以由编译器（显式SIMD）完成，也可以在运行时由硬件完成</li>
<li>在执行之前已知没有依赖关系（通常由程序员声明，但可以由高级编译器通过循环分析推断）</li>
</ul>
</li>
<li>超标量：在指令流中利用ILP。并行处理来自同一指令流的不同指令（在内核内）<ul>
<li>硬件在执行过程中自动动态发现并行性（程序员不可见）</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>stalls:</p>
<ul>
<li>由于依赖于上一条指令，处理器无法运行指令流中的下一条指令时会“暂停”（stalls）。</li>
<li>访问内存是暂停的主要来源</li>
<li>内存访问时间约为100个周期<ul>
<li>内存“访问时间”是对延迟的度量</li>
</ul>
</li>
</ul>
<p>当数据驻留在缓存中时，处理器会高效运行，cache减少了stalls的时间长度（隐藏延迟）</p>
<ul>
<li>所有现代CPU都具有将数据预取到缓存中的逻辑<ul>
<li>动态分析程序的访问模式，预测它将很快访问什么</li>
</ul>
</li>
<li>减少暂停，因为访问时数据驻留在缓存中</li>
<li>注意：如果猜测错误，预取也会降低性能（占用带宽，污染缓存）</li>
</ul>
<p><img src="/img/1639816915.jpg" alt=""></p>
<p>多线程也能减少stalls：</p>
<ul>
<li>想法：在同一个内核上交错处理多个线程以隐藏暂停</li>
<li>与预取一样，多线程也是一种延迟<strong>隐藏</strong>技术，而不是一种减少延迟的技术</li>
</ul>
<p>面向吞吐量的系统的关键思想：潜在地增加任何一个线程完成工作的时间，以便在运行多个线程时提高总体系统吞吐量。在结束引起stalls的异常之后，此线程是可运行的，但处理器不会执行它，内核正在运行其他线程，需要等待操作系统进行调度。<br><img src="/img/1639817132.jpg" alt=""></p>
<p>存储执行上下文：有限资源下执行上下文的片上存储问题。如果有16个线程的话，将之前整个的context storage改为16个小块，存储每个线程的小工作上下文。<br><img src="/img/1639817978.jpg" alt=""></p>
<p>硬件支持的多线程</p>
<ul>
<li>Core管理多线程的执行上下文<ul>
<li>从可运行线程运行指令（处理器决定运行每个时钟运行的线程，而不是操作系统）</li>
<li>Core仍然拥有相同数量的ALU资源：多线程只在面临内存访问等高延迟操作时有助于更有效地使用它们</li>
</ul>
</li>
<li>交错多线程（又称时态多线程）<ul>
<li>每个时钟，内核选择一个线程，并从ALU上的线程运行一条指令</li>
</ul>
</li>
<li>同步多线程（SMT）<ul>
<li>每个时钟，内核从多个线程中选择指令在ALU上运行</li>
<li>超标量CPU设计的扩展</li>
<li>示例：英特尔超线程（每个核心2个线程）</li>
</ul>
</li>
</ul>
<p>GPU：面向吞吐量的处理器</p>
<ul>
<li>“回”字形黄色方框=SIMD功能单元，16个单元共享控制（每个时钟1个MUL-ADD）</li>
<li>两个取指/编码器，共32个SIMD功能单元</li>
<li>指令一次操作32条数据（称为“warp”）。</li>
<li>warp=发出32条宽向量指令的线程</li>
<li>多达48条warps同时交错</li>
<li>一个核心可同时处理1500多个元素</li>
<li>为什么warp是32个元素，只有16个SIMD ALU？<ul>
<li>这有点复杂：ALU的运行速度是芯片其他部分时钟频率的两倍。因此，每条解码指令在两个ALU时钟上运行在16个ALU上的32条数据上。（但对于程序员来说，它的行为类似于32宽的SIMD操作）</li>
</ul>
</li>
</ul>
<p><img src="/img/1639819364.jpg" alt=""></p>
<p>带宽是一项关键资源，高性能并行程序将：</p>
<ul>
<li>组织计算以减少从内存中提取数据的频率<ul>
<li>重用以前由同一线程加载的数据（传统的线程内时间局部性优化）</li>
<li>跨线程共享数据（线程间协作）</li>
</ul>
</li>
<li>减少请求数据的频率（相反，多做算术运算：它是“免费的”）<ul>
<li>有用术语：“算术强度”-指令流中数学运算与数据访问运算的比率</li>
<li>要点：为了有效利用现代处理器，程序必须具有较高的运算强度，也就是运算指令要大于取值指令的数量</li>
</ul>
</li>
</ul>
<p>总结</p>
<ul>
<li>所有现代处理器在不同程度上采用的三大理念<ul>
<li>使用多个处理核心<ul>
<li>更简单的内核（采用线程级并行而不是指令级并行）</li>
</ul>
</li>
<li>在多个ALU上摊销指令流处理（SIMD）<ul>
<li>以很少的额外成本提高计算能力</li>
</ul>
</li>
<li>使用多线程来更有效地利用处理资源（隐藏延迟、填充所有可用资源）</li>
</ul>
</li>
<li>由于现代芯片的高运算能力，许多并行应用程序（在CPU和GPU上）都有带宽瓶颈</li>
<li>GPU架构使用与CPU相同的吞吐量计算思想：但GPU将这些概念推向了极限</li>
</ul>
<p>总结：</p>
<ul>
<li>最开始普通的串行程序+普通的仅有（取指、ALU、上下文存储）三部分的处理器，</li>
<li>改进为有两套取指+ALU的超标量处理器，这样可以在单指令流中每个时钟同时执行两个没有依赖关系的指令</li>
<li>创建多线程程序的话，需要在一个处理器上设置两套（取指、ALU、上下文存储），每个核在每个时钟只执行一个指令</li>
<li>多线程+超标量，两个核+两套（取指、ALU、上下文存储）</li>
<li>四核处理器，四个核每个核一套（取指、ALU、上下文存储）</li>
<li>进化到SIMD时代，四个核每个核都有一个取指器，八个ALU执行运算，一个上下文存储器存储上下文。</li>
<li>在SIMD基础上增加多线程，每个核除了一个取指器，八个ALU，再来两个存放上下文的切换器。<ul>
<li>观察：内存操作有很长的延迟</li>
<li>解决方案：通过执行其他迭代的算术指令来隐藏一次迭代加载数据的延迟</li>
<li>多线程SIMD四核处理器：从每个核上的一条指令流中，每个时钟执行一条SIMD指令。但当遇到暂停时，可以切换到处理其他指令流。</li>
</ul>
</li>
<li>四个超标量、SIMD、多线程内核<ul>
<li>多线程、超标量、SIMD四核处理器：从每个核上的一条指令流中，每个时钟最多执行两条指令（在本例中：一条SIMD指令+一条标量指令）。当遇到暂停时，处理器可以切换到执行其他指令流。</li>
</ul>
</li>
<li>以上，上下文切换器提供了进行切换指令流的能力；有多个取指器的话能同时执行两个指令流</li>
</ul>
<p><img src="/img/1639823726.png" alt=""></p>
<p><img src="/img/1639824277.png" alt=""></p>
<h1 id="lecture-3"><a href="#lecture-3" class="headerlink" title="lecture 3"></a>lecture 3</h1><p>Intel SPMD Program Compiler (ISPC)</p>
<p>在ISPC上计算之前的<code>sin(x)</code>函数，<code>sin(x) = x - x^3/3! + x^5/5! - x^7/7! + ...</code></p>
<p>C++ code: main.cpp<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> “sinx_ispc.h”</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> N = <span class="number">1024</span>; </span><br><span class="line"><span class="type">int</span> terms = <span class="number">5</span>; </span><br><span class="line"><span class="type">float</span>* x = new <span class="type">float</span>[N]; </span><br><span class="line"><span class="type">float</span>* result = new <span class="type">float</span>[N]; </span><br><span class="line"><span class="comment">// initialize x here </span></span><br><span class="line"><span class="comment">// execute ISPC code </span></span><br><span class="line">sinx(N, terms, x, result);</span><br></pre></td></tr></table></figure></p>
<p>ISPC code: sinx.ispc<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">export</span> <span class="type">void</span> <span class="title">sinx</span><span class="params">( </span></span></span><br><span class="line"><span class="params"><span class="function">uniform <span class="type">int</span> N, </span></span></span><br><span class="line"><span class="params"><span class="function">uniform <span class="type">int</span> terms, </span></span></span><br><span class="line"><span class="params"><span class="function">uniform <span class="type">float</span>* x, </span></span></span><br><span class="line"><span class="params"><span class="function">uniform <span class="type">float</span>* result)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// assume N % programCount = 0 </span></span><br><span class="line">    <span class="keyword">for</span> (uniform <span class="type">int</span> i=<span class="number">0</span>; i&lt;N; i+=programCount)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> idx = i + programIndex; </span><br><span class="line">        <span class="type">float</span> value = x[idx]; </span><br><span class="line">        <span class="type">float</span> numer = x[idx] * x[idx] * x[idx]; </span><br><span class="line">        uniform <span class="type">int</span> denom = <span class="number">6</span>;  <span class="comment">// 3! </span></span><br><span class="line">        uniform <span class="type">int</span> sign = <span class="number">-1</span>; </span><br><span class="line">        <span class="keyword">for</span> (uniform <span class="type">int</span> j=<span class="number">1</span>; j&lt;=terms; j++) </span><br><span class="line">        &#123;  </span><br><span class="line">            value += sign * numer / denom </span><br><span class="line">            numer *= x[idx] * x[idx]; </span><br><span class="line">            denom *= (<span class="number">2</span>*j<span class="number">+2</span>) * (<span class="number">2</span>*j<span class="number">+3</span>); </span><br><span class="line">            sign *= ­‐<span class="number">1</span>; </span><br><span class="line">        &#125; </span><br><span class="line">        result[idx] = value; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>SPMD编程抽象：对ISPC函数的调用产生“一组”ISPC“程序实例”，所有实例同时运行ISPC代码，将数组元素“交错”分配给程序实例。返回后，所有实例都已完成。</p>
<p><img src="/img/1639825906.png" alt=""></p>
<p>ISPC关键字：</p>
<ul>
<li><code>programCount</code>：组中同时执行的实例数（统一值）</li>
<li><code>programIndex</code>：组中当前实例的id。（非均匀值：“变化”）</li>
<li><code>uniform</code>：类型修饰符。所有实例对此变量具有相同的值。它的使用纯粹是一种优化。不需要正确性。</li>
</ul>
<p>程序实例到循环迭代的交错分配<br><img src="/img/1639826639.jpg" alt=""></p>
<p>SPMD编程抽象：</p>
<ul>
<li>对ISPC函数的调用会产生一组ISPC“程序实例”</li>
<li>所有实例同时运行ISPC代码</li>
<li>返回后，所有实例都已完成</li>
</ul>
<p>ISPC编译器生成SIMD实现：</p>
<ul>
<li>组中的实例数是硬件的SIMD宽度（或SIMD宽度的小倍数）</li>
<li>ISPC编译器使用SIMD指令生成二进制（.o）</li>
<li>与常规文件一样的C++代码链接</li>
</ul>
<p>将元素“分块”分配给实例<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">export</span> <span class="type">void</span> <span class="title">sinx</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">uniform <span class="type">int</span> N, </span></span></span><br><span class="line"><span class="params"><span class="function">uniform <span class="type">int</span> terms, </span></span></span><br><span class="line"><span class="params"><span class="function">uniform <span class="type">float</span>* x, </span></span></span><br><span class="line"><span class="params"><span class="function">uniform <span class="type">float</span>* result)</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="comment">// assume N % programCount = 0 </span></span><br><span class="line">    uniform <span class="type">int</span> count = N / programCount; </span><br><span class="line">    <span class="type">int</span> start = programIndex * count; </span><br><span class="line">    <span class="keyword">for</span> (uniform <span class="type">int</span> i=<span class="number">0</span>; i&lt;count; i++)</span><br><span class="line">    &#123; </span><br><span class="line">        <span class="type">int</span> idx = start + i; </span><br><span class="line">        <span class="type">float</span> value = x[idx]; </span><br><span class="line">        <span class="type">float</span> numer = x[idx] * x[idx] * x[idx]; </span><br><span class="line">        uniform <span class="type">int</span> denom = <span class="number">6</span>;  <span class="comment">// 3! </span></span><br><span class="line">        uniform <span class="type">int</span> sign = ‐<span class="number">1</span>; </span><br><span class="line">        <span class="keyword">for</span> (uniform <span class="type">int</span> j=<span class="number">1</span>; j&lt;=terms; j++) </span><br><span class="line">        &#123;  </span><br><span class="line">            value += sign * numer / denom </span><br><span class="line">            numer *= x[idx] * x[idx]; </span><br><span class="line">            denom *= (j<span class="number">+3</span>) * (j<span class="number">+4</span>); </span><br><span class="line">            sign *= ‐<span class="number">1</span>; </span><br><span class="line">        &#125;</span><br><span class="line">        result[idx] = value; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/1639827312.png" alt=""></p>
<p>第一个版本是轮转的方式，使用<code>_mm_load_ps1</code>SSE指令为四个实例分别分配值，这四个元素在内存中是连续的，因此很高效。但是分块的方式在每次为四个实例分配值的时候，会按照“0，4，8，12”、“1，5，9，13”的方式分配，现在涉及内存中的四个非连续值。需要执行“gather”指令（gather是一种更复杂、更昂贵的SIMD指令：在2013年开始作为AVX2的一部分提供）</p>
<p>使用foreach提高抽象级别：foreach是关键的ISPC语言构造</p>
<ul>
<li>foreach声明并行循环迭代<ul>
<li>表示：这些是团队中的实例必须协同执行的迭代</li>
</ul>
</li>
<li>ISPC实现将迭代分配给组中的程序实例<ul>
<li>当前ISPC实现将执行静态交错分配</li>
</ul>
</li>
</ul>
<p>错误的sum 规约<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">export</span> uniform <span class="type">float</span> <span class="title">sumall1</span><span class="params">( </span></span></span><br><span class="line"><span class="params"><span class="function">   uniform <span class="type">int</span> N, </span></span></span><br><span class="line"><span class="params"><span class="function">   uniform <span class="type">float</span>* x)</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    uniform <span class="type">float</span> sum = <span class="number">0.0f</span>; </span><br><span class="line">    foreach (i = <span class="number">0</span> ... N) </span><br><span class="line">    &#123; </span><br><span class="line">       sum += x[i]; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">return</span> sum; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>正确的sum 规约<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">export</span> uniform <span class="type">float</span> <span class="title">sumall2</span><span class="params">( </span></span></span><br><span class="line"><span class="params"><span class="function">   uniform <span class="type">int</span> N, </span></span></span><br><span class="line"><span class="params"><span class="function">   uniform <span class="type">float</span>* x)</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    uniform <span class="type">float</span> sum; </span><br><span class="line">    <span class="type">float</span> partial = <span class="number">0.0f</span>; </span><br><span class="line">    foreach (i = <span class="number">0</span> ... N) </span><br><span class="line">    &#123; </span><br><span class="line">        partial += x[i]; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="comment">// from ISPC math library </span></span><br><span class="line">    sum = <span class="built_in">reduce_add</span>(partial); </span><br><span class="line">    <span class="keyword">return</span> sum; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>sum的类型为<code>uniform float</code>（所有程序实例都有一个变量副本），<code>x[i]</code>不是统一表达式（每个程序实例的值不同）结果：编译时类型错误。</p>
<p>并行计算所有数组元素的总和。每个实例累积一个私有部分和（无通信）。</p>
<p>使用<code>reduce_add()</code>通信原语将部分和相加。结果是所有程序实例的总和相同（<code>reduce_add()</code>返回一个统一的浮点数）。</p>
<p>下面的ISPC代码将以类似于下面的手写C+AVX intrinsics实现的方式执行<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">export</span> uniform <span class="type">float</span> <span class="title">sumall2</span><span class="params">( </span></span></span><br><span class="line"><span class="params"><span class="function">   uniform <span class="type">int</span> N, </span></span></span><br><span class="line"><span class="params"><span class="function">   uniform <span class="type">float</span>* x)</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    uniform <span class="type">float</span> sum; </span><br><span class="line">    <span class="type">float</span> partial = <span class="number">0.0f</span>; </span><br><span class="line">    foreach (i = <span class="number">0</span> ... N) </span><br><span class="line">    &#123; </span><br><span class="line">        partial += x[i]; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="comment">// from ISPC math library </span></span><br><span class="line">    sum = <span class="built_in">reduce_add</span>(partial); </span><br><span class="line">    <span class="keyword">return</span> sum; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">sumall2</span><span class="params">(<span class="type">int</span> N, <span class="type">float</span>* x)</span> </span>&#123; </span><br><span class="line">    <span class="type">float</span> tmp[<span class="number">8</span>];  <span class="comment">// assume 16­‐byte alignment </span></span><br><span class="line">    __mm256 partial = _mm256_broadcast_ss(<span class="number">0.0f</span>); </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;N; i+=<span class="number">8</span>) </span><br><span class="line">        partial = _mm256_add_ps(partial, _mm256_load_ps(&amp;x[i]));</span><br><span class="line">    _mm256_store_ps(tmp, partial); </span><br><span class="line">    <span class="type">float</span> sum = <span class="number">0.f</span>; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;<span class="number">8</span>; i++) </span><br><span class="line">        sum += tmp[i];</span><br><span class="line">    <span class="keyword">return</span> sum; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>ISPC task</p>
<ul>
<li>ISPC组抽象由单核上的SIMD指令实现。</li>
<li>ISPC包含另一个抽象：用于实现多核执行的“task”。</li>
</ul>
<p>用pthreads表示并行性<br><img src="/img/1639828816.jpg" alt=""></p>
<p>用ISPC表示并行性：</p>
<ul>
<li>用于指定同时执行（真正的并行性）</li>
<li>用于指定独立工作（可能并行）</li>
</ul>
<p><img src="/img/1639828860.jpg" alt=""></p>
<p>三种通信模式</p>
<ol>
<li>共享地址空间</li>
<li>消息传递</li>
<li>数据并行</li>
</ol>
<p>共享地址空间模型（抽象）：</p>
<ul>
<li>线程通过读/写共享变量进行通信</li>
<li>共享变量就像一个大公告板<ul>
<li>任何线程都可以读取或写入共享变量</li>
</ul>
</li>
</ul>
<p>两个线程如下：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int x = 0;</span><br><span class="line">spawn_thread(foo, &amp;x);</span><br><span class="line">x = 1;</span><br></pre></td></tr></table></figure><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">void foo(int* x) &#123; </span><br><span class="line">while (x == 0) &#123;&#125; </span><br><span class="line">    print x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/1639829805.jpg" alt=""></p>
<p>同步原语也是共享变量：例如锁<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> x = <span class="number">0</span>;</span><br><span class="line">Lock my_lock; </span><br><span class="line"><span class="built_in">spawn_thread</span>(foo, &amp;x, &amp;my_lock); </span><br><span class="line">mylock.<span class="built_in">lock</span>(); </span><br><span class="line">x++; </span><br><span class="line">mylock.<span class="built_in">unlock</span>();</span><br></pre></td></tr></table></figure></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">int</span>* x, lock* my_lock)</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    my_lock‐&gt;<span class="built_in">lock</span>();</span><br><span class="line">    x++; </span><br><span class="line">    my_lock­‐&gt;<span class="built_in">unlock</span>(); </span><br><span class="line">    print x; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>共享地址空间模型（抽象）</p>
<ul>
<li>线程通过以下方式进行通信：<ul>
<li>读取/写入共享变量<ul>
<li>线程间通信隐含在内存操作中</li>
<li>线程1存储到X</li>
<li>稍后，线程2读取X（并观察线程1对值的更新）</li>
</ul>
</li>
<li>操作同步原语<ul>
<li>例如，通过使用锁确保相互排斥</li>
</ul>
</li>
</ul>
</li>
<li>这是顺序编程的自然扩展<ul>
<li>事实上，到目前为止，我们在课堂上的所有讨论都假设有一个共享的地址空间！</li>
</ul>
</li>
</ul>
<p>共享地址空间的硬件实现：每个处理器可以直接访问任何内存地址。<br><img src="/img/1639830087.png" alt=""></p>
<p>非统一内存访问（NUMA）</p>
<ul>
<li>所有处理器都可以访问任何内存位置，但是内存访问的成本（延迟和/或带宽）对于不同的处理器是不同的<ul>
<li>在系统中保持统一访问时间的问题：可扩展性<ul>
<li>好：开销是一致的，坏：它们是一致的坏（内存是一致的远）</li>
</ul>
</li>
<li>NUMA设计更具可扩展性<ul>
<li>对本地内存的低延迟访问</li>
<li>为本地内存提供高带宽</li>
</ul>
</li>
</ul>
</li>
<li>开销是程序员为性能调优所做的工作增加<ul>
<li>发现、利用局部性对性能非常重要（希望大多数内存访问都指向本地内存）</li>
</ul>
</li>
</ul>
<p><img src="/img/1639830216.jpg" alt=""></p>
<p>下面的图中对x的访问，如果x在1-4核上，访问开销远远小于5-8核。<br><img src="/img/1639830245.jpg" alt=""></p>
<p>消息传递（实现）</p>
<ul>
<li>流行软件库：MPI（消息传递接口）</li>
<li>硬件不需要实现系统范围的加载和存储来执行消息传递程序（只需要能够传递消息）</li>
</ul>
<p>ISPC中的数据并行</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ISPC code:</span></span><br><span class="line"><span class="function"><span class="keyword">export</span> <span class="type">void</span> <span class="title">absolute_value</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    uniform <span class="type">int</span> N,</span></span></span><br><span class="line"><span class="params"><span class="function">    uniform <span class="type">float</span>* x,</span></span></span><br><span class="line"><span class="params"><span class="function">    uniform <span class="type">float</span>* y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    foreach(i = <span class="number">0</span> ... N)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (i &gt; <span class="number">0</span> &amp;&amp; x[i] &lt; <span class="number">0</span>)</span><br><span class="line">            y[i<span class="number">-1</span>] = x[i];</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            y[i] = x[i]; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>将循环体视为函数，foreach构造是一个映射。给定此程序，可以将该程序视为将循环体映射到数组X和Y的每个元素上。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// main C++ code: </span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1024</span>; </span><br><span class="line"><span class="type">float</span>* x = <span class="keyword">new</span> <span class="type">float</span>[N]; </span><br><span class="line"><span class="type">float</span>* y = <span class="keyword">new</span> <span class="type">float</span>[N]; </span><br><span class="line"><span class="comment">// initialize N elements of x here </span></span><br><span class="line"><span class="built_in">absolute_value</span>(N, x, y);</span><br></pre></td></tr></table></figure></p>
<p>但如果我们想说得更准确一些：该系列不是一流的ISPC概念。它是由程序如何实现数组索引逻辑隐式定义的。</p>
<p>这个程序是不确定的！循环体的多次迭代可能写入同一内存位置。数据并行模型（foreach）没有规定迭代发生的顺序，模型不提供用于细粒度互斥/同步的原语）。它不是为了帮助程序员用这种结构编写程序。</p>
<p>一种更“合适”的数据并行方法<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1024</span>; </span><br><span class="line"><span class="function">stream&lt;<span class="type">float</span>&gt; <span class="title">x</span><span class="params">(N)</span></span>;  <span class="comment">// define collection </span></span><br><span class="line"><span class="function">stream&lt;<span class="type">float</span>&gt; <span class="title">y</span><span class="params">(N)</span></span>;  <span class="comment">// define collection </span></span><br><span class="line"><span class="comment">// initialize N elements of x here </span></span><br><span class="line"><span class="comment">// map function absolute_value onto </span></span><br><span class="line"><span class="comment">// streams (collections) x, y </span></span><br><span class="line"><span class="built_in">absolute_value</span>(x, y);</span><br><span class="line"></span><br><span class="line"><span class="comment">// kernel: </span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">absolute_value</span><span class="params">(<span class="type">float</span> x, <span class="type">float</span> y)</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="keyword">if</span> (x &lt; <span class="number">0</span>) </span><br><span class="line">        y = ‐x; </span><br><span class="line">    <span class="keyword">else</span> </span><br><span class="line">        y = x; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>注意：这不是ISPC语法（更多的是Kayvon编造的语法），以这种函数形式表示的数据并行性有时被称为流编程模型。</p>
<ul>
<li>stream：元素的集合。元素可以独立处理</li>
<li>kernel：没有副作用的函数。对集合进行元素操作</li>
</ul>
<p>gather/scatter：两个关键的数据并行通信原语</p>
<p>把absolute_value映射到gather产生的流上：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1024</span>; </span><br><span class="line"><span class="function">stream&lt;<span class="type">float</span>&gt; <span class="title">input</span><span class="params">(N)</span></span>; </span><br><span class="line">stream&lt;<span class="type">int</span>&gt; indices; </span><br><span class="line"><span class="function">stream&lt;<span class="type">float</span>&gt; <span class="title">tmp_input</span><span class="params">(N)</span></span>;   </span><br><span class="line"><span class="function">stream&lt;<span class="type">float</span>&gt; <span class="title">output</span><span class="params">(N)</span></span>; </span><br><span class="line"><span class="built_in">stream_gather</span>(input, indices, tmp_input);</span><br><span class="line"><span class="built_in">absolute_value</span>(tmp_input, output);</span><br></pre></td></tr></table></figure></p>
<p>用ISPC 等价于:<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">export</span> <span class="type">void</span> <span class="title">absolute_value</span><span class="params">( </span></span></span><br><span class="line"><span class="params"><span class="function">   uniform <span class="type">float</span> N, </span></span></span><br><span class="line"><span class="params"><span class="function">   uniform <span class="type">float</span>* input, </span></span></span><br><span class="line"><span class="params"><span class="function">   uniform <span class="type">float</span>* output, </span></span></span><br><span class="line"><span class="params"><span class="function">   uniform <span class="type">int</span>* indices)</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    foreach (i = <span class="number">0</span> ... n) </span><br><span class="line">    &#123; </span><br><span class="line">        <span class="type">float</span> tmp = input[indices[i]]; </span><br><span class="line">        <span class="keyword">if</span> (tmp &lt; <span class="number">0</span>) </span><br><span class="line">            output[i] = ‐tmp; </span><br><span class="line">        <span class="keyword">else</span> </span><br><span class="line">            output[i] = tmp; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>把absolute_value映射到scatter的值上：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1024</span>; </span><br><span class="line"><span class="function">stream&lt;<span class="type">float</span>&gt; <span class="title">input</span><span class="params">(N)</span></span>; </span><br><span class="line">stream&lt;<span class="type">int</span>&gt; indices; </span><br><span class="line"><span class="function">stream&lt;<span class="type">float</span>&gt; <span class="title">tmp_output</span><span class="params">(N)</span></span>;</span><br><span class="line"><span class="function">stream&lt;<span class="type">float</span>&gt; <span class="title">output</span><span class="params">(N)</span></span>; </span><br><span class="line"><span class="built_in">absolute_value</span>(input, tmp_output); </span><br><span class="line"><span class="built_in">stream_scatter</span>(tmp_output, indices, output);</span><br></pre></td></tr></table></figure></p>
<p>用ISPC等价于：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">export</span> <span class="type">void</span> <span class="title">absolute_value</span><span class="params">( </span></span></span><br><span class="line"><span class="params"><span class="function">    uniform <span class="type">float</span> N, </span></span></span><br><span class="line"><span class="params"><span class="function">    uniform <span class="type">float</span>* input, </span></span></span><br><span class="line"><span class="params"><span class="function">    uniform <span class="type">float</span>* output, </span></span></span><br><span class="line"><span class="params"><span class="function">    uniform <span class="type">int</span>* indices)</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    foreach (i = <span class="number">0</span> ... n) </span><br><span class="line">    &#123; </span><br><span class="line">        <span class="keyword">if</span> (input[i] &lt; <span class="number">0</span>) </span><br><span class="line">            output[indices[i]] = ‐input[i]; </span><br><span class="line">        <span class="keyword">else</span> </span><br><span class="line">            output[indices[i]] = input[i]; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>gather操作：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gather(R1, R0, mem_base);</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/1639835717.png" alt=""></p>
<p>概要：数据并行模型</p>
<ul>
<li>基本结构：将函数映射到大量数据集合上<ul>
<li>功能性：无副作用执行</li>
<li>不同函数调用之间没有通信（允许以任何顺序调度调用，包括并行调度）</li>
</ul>
</li>
<li>实际上，这就是许多简单程序的工作原理</li>
<li>但是许多现代面向性能的数据并行语言并不严格执行这种结构<ul>
<li>ISPC、OpenCL、CUDA等。</li>
<li>他们选择命令式C风格语法的灵活性/熟悉性，而不是功能更强大的形式的安全性：这是他们采用命令式C风格的关键</li>
<li>观点：功能性思维是很好的，但编程系统确实应该采用结构来促进实现高性能的实现，而不是阻碍它们</li>
</ul>
</li>
</ul>
<p>现代实践：混合编程模型</p>
<ul>
<li>在集群的多核节点内使用共享地址空间编程，在节点之间使用消息传递<ul>
<li>在实践中非常非常普遍</li>
<li>使用共享地址空间的便利性（在节点内）可以有效地实现，需要在其他地方进行显式通信</li>
</ul>
</li>
<li>数据并行编程模型支持内核中的共享内存式同步原语<ul>
<li>允许有限形式的迭代间通信（如CUDA、OpenCL）</li>
</ul>
</li>
<li>CUDA/OpenCL使用数据并行模型扩展到多个内核，但采用共享地址空间模型，允许在同一内核上运行的线程进行通信。</li>
</ul>
<h1 id="lecture-4"><a href="#lecture-4" class="headerlink" title="lecture 4"></a>lecture 4</h1><p>如何创建一个并行程序</p>
<ul>
<li>剖分</li>
<li>分配给线程/进程<ul>
<li>负载平衡，可以动态/静态分配</li>
</ul>
</li>
<li>编排依赖关系</li>
<li>在并行机器上并行执行，通信</li>
</ul>
<p>阿姆达尔定律：依赖性限制了并行性带来的最大加速比</p>
<ul>
<li>运行顺序程序。。。</li>
<li>设S=固有顺序的顺序执行部分（依赖项阻止并行执行）</li>
<li>然后是并行执行带来的最大加速≤ 1/S</li>
</ul>
<p>一个使用pthread的例子，进行了任务的划分：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> &#123; </span><br><span class="line">    <span class="type">int</span> N, terms; </span><br><span class="line">    <span class="type">float</span>* x, *result; </span><br><span class="line">&#125; my_args; </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">parallel_sinx</span><span class="params">(<span class="type">int</span> N, <span class="type">int</span> terms, <span class="type">float</span>* x, <span class="type">float</span>* result)</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="type">pthread_t</span> thread_id; </span><br><span class="line">    my_args args; </span><br><span class="line">    args.N = N/<span class="number">2</span>; </span><br><span class="line">    args.terms = terms; </span><br><span class="line">    args.x = x; </span><br><span class="line">    args.result = result; </span><br><span class="line">    <span class="comment">// launch second thread, do work on first half of array </span></span><br><span class="line">    <span class="built_in">pthread_create</span>(&amp;thread_id, <span class="literal">NULL</span>, my_thread_start, &amp;args);</span><br><span class="line">    <span class="comment">// do work on second half of array in main thread</span></span><br><span class="line">    <span class="built_in">sinx</span>(N ‐ args.N, terms, x + args.N, result + args.N);</span><br><span class="line">    <span class="built_in">pthread_join</span>(thread_id, <span class="literal">NULL</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">my_thread_start</span><span class="params">(<span class="type">void</span>* thread_arg)</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    my_args* thread_args = (my_args*)thread_arg; </span><br><span class="line">    <span class="built_in">sinx</span>(args‐&gt;N, args‐&gt;terms, args‐&gt;x, args‐&gt;result); <span class="comment">// do work </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>循环迭代分解任务</p>
<ul>
<li>静态分配</li>
<li>以块的方式（一个连续的部分）将迭代各步分配给pthreads（数组的前半部分分配给派生线程，后半部分分配给主线程）</li>
</ul>
<p>使用ISPC task进行动态分配：ISPC在运行时将任务分配给工作线程</p>
<ul>
<li>分配策略：完成当前任务后，工作线程检查列表并为自己分配下一个未完成的任务。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">void foo(uniform float* input, </span><br><span class="line">         uniform float* output, </span><br><span class="line">         uniform int N) </span><br><span class="line">&#123; </span><br><span class="line">    // create a bunch of tasks</span><br><span class="line">    launch[100] my_ispc_task(input, output, N);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>编排Orchestration</p>
<ul>
<li>涉及：<ul>
<li>结构化通信</li>
<li>如有必要，添加同步以保留依赖项</li>
<li>在内存中组织数据结构</li>
<li>调度任务</li>
</ul>
</li>
<li>目标：降低通信/同步成本，保留数据引用的位置，减少开销等。</li>
<li>机器细节会影响许多决策<ul>
<li>如果同步比较昂贵，可能会少用</li>
</ul>
</li>
</ul>
<p>映射到硬件</p>
<ul>
<li>将“线程”（“工作线程”）映射到硬件执行单元</li>
<li>示例1：操作系统映射<ul>
<li>例如，将pthread映射到CPU核心上的硬件执行上下文</li>
</ul>
</li>
<li>示例2：编译器的映射<ul>
<li>将ISPC程序实例映射到向量指令通道</li>
</ul>
</li>
<li>示例3：硬件映射<ul>
<li>将CUDA线程块映射到GPU内核</li>
</ul>
</li>
<li>一些有趣的映射决策：<ul>
<li>将相关线程（协作线程）放在同一处理器上（最大化本地性、数据共享、最小化通信/同步成本）</li>
<li>将不相关的线程放在同一个处理器上（一个可能是带宽受限的，另一个可能是计算受限的），以更有效地使用机器</li>
</ul>
</li>
</ul>
<p>共享地址空间表达式</p>
<ul>
<li>程序员负责同步</li>
<li>通用同步原语：<ul>
<li>锁（提供互斥）：一次仅在关键区域中有一个线程</li>
<li>barrier：等待线程到达此点<ul>
<li>barrier是表示依赖关系的一种保守方式</li>
<li>barrier将计算分为几个阶段</li>
<li>在barrier开始后的任何线程中的任何计算之前，barrier之前所有线程的所有计算都已完成</li>
</ul>
</li>
</ul>
</li>
<li>保持原子性的机制<ul>
<li>锁定/解锁关键部分周围的互斥锁</li>
<li>硬件支持的原子读修改写操作的内部函数</li>
<li>有些语言对代码块的原子性具有一流的支持<code>atmoic</code></li>
</ul>
</li>
</ul>
<h1 id="lecture-5"><a href="#lecture-5" class="headerlink" title="lecture 5"></a>lecture 5</h1><p>GPU结构和CUDA编程</p>
<p>在CPU上，操作系统把程序加载到内存中，选择CPU的执行上下文，执行中断，加载上下文，运行。在GPU上，</p>
<p>NVIDIA Tesla architecture（2007）</p>
<ul>
<li>第一个GPU硬件的非图形特定（“计算模式”）接口（GeForce 8xxx系列GPU）</li>
<li>应用程序可以在GPU内存中分配缓冲区，并将数据复制到缓冲区或从缓冲区复制数据</li>
<li>应用程序（通过图形驱动程序）为GPU提供单一内核二进制程序</li>
<li>应用程序告诉GPU以SPMD模式“运行N个实例”</li>
</ul>
<p>CUDA程序由并发线程的层次结构组成，线程ID可以是三维的（下面的2D示例）。多维线程ID对于自然为N-D的问题非常方便。</p>
<p><img src="/img/1639839915.jpg" alt=""></p>
<p>基本的CUDA语法：</p>
<ul>
<li>主机和设备执行的代码是被程序员人为分开的</li>
<li>host代码：串行执行<ul>
<li>在CPU上作为普通C/C++应用程序的一部分运行</li>
<li>最后一行代码大量启动多个CUDA线程，“启动CUDA线程块网格”，调用在所有线程终止时返回</li>
</ul>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> Nx = <span class="number">12</span>; </span><br><span class="line"><span class="type">const</span> <span class="type">int</span> Ny = <span class="number">6</span>;</span><br><span class="line"><span class="function">dim3 <span class="title">threadsPerBlock</span><span class="params">(<span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>)</span></span>; </span><br><span class="line"><span class="function">dim3 <span class="title">numBlocks</span><span class="params">(Nx/threadsPerBlock.x, </span></span></span><br><span class="line"><span class="params"><span class="function">               Ny/threadsPerBlock.y, <span class="number">1</span>)</span></span>; </span><br><span class="line"><span class="comment">// assume A, B, C are allocated Nx x Ny float arrays </span></span><br><span class="line"><span class="comment">// this call will trigger execution of 72 CUDA threads:</span></span><br><span class="line"><span class="comment">// 6 thread blocks of 12 threads each  </span></span><br><span class="line">matrixAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C);</span><br></pre></td></tr></table></figure>
<ul>
<li>设备内核函数(device kernel function)的SPMD执行：<ul>
<li>每个线程从其在其块中的位置（threadIdx）和其块在网格中的位置（blockIdx）计算其整个网格线程id。</li>
<li>device代码：内核函数（<code>__global__</code>表示CUDA内核函数）在GPU上运行</li>
</ul>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__device__ <span class="type">float</span> <span class="title">doubleValue</span><span class="params">(<span class="type">float</span> x)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// kernel definition </span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">matrixAdd</span><span class="params">(<span class="type">float</span> A[Ny][Nx], </span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">float</span> B[Ny][Nx], </span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">float</span> C[Ny][Nx])</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">   <span class="type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x; </span><br><span class="line">   <span class="type">int</span> j = blockIdx.y * blockDim.y + threadIdx.y; </span><br><span class="line">   C[j][i] = A[j][i] + <span class="built_in">doubleValue</span>(B[j][i]); </span><br></pre></td></tr></table></figure>
<p>SPMD线程数在程序中是显式的，内核调用的数量不是由数据的大小决定。</p>
<p>CUDA中GPU设备的内存和CPU的内存是完全分开的，需要数据时用<code>cudaMemcpy</code>从CPU中拷到GPU上。</p>
<p>CUDA中有三种不同的内存</p>
<ul>
<li>每个线程自己的内存，只能被线程读写</li>
<li>每个block自己的内存，能被block中所有的线程读写</li>
<li>全局内存，能被所有的线程读写。</li>
</ul>
<p>举例子：1D卷积：<code>output[i] = (input[i] + input[i+1] + input[i+2]) / 3.f</code><br><img src="/img/1639840798.png" alt=""></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> THREADS_PER_BLK 128</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">convolve</span><span class="params">(<span class="type">int</span> N, <span class="type">float</span>* input, <span class="type">float</span>* output)</span> </span>&#123; </span><br><span class="line">    <span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x; <span class="comment">// thread local variable </span></span><br><span class="line">    <span class="type">float</span> result = <span class="number">0.0f</span>; <span class="comment">// thread-local variable </span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;<span class="number">3</span>; i++) <span class="comment">// each thread computes result for one element</span></span><br><span class="line">        result += input[index + i]; </span><br><span class="line">    output[index] = result / <span class="number">3.f</span>; <span class="comment">// write result to global memory</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>host上的代码：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> N = <span class="number">1024</span> * <span class="number">1024</span> </span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;devInput, <span class="built_in">sizeof</span>(<span class="type">float</span>) * (N<span class="number">+2</span>) ); <span class="comment">// allocate array in device memory </span></span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;devOutput, <span class="built_in">sizeof</span>(<span class="type">float</span>) * N); <span class="comment">// allocate array in device memory </span></span><br><span class="line"><span class="comment">// property initialize contents of devInput here ... </span></span><br><span class="line">convolve&lt;&lt;&lt;N/THREADS_PER_BLK, THREADS_PER_BLK&gt;&gt;&gt;(N, devInput, devOutput);</span><br></pre></td></tr></table></figure></p>
<p>每个输出元素一个线程：在每个块共享内存中暂存输入数据<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> THREADS_PER_BLK 128 </span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">convolve</span><span class="params">(<span class="type">int</span> N, <span class="type">float</span>* input, <span class="type">float</span>* output)</span> </span>&#123; </span><br><span class="line">    __shared__ <span class="type">float</span> support[THREADS_PER_BLK<span class="number">+2</span>];           <span class="comment">// per-block allocation</span></span><br><span class="line">    <span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x; <span class="comment">// thread local variable </span></span><br><span class="line">    support[threadIdx.x] = input[index]; </span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x &lt; <span class="number">2</span>) &#123; </span><br><span class="line">        support[THREADS_PER_BLK + threadIdx.x] = input[index+THREADS_PER_BLK];</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="comment">// 所有线程协同地将块的支持区域从全局内存加载到共享内存中（总共130条加载指令，而不是3*128条加载指令）</span></span><br><span class="line"></span><br><span class="line">    __syncthreads(); <span class="comment">// barrier (all threads in block)</span></span><br><span class="line">    <span class="type">float</span> result = <span class="number">0.0f</span>;   <span class="comment">// thread-local variable </span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;<span class="number">3</span>; i++)</span><br><span class="line">        result += support[threadIdx.x + i]; </span><br><span class="line">    output[index] = result / <span class="number">3.f</span>; <span class="comment">// write result to global memory</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>CUDA同步结构</p>
<ul>
<li><code>__syncthread()</code><ul>
<li>屏障：等待块中的所有线程到达该点</li>
</ul>
</li>
<li>原子操作<ul>
<li>例如，<code>float atomicAdd(float* addr, float amount)</code></li>
<li>全局内存和共享内存变量上的原子操作</li>
</ul>
</li>
<li>主机/设备同步<ul>
<li>内核返回时跨越所有线程的隐式屏障</li>
</ul>
</li>
</ul>
<p>CUDA摘要</p>
<ul>
<li>执行：线程层次结构<ul>
<li>大量启动多个线程</li>
<li>两级层次结构：线程被分组到线程块中</li>
</ul>
</li>
<li>分布式地址空间<ul>
<li>用于在主机和设备地址空间之间复制的内置memcpy原语</li>
<li>三种不同类型的设备地址空间</li>
<li>分为三个层级的内存：每个线程、每个块（“共享”）或每个程序（“全局”）</li>
</ul>
</li>
<li>线程块中线程的屏障同步原语</li>
<li>用于附加同步的原子原语（共享和全局变量）</li>
</ul>
<p>启动超过100万个CUDA线程（超过8K个线程块）<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> THREADS_PER_BLK 128 </span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">convolve</span><span class="params">(<span class="type">int</span> N, <span class="type">float</span>* input, <span class="type">float</span>* output)</span> </span>&#123; </span><br><span class="line">__shared__ <span class="type">float</span> support[THREADS_PER_BLK<span class="number">+2</span>];  <span class="comment">// per-block allocation</span></span><br><span class="line">    <span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x; <span class="comment">// thread local var</span></span><br><span class="line">    support[threadIdx.x] = input[index]; </span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x &lt; <span class="number">2</span>) &#123; </span><br><span class="line">        support[THREADS_PER_BLK+threadIdx.x] = input[index+THREADS_PER_BLK];   </span><br><span class="line">    &#125; </span><br><span class="line">    __syncthreads(); </span><br><span class="line">    <span class="type">float</span> result = <span class="number">0.0f</span>;   <span class="comment">// thread-local variable </span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;<span class="number">3</span>; i++)</span><br><span class="line">        result += support[threadIdx.x + i]; </span><br><span class="line">    output[index] = result / <span class="number">3.f</span>; </span><br><span class="line">&#125; </span><br><span class="line"><span class="comment">// host code ////////////////////////////////////////////////////// </span></span><br><span class="line"><span class="type">int</span> N = <span class="number">1024</span> * <span class="number">1024</span>; </span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;devInput, N<span class="number">+2</span>);   <span class="comment">// allocate array in device memory </span></span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;devOutput, N);    <span class="comment">// allocate array in device memory </span></span><br><span class="line"><span class="comment">// property initialize contents of devInput here ... </span></span><br><span class="line">convolve&lt;&lt;&lt;N/THREADS_PER_BLK, THREADS_PER_BLK&gt;&gt;&gt;(N, devInput, devOutput); </span><br></pre></td></tr></table></figure></p>
<p>8k个线程blocks分布在Grid上，blocks所需资源：（包含在已编译的内核二进制文件中）</p>
<ul>
<li>128线程</li>
<li>520字节的共享内存</li>
<li>（128 x B）字节的本地内存</li>
</ul>
<p>从主机中执行的启动命令<code>launch(blockDim, convolve)</code></p>
<ul>
<li>主要CUDA假设：线程块执行可以以任何顺序执行（块之间没有依赖关系）</li>
<li>GPU实现使用尊重资源需求的动态调度策略将线程块（“工作”）映射到内核</li>
</ul>
<p><img src="/img/1639844349.jpg" alt=""></p>
<p>我们常见设计模式的另一个实例：</p>
<ul>
<li>最佳实践：创建足够的worker来“填充”并行机，不再：<ul>
<li>每个并行执行资源（例如，CPU核心、核心执行上下文）一个worker thread</li>
<li>每个核心可能需要N个工作线程（其中N足够大，可以隐藏内存/IO延迟）</li>
<li>为每个worker预先分配资源</li>
<li>动态地将任务分配给工作线程（对许多任务重用分配）</li>
</ul>
</li>
<li>其他例子：<ul>
<li>ISPC执行发射任务的情况</li>
<li>为CPU上的每个超线程创建一个pthread。线程在程序的其余部分保持活动状态</li>
<li>线程数是内核数的函数，而不是未完成请求数的函数</li>
</ul>
</li>
</ul>
<p><img src="/img/1639844935.jpg" alt=""></p>
<p>回想一下，CUDA内核作为SPMD程序执行。在NVIDIA GPU上，32个CUDA线程组共享一个指令流。这些组织被称为“warp”。</p>
<p><code>convolve</code>线程块由4个warp执行（4个warp x 32个线程/warp = 每个block 128个CUDA线程）（WAPS是一个重要的GPU实现细节，但不是CUDA抽象！）</p>
<p>每个时钟时SMX核心操作：</p>
<ul>
<li>从驻留在SMM core上的64个线程中选择最多四个可运行的warp（线程级并行）</li>
<li>每个warp最多选择两条可运行指令（指令级并行）</li>
</ul>
<p>运行GPU的流程：</p>
<ul>
<li><code>convolve</code>的运行需要：<ul>
<li>每个线程block必须执行128个线程</li>
<li>每个线程block必须分配130*sizeof(float)=520Bytes内存</li>
<li>让我们假设数组大小N非常大，因此主机端内核启动会生成数千个线程块。</li>
<li><code>#define THREADS_PER_BLK 128</code></li>
<li><code>convolve&lt;&lt;&lt;N/THREADS_PER_BLK,  THREADS_PER_BLK&gt;&gt;&gt;(N, input_array, output_array);</code></li>
</ul>
</li>
<li>步骤1：主机向CUDA设备（GPU）发送命令（“执行此内核”）</li>
</ul>
<p><img src="/img/1639845602.jpg" alt=""></p>
<ul>
<li>步骤2：调度器将块0映射到核心0（为128个线程和520字节的共享存储保留执行上下文）</li>
<li>步骤3：调度器继续将块映射到可用的执行上下文（显示交错映射）</li>
</ul>
<p><img src="/img/1639845791.jpg" alt=""></p>
<ul>
<li>步骤3：调度器继续将块映射到可用的执行上下文（显示交错映射）。一个内核上只能容纳两个线程块（第三个线程块无法容纳，因为共享存储不足3 x 520字节&gt;1.5 KB）</li>
</ul>
<p><img src="/img/1639845945.jpg" alt=""></p>
<ul>
<li>步骤4：线程块0在核心0上完成</li>
</ul>
<p><img src="/img/1639846029.jpg" alt=""></p>
<ul>
<li>步骤5：在核心0上调度块4（映射到执行上下文0-127）</li>
<li>步骤6：线程块2在核心0上完成</li>
<li>步骤7：线程块5在核心0上调度（映射到执行上下文128-255）</li>
</ul>
<p><img src="/img/1639846114.jpg" alt=""></p>
<p>复习：什么是“warp”？</p>
<ul>
<li>warp是NVIDIA GPU上的CUDA实现细节</li>
<li>在现代NVIDIA硬件上，线程块中32个CUDA线程组使用32宽SIMD执行同时执行。<ul>
<li>这32个逻辑CUDA线程共享一个指令流，因此由于执行不一致，性能可能会受到影响。</li>
<li>此映射类似于ISPC在一个组中运行程序实例的方式。</li>
</ul>
</li>
<li>共享一个指令流的32个线程组称为warp。<ul>
<li>在thread block 中，thread0-31落在同一warp中（thread32-63等也落在同一warp中）</li>
<li>因此，包含256个CUDA线程的线程块映射到8个warp。</li>
<li>我们上次讨论的GTX 980中的每个“SMM”核心都能够调度和交错执行多达64个warp。</li>
<li>因此，“SMM”内核能够并发执行多个CUDA线程块。</li>
</ul>
</li>
</ul>
<p>在这个虚构的NVIDIA GPU示例中：Core维护12个warp的上下文，并选择一个warp来运行每个时钟<br><img src="/img/1639846224.jpg" alt=""></p>
<p>为什么为block中的所有线程分配执行上下文？</p>
<ul>
<li>假设一个线程块有256个CUDA线程</li>
<li>假设一个虚构的SMM内核，在硬件中只有4个可并行执行的warp（如上图所示）</li>
<li>为什么不运行四个warp（线程0-127）以完成，然后运行下四个warp（线程128-255）以完成，以便执行整个线程块？</li>
</ul>
<p>因为CUDA内核可能会在块中的线程之间创建依赖关系。</p>
<ul>
<li>最简单的例子是<code>__syncthreads()</code></li>
<li>当存在依赖项时，系统不能以任何顺序执行块中的线程。</li>
<li>CUDA语义：块中的线程同时运行。如果块中的线程是可运行的，那么它最终将运行！（没有deadlock）</li>
</ul>
<p>CUDA抽象的实现</p>
<ul>
<li>系统可以按任何顺序安排线程块<ul>
<li>系统假定块之间没有依赖关系</li>
<li>逻辑并发</li>
</ul>
</li>
<li>同一块中的CUDA线程不会同时运行<ul>
<li>当块开始执行时，所有线程都在运行（这些语义对系统施加调度约束）</li>
<li>CUDA线程块本身就是一个SPMD程序（类似于一组ISPC程序实例）</li>
<li>线程块中的线程是并发的、协作的“工作线程”</li>
</ul>
</li>
<li>CUDA实施：<ul>
<li>GPU warp具有类似于ISPC实例组的性能特征（但与ISPC实例组不同，warp概念不存在于编程模型）</li>
<li>线程块中的所有warp都调度到同一个内核上，允许通过共享内存变量进行高带宽/低延迟通信</li>
<li>当块中的所有线程完成时，块资源（共享内存分配、warp执行上下文）将可用于下一个块</li>
</ul>
</li>
</ul>
<p>CUDA摘要</p>
<ul>
<li>执行语义<ul>
<li>将问题划分为线程块符合数据并行模型的精神（旨在与机器无关：系统将块调度到任意数量的核上）</li>
<li>线程块中的线程实际上是并发运行的（它们必须并发运行，因为它们相互协作）</li>
<li>单线程块内部：SPMD共享地址空间编程</li>
<li>这些执行模式之间存在细微但显著的差异。</li>
</ul>
</li>
<li>内存语义<ul>
<li>分布式地址空间：主机/设备存储器</li>
<li>设备内存中的线程本地/块共享/全局变量</li>
<li>加载/存储在它们之间移动数据（因此将本地/共享/全局内存视为不同的地址空间是正确的）</li>
</ul>
</li>
<li>主要实施细节：<ul>
<li>线程块中的线程被调度到同一GPU内核上，以允许通过共享内存进行快速通信</li>
<li>线程块中的线程被分组为warp，以便在GPU硬件上执行SIMD</li>
</ul>
</li>
</ul>
<h1 id="lecture-6"><a href="#lecture-6" class="headerlink" title="lecture 6"></a>lecture 6</h1><p>高性能编程</p>
<ul>
<li>优化并行程序的性能是一个优化分解、分配和编排选择的迭代过程</li>
<li>关键目标<ul>
<li>将工作负载平衡到可用的执行资源上</li>
<li>减少通信（避免停顿）</li>
<li>减少额外的工作（开销），以提高并行性、管理分配、减少通信等。</li>
</ul>
</li>
</ul>
<p>平衡各进程间的工作量</p>
<ul>
<li>理想情况下：所有处理器在程序执行期间都在计算（它们同时计算，同时完成部分工作）</li>
<li>回顾阿姆达尔定律：<ul>
<li>少量的负载不平衡就能显著限制最大加速比</li>
<li>P4多做20%的工作→P4完成所需时间延长20%→并行程序的20%运行是串行执行，就很严重了。</li>
</ul>
</li>
</ul>
<p><img src="/img/1639880273.jpg" alt=""></p>
<p>静态赋值</p>
<ul>
<li>线程的工作分配是预先确定的<ul>
<li>不一定在编译时确定（分配算法可能取决于运行时参数，如输入数据大小、线程数等）</li>
</ul>
</li>
<li>示例：为每个线程分配相等数量的网格单元<ul>
<li>我们讨论了两种静态工作分配（分块和交替）</li>
</ul>
</li>
<li>静态赋值的良好特性：简单，基本上零运行时开销（在本例中：实现赋值的额外工作是一点索引的计算）</li>
</ul>
<p><img src="/img/1639880425.jpg" alt=""></p>
<p>静态分配何时适用？</p>
<ul>
<li>当工作的成本（执行时间）和工作量是可预测的（这样程序员就可以提前完成一个好的任务）</li>
<li>当工作是可预测的，但不是所有的工作都有相同的开销</li>
<li>当已知执行时间统计信息时（例如，平均成本相同）</li>
</ul>
<p>“半静态”分配</p>
<ul>
<li>工作成本在短期内是可预测的<ul>
<li>想法：最近的过去很好地预测了不久的将来</li>
</ul>
</li>
<li>应用程序定期配置应用程序并重新调整分配<ul>
<li>对于重新调整之间的间隔，分配是“静态”的</li>
</ul>
</li>
<li>自适应网格：网格随着对象移动或流过对象的更改而更改，但更改速度较慢（颜色表示网格部分已分配给处理器）</li>
<li>粒子模拟：粒子在模拟过程中移动时重新分布（如果运动缓慢，则不需要经常进行重新分布）</li>
</ul>
<p><img src="/img/1639880672.jpg" alt=""></p>
<p>动态分配：程序在运行时动态确定分配，以确保负载分布均匀。（任务的执行时间或任务总数是不可预测的。）</p>
<p>顺序程序（独立循环迭代）<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> N = <span class="number">1024</span>; </span><br><span class="line"><span class="type">int</span>* x = <span class="keyword">new</span> <span class="type">int</span>[N]; </span><br><span class="line"><span class="type">bool</span>* prime = <span class="keyword">new</span> <span class="type">bool</span>[N]; </span><br><span class="line"><span class="comment">// initialize elements of x here</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;N; i++) </span><br><span class="line">&#123; </span><br><span class="line">    <span class="comment">// unknown execution time </span></span><br><span class="line">    is_prime[i] = <span class="built_in">test_primality</span>(x[i]); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>并行程序（多线程执行SPMD，共享地址空间模型）<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> N = <span class="number">1024</span>; </span><br><span class="line"><span class="comment">// assume allocations are only executed by 1 thread</span></span><br><span class="line"><span class="type">int</span>* x = <span class="keyword">new</span> <span class="type">int</span>[N]; </span><br><span class="line"><span class="type">bool</span>* is_prime = <span class="keyword">new</span> <span class="type">bool</span>[N];</span><br><span class="line"><span class="comment">// initialize elements of x here</span></span><br><span class="line">LOCK counter_lock; </span><br><span class="line"><span class="type">int</span> counter = <span class="number">0</span>; <span class="comment">// shared variable</span></span><br><span class="line"><span class="keyword">while</span> (<span class="number">1</span>) &#123; </span><br><span class="line">    <span class="type">int</span> i; </span><br><span class="line">    <span class="built_in">lock</span>(counter_lock); </span><br><span class="line">    i = counter++; </span><br><span class="line">    <span class="built_in">unlock</span>(counter_lock);</span><br><span class="line">    <span class="keyword">if</span> (i &gt;= N) </span><br><span class="line">        <span class="keyword">break</span>; </span><br><span class="line">    is_prime[i] = <span class="built_in">test_primality</span>(x[i]); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>使用工作队列的动态分配</p>
<ul>
<li>子问题（也称为“任务”、“工作”）</li>
<li>共享工作队列：要做的工作的列表（现在，让我们假设每个工作都是独立的）</li>
<li>工作线程：从共享工作队列中提取数据，在创建新工作时将其推送到队列中</li>
</ul>
<p>在分配任务时，如果是一个元素就分配一次的话，可能具有良好的工作负载平衡（许多小任务），但是可能导致高同步成本（关键部分的序列化）。因此可以每多少个元素成为一个任务，降低同步或者加锁的开销。</p>
<p>选择任务大小</p>
<ul>
<li>拥有比处理器多得多的任务非常有用（许多小任务通过动态分配实现良好的工作负载平衡）</li>
<li>但希望尽可能少的任务，以最大限度地减少<ul>
<li>鼓励大粒度任务</li>
</ul>
</li>
<li>理想的粒度取决于许多因素，必须了解您的工作负载和您的机器</li>
</ul>
<p>如果某些任务比其他任务更耗时，不平衡问题的一种可能解决方案：</p>
<ul>
<li>将工作划分为大量较小的任务<ul>
<li>希望最长的任务相对于总执行时间变得更短</li>
<li>可能会增加同步开销</li>
<li>可能不可能（也许长任务基本上是连续的）</li>
</ul>
</li>
<li>另一个解决方案：智能调度<ul>
<li>先安排长任务</li>
<li>执行长任务的线程执行的总体任务较少，但与其他线程的工作量大致相同。</li>
<li>需要一些工作量方面的知识（一些成本的可预测性）</li>
</ul>
</li>
</ul>
<p>使用一组分布式队列减少同步开销（避免所有work在单个工作队列上同步），工作线程需要：</p>
<ul>
<li>从自己的工作队列中提取数据</li>
<li>将新工作推送到自己的工作队列</li>
<li>当本地工作队列为空时从另一个工作队列窃取工作</li>
</ul>
<p><img src="/img/1639882716.jpg" alt=""></p>
<p>分布式工作队列</p>
<ul>
<li>窃取期间会发生代价高昂的同步/通信<ul>
<li>但并非每次线程都有新的工作</li>
<li>只有在确保良好负载平衡的必要情况下才会发生抢夺</li>
</ul>
</li>
<li>导致局部性增加（好事啊）<ul>
<li>常见情况：线程处理它们创建的任务（生产者-消费者位置）</li>
</ul>
</li>
<li>实施挑战<ul>
<li>偷谁的？</li>
<li>偷多少？</li>
<li>如何检测程序终止？</li>
<li>确保本地队列访问速度快（同时保持互斥）</li>
</ul>
</li>
</ul>
<p>总结</p>
<ul>
<li>挑战：实现良好的工作负载平衡<ul>
<li>希望所有处理器始终工作（否则，资源将处于空闲状态！）</li>
<li>但我们需要低成本的解决方案来实现这一平衡</li>
<li>最小化计算开销（例如，调度/分配逻辑）</li>
<li>最小化同步成本</li>
</ul>
</li>
<li>静态分配与动态分配<ul>
<li>尽可能使用有关工作负载的预先知识，以减少负载不平衡和任务管理/同步成本（在极限情况下，如果系统知道一切，则使用完全静态分配）</li>
</ul>
</li>
</ul>
<p>通用并行编程模式</p>
<ul>
<li>线程并行性的显式管理：<ul>
<li>每个执行单元（或每个所需并发量）创建一个线程</li>
<li>下面的示例：带有pthreads的C代码</li>
</ul>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">thread_args</span> &#123; </span><br><span class="line">    <span class="type">float</span>* A; </span><br><span class="line">    <span class="type">float</span>* B; </span><br><span class="line">&#125;;</span><br><span class="line"><span class="type">int</span> thread_id[MAX_THREADS]; </span><br><span class="line">thread_args args; </span><br><span class="line">args.A = A; </span><br><span class="line">args.B = B; </span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;num_cores; i++) &#123; </span><br><span class="line">    <span class="built_in">pthread_create</span>(&amp;thread_id[i], <span class="literal">NULL</span>, myFunctionFoo, &amp;args); </span><br><span class="line">&#125; </span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;num_cores; i++) &#123; </span><br><span class="line">    <span class="built_in">pthread_join</span>(&amp;thread_id[i]); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>考虑分治算法<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sort elements from ‘begin’ up to (but not including) ‘end’ </span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">quick_sort</span><span class="params">(<span class="type">int</span>* begin, <span class="type">int</span>* end)</span> </span>&#123; </span><br><span class="line">    <span class="keyword">if</span> (begin &gt;= end<span class="number">-1</span>)  </span><br><span class="line">        <span class="keyword">return</span>; </span><br><span class="line">    <span class="keyword">else</span> &#123; </span><br><span class="line">        <span class="comment">// choose partition key and partition elements </span></span><br><span class="line">        <span class="comment">// by key, return position of key as `middle` </span></span><br><span class="line">        <span class="type">int</span>* middle = <span class="built_in">partition</span>(begin, end);  </span><br><span class="line">        <span class="built_in">quick_sort</span>(begin, middle); </span><br><span class="line">        <span class="built_in">quick_sort</span>(middle<span class="number">+1</span>, last); </span><br><span class="line">        <span class="comment">// independent work</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>fork-join模式</p>
<ul>
<li>表示分治算法固有的独立工作的自然方式</li>
<li>本课程的代码示例将使用Cilk Plus<ul>
<li>C++语言扩展</li>
<li>最初由麻省理工学院开发，现在改为开放标准（在GCC、英特尔ICC中）</li>
</ul>
</li>
</ul>
<p><code>cilk_spawn foo(args)</code>：</p>
<ul>
<li>语义：调用<code>foo</code>，但与标准函数调用不同，调用方可以继续异步执行<code>foo</code>。</li>
</ul>
<p><code>cilk_sync</code>：</p>
<ul>
<li>语义：当前函数生成的所有调用完成时返回。</li>
<li>注意：在包含<code>cilk_sync</code>的每个函数的末尾都有一个隐式<code>cilk_barrier</code>（暗示：当cilk函数返回时，与该函数相关的所有工作都已完成）</li>
</ul>
<p>基本Cilk示例<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// foo() and bar() may run in parallel</span></span><br><span class="line"><span class="function">cilk_spawn <span class="title">foo</span><span class="params">()</span></span>; </span><br><span class="line"><span class="built_in">bar</span>(); </span><br><span class="line">cilk_sync;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// foo() and bar() may run in parallel </span></span><br><span class="line"><span class="function">cilk_spawn <span class="title">foo</span><span class="params">()</span></span>; </span><br><span class="line"><span class="function">cilk_spawn <span class="title">bar</span><span class="params">()</span></span>; </span><br><span class="line">cilk_sync;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// foo, bar, fizz, buzz, may run in parallel </span></span><br><span class="line"><span class="function">cilk_spawn <span class="title">foo</span><span class="params">()</span></span>; </span><br><span class="line"><span class="function">cilk_spawn <span class="title">bar</span><span class="params">()</span></span>; </span><br><span class="line"><span class="function">cilk_spawn <span class="title">fizz</span><span class="params">()</span></span>; </span><br><span class="line"><span class="built_in">buzz</span>(); </span><br><span class="line">cilk_sync;</span><br></pre></td></tr></table></figure>
<p><img src="/img/1639883901.jpg" alt=""></p>
<p>Cilk Plus中的并行快速排序</p>
<p>如果问题规模足够小，则按顺序排序（生成的开销超过了潜在并行化的好处）<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">quick_sort</span><span class="params">(<span class="type">int</span>* begin, <span class="type">int</span>* end)</span> </span>&#123; </span><br><span class="line">    <span class="keyword">if</span> (begin &gt;= end - PARALLEL_CUTOFF) </span><br><span class="line">        std::<span class="built_in">sort</span>(begin, end); </span><br><span class="line">    <span class="keyword">else</span> &#123; </span><br><span class="line">        <span class="type">int</span>* middle = <span class="built_in">partition</span>(begin, end);  </span><br><span class="line">        <span class="function">cilk_spawn <span class="title">quick_sort</span><span class="params">(begin, middle)</span></span>; </span><br><span class="line">        <span class="built_in">quick_sort</span>(middle<span class="number">+1</span>, last); </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/1639884148.jpg" alt=""></p>
<p>编写fork-join程序</p>
<ul>
<li>主要思想：使用<code>cilk_spawn</code>向系统公开独立工作（潜在并行性）</li>
<li>回忆并行编程的经验法则<ul>
<li>需要至少和并行执行能力一样多的工作（例如，程序可能产生至少和内核一样多的工作）</li>
<li>需要更多的独立工作而不是执行能力，以便在核心上实现所有工作的良好工作负载平衡</li>
<li>“并行松弛”=独立工作与机器并行执行能力的比率（实际上，~8是一个很好的比率）</li>
<li>但是不要做太多的独立工作，这样工作的粒度就太小了（太多的松弛会导致管理细粒度工作的开销）</li>
</ul>
</li>
</ul>
<p>调度fork-join程序</p>
<ul>
<li>考虑非常简单的调度器：<ul>
<li>使用<code>pthread_create</code>为每个<code>cilk_sync</code>启动pthread</li>
<li>将<code>cilk_sync</code>转换为适当的<code>pthread_join</code>调用</li>
</ul>
</li>
<li>潜在的性能问题？<ul>
<li>重量级spawn操作</li>
<li>并发运行的线程比内核多得多</li>
<li>上下文切换开销</li>
<li>工作集比需要的工作集大，缓存位置少</li>
</ul>
</li>
</ul>
<p>以下程序的工作步骤？<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cilk_spawn <span class="title">foo</span><span class="params">()</span></span>;</span><br><span class="line"><span class="built_in">bar</span>(); </span><br><span class="line">cilk_sync;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>每线程工作队列存储“要做的工作”<ul>
<li>到达<code>cilk_spawn foo()</code>后，线程将后续工作（<code>bar()</code>）放入其工作队列，并开始执行<code>foo()</code></li>
</ul>
</li>
<li>空闲线程从繁忙线程“窃取”工作<ul>
<li>空闲线程在忙线程的队列中查找工作</li>
<li>如果线程1处于空闲状态（也就是说，它自己的队列中没有工作），那么它会在线程0的队列中查找要做的工作</li>
<li>空闲线程将工作从繁忙线程的队列移动到自己的队列</li>
<li>空闲线程开始执行任务。</li>
</ul>
</li>
</ul>
<p><img src="/img/1639884533.png" alt=""></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;N; i++) &#123;</span><br><span class="line">    <span class="function">cilk_spawn <span class="title">foo</span><span class="params">(i)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line">cilk_sync;</span><br></pre></td></tr></table></figure>
<ul>
<li>先运行后续工作<ul>
<li>调用线程在执行任何迭代之前生成所有迭代的工作</li>
<li>思考：调用图的宽度优先遍历。O(N)生成工作的空间（最大空间）</li>
<li>如果没有窃取，执行顺序与删除<code>cilk_spawn</code>的程序非常不同</li>
</ul>
</li>
<li>先运行孩子线程<ul>
<li>调用线程只创建一个要窃取的项（表示所有剩余迭代的延续）</li>
<li>若并没有发生窃取，线程将继续从工作队列中弹出后续，将新的后续排入队列（更新后的值为i）</li>
<li>执行顺序与删除spawn的程序相同。</li>
<li>思考：调用图的深度优先遍历</li>
<li>若后续被窃取，则线程将生成并执行下一次迭代</li>
<li>排队继续，i前进1</li>
<li>可以证明具有T线程的系统的工作队列存储不超过单线程执行的堆栈存储的T倍</li>
</ul>
</li>
</ul>
<p>实现工作窃取：每个工作线程实现一个dequeue</p>
<ul>
<li>作为dequeue实现的工作队列（双端队列）<ul>
<li>本地线程从“尾部”（底部）推动/弹出</li>
<li>远程线程从“头”（顶部）窃取</li>
<li>存在有效的无锁出列实现</li>
</ul>
</li>
<li>空闲线程随机选择要尝试从中窃取的线程</li>
<li>从出列的顶端窃取工作<ul>
<li>减少与本地线程的争用：本地线程访问的出列部分与窃取线程访问的出列部分不同！</li>
<li>窃取在调用树开始方向的工作：这是一个“更大”的工作，因此执行窃取的成本在未来较长的计算时间内摊销</li>
<li>最大化局部性：（结合运行子级优先策略）局部线程在调用树的局部部分工作</li>
</ul>
</li>
</ul>
<p><img src="/img/1639886382.jpg" alt=""><br><img src="/img/1639886607.png" alt=""></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">recursive_for</span><span class="params">(<span class="type">int</span> start, <span class="type">int</span> end)</span> </span>&#123; </span><br><span class="line">    <span class="keyword">while</span> (start &lt;= end - GRANULARITY) &#123; </span><br><span class="line">        <span class="type">int</span> mid = (end - start) / <span class="number">2</span>; </span><br><span class="line">        <span class="function">cilk_spawn <span class="title">recursive_for</span><span class="params">(start, mid)</span></span>; </span><br><span class="line">        start = mid; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=start; i&lt;end; i++) </span><br><span class="line">        <span class="built_in">foo</span>(i); </span><br><span class="line">&#125; </span><br><span class="line"><span class="built_in">recursive_for</span>(<span class="number">0</span>, N);</span><br></pre></td></tr></table></figure>
<p><img src="/img/1639886794.jpg" alt=""></p>
<p>两种sync的实现方法</p>
<ul>
<li>“暂停”加入策略<ul>
<li>启动fork的线程必须执行同步，因此，它将等待所有生成的工作完成，在这种情况下，线程0是启动fork的线程，它也将等待所有其他线程完成工作后继续执行之后的任务</li>
</ul>
</li>
<li>贪心<ul>
<li>当启动fork的线程处于空闲状态时，它看起来会窃取新的工作</li>
<li>到达连接点的最后一个线程在同步后继续执行</li>
</ul>
</li>
</ul>
<h1 id="lecture-7"><a href="#lecture-7" class="headerlink" title="lecture 7"></a>lecture 7</h1><p>关于消息传递示例的说明</p>
<ul>
<li>计算<ul>
<li>数组索引相对于本地地址空间（而不是全局网格坐标）</li>
</ul>
</li>
<li>通讯：<ul>
<li>通过发送和接收消息来执行</li>
<li>批量传输：一次传输整行（而不是单个元素）</li>
</ul>
</li>
<li>同步：<ul>
<li>通过发送和接收消息来执行</li>
</ul>
</li>
<li>为方便起见，消息传递库通常包括更高级的原语（通过发送和接收实现）</li>
</ul>
<p>同步（阻塞）发送和接收</p>
<ul>
<li><code>send()</code>：当发送方收到消息数据驻留在接收方地址空间的确认时，调用返回</li>
<li><code>recv()</code>：当接收到的消息中的数据复制到接收方的地址空间并将确认发送回发送方时，调用返回</li>
</ul>
<p>call SEND(foo)：</p>
<ul>
<li>将数据从发送方地址空间中的缓冲区“foo”复制到网络缓冲区</li>
<li>send message</li>
<li>receive ack</li>
<li>SEND()返回</li>
</ul>
<p>Call RECV(bar)：</p>
<ul>
<li>接收消息</li>
<li>将数据复制到接收方地址空间的缓冲区“bar”中</li>
<li>send ack</li>
<li>RECV()返回</li>
</ul>
<p>非阻塞异步发送/接收</p>
<ul>
<li><code>send()</code>：调用立即返回<ul>
<li>调用线程无法修改提供给send()的缓冲区，因为消息处理与线程执行同时发生</li>
<li>调用线程可以在等待消息发送时执行其他工作</li>
</ul>
</li>
<li><code>recv()</code>：发布打算在将来接收的内容，立即返回<ul>
<li>使用<code>checksend()</code>、<code>checkrecv()</code>确定发送/接收的实际状态</li>
<li>调用线程可以在等待接收消息时执行其他工作</li>
</ul>
</li>
</ul>
<p>一种简单的非流水线通信模型：<code>T(n) = T0 + n/B</code></p>
<ul>
<li>T(n)=传输时间（操作的总延迟）</li>
<li>T0=启动延迟（例如，直到第一位到达目的地的时间）</li>
<li>n=操作中传输的字节数</li>
<li>B=传输速率（链路带宽）</li>
</ul>
<p>如果处理器仅在上一条消息发送完成后发送下一条消息，“有效带宽”=<code>n/T(n)</code>，有效带宽取决于传输大小（大传输分摊启动延迟）。</p>
<p>比较通用的通信开销模型：总通信时间=开销+占用率+网络延迟</p>
<ul>
<li>开销（处理器在通信上花费的时间，调用API，缓冲区拷贝等）</li>
<li>占用率（数据通过系统最慢组件的时间）</li>
<li>网络延迟（所有其他）</li>
</ul>
<p><img src="/img/1639888794.png" alt=""></p>
<p>流水线通信：</p>
<ul>
<li>当网络忙时，消息被缓冲，直到之前的数据发送完</li>
<li>由于网络缓冲区已满，发送者无法发送其他数据</li>
</ul>
<p><img src="/img/1639888976.jpg" alt=""></p>
<p>通信计算比</p>
<ul>
<li>通信量（bytes）/计算量（指令数）</li>
<li>如果分母是计算的执行时间，则比率给出代码的平均带宽要求</li>
<li>“运算强度”=1/通信与计算比率</li>
<li>高效利用现代并行处理器需要高运算强度（低通信计算比），因为计算能力与可用带宽的比率很高</li>
</ul>
<p>良好的剖分可以减少固有的通信开销（增加运算强度）</p>
<ul>
<li>一个是N/P，另一个是1/2</li>
</ul>
<p><img src="/img/1639889381.jpg" alt=""></p>
<p>人为通信</p>
<ul>
<li>固有通信：基本上必须在处理器之间移动的信息，以执行给定分配的算法（假设无限容量缓存、最小粒度传输等）</li>
<li>人为通信：所有其他通信（人为通信源于系统实现的实际细节）</li>
<li>系统可能具有最小的传输粒度（结果：系统必须传输比所需更多的数据）<ul>
<li>程序加载一个4字节浮点值，但必须从内存传输整个64字节缓存线（通信量比需要多16倍）</li>
</ul>
</li>
<li>系统可能具有导致不必要通信的操作规则：<ul>
<li>程序存储16个连续的4字节浮点值，因此整个64字节缓存线从内存加载，然后存储到内存中（开销为2倍）</li>
</ul>
</li>
<li>数据在分布式内存中的位置不佳（数据不在访问最多的处理器附近）</li>
<li>有限的复制容量（同一数据多次传输到处理器，因为缓存太小，无法在访问之间保留）</li>
</ul>
<p>通过融合循环改进时间局部性</p>
<p>下面的程序中的两个函数，都先执行两个load，再执行一个数学运算，进行一次store（计算强度=1/3），总的计算强度就是1/3。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">int</span> n, <span class="type">float</span>* A, <span class="type">float</span>* B, <span class="type">float</span>* C)</span> </span>&#123; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;n; i++) </span><br><span class="line">        C[i] = A[i] + B[i];</span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">mul</span><span class="params">(<span class="type">int</span> n, <span class="type">float</span>* A, <span class="type">float</span>* B, <span class="type">float</span>* C)</span> </span>&#123; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;n; i++)</span><br><span class="line">        C[i] = A[i] * B[i];</span><br><span class="line">&#125; </span><br><span class="line"><span class="type">float</span>* A, *B, *C, *D, *E, *tmp1, *tmp2; </span><br><span class="line"><span class="comment">// assume arrays are allocated here </span></span><br><span class="line"><span class="comment">// compute E = D + ((A + B) * C) </span></span><br><span class="line"><span class="built_in">add</span>(n, A, B, tmp1); </span><br><span class="line"><span class="built_in">mul</span>(n, tmp1, C, tmp2); </span><br><span class="line"><span class="built_in">add</span>(n, tmp2, D, E);</span><br></pre></td></tr></table></figure></p>
<p>四个load，每3个数学运算一个load（计算强度=3/5）<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">fused</span><span class="params">(<span class="type">int</span> n, <span class="type">float</span>* A, <span class="type">float</span>* B, <span class="type">float</span>* C, <span class="type">float</span>* D, <span class="type">float</span>* E)</span> </span>&#123; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;n; i++) </span><br><span class="line">       `E[i] = D[i] + (A[i] + B[i]) * C[i];     </span><br><span class="line">&#125; </span><br><span class="line"><span class="comment">// compute E = D + (A + B) * C </span></span><br><span class="line"><span class="built_in">fused</span>(n, A, B, C, D, E);</span><br></pre></td></tr></table></figure></p>
<p>上面的代码更加模块化（例如，基于数组的数学库，如Python中的Numaray）。下面的代码执行得更好。</p>
<p>通过共享数据提高算法强度</p>
<ul>
<li>利用共享：将在同一数据上运行的任务放在同一位置<ul>
<li>在同一处理器上同时调度在同一数据结构上工作的线程</li>
<li>减少固有的通信</li>
</ul>
</li>
<li>示例：CUDA的线程块<ul>
<li>CUDA程序中用于本地化相关处理的抽象</li>
<li>块中的线程经常协作执行操作（利用CUDA共享内存快速访问/同步）</li>
<li>因此，GPU实现总是在同一GPU内核上调度来自同一块的线程</li>
</ul>
</li>
</ul>
<p>利用空间局部性</p>
<ul>
<li>通信的粒度可能很重要，因为它可能会引入伪通信<ul>
<li>通信/数据传输的粒度</li>
<li>缓存一致性的粒度</li>
</ul>
</li>
</ul>
<p>通信粒度导致的人为通信</p>
<ul>
<li>假设：通信粒度是cache line，cache line包含四个元素</li>
<li>良好的空间局部性，便于对上下行的非局部访问</li>
<li>对左右列的非本地访问的空间局部性较差</li>
<li>本质上需要来自左右邻域的一个元素，但系统必须通信四个元素。</li>
</ul>
<p><img src="/img/1639891385.png" alt=""></p>
<p>竞争：</p>
<ul>
<li>资源可以在给定吞吐量（单位时间内的事务数）下执行操作<ul>
<li>内存、通信链路、服务器等。</li>
</ul>
</li>
<li>当在一个小的时间窗口内对一个资源发出许多请求时（该资源是一个“热点”），就会发生争用</li>
<li>示例：CUDA中的内存系统争用</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> THREADS_PER_BLK 128 </span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">my_cuda_program</span><span class="params">(<span class="type">int</span> N, <span class="type">float</span>* input, <span class="type">float</span>* output)</span></span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    __shared__ <span class="type">float</span> local_data[THREADS_PER_BLK];</span><br><span class="line">    <span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x; </span><br><span class="line">    <span class="comment">// COOPERATIVELY LOAD DATA HERE</span></span><br><span class="line">    local_data[threadIdx.x] = input[index];</span><br><span class="line">    <span class="comment">// WAIT FOR ALL LOADS TO COMPLETE</span></span><br><span class="line">    __syncthreads(); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所有线程都会访问内存，因此没有线程可以运行，因为所有线程要么正在访问内存，要么在屏障处被阻塞。</p>
<p>一般来说，CUDA编程时的一个好的经验法则是确保调整线程块的大小，以便GPU可以在每个GPU内核上安装几个线程块。（这允许一个线程块中的线程覆盖分配给同一内核的另一个块中线程的延迟。）</p>
<p>示例：在大型并行机（例如GPU）上创建粒子网格数据结构，这一般用在N-body问题上，也有其他的方法<br><img src="/img/1639892682.jpg" alt=""></p>
<p>解决方案1：在cell上并行化</p>
<ul>
<li>一个可能的答案是按cell剖分：对于每个cell，独立计算其中的粒子（消除争用，因为不需要同步）</li>
<li>并行性不足：只有16个并行任务，但需要数千个独立任务才能有效利用GPU）</li>
<li>工作效率低下：在单元中执行粒子计算的次数是顺序算法的16倍</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">list cell_lists[<span class="number">16</span>];      <span class="comment">// 2D array of lists </span></span><br><span class="line"><span class="keyword">for</span> each cell c           <span class="comment">// in parallel </span></span><br><span class="line">    <span class="keyword">for</span> each particle p    <span class="comment">// sequentially </span></span><br><span class="line">        <span class="keyword">if</span> (p is within c) </span><br><span class="line">            append p to cell_lists[c]</span><br></pre></td></tr></table></figure>
<p>解决方案2：在粒子上并行化</p>
<ul>
<li>另一个答案：为每个CUDA线程指定一个粒子。线程计算包含粒子的单元，然后原子地更新列表。</li>
<li>大规模争用：数千个线程争用更新单个共享数据结构的权限</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">list cell_list[<span class="number">16</span>]; <span class="comment">// 2D array of lists  </span></span><br><span class="line">lock cell_list_lock;  </span><br><span class="line"><span class="keyword">for</span> each particle p <span class="comment">//  in  parallel  </span></span><br><span class="line">    c = compute cell containing p</span><br><span class="line">    <span class="built_in">lock</span>(cell_list_lock)</span><br><span class="line">    append p to cell_list[c]</span><br><span class="line">    <span class="built_in">unlock</span>(cell_list_lock)</span><br></pre></td></tr></table></figure>
<p>解决方案3：使用更细粒度的锁</p>
<ul>
<li>通过使用每cell锁缓解单个全局锁的争用<ul>
<li>假设粒子在二维空间中均匀分布~比解决方案2少16倍的争用</li>
</ul>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">list cell_list[<span class="number">16</span>];     <span class="comment">// 2D array of lists </span></span><br><span class="line">lock cell_list_lock[<span class="number">16</span>]; </span><br><span class="line"><span class="keyword">for</span> each particle p             <span class="comment">// in parallel </span></span><br><span class="line">    c = compute cell containing p </span><br><span class="line">    <span class="built_in">lock</span>(cell_list_lock[c]) </span><br><span class="line">    append p to cell_list[c] </span><br><span class="line">    <span class="built_in">unlock</span>(cell_list_lock[c])</span><br></pre></td></tr></table></figure>
<p>解决方案4：计算部分结果+合并</p>
<ul>
<li>另一个答案是：并行生成N个“部分”网格，然后合并<ul>
<li>示例：创建N个线程块（至少与SMX内核的线程块数量相同）</li>
<li>线程块中的所有线程更新相同的网格</li>
<li>支持更快的同步：争用减少了N倍，而且同步成本更低，因为它是在块本地变量上执行的（在CUDA共享内存中）</li>
<li>需要额外的工作：在计算结束时合并N个网格</li>
<li>需要额外的内存占用：存储N个列表网格，而不是1个</li>
</ul>
</li>
</ul>
<p><img src="/img/1639893108.jpg" alt=""></p>
<p>解决方案5：数据并行方法</p>
<ul>
<li>步骤1：计算每个粒子被哪个cell包含（对输入粒子是平行处理的）</li>
<li>步骤2：按cell序号对结果排序（基于排序排列的粒子索引数组）</li>
<li>步骤3：查找每个cell的开始/结束（基于粒子索引元素的平行）</li>
<li>此解决方案保持了大量并行性，并消除了细粒度同步的需要。。。以对数据进行排序和额外传递为代价（额外BW）</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cell = grid_index[index] </span><br><span class="line"><span class="keyword">if</span> (index == <span class="number">0</span>)</span><br><span class="line">    cell_starts[cell] = index;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (cell != grid_index[index<span class="number">-1</span>]) &#123; </span><br><span class="line">    cell_starts[cell] = index;</span><br><span class="line">    cell_ends[grid_index[index<span class="number">-1</span>]] = index; </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (index == numParticles<span class="number">-1</span>) <span class="comment">// special case for last cell </span></span><br><span class="line">    cell_ends[cell] = index<span class="number">+1</span>;</span><br></pre></td></tr></table></figure>
<p>降低通信成本</p>
<ul>
<li>减少与发送方/接收方的通信开销<ul>
<li>发送更少的消息，使消息更大（分摊开销）</li>
<li>将许多小消息合并成大消息</li>
</ul>
</li>
<li>减少延迟<ul>
<li>应用程序编写器：重新构造代码以利用局部性</li>
<li>硬件实现者：改进通信架构</li>
</ul>
</li>
<li>减少争用<ul>
<li>复制争用资源（例如，本地副本、细粒度锁）</li>
<li>错开对竞争资源的访问</li>
</ul>
</li>
<li>增加通信/计算重叠<ul>
<li>应用程序编写器：使用异步通信（例如，异步消息）</li>
<li>硬件实现者：流水线、多线程、预取、无序执行</li>
<li>在应用程序中需要额外的并发性（并发性大于执行单元的数量）</li>
</ul>
</li>
</ul>
<p>总结：优化通信</p>
<ul>
<li>固有的通信<ul>
<li>考虑到问题是如何分解的，工作是如何分配的，固有的通信是最基本的</li>
<li>人为通信取决于机器实现细节（通常与固有通信对性能同样重要）</li>
</ul>
</li>
<li>提高程序性能<ul>
<li>识别和利用位置：减少通信（增加运算强度）</li>
<li>减少开销（更少、更大的消息）</li>
<li>减少争用</li>
<li>最大化通信和处理的重叠（隐藏延迟，以免产生成本）</li>
</ul>
</li>
</ul>
<h1 id="lecture-8"><a href="#lecture-8" class="headerlink" title="lecture 8"></a>lecture 8</h1><p>一些case study，讲解多个并行应用示例</p>
<ul>
<li>海洋模拟</li>
<li>星系模拟（Barnes-Hut 算法）</li>
<li>平行扫描</li>
<li>数据并行分段扫描</li>
<li>光线追踪</li>
</ul>
<p>下图中方框对应于网格上的计算，线条表示网格上计算之间的依赖关系，“网格求解器”对应于应用程序的这些部分。这个图中表示了网格内的并行（数据并行）和不同网格之间的操作。该实现仅利用数据并行性。</p>
<p><img src="/img/1640008094.jpg" alt=""></p>
<p>海洋实现细节</p>
<ul>
<li>分解：<ul>
<li>网格的空间划分：每个处理器接收网格的二维剖分</li>
</ul>
</li>
<li>分配<ul>
<li>将剖分静态分配给处理器</li>
</ul>
</li>
<li>同步<ul>
<li>barrier（将不同的计算阶段分开）</li>
<li>更新共享变量时锁定互斥（“diff”的原子更新）</li>
</ul>
</li>
</ul>
<p><img src="/img/1640008248.png" alt=""></p>
<p>一种对区域格点进行分割的方法：</p>
<ul>
<li>叶节点是粒子，中间节点是方框，存储着若干点</li>
<li>内部节点存储所有子实体的质心 + 总质量</li>
<li>要计算每个物体上的力，请遍历树…累积所有其他物体的力<ul>
<li>如果 L/D &lt; ϴ，则聚合内部节点计算力，否则下降到子节点</li>
</ul>
</li>
<li>预期接触节点数 ~ lg N / ϴ2</li>
</ul>
<p>Barnes-Hut 树形结构的挑战：</p>
<ul>
<li>每个进程的工作量不统一，通信不均匀（取决于物体的局部密度）</li>
<li>格点移动：因此成本和沟通模式会随着时间而变化</li>
<li>不规则、细粒度的计算</li>
<li>但是，计算中有很多局部性（空间附近的物体需要类似的数据来计算力）</li>
</ul>
<p><img src="/img/1640008407.jpg" alt=""></p>
<p>工作分配</p>
<ul>
<li>挑战：<ul>
<li>每个处理器的主体数量相等！= 每个处理器的工作量相等</li>
<li>希望每个处理器的工作量均等，并且分配应保留局部性</li>
</ul>
</li>
<li>观察：物体的空间分布变化缓慢</li>
<li>使用半静态赋值<ul>
<li>每个时间步长，对于每个主体，记录与其他主体的交互次数</li>
<li>计算成本低。 只需增加本地的 per-body 计数器</li>
<li>使用值定期重新计算分配</li>
</ul>
</li>
</ul>
<p>Barnes-Hut：工作集</p>
<ul>
<li>工作集 1：计算体-体（或体-节点）对之间的力所需的数据</li>
<li>工作集 2：在整个树遍历中遇到的数据<ul>
<li>一个物体接触的预期节点数：~ lg N / ϴ^2</li>
<li>计算具有高度局部性：连续处理的物体就在附近，因此对一个点的处理几乎在完全相同的节点！</li>
</ul>
</li>
</ul>
<p>应该是一个树形的扫描结构，用来遍历或者广播。<br><img src="/img/1640009631.jpg" alt=""></p>
<p>Up-sweep:<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for d=0 to (log2(n) - 1) do</span><br><span class="line">    forall k=0 to n-1 by 2^(d+1) do </span><br><span class="line">        a[k + 2^(d+1) - 1] = a[k + 2^(d) - 1] + a[k + 2^(d+1) - 1]</span><br></pre></td></tr></table></figure></p>
<p>Down-sweep:<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x[n-1] = 0</span><br><span class="line">for d=(log2(n) - 1) down to 0 do</span><br><span class="line">    forall k=0 to n-1 by 2^(d+1) do </span><br><span class="line">        tmp = a[k + 2^(d) - 1] </span><br><span class="line">        a[k + 2^(d) - 1] = a[k + 2^(d+1) - 1] </span><br><span class="line">        a[k + 2^(d+1) - 1] = tmp + a[k + 2^(d+1) - 1]</span><br></pre></td></tr></table></figure></p>
<p>加速光线相交场景</p>
<ul>
<li>预处理场景以构建数据结构，加速沿射线寻找“最接近”的几何体</li>
<li>想法：对空间接近的对象进行分组（如 Barnes-Hut 中的四叉树）<ul>
<li>分层分组适应场景对象的非均匀密度</li>
</ul>
</li>
</ul>
<p>简单的光线追踪器（使用 BVH）<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// stores information about closest hit found so far </span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ClosestHitInfo</span> &#123;</span> </span><br><span class="line">    Primitive primitive; </span><br><span class="line">    <span class="type">float</span> distance; </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">trace(Ray ray, BVHNode node, ClosestHitInfo hitInfo) </span><br><span class="line">&#123; </span><br><span class="line">    <span class="keyword">if</span> (!intersect(ray, node.bbox) || (closest point on box is farther than hitInfo.distance)) </span><br><span class="line">        <span class="keyword">return</span>; </span><br><span class="line">    <span class="keyword">if</span> (node.leaf) &#123; </span><br><span class="line">        <span class="keyword">for</span> (each primitive in node) &#123; </span><br><span class="line">            (hit, distance) = intersect(ray, primitive); </span><br><span class="line">            <span class="keyword">if</span> (hit &amp;&amp; distance &lt; hitInfo.distance) &#123; </span><br><span class="line">                hitInfo.primitive = primitive; </span><br><span class="line">                hitInfo.distance = distance; </span><br><span class="line">            &#125; </span><br><span class="line">        &#125; </span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; </span><br><span class="line">        trace(ray, node.leftChild, hitInfo); </span><br><span class="line">        trace(ray, node.rightChild, hitInfo); </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>射线打包追踪：程序一次明确地将一组光线与 BVH 相交<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">RayPacket </span><br><span class="line">&#123; </span><br><span class="line">    Ray rays[PACKET_SIZE]; </span><br><span class="line">    <span class="type">bool</span> active[PACKET_SIZE]; </span><br><span class="line">&#125;; </span><br><span class="line">trace(RayPacket rays, BVHNode node, ClosestHitInfo packetHitInfo) </span><br><span class="line">&#123; </span><br><span class="line">    <span class="keyword">if</span> (!ANY_ACTIVE_intersect(rays, node.bbox) || (closest point on box (<span class="keyword">for</span> all active rays) is farther than hitInfo.distance)) </span><br><span class="line">        <span class="keyword">return</span>; </span><br><span class="line">    </span><br><span class="line">    update packet active mask</span><br><span class="line">    </span><br><span class="line">    <span class="title function_">if</span> <span class="params">(node.leaf)</span> &#123; </span><br><span class="line">        <span class="keyword">for</span> (each primitive in node) &#123; </span><br><span class="line">            <span class="keyword">for</span> (each ACTIVE ray r in packet) &#123; </span><br><span class="line">                (hit, distance) = intersect(ray, primitive); </span><br><span class="line">                <span class="keyword">if</span> (hit &amp;&amp; distance &lt; hitInfo.distance) &#123; </span><br><span class="line">                    hitInfo[r].primitive = primitive; </span><br><span class="line">                    hitInfo[r].distance = distance; </span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; </span><br><span class="line">        trace(rays, node.leftChild, hitInfo); </span><br><span class="line">        trace(rays, node.rightChild, hitInfo); </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>首先按照多叉树进行分割，再把光线进行打包追踪<br><img src="/img/1640010466.jpg" alt=""></p>
<p>数据包的优点</p>
<ul>
<li>将数据包操作映射到宽 SIMD 执行<ul>
<li>每条射线一个矢量</li>
</ul>
</li>
<li>Amortize BVH 数据获取：包中的所有光线同时访问节点<ul>
<li>为数据包中的所有光线加载一次 BVH 节点（不是每条光线一次）</li>
<li>注意：使数据包大于 SIMD 宽度是有价值的！（例如，大小 = 64）</li>
</ul>
</li>
<li>摊销工作（数据包是光线的层次结构）<ul>
<li>使用区间算法保守地针对节点 bbox 测试整个光线集（例如，将数据包视为光束）</li>
<li>当所有光线共享原点时，可以进行进一步的算术优化</li>
<li>注意：使数据包比 SIMD 宽度大得多是有价值的！</li>
</ul>
</li>
</ul>
<p>数据包的缺点</p>
<ul>
<li>如果任何光线必须访问一个节点，它会拖动数据包中的所有光线与它一起）</li>
<li>效率损失：节点遍历、交叉等，分摊在少于一个数据包的射线价值上</li>
<li>并非所有 SIMD 通道都在做有用的工作</li>
</ul>
<p>当光线不相干时，数据包的好处会显着降低。本例：数据包访问所有树节点。<br><img src="/img/1640011038.jpg" alt=""></p>
<p>通过光线重新排序改进数据包跟踪：想法：当数据包利用率低于阈值时，重排光线并继续使用较小的数据包</p>
<ul>
<li>提高 SIMD 利用率</li>
<li>这项工作用小包更好</li>
</ul>
<p>示例：考虑 8-wide SIMD 处理器和 16-ray 数据包（对数据包中的所有光线执行每个操作需要 2 个 SIMD 指令）</p>
<p>16 射线包：16 条射线中的 7 条处于活动状态，重新排列光线，重新计算活动射线的间隔/边界，使用 8 射线数据包继续跟踪：8 条射线中的 7 条处于活动状态</p>
<p><img src="/img/1640011267.jpg" alt=""></p>
<p>数据包跟踪的最佳实践</p>
<ul>
<li>对眼睛/反射/点光阴影光线或更高级别的 BVH 使用大包<ul>
<li>相干的光线始终位于树的顶部</li>
</ul>
</li>
<li>当数据包利用率低于阈值时切换到单射线（射线内 SIMD）<ul>
<li>对于宽 SIMD 机器，分支因子 4 BVH 适用于数据包遍历和单射线遍历</li>
</ul>
</li>
<li>可以使用数据包重新排序来推迟切换时间<ul>
<li>重新排序允许数据包提供利用树的好处</li>
<li>由于实现复杂度高，在实践中不经常使用</li>
</ul>
</li>
</ul>
<h1 id="lecture-9"><a href="#lecture-9" class="headerlink" title="lecture 9"></a>lecture 9</h1><p>粒子分块的并行实现</p>
<p>串行方法：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span> cell_lists[<span class="number">16</span>];      <span class="comment">// 2D array of lists </span></span><br><span class="line"><span class="keyword">for</span> each particle p</span><br><span class="line">    c = compute cell containing p   </span><br><span class="line">    append p to cell_lists[c]</span><br></pre></td></tr></table></figure></p>
<p>并行实现1：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span> cell_list[<span class="number">16</span>];    <span class="comment">// 2D array of lists </span></span><br><span class="line">lock cell_list_lock; </span><br><span class="line"><span class="keyword">for</span> each particle p    <span class="comment">// in parallel </span></span><br><span class="line">    c = compute cell containing p </span><br><span class="line">    lock(cell_list_lock) </span><br><span class="line">    append p to cell_list[c] </span><br><span class="line">    unlock(cell_list_lock)</span><br></pre></td></tr></table></figure></p>
<p>并行实现2：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">list</span> cell_lists[<span class="number">16</span>]; <span class="comment">// 2D array of lists </span></span><br><span class="line"><span class="keyword">for</span> each cell c <span class="comment">// in parallel </span></span><br><span class="line">    <span class="keyword">for</span> each particle p     <span class="comment">// sequentially </span></span><br><span class="line">        <span class="keyword">if</span> (p is within c) </span><br><span class="line">            append p to cell_lists[c]</span><br></pre></td></tr></table></figure></p>
<p>实现3：每个进程构建一个网格再合并<br><img src="/img/1640088818.jpg" alt=""></p>
<p>实现4：数据并行排序</p>
<ul>
<li>计算每个粒子包含在哪个cell中</li>
<li>根据cell号对粒子排序</li>
<li>找到每个cell包含粒子下标的起始和结束</li>
</ul>
<p>下图是一个海洋求解器在海洋网格规模固定时的加速比，最上方的是超线性加速，有了足够的处理器，分配给每个处理器的块开始适合于缓存(关键的工作集适合于每个处理器的缓存)。另一个例子是，如果问题大小对单个机器来说太大，那么工作集可能不适合内存，从而导致磁盘的抖动(这将会使加速比在有更大的的内存的机器上看起来令人惊讶!)<br><img src="/img/1640093948.jpg" alt=""></p>
<p>理解扩展性</p>
<ul>
<li>在问题的大小和并行计算机的大小之间可能有复杂的关系。<ul>
<li>可以影响负载平衡，开销，算术强度，数据访问的位置</li>
</ul>
</li>
<li>用一个有固定问题大小的问题去评估一个机器可能是有问题的<ul>
<li>问题过小:<ul>
<li>并行性的开销掩盖了并行性的优势(甚至可能导致性能下降)</li>
<li>问题大小可能适合小型机器，但对大型机器不合适(不反映大机器的实际使用!)</li>
</ul>
</li>
<li>过大的问题<ul>
<li>关键的工作集可能不“适合”在小机器(导致对磁盘的攻击)，或者关键工作集超过缓存容量，或者根本不能运行</li>
<li>当有问题的工作集更“适合”在一个大型机器上，而不适合小的机器上，可以发生超线性加速比</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>资源导向的扩展性属性</p>
<p>工作问题约束(PC)：</p>
<ul>
<li>使用一个并行计算机更快的解决问题</li>
<li>加速比：(time 1 processor) / (time p processor)</li>
</ul>
<p>工作时间约束(TC)</p>
<ul>
<li>固定时间内完成更多的工作</li>
<li>加速比：(work done by p processor) / (work done by 1 processor)</li>
<li>如何衡量“工作”?<ul>
<li>挑战：“工作完成”可能不是问题输入值的线性函数(例如矩阵乘法是O(N^3)，对O(N^2)大小输入的工作)</li>
<li>一种方法：“工作完成”是通过单个处理器的相同计算的执行时间定义的</li>
<li>理想情况下，一项工作是:<ul>
<li>理解简单</li>
<li>随顺序运行时间保持线性扩展(因此理想的加速保持线性P)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>内存约束(MC)</p>
<ul>
<li>在不溢出内存的情况下运行的最大问题，且每个处理器的内存是固定的，任务和执行时间都不是固定的</li>
<li>加速比：(work(P processors) ✖ time(1 processor)) / (work(1 processors) ✖ time(p processor))<ul>
<li>可以化简为：(work per unit time on P processors) / (work per unit time on 1 processors)</li>
</ul>
</li>
<li>例如：大规模N体问题，大规模机器学习等</li>
</ul>
<p>对一个有N^2个格点，P个进程的海洋求解器，需要<code>O(N^2)</code>内存，在<code>O(N)</code>下迭代收敛，总共的工作量是<code>O(N^3)</code>。每个处理器计算<code>N^2/P</code>个格点，每个进程的通信量是<code>N/√P</code>个。</p>
<p>在问题规模确定的情况下，N是固定的</p>
<ul>
<li>执行时间是<code>O(1/P)</code></li>
<li>每个处理器<code>O(1/P)</code>个格点</li>
<li>每个处理器的通信<code>O(1/P^(1/2))</code></li>
<li>通信计算比<code>O(P^(1/2))</code></li>
</ul>
<p>在内存确定的情况下</p>
<ul>
<li>让网格大小是<code>NP^(1/2) * NP^(1/2)</code></li>
<li>执行时间是<code>O(NP^(1/2)^3 / P)</code> = <code>O(P^(1/2))</code></li>
<li>每个处理器<code>N^2</code>个格点</li>
<li>通信计算比<code>1/N</code></li>
</ul>
<p>在时间固定情况下</p>
<ul>
<li>让网格大小固定为<code>K * K</code></li>
<li>假定线性加速比：<code>K^3 / P = N^3</code>，所以<code>K = NP^(1/3)</code></li>
<li>执行时间是固定的<code>O(N^3)</code></li>
<li>每个处理器<code>K^2/P</code>个格点，即<code>N^2 / P^(1/3)</code></li>
<li>每个处理器通信<code>K/P^(1/2) = O(1/P^(1/6))</code></li>
<li>通信计算比<code>O(P^(1/6))</code></li>
</ul>
<p>关于问题扩展性的警告</p>
<ul>
<li>在前面的例子中，问题大小是一个参数n</li>
<li>在实践中，问题大小是参数的组合<ul>
<li>回忆海洋的例子:问题大小是<code>=(n,Δt,T)</code>的函数</li>
<li>问题参数通常相关(不独立)</li>
</ul>
</li>
</ul>
<p>一个有用的性能分析策略</p>
<ul>
<li>您可以确定您的性能是否受到计算、内存带宽(或内存延迟)或同步的限制?</li>
<li>试着建立“高水准”<ul>
<li>你在实践中能做的最好的是什么?</li>
<li>你的实现离最好的case有多近?</li>
</ul>
</li>
</ul>
<p>使用roofline模型：利用微基准计算机器的峰值性能，作为应用的计算强度函数。然后，将应用程序的性能与已知的峰值值进行比较。斜线是受内存带宽的限制，水平区域是受计算能力的限制了。<br><img src="/img/1640097950.jpg" alt=""></p>
<p>使用不同级别的优化方法，得到的曲线图<br><img src="/img/1640098118.jpg" alt=""></p>
<p>建立高水准程序</p>
<ul>
<li>添加“数学”(运算指令而非内存指令)<ul>
<li>执行时间与运算数量的增长是线性增加的吗?(如果是这样的话，这就是代码是指令限制的证据)</li>
</ul>
</li>
<li>将所有数组访问更改为<code>A[0]</code><ul>
<li>你的代码得到了多少速度?(这为改善数据访问的局部性建立了一个上限)</li>
</ul>
</li>
<li>删除所有原子操作或锁<ul>
<li>你的代码得到了多少速度?(如果它仍然做了大约相同数量的工作)(这在降低同步开销的好处上建立了一个上限。)</li>
</ul>
</li>
</ul>
<blockquote>
<p>计算、内存访问和同步几乎无法完全重叠。因此，整体性能很少会完全通过计算或带宽或同步来决定。即便如此，性能对上述程序修改的敏感性可以很好地表明主要开销</p>
</blockquote>
<h1 id="lecture-10"><a href="#lecture-10" class="headerlink" title="lecture 10"></a>lecture 10</h1><p>以下是一个64位cache line：<br><img src="/img/1640098677.jpg" alt=""></p>
<p>回顾：写回、写分配的行为。当处理器执行<code>int x = 1;</code>时：</p>
<ol>
<li>工作处理器希望在没有驻留在缓存中的地址写</li>
<li>缓存选择位置在缓存中放置行，如果目前这个位置有脏标记，这个脏的cache line被写到内存中</li>
<li>工作缓存从内存中加载<code>x</code>到这个行(“在缓存分配行”)</li>
<li>更新32位缓存行</li>
<li>缓存行标记为脏（因为是执行了把1写到<code>x</code>的操作）</li>
</ol>
<p>共享内存多处理器</p>
<ul>
<li>处理器读取和写入共享变量<ul>
<li>更准确地说：处理器发布加载和存储指令</li>
</ul>
</li>
<li>对内存的合理期望是:<ul>
<li>读取地址X中的值应该返回写入的最后一个值，以处理任何处理器的X</li>
</ul>
</li>
</ul>
<p>缓存一致性问题</p>
<ul>
<li>现代处理器在本地缓存中复制内存的内容</li>
<li>问题：处理器可能观察相同内存位置出现不同值</li>
</ul>
<p><img src="/img/1640099335.jpg" alt=""></p>
<p>下边的图表显示了变量foo(在地址X中存储的)和每个处理器缓存中的值。假设在地址X中存储的初始值为0，假设回写缓存行为。这是一个由存储在本地缓存中的地址X中的数据引起的问题(硬件实现细节)。<br><img src="/img/1640099367.jpg" alt=""></p>
<p>内存一致性问题</p>
<ul>
<li>内存系统的逻辑行为：地址X的读取值应该返回写入的最后一个值，以处理任何处理器的X。</li>
<li>由于存在全局存储(主内存)和每个处理器本地存储(处理器缓存)实现单个共享地址空间的抽象，因此，内存一致性问题就存在了。</li>
</ul>
<p>下图中32KB的L1是每个处理器私有的，8路组相联，每次延迟只有4-6个钟。256KB的L2是每个处理器私有的，8路组相联且为写回策略，8MB的L3是每个片私有的，16路组相联。<br><img src="/img/1640099630.png" alt=""></p>
<p>共享内存的期望</p>
<ul>
<li>内存系统的逻辑行为：地址X的读取值应该返回写入的最后一个值，以处理任何处理器的X。</li>
<li>在单处理器上，提供这种行为是相当简单的，因为通常来自一个处理器。<ul>
<li>异常:设备通过直接内存访问(DMA)进行I/O</li>
</ul>
</li>
</ul>
<p>一致性是单个CPU系统中的一个问题。常见解决方案:</p>
<ul>
<li>CPU使用未缓存的存储(例如，琼代码)写入共享缓冲区。</li>
<li>OS支持:<ul>
<li>标记虚拟内存页面,包含可访问的共享缓冲区</li>
<li>当I / O完成时，明确地将页面从缓存中刷新</li>
</ul>
</li>
<li>在实践中，与CPU load和store相比，DMA传输是不常见的(因此这些重量级的软件解决方案是可以接受的)</li>
</ul>
<p>案例1:</p>
<ul>
<li>处理器写入主内存中的缓冲区。处理器告诉网络卡异步发送缓冲区。</li>
<li>问题：如果处理器的写入(反映在缓存的数据副本中)没有刷新到内存中，则会发送陈旧的数据</li>
</ul>
<p>案例2:</p>
<ul>
<li>网络卡接收消息。网络卡使用DMA把数据传输到在主内存中的缓冲区中。网卡通知CPU消息已被接收，缓冲区已经就绪读取。</li>
<li>问题：如果网络卡更新的地址还只在缓存中，CPU可能会读取陈旧的数据</li>
</ul>
<p>直觉行为：读取地址X的值应该返回由任何处理器写入地址X的最后一个值。</p>
<ul>
<li>“最后一个”是什么意思?<ul>
<li>如果两个处理器同时写怎么办?</li>
<li>如果P1的写操作之后紧接着P2的读操作发生的时间非常接近，以至于无法及时通知P2的写操作，该怎么办?</li>
</ul>
</li>
<li>在顺序程序中，“last”由程序顺序(而不是时间)决定。<ul>
<li>在并行程序的一个线程为真正的最后一个</li>
<li>但我们需要想出一个有意义的方式来描述并行程序中的所有线程</li>
</ul>
</li>
</ul>
<p>在以下情况下，内存系统是一致的：</p>
<ul>
<li>并行程序的执行结果是，对于每个内存位置，所有程序操作（由所有处理器执行）到与执行结果一致的位置都有一个假定的串行顺序，并且：<ul>
<li>任何一个处理器发出的内存操作按照处理器发出的顺序进行</li>
<li>读取返回的值是最后一次写入位置…时写入的值</li>
</ul>
</li>
</ul>
<p>在以下情况下，内存系统是一致的：</p>
<ol>
<li>处理器P对地址X的读取，紧接着处理器P对地址X的写入，应返回P写入的值（假设其间没有其他处理器写入X）</li>
<li>在处理器P2对X的写入之后，处理器P1对地址X的读取返回写入的值，如果读取和写入在时间上“足够分离”（假设其间没有对X进行其他写入）</li>
<li>对同一地址的写入被序列化：任何两个处理器对地址X的两次写入被所有处理器以相同的顺序观察到。</li>
<li>示例：如果将值1和2写入地址X，则没有处理器观察到X在值1之前有值2</li>
</ol>
<ul>
<li>条件1：遵守程序顺序（如单处理器系统所预期的）</li>
<li>条件2：“写入传播”：写入通知最终必须到达其他处理器。请注意，一致性的定义中没有明确规定何时传播有关写入的信息。</li>
<li>条件3：“写序列化”</li>
</ul>
<p>实施一致性</p>
<ul>
<li>基于软件的解决方案<ul>
<li>操作系统使用页面错误机制来传播写操作</li>
<li>可用于在工作站集群上实现内存一致性</li>
<li>我们不会讨论这些解决方案</li>
</ul>
</li>
<li>基于硬件的解决方案<ul>
<li>基于“监听”的一致性实现</li>
<li>基于目录的一致性实现</li>
</ul>
</li>
</ul>
<p>共享缓存：一致性变得容易</p>
<ul>
<li>一个由所有处理器共享的单一缓存<ul>
<li>消除了在多个缓存中复制状态的问题</li>
</ul>
</li>
<li>明显的可扩展性问题（因为缓存的关键是本地和快速）<ul>
<li>由多个客户端引起的干扰/争用</li>
</ul>
</li>
<li>但共享缓存有以下好处：<ul>
<li>促进细粒度共享（重叠工作集）</li>
<li>一个处理器的加载/存储可能会预取另一个处理器的行</li>
</ul>
</li>
</ul>
<p>缓存缓存一致性方案</p>
<ul>
<li>主要思想：所有与一致性相关的活动都会广播到系统中的所有处理器（更具体地说：广播到处理器的缓存控制器）</li>
<li>缓存控制器监视内存操作，并相应地作出反应以保持内存一致性</li>
<li>注意：现在缓存控制器必须响应“两端”的操作：<ul>
<li>来自本地处理器的LD/ST请求</li>
<li>通过芯片互连进行一致性相关活动广播</li>
</ul>
</li>
</ul>
<p>非常简单的一致性实现。让我们假设：</p>
<ol>
<li>write-through缓存</li>
<li>一致性的粒度是cache line</li>
</ol>
<ul>
<li>写入时，缓存控制器广播失效消息</li>
<li>因此，从其他处理器的下一次读取将触发缓存未命中（由于直写策略，处理器从内存中检索更新的值）</li>
</ul>
<p><img src="/img/1640101577.jpg" alt=""></p>
<p>说明</p>
<ul>
<li>我们将要描述的逻辑由每个处理器的缓存控制器执行，以响应：<ul>
<li>由本地处理器加载和存储</li>
<li>它从其他缓存接收的消息</li>
</ul>
</li>
<li>如果所有高速缓存控制器都按照所描述的协议操作，则将保持一致性<ul>
<li>缓存“合作”以确保保持一致性</li>
</ul>
</li>
</ul>
<p>写直达（write-through）失效状态图。蓝色虚线表示远端处理器发起事务，黑色实线表示本地处理器发起事务。<br><img src="/img/1640101756.jpg" alt=""></p>
<ul>
<li>两种状态（与单处理器缓存中无效的含义相同）<ul>
<li>无效（I）</li>
<li>有效（V）</li>
</ul>
</li>
<li>两个处理器操作（由本地处理器触发）<ul>
<li>PrRd（已读）</li>
<li>PrWr（写入）</li>
</ul>
</li>
<li>两个总线事务（来自远程缓存）<ul>
<li>BusRd（另一个处理器打算读取）</li>
<li>BusRw（另一个处理器打算写入）</li>
</ul>
</li>
</ul>
<p>互连的要求：</p>
<ol>
<li>所有缓存控制器可见的所有写事务</li>
<li>所有缓存控制器以相同顺序发现所有写入事务</li>
</ol>
<p>简化此处的假设：</p>
<ol>
<li>互连和内存事务是原子事务</li>
<li>处理器在发出下一个内存操作之前，将等待上一个内存操作完成</li>
<li>作为接收失效广播的一部分，立即申请失效</li>
</ol>
<p>写直达策略效率低下</p>
<ul>
<li>每个写操作都会输出到内存中<ul>
<li>非常高的带宽要求</li>
</ul>
</li>
<li>写回缓存在缓存命中时吸收大部分写流量<ul>
<li>显著降低带宽需求</li>
<li>但现在我们如何确保写入传播/序列化？</li>
<li>这需要更复杂的一致性协议</li>
</ul>
</li>
</ul>
<p>具有写回缓存的缓存一致性</p>
<ul>
<li>cache line的脏状态现在表示独占所有权<ul>
<li>独占：缓存是唯一具有行的有效副本的缓存（可以安全地写入）</li>
<li>所有者：这个缓存行所在的处理器负责在其他处理器尝试从内存加载该行时将其提供给其他处理器（否则，来自其他处理器的加载将从内存中获取过时数据）</li>
</ul>
</li>
</ul>
<p><img src="/img/1640102215.png" alt=""></p>
<p>基于失效的写回协议关键思想：</p>
<ul>
<li>处于“独占”状态的行可以在不通知其他缓存的情况下进行修改</li>
<li>处理器只能写入处于独占状态的行<ul>
<li>因此，他们需要一种方法来告诉其他缓存，他们希望以独占方式访问该线路</li>
<li>他们将通过向所有其他处理器发送缓存消息来实现这一点</li>
</ul>
</li>
<li>当缓存控制器监听对其包含的cache line的独占访问请求时<ul>
<li>它必须使自己缓存中的行无效</li>
</ul>
</li>
</ul>
<p>MSI写回失效协议</p>
<ul>
<li>协议的关键任务<ul>
<li>确保处理器获得写入的独占访问权</li>
<li>在缓存未命中上查找cache line数据的最新副本</li>
</ul>
</li>
<li>三种缓存线状态<ul>
<li>无效（I）：与单处理器缓存中无效的含义相同</li>
<li>共享（S）：在一个或多个缓存中有效的行</li>
<li>修改（M）：行在一个缓存中有效（也称为“脏”或“独占”状态）</li>
</ul>
</li>
<li>两个处理器操作（由本地CPU触发）<ul>
<li>PrRd（已读）</li>
<li>PrWr（写入）</li>
</ul>
</li>
<li>三个一致性相关总线事务（来自远程缓存）<ul>
<li>BusRd：获取cache line副本，无需修改</li>
<li>BusRdX：获取cache line副本，以便修改</li>
<li>刷新：将脏行写入内存</li>
</ul>
</li>
</ul>
<p><img src="/img/1640102456.jpg" alt=""></p>
<p>小结：MSI</p>
<ul>
<li>可以在不通知其他缓存的情况下修改处于M状态的行<ul>
<li>没有其他缓存具有常驻行，因此其他处理器无法读取这些值（不生成内存读取事务）</li>
</ul>
</li>
<li>处理器只能写入处于M状态的行<ul>
<li>若处理器对缓存中非独占的行执行写操作，则缓存控制器必须首先广播读独占事务，以将该行移动到该状态</li>
<li>Read exclusive告诉其他缓存即将写入的信息（“你不能再读取了，因为我要写了”）</li>
<li>即使行在处理器的本地缓存中有效（但不是独占的…它处于s状态），也需要读独占事务</li>
<li>脏状态意味着排他性</li>
</ul>
</li>
<li>当缓存控制器监听其包含的行的“只读独占”时<ul>
<li>必须使缓存中的行无效</li>
<li>因为如果没有，那么多个缓存都将有这一行（因此它在另一个缓存中不是独占的！）</li>
</ul>
</li>
</ul>
<p>MSI是否满足一致性？</p>
<ul>
<li>写传播<ul>
<li>通过组合BusRdX上的失效和从其他处理器在后续BusRd/BusRdX上的M状态刷新来实现</li>
</ul>
</li>
<li>写序列化<ul>
<li>出现在互连上的写入按它们出现在互连上的顺序排列（BusRdX）</li>
<li>显示在互连上的读取按它们在互连上的显示顺序排序（BusRd）</li>
<li>未出现在互连上的写入（PrWr到cache line已处于M状态）：<ul>
<li>对cache line的写入序列位于线路的两个互连事务之间</li>
<li>由同一处理器P按顺序执行的所有写入操作（该处理器肯定会按正确的顺序观察它们）</li>
<li>所有其他处理器仅在cache line的互连事务之后才观察这些写入的通知。因此，所有写入都在事务之前。</li>
<li>因此，所有处理器都以相同的顺序看到写入。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>MESI失效协议</p>
<ul>
<li>即使应用程序根本没有共享，也存在这种低效率</li>
<li>解决方案：添加附加状态E（“exclusive clean”）<ul>
<li>尚未修改行，但只有此缓存具有该行的副本</li>
<li>将排他性与行所有权分离（行不脏，所以内存中的副本是数据的有效副本）</li>
<li>从E升级到M不需要互连事务</li>
</ul>
</li>
<li>MSI需要两个互连事务，用于读取地址然后写入地址的常见情况<ul>
<li>事务1:BusRd从I状态移动到S状态</li>
<li>事务2:BusRdX从S状态移动到M状态</li>
</ul>
</li>
</ul>
<p><img src="/img/1640103269.jpg" alt=""></p>
<ul>
<li>当缓存线处于另一个缓存的E或S状态时，谁应该提供缓存未命中的数据？<ul>
<li>可以从内存中获取缓存线数据，也可以从另一个缓存中获取数据</li>
</ul>
</li>
<li>如果源是另一个缓存，应该由哪个缓存提供？<ul>
<li>缓存到缓存的传输增加了复杂性，但通常用于减少数据访问的延迟和减少应用程序所需的内存带宽</li>
</ul>
</li>
</ul>
<p>提高效率（和复杂性）</p>
<ul>
<li>MESIF（基于五阶段失效的协议）<ul>
<li>与MESI类似，但一个缓存在F状态而不是S状态下保存共享cache line（F=“forward”）</li>
<li>cache line处于F状态服务未命中</li>
<li>简化了应丢失哪个缓存的决策（基本MESI：所有缓存都响应）</li>
<li>由英特尔处理器使用</li>
</ul>
</li>
<li>MOESI（基于五阶段失效的协议）<ul>
<li>在MESI协议中，从M到S的转换需要刷新到内存</li>
<li>作为替代，从M转换到O（O=“拥有，但不独占”），并且不刷新到内存</li>
<li>其他处理器将共享cache line保持在S状态，一个处理器将cache line保持在O状态</li>
<li>内存中的数据已过时，因此cache line处于O状态的缓存必须为缓存未命中提供服务</li>
<li>用于AMD Opteron</li>
</ul>
</li>
</ul>
<p>Dragon写回更新协议</p>
<ul>
<li>状态：（无无效状态，但在第一次加载之前可以认为行无效）<ul>
<li>独占清除（E）：只有一个缓存具有最新的行、内存</li>
<li>共享清理（SC）：多个缓存可能有这一行，内存可能是最新的，也可能不是最新的</li>
<li>共享修改（SM）：多个缓存可能有这一行，内存不是最新的<ul>
<li>对于给定的行，只有一个缓存可以处于这种状态（但其他缓存可以处于SC状态）</li>
<li>行处于SM状态的缓存是数据的“所有者”。必须在退出时更新内存</li>
</ul>
</li>
<li>修改（M）：只有一个缓存有行，它是脏的，内存不是最新的<ul>
<li>缓存是数据的所有者。更换时必须更新内存</li>
</ul>
</li>
</ul>
</li>
<li>处理器操作：<ul>
<li>PrRd，PrWr，PrRdMiss，PrWrMiss</li>
</ul>
</li>
<li>总线事务：<ul>
<li>总线读取（BusRd）、刷新（提供线路）、总线更新（BusUpd）</li>
</ul>
</li>
</ul>
<p><img src="/img/1640103585.jpg" alt=""></p>
<p>现实：多级缓存层次结构</p>
<ul>
<li>挑战：对一级缓存中的数据所做的更改可能对二级缓存控制器不可见，而只是监听互连。</li>
<li>监听如何在缓存层次结构中工作？<ul>
<li>所有缓存监听是否独立互连？（效率低下）</li>
<li>保持“包容”</li>
</ul>
</li>
</ul>
<p>缓存的包含性</p>
<ul>
<li>离处理器近的缓存中的所有行也位于离处理器较远的缓存中<ul>
<li>例如，L1的内容是L2内容的子集</li>
<li>因此，与L1相关的所有事务也与L2相关，因此仅L2监听互连就足够了</li>
</ul>
</li>
<li>若线路在L1中处于自有状态（MSI/MESI中为M），则在L2中也必须处于自有状态<ul>
<li>允许L2确定总线事务是否在L1中请求修改的cache line，而不需要L1提供信息</li>
</ul>
</li>
</ul>
<p>如果L2大于L1，是否自动保持包含？</p>
<ul>
<li>考虑这个例子：<ul>
<li>让二级缓存的大小是一级缓存的两倍</li>
<li>让L1和L2具有相同的行大小，是2路组相联，并使用LRU替换策略</li>
<li>让A、B、C映射到同一组L1缓存<br>有以下事务：</li>
</ul>
</li>
<li>处理器访问A（L1+L2未命中）</li>
<li>处理器访问B（L1+L2未命中）。</li>
<li>处理器多次访问A（所有L1命中）。</li>
<li>处理器现在访问C，触发L1和L2未命中。L1和L2可能会选择逐出不同的行，因为访问历史记录不同。</li>
<li>因此，包容不再适用！</li>
</ul>
<p>当二级缓存中的行X由于来自另一个缓存的BusRdX而无效时，还必须使L1中的X行无效。</p>
<ul>
<li>一种解决方案：每个L2行包含一个额外的状态位，指示L1中是否也存在该行</li>
<li>该位告诉L2的这个cache line失效，因为一致性通信需要传播到一级</li>
</ul>
<p><img src="/img/1640104101.jpg" alt=""></p>
<p>保持包含：L1写命中</p>
<ul>
<li>假设L1是回写缓存。处理器写入X行（L1写入命中）</li>
<li>二级缓存中的X行在一致性协议中处于修改状态，但它有过时的数据！</li>
<li>当一致性协议要求从二级刷新X时（例如，另一个处理器加载X），二级缓存必须从一级缓存请求数据。</li>
<li>为“修改但过时”添加另一位（刷新“修改但过时”L2行需要首先从L1获取真实数据。）</li>
</ul>
<p><img src="/img/1640104259.jpg" alt=""></p>
<p>实施一致性的硬件影响</p>
<ul>
<li>每个缓存必须侦听并响应互连广播的所有一致性通信，造成互连网络上的额外流量<ul>
<li>在扩展到更高的核心数时可能非常重要</li>
</ul>
</li>
<li>大多数现代多核CPU实现缓存一致性</li>
<li>迄今为止，多数多核GPU未实现缓存一致性<ul>
<li>到目前为止，对于图形和科学计算应用程序，一致性的开销被认为是不值得的（NVIDIA GPU提供单一共享L2+原子内存操作）</li>
<li>但最新的Intel集成GPU确实实现了缓存一致性</li>
</ul>
</li>
</ul>
<p>虚假共享问题，此代码的潜在性能问题是什么？<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// allocate per-thread variable for local per-thread accumulation </span></span><br><span class="line"><span class="type">int</span> myPerThreadCounter[NUM_THREADS];</span><br></pre></td></tr></table></figure></p>
<p>为什么这样更好？因为每个线程都可以把自己要读取的数据加载到一个cache line里。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// allocate per thread variable for local accumulation </span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">PerThreadState</span> &#123;</span> </span><br><span class="line">  <span class="type">int</span> myPerThreadCounter; </span><br><span class="line">  <span class="type">char</span> padding[CACHE_LINE_SIZE ‐ <span class="keyword">sizeof</span>(<span class="type">int</span>)]; </span><br><span class="line">&#125;; </span><br><span class="line">PerThreadState myPerThreadCounter[NUM_THREADS];</span><br></pre></td></tr></table></figure></p>
<p>虚假分享</p>
<ul>
<li>两个处理器写入不同地址，但地址映射到同一cache line的情况</li>
<li>写入处理器缓存之间的cache line摆动，由于一致性协议，产生大量通信</li>
<li>没有内在的通信，这完全是人为的通信</li>
<li>在为缓存一致性体系结构编程时，错误共享可能是一个因素</li>
</ul>
<p>概述：基于监听的一致性</p>
<ul>
<li>缓存一致性问题的存在是因为单个共享地址空间的抽象不是由单个存储单元实现的<ul>
<li>存储分布在主内存和本地处理器缓存之间</li>
<li>在本地缓存中复制数据以提高性能</li>
</ul>
</li>
<li>基于监听的缓存一致性的主要思想：每当发生可能影响一致性的缓存操作时，缓存控制器都会向所有其他缓存控制器广播通知<ul>
<li>硬件架构师面临的挑战：最小化一致性实现的开销</li>
<li>软件开发人员面临的挑战：由于一致性协议（例如，虚假共享），要警惕人为造成的通信</li>
</ul>
</li>
<li>监听实现的可扩展性受到向所有缓存广播一致性消息的能力的限制！<ul>
<li>下次：通过基于目录的方法扩展缓存一致性</li>
</ul>
</li>
</ul>
<h1 id="lecture-11"><a href="#lecture-11" class="headerlink" title="lecture 11"></a>lecture 11</h1><p>监听缓存一致性协议依赖于通过芯片互连向所有处理器广播一致性信息。每次发生缓存未命中时，触发未命中的缓存都会与所有其他缓存通信！我们讨论了传达了哪些信息以及采取了哪些行动来实施一致性协议。但是我们没有讨论如何在互连上实现广播。 （一个例子是使用共享总线进行互连）</p>
<p>问题：将缓存一致性扩展到大型机器</p>
<ul>
<li>调用非统一内存访问 (NUMA) 共享内存系统</li>
<li>想法：在处理器附近定位内存区域可提高可扩展性：它会产生更高的总带宽并减少延迟（尤其是在应用程序中存在局部性时）。但是……如果一致性协议也不能扩展，那么 NUMA 系统的效率就没有多大用处！</li>
<li>考虑这种情况：处理器访问附近的内存（好情况），但为了确保一致性仍然必须向所有其他处理器广播它正在这样做（坏事情）</li>
</ul>
<p>一些术语：</p>
<ul>
<li>cc-NUMA = “缓存一致的非统一内存访问”</li>
<li>分布式共享内存系统 (DSM)：缓存一致、共享地址空间，但架构由物理分布式内存实现</li>
</ul>
<p><img src="/img/1640162740.jpg" alt=""></p>
<p>一种可能的解决方案：分层监听。在每个级别使用监听一致性。另一个例子是：使用处理器组本地化内存，而不是集中式</p>
<ul>
<li>好处<ul>
<li>构建相对简单（由于多级缓存，已经必须处理类似问题）</li>
</ul>
</li>
<li>缺点<ul>
<li>网络的根节点可能成为瓶颈</li>
<li>比直接通信更大的延迟</li>
<li>不适用于更通用的网络拓扑（网格、立方体）</li>
</ul>
</li>
</ul>
<p><img src="/img/1640162808.jpg" alt=""></p>
<p>使用目录的可扩展缓存一致性</p>
<ul>
<li>基于监听的方案广播一致性消息以确定其他缓存中的行的状态</li>
<li>另一种想法：通过在一个地方存储有关线路状态的信息来避免广播：“目录”<ul>
<li>缓存行的目录条目包含有关所有缓存中缓存行状态的信息。</li>
<li>缓存根据需要从目录中查找信息</li>
<li>缓存一致性由缓存之间的点对点消息在“需要知道”的基础上维护（而不是通过广播机制）</li>
</ul>
</li>
</ul>
<p>分布式目录</p>
<ul>
<li>线路的“主节点”：具有保存线路相应数据的内存的节点<ul>
<li>例子：节点0是黄线的home节点，节点1是蓝线的home节点</li>
</ul>
</li>
<li>“请求节点”：包含处理器请求行的节点</li>
</ul>
<p><img src="/img/1640163285.jpg" alt=""></p>
<p>第一个例子是一个干净的缓存行读缺失。蓝线的处理器 0 从主内存中读取：cache line不脏。</p>
<ul>
<li>读未命中消息发送到请求cache line的主节点</li>
<li>主目录检查行的条目</li>
<li>如果缓存行的脏位为 OFF，则响应内存中的内容，将<code>Presence[0]</code>设置为 true（表示行被处理器 0 缓存）</li>
</ul>
<p><img src="/img/1640163929.jpg" alt=""></p>
<p>示例 2：读取未命中脏行</p>
<ul>
<li>处理器 0 从主内存中读取蓝cache line：缓存是脏的（P2 缓存中的内容）<ul>
<li>如果脏位为 ON，则数据必须来自另一个处理器（具有该行的最新副本）</li>
<li>主节点必须告诉请求节点在哪里可以找到数据</li>
<li>回复提供线路所有者身份的消息（“从 P2 获取”）</li>
</ul>
</li>
</ul>
<p><img src="/img/1640164303.jpg" alt=""></p>
<ol>
<li>如果脏位为 ON，则数据必须来自另一个处理器</li>
<li>这个cache line归属的节点响应提供cache line的owner身份的消息</li>
<li>请求节点向owner请求数据</li>
<li>Owner 将缓存中的状态更改为 SHARED（只读），响应请求节点</li>
<li>Owner也响应home节点，home清除dirty，更新presence bits，更新内存</li>
</ol>
<p><img src="/img/1640164900.jpg" alt=""></p>
<p>示例 3：写未命中</p>
<ul>
<li>由处理器 0 写入内存：行是干净的，但驻留在 P1 和 P2 的缓存中</li>
</ul>
<p>请求写缺失的缓存行<br><img src="/img/1640164968.jpg" alt=""></p>
<p>归属节点返回这个cache line的owner信息<br><img src="/img/1640165011.jpg" alt=""></p>
<p>请求节点发送cache line失效的消息<br><img src="/img/1640165048.jpg" alt=""></p>
<p>另两个处理器返回失效确认<br><img src="/img/1640165079.jpg" alt=""></p>
<p>目录优势</p>
<ul>
<li>在读取时，目录会告诉请求节点确切的位置<ul>
<li>来自主节点（如果cache line干净）</li>
<li>或者来自拥有节点（如果cache line脏）</li>
<li>无论哪种方式，检索数据都只涉及点对点通信</li>
</ul>
</li>
<li>在写入时，目录的优势取决于共享cache line的处理器数量<ul>
<li>在限制中，如果所有缓存都共享数据，则所有cache必须相互通信（就像在监听协议中广播）</li>
</ul>
</li>
</ul>
<p>一般而言，写入期间只有少数共享者</p>
<ul>
<li>访问模式<ul>
<li>“主要读取”对象：很多共享者但写入很少，因此对性能的影响最小（例如，Barnes-Hut 中的根节点）</li>
<li>迁移对象（一个处理器读/写一段时间，然后是另一个，等等）：很少的共享者，计数不随处理器数量而扩展</li>
<li>频繁读/写对象：频繁失效，但共享者数量很少，因为共享的数量不能在失效之间的短时间内建立（例如，共享任务队列）</li>
<li>低争用锁：不经常失效，没有性能问题</li>
<li>高争用锁：可能是一个挑战，因为当锁释放时会出现许多读者</li>
</ul>
</li>
<li>含义 1：目录可用于限制一致性流量<ul>
<li>不需要广播机制来“告诉所有人”</li>
</ul>
</li>
<li>含义 2：建议优化目录实现的方法（减少存储开销）</li>
</ul>
<p>非常简单的目录存储要求</p>
<ul>
<li>一个cache line内存</li>
<li>每个内存缓存行都有一个目录条目</li>
<li>P 存在位：指示处理器 P 的缓存中是否有行</li>
<li>脏位：指示处理器缓存之一中的行是脏的</li>
</ul>
<p><img src="/img/1640165411.jpg" alt=""></p>
<p>全位向量目录表示</p>
<ul>
<li>每个节点一个存在位</li>
<li>存储与 P x M 成正比<ul>
<li>P = 节点数（例如，处理器）</li>
<li>M = 内存中的行数</li>
</ul>
</li>
<li>存储开销随 P 增加<ul>
<li>假设 64 字节高速cache line大小（512 位）</li>
<li>64 个节点 (P=64) → 12% 开销</li>
<li>256 个节点 (P=256) → 50% 开销</li>
<li>1024 个节点 (P=1024) → 200% 开销</li>
</ul>
</li>
</ul>
<p>减少目录的存储开销</p>
<ul>
<li>全位向量方案的优化<ul>
<li>增加cache line大小（减少 M 项）</li>
<li>将多个处理器分组到一个目录“节点”中（减少 P 项）<ul>
<li>每个节点只需要一个目录位，每个处理器不需要一个位</li>
<li>分层：可以使用监听协议来保持节点中处理器之间的一致性，目录跨节点</li>
</ul>
</li>
</ul>
</li>
<li>我们现在将讨论两种替代方案<ul>
<li>有限的指针方案（减少 P）</li>
<li>稀疏目录</li>
</ul>
</li>
</ul>
<p>有限的指针方案</p>
<ul>
<li>由于预计数据一次只会出现在几个缓存中，因此每个目录条目存储有限数量的指针就足够了（只需要一个包含行的有效副本的节点列表！）</li>
<li>一个有着1024处理器的系统，全位向量方案每行需要 1024 位。作为优化，这1024位可以存储 100 个指向保存该行的节点的指针（每个指针log2(1024)=10 位）</li>
</ul>
<p>在有限的指针方案中管理溢出</p>
<ul>
<li>回退到广播（如果存在广播机制）<ul>
<li>当超过最大共享者数时，恢复广播</li>
</ul>
</li>
<li>如果机器上没有广播机制<ul>
<li>不允许超过最大数量的共享者</li>
<li>溢出时，最新的共享者替换现有的共享者（必须使旧共享者缓存中的行无效）</li>
</ul>
</li>
<li>向量回退<ul>
<li>恢复到位向量表示表示</li>
<li>每一位对应K个节点</li>
<li>在写入时，使所有节点无效</li>
</ul>
</li>
</ul>
<p>有限指针方案是巧妙理解和优化常见情况的一个很好的例子：</p>
<ol>
<li>工作负载驱动观察：一般情况下缓存行共享器的数量很少</li>
<li>使常见情况简单快速：前 N 个共享者的指针数组</li>
<li>不常见的情况仍然正确处理，只是使用了更慢，更复杂的机制（程序仍然有效！）</li>
<li>复杂解决方案的额外开销是可以容忍的，因为它很少发生</li>
</ol>
<p>限制目录大小：稀疏目录</p>
<ul>
<li>关键观察：大部分内存并不驻留在缓存中。并且为了执行一致性协议，系统只需要共享当前在缓存中的行的信息</li>
<li>大多数目录条目大部分时间都是空的</li>
<li>例如，1 MB 缓存，1 GB 内存，在单节点系统中，≥ 99.9% 的目录条目为空</li>
</ul>
<p>稀疏目录</p>
<ul>
<li>主节点的目录只维护指向一个节点缓存行的指针（不是共享者列表）</li>
<li>指向列表中下一个节点的指针作为额外信息存储在缓存行中（就像行的标签、脏位等）</li>
<li>保存在某个缓存中的内存的每个缓存行一个目录条目<ul>
<li>读取未命中：将请求节点添加到列表的头部</li>
<li>写入未命中：沿列表传播失效</li>
<li>关于缓存换出：需要修补链表（链表移除）</li>
</ul>
</li>
</ul>
<p><img src="/img/1640166452.jpg" alt=""></p>
<ul>
<li>好处：<ul>
<li>低内存存储开销（每行一个指向列表头的指针）</li>
<li>额外的目录存储与缓存大小成正比（存储在 SRAM 中的列表）</li>
<li>写入流量仍然与共享者数量成正比</li>
</ul>
</li>
<li>坏处：<ul>
<li>写入延迟与共享者数量成正比（行的无效是连续的）</li>
<li>更高的实现复杂度</li>
</ul>
</li>
</ul>
<p>干预转发</p>
<p>读缺失时向cache line的拥有者请求<br><img src="/img/1640167121.jpg" alt=""></p>
<p>拥有者向保有此cache line缓存的p2转发读请求。p2把数据和dir返回给拥有者<br><img src="/img/1640167162.jpg" alt=""></p>
<p>拥有者再把cache line发给请求者<br><img src="/img/1640167231.jpg" alt=""></p>
<p>原始的基于目录的协议一共五次总线事务，其中 4 个事务在“关键路径”上（事务 4 和 5 可以并行完成）。干预转发则总共四个总线事务（更少的流量）但所有四次事务都在“关键路径”上。</p>
<p>请求转发</p>
<p>读缺失时向cache line的拥有者请求<br><img src="/img/1640167369.jpg" alt=""></p>
<p>拥有者向保有此cache line缓存的p2转发读请求<br><img src="/img/1640167392.jpg" alt=""></p>
<p>p2把数据发给拥有者和请求者<br><img src="/img/1640167455.jpg" alt=""></p>
<p>请求转发一共四次总线事务，只有三个事务在关键路径上（事务 3 和 4 可以并行完成）。注意：系统不再是纯请求/响应（因为 P0 向 P1 发送请求，但从 P2 接收响应）</p>
<p>Intel Core i7 CPU 中的目录一致性</p>
<ul>
<li>L3 作为 L3 缓存中所有行的集中目录（注意包含属性的重要性……L2 中的任何行都会有一个目录条目）</li>
<li>目录维护包含行的 L2 缓存列表</li>
<li>不向所有 L2 广播一致性流量，只向包含该行的 L2 发送一致性消息（Core i7 互连是环，不是总线）</li>
<li>目录维度：<ul>
<li>P=4</li>
<li>M = L3 缓存行数</li>
</ul>
</li>
</ul>
<p><img src="/img/1640167654.jpg" alt=""></p>
<p>Xeon Phi</p>
<ul>
<li>芯片上的英特尔 NUMA</li>
<li>50+个 x86 内核<ul>
<li>4 路超线程</li>
<li>每个有 1–2 个向量单位</li>
</ul>
</li>
<li>缓存一致性内存系统</li>
<li>整体系统：<ul>
<li>最大 8GB内存</li>
<li>最大 2 TFLOPS</li>
<li>0.004 字节/flop</li>
<li>300 瓦</li>
</ul>
</li>
</ul>
<p>Xeon Phi围绕双向环发送的消息</p>
<ul>
<li>将所有内容都集成在单芯片上可实现非常广泛的通信路径</li>
<li>可以通过在整个环中循环消息来获得广播的效果</li>
<li>优于点对点</li>
</ul>
<p>Xeon Phi目录结构</p>
<ul>
<li>目录跟踪哪些线路驻留在本地 L2 中<ul>
<li>与单节点系统相同</li>
</ul>
</li>
<li>P 读取或写入的最坏情况内存：<ul>
<li>检查本地缓存</li>
<li>请求某些line时，围绕环循环请求</li>
<li>围绕环向内存控制器发送请求</li>
</ul>
</li>
</ul>
<h1 id="lecture-12"><a href="#lecture-12" class="headerlink" title="lecture 12"></a>lecture 12</h1><p>缓存的包含属性</p>
<ul>
<li>靠近处理器的缓存的所有行也位于离处理器更远的缓存中<ul>
<li>例如，L1 的内容是 L2 内容的子集</li>
<li>因此，所有与 L1 相关的事务也与 L2 相关，因此只有 L2 监听互连就足够了</li>
</ul>
</li>
<li>如果cache line在 L1 中处于拥有状态（MSI/MESI 中的 M），则它在 L2 中也必须处于拥有状态<ul>
<li>允许 L2 确定总线事务是否正在请求 L1 中修改的缓存行，而无需来自 L1 的信息</li>
</ul>
</li>
</ul>
<p>维护包含关系：处理失效</p>
<ul>
<li>当 L2 缓存中的 X 行由于来自另一个缓存的 BusRdX 无效时。 还必须使 L1 Invalidate 中的 X 行无效</li>
<li>一种解决方案：每个 L2 行包含一个额外的状态位，指示 L1 中是否也存在该行</li>
<li>该位告诉由于一致性流量需要将缓存行的 L2 失效需要传播到 L1。</li>
</ul>
<p><img src="/img/1640173839.jpg" alt=""></p>
<p>保持包含性：L1 写入命中</p>
<ul>
<li>假设 L1 是写回缓存。 处理器写入 X 行（L1 写入命中）</li>
<li>L2 缓存中的 X 行在一致性协议中处于修改状态，但它有陈旧的数据！</li>
<li>当一致性协议要求从 L2 刷新 X（例如，另一个处理器加载 X）时，L2 缓存必须从 L1 请求数据。</li>
<li>因为“已修改但已过时”，所以要添加额外的一位（刷新“已修改但已过时”的 L2 行需要首先从 L1 获取真实数据。）</li>
</ul>
<p><img src="/img/1640174084.jpg" alt=""></p>
<h2 id="死锁活锁和饥饿"><a href="#死锁活锁和饥饿" class="headerlink" title="死锁活锁和饥饿"></a>死锁活锁和饥饿</h2><p>死锁的必要条件</p>
<ol>
<li>互斥：一个处理器可以同时持有一个给定的资源</li>
<li>保持并等待：处理器必须保持资源，同时等待完成操作所需的其他资源</li>
<li>无抢占：处理器在他们希望执行的操作完成之前不会放弃资源</li>
<li>循环等待：等待的处理器相互依赖（资源依赖图中存在循环）</li>
</ol>
<p>活锁是一种状态，系统正在执行许多操作，但没有线程正在取得有意义的进展。计算机系统示例：操作不断中止并重试。</p>
<p>饥饿是一种系统正在取得整体进展，但某些进程没有进展的状态。饥饿通常不是永久状态。</p>
<h2 id="监听的基本实现（假设是原子总线）"><a href="#监听的基本实现（假设是原子总线）" class="headerlink" title="监听的基本实现（假设是原子总线）"></a>监听的基本实现（假设是原子总线）</h2><p>考虑一个基本的系统设计</p>
<ul>
<li>每个处理器一个未完成的内存请求</li>
<li>每个处理器的单级写回缓存</li>
<li>缓存可以在处理器执行一致性操作时停止处理器</li>
<li>系统互连是一个原子共享总线（一次一个缓存通信）</li>
</ul>
<p>原子总线上的事务</p>
<ol>
<li>客户端被授予总线访问权（仲裁结果）</li>
<li>客户端在总线上放置命令（也可以在总线上放置数据）</li>
<li>总线上另一个总线客户端对命令的响应</li>
<li>下一个客户端获得总线访问权（仲裁）</li>
</ol>
<p>单处理器上的缓存未命中逻辑</p>
<ol>
<li>确定缓存集（使用适当的地址位）</li>
<li>检查缓存标签（以确定行是否在缓存中）</li>
<li>断言访问总线的请求</li>
<li>等待总线授权（由总线仲裁员决定）</li>
<li>在总线上发送地址+命令</li>
<li>等待命令被接受</li>
<li>接收总线上的数据</li>
</ol>
<p>原子总线在多处理器场景中意味着什么？</p>
<ul>
<li>BusRd、BusRdX：在发出地址和接收数据之间不允许其他总线事务</li>
<li>Flush：地址和数据同时发送，在允许任何其他事务之前由内存接收</li>
</ul>
<p>多处理器缓存控制器行为的挑战：来自处理器和总线的请求都需要标签查找</p>
<ul>
<li>如果总线获得优先权：在总线事务期间，处理器被锁定在它自己的缓存之外。</li>
<li>如果处理器获得优先权：在处理器缓存访问期间，缓存无法响应监听结果（因此即使不存在任何形式的共享，也会延迟其他处理器）</li>
</ul>
<p>缓解争用：允许处理器端和监听控制器同时访问</p>
<ul>
<li>选项 1：缓存重复标签</li>
<li>选项 2：多端口标签存储器</li>
<li>注意：标签必须保持同步以确保正确性，因此一个控制器的标签更新仍然需要阻止另一个控制器（但与检查标签相比，修改标签并不常见）</li>
</ul>
<p><img src="/img/1640175249.jpg" alt=""></p>
<p>报告监听结果：红色的线是额外的总线硬件，是所有的处理器的“或”结果。</p>
<p><img src="/img/1640176806.jpg" alt=""></p>
<p>何时报告监听结果？</p>
<ul>
<li>内存控制器可以立即开始访问 DRAM，但如果来自另一个缓存的监听结果表明它有最新数据的副本，则不会响应<ul>
<li>缓存应该提供数据，而不是内存</li>
</ul>
</li>
<li>内存可以假设其中一个缓存将为请求提供服务，直到监听结果有效（如果监听指示没有缓存有数据，则内存必须响应）</li>
</ul>
<p>处理回写</p>
<ul>
<li>回写涉及两个总线事务<ul>
<li>传入线路（处理器请求的线路）</li>
<li>输出行（缓存中被驱逐的脏行，必须刷新）</li>
</ul>
</li>
<li>理想情况下希望处理器尽快继续运行（它不应该等待刷新完成）</li>
<li>解决方案：回写缓冲区<ul>
<li>要在回写缓冲区中放被刷新的cache line</li>
<li>立即加载请求的行（允许处理器继续）</li>
<li>稍后刷新回写缓冲区的内容</li>
</ul>
</li>
</ul>
<p>带有回写缓冲区的缓存</p>
<ul>
<li>如果总线上出现对回写缓冲区中数据地址的请求怎么办？</li>
<li>除了缓存标签之外，监听控制器还必须检查回写缓冲区地址。</li>
<li>如果有回写缓冲区匹配：<ul>
<li>响应来自写回缓冲区而不是缓存的数据</li>
<li>取消未完成的总线访问请求（用于回写）</li>
</ul>
</li>
</ul>
<p>取回死锁</p>
<ul>
<li>P1 有缓存行 B 的修改副本</li>
<li>P1 正在等待总线，因此它可以在缓存线 A 上发出 BusRdX</li>
<li>当 P1 正在等待时，B 的 BusRd 出现在总线上</li>
<li>为避免死锁，P1 必须能够在等待发出请求时为到来的事务提供服务</li>
</ul>
<p>活锁</p>
<ul>
<li>两个处理器写入缓存线 B</li>
<li>P1 获取总线，发出 BusRdX</li>
<li>P2 失效</li>
<li>在 P1 执行缓存行更新之前，P2 获取总线，发出 BusRdX</li>
<li>P1 无效</li>
<li>为了避免livelock，必须允许获得独占所有权的写入在独占所有权放弃之前完成。</li>
</ul>
<p>自检：何时写入“提交”</p>
<ul>
<li>当读独占事务出现在总线上并被所有其他缓存确认时，写操作提交<ul>
<li>此时，写入已“提交”</li>
<li>将来的所有读取都将反映此写操作的值（即使来自P的数据尚未写入P的脏缓存线或内存）</li>
<li>关键思想：总线上的事务顺序定义并行程序中全局写入顺序（写入序列化）</li>
</ul>
</li>
</ul>
<p>饥饿</p>
<ul>
<li>多处理器竞争总线接入<ul>
<li>必须小心避免（或尽量减少）饥饿</li>
<li>例如，如果具有“最低id”的处理器获胜怎么办。</li>
</ul>
</li>
<li>实现更大公平性的示例政策：<ul>
<li>先进先出仲裁</li>
<li>基于优先级的启发式（频繁的总线用户优先级下降）</li>
</ul>
</li>
</ul>
<p>前半部分总结：一致性实现中的并行性和并发性是复杂性的来源</p>
<ul>
<li>处理器、缓存和总线都是并行运行的资源<ul>
<li>经常争夺共享资源：</li>
<li>处理器和总线争夺缓存</li>
<li>缓存争用总线访问</li>
</ul>
</li>
<li>体系结构将“内存操作”抽象为原子操作（例如，加载、存储），通过涉及所有这些硬件组件的多事务来实现</li>
<li>性能优化通常需要将操作拆分为几个较小的事务<ul>
<li>将工作拆分为更小的事务显示出更多的并行性</li>
<li>开销：需要更多的硬件来利用额外的并行性</li>
<li>开销：需要注意确保抽象仍然有效（机器是正确的）</li>
</ul>
</li>
</ul>
<h2 id="围绕非原子总线事务构建系统"><a href="#围绕非原子总线事务构建系统" class="headerlink" title="围绕非原子总线事务构建系统"></a>围绕非原子总线事务构建系统</h2><p>分割事务的总线</p>
<p>总线事务分为两个事务：</p>
<ol>
<li>请求</li>
<li>回应</li>
</ol>
<p>基本设计</p>
<ul>
<li>一次最多八个未完成的请求（全系统）</li>
<li>响应的顺序不必与请求的顺序相同<ul>
<li>但是请求顺序确定了系统的总顺序</li>
</ul>
</li>
<li>通过否定确认（NACKs）进行流量控制<ul>
<li>当缓冲区已满时，客户端可以NACK事务，从而导致重试</li>
</ul>
</li>
</ul>
<p>发起请求：可以将分割事务总线看作两个独立的总线：请求总线和响应总线。</p>
<ul>
<li>请求总线：cmd+地址</li>
<li><p>响应总线：数据</p>
</li>
<li><p>步骤1：请求者请求总线访问</p>
</li>
<li>步骤2：总线仲裁器授予访问权，为事务分配一个标记</li>
<li>步骤3：请求者在请求总线上放置命令+地址</li>
</ul>
<p>读取未命中：逐周期总线行为：</p>
<ul>
<li>addr req/请求仲裁：高速缓存控制器向总线提供地址请求（许多高速缓存可能在同一周期内执行此操作）</li>
<li>grant/请求解析：地址总线仲裁器为一个请求者授权，为请求分配一个请求表条目</li>
<li>addr/总线“获胜者”将命令/地址放置在总线上</li>
<li>dcd/缓存执行监听：查找标记、更新缓存状态等。内存操作在此提交！（没有总线）</li>
<li>addr ack/缓存确认此监听结果已准备就绪（或在此发出无法及时完成监听的信号</li>
<li>data req/数据响应仲裁：响应者表示打算用标记T响应请求（许多缓存或内存可能在同一个周期内这样做）</li>
<li>grant/数据总线仲裁器授予一个响应器总线访问权限</li>
<li>tag check/原始请求者表示准备接收响应（或缺少响应：请求者此时可能很忙）</li>
<li>响应程序将响应数据放置在数据总线上</li>
<li>缓存为请求提供带有数据的监听结果</li>
<li>请求表项被释放</li>
<li>这里：假设128字节缓存线→256位总线上的4个周期</li>
</ul>
<p><img src="/img/1640178658.jpg" alt=""></p>
<p><img src="/img/1640178802.jpg" alt=""></p>
<p>为什么在并行系统中有队列？</p>
<ul>
<li>答：适应可变（不可预测）的生产和消费率。</li>
<li>只要A和B平均以相同的速度生产和消费，两个工人就可以全速运转。</li>
<li>无队列：注意A暂停等待B接受新输入（B有时暂停等待A产生新输入）。</li>
</ul>
<p><img src="/img/1640178886.jpg" alt=""></p>
<p>多级缓存层次结构:</p>
<ul>
<li>假设每个处理器有一个未完成的内存请求。</li>
<li>考虑获取死锁问题：Cache必须能够在等待响应自身请求时服务请求（层次结构增加响应延迟）</li>
<li>调整所有缓冲区的大小以适应总线上最大数量的未完成请求是避免死锁的一种解决方案。</li>
</ul>
<p><img src="/img/1640179330.jpg" alt=""></p>
<p>因为队列满导致的死锁：</p>
<ul>
<li>传出读取请求（由处理器启动）</li>
<li>传入读取请求（由于另一个缓存）</li>
<li>这两个请求生成的响应都需要另一个队列中的空间（循环依赖）</li>
</ul>
<p><img src="/img/1640179399.jpg" alt=""></p>
<p>使用单独的请求/响应队列避免缓冲区死锁</p>
<ul>
<li>系统将所有事务分类为请求或响应</li>
<li>响应可以在不生成进一步事务的情况下完成！</li>
<li>请求会增加队列长度</li>
<li>但是响应减少了队列长度</li>
<li>在尝试发送请求时，缓存必须能够为响应提供服务。</li>
<li>响应将取得进展（它们不会生成新的工作，因此不存在循环依赖），最终为请求释放资源</li>
</ul>
<p><img src="/img/1640179544.jpg" alt=""></p>
<h1 id="lecture-13"><a href="#lecture-13" class="headerlink" title="lecture 13"></a>lecture 13</h1><p>内存coherence与内存consistency</p>
<ul>
<li>内存coherence定义了对同一内存位置的读取和写入行为的观察要求<ul>
<li>所有处理器必须就读/写 X 的顺序达成一致</li>
<li>换句话说：可以将涉及 X 的操作放在时间线上，以便所有处理器的观察结果与该时间线一致</li>
</ul>
</li>
<li>内存consistency定义了对不同位置的读写行为（其他处理器观察到的）<ul>
<li>Coherence 仅保证对地址 X 的写入最终会传播到其他处理器</li>
<li>Consistency处理何时写入 X 传播到其他处理器，相对于读取和写入其他地址</li>
</ul>
</li>
</ul>
<p>Coherence vs. consistency</p>
<ul>
<li>Coherence的目标是确保并行计算机中的内存系统表现得好像缓存不存在一样<ul>
<li>就像单处理器系统中的内存系统表现得好像缓存不存在一样</li>
</ul>
</li>
<li>没有缓存的系统不需要缓存Coherence<ul>
<li>Consistency定义了对并行系统中不同地址的加载和存储的允许行为</li>
<li>无论是否存在缓存，都应该指定内存的允许行为（这就是内存一致性模型所做的）</li>
</ul>
</li>
</ul>
<p>内存操作排序</p>
<ul>
<li>程序定义了加载和存储的序列（这是加载和存储的“程序顺序”）</li>
<li>四种内存操作顺序<ul>
<li>W→R：写入 X 必须在随后从 Y 读取之前提交</li>
<li>R→R：从 X 读取必须在随后从 Y 读取之前提交</li>
<li>R→W：读取到 X 必须在随后写入 Y 之前提交</li>
<li>W→W：写入 X 必须在后续写入 Y 之前提交</li>
</ul>
</li>
<li>顺序一致的内存系统维护所有四种内存操作顺序</li>
</ul>
<p>顺序一致性</p>
<ul>
<li>一个并行系统是顺序一致的，如果任何一个并行执行的结果是相同的，就好像所有的内存操作都是按照某种顺序执行的，并且任何一个处理器的内存操作都是按照程序顺序执行的。</li>
<li>存在与观察值一致的所有内存操作的序列表</li>
</ul>
<p>快速示例<br>线程 1（在 P1 上）<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = 1;</span><br><span class="line">if (B == 0)</span><br><span class="line">    print(&quot;hello&quot;);</span><br></pre></td></tr></table></figure></p>
<p>线程 2（在 P2 上）<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">B = 1;</span><br><span class="line">if (A == 0)</span><br><span class="line">    print(&quot;world&quot;);</span><br></pre></td></tr></table></figure></p>
<p>假设 A 和 B 被初始化为 0。想象一下线程 1 和 2 同时运行，在双处理器系统上，会打印什么？</p>
<p>答案：假设写入立即传播（例如，直到 P2 观察到对 A 的写入，P1才会继续“if”语句），然后代码将打印“hello”或“world”，但不是两者兼而有之。</p>
<p>放宽对内存操作顺序的限制</p>
<ul>
<li>顺序一致的内存系统维护所有四种内存操作顺序（W→R、R→R、R→W、W→W）</li>
<li>宽松的内存一致性模型允许违反某些顺序</li>
</ul>
<p>放宽一致性的动机：隐藏延迟</p>
<ul>
<li>为什么我们对放宽顺序的要求感兴趣？<ul>
<li>获得性能的提升</li>
<li>具体来说，隐藏内存延迟：当它们独立时，内存访问操作与其他操作重叠</li>
<li>请记住，缓存一致性系统中的内存访问可能需要比简单地从内存中读取位（查找数据、发送无效等）更多的工作，当然了，需要同步操作、加锁等。</li>
</ul>
</li>
</ul>
<p>允许读取先于写入</p>
<ul>
<li>四种内存操作顺序<ul>
<li>W→R：写入必须在后续读取之前完成（划掉了，可能不是必要）</li>
<li>R→R：读取必须在后续读取之前完成</li>
<li>R→W：读取必须在后续写入之前完成</li>
<li>W→W：写入必须在后续写入之前完成</li>
</ul>
</li>
<li>允许处理器隐藏写入延迟<ul>
<li>Total Store Ordering (TSO)</li>
<li>Processor Consistency (PC)</li>
</ul>
</li>
</ul>
<p><img src="/img/1640267417.jpg" alt=""></p>
<p>写缓冲示例</p>
<ul>
<li>写入缓冲是一种常见的处理器优化，它允许读取在先前的写入之前进行<ul>
<li>当 store 被发出时，处理器缓冲区存储在写缓冲区中（假设 store 是地址 X）</li>
<li>处理器立即开始执行后续load，前提是它们没有访问地址 X（在程序中利用 ILP）</li>
<li>也可以进一步写入“写入缓冲区”（写入缓冲区是按顺序处理的，没有W→W重新排序）</li>
</ul>
</li>
<li>写缓冲放宽了 W→R 排序</li>
</ul>
<ul>
<li>不要将写缓冲区（此处显示）与缓存的回写缓冲区混淆。两个缓冲区的存在都是为了隐藏内存操作的延迟。但是，写入缓冲区保存了处理器已发出但尚未在系统中提交的写入。回写缓冲区包含必须刷新到内存的脏缓存行，以便内存保持最新。这些行很脏，因为很久以前处理器完成了对它们的一些写入。</li>
</ul>
<p><img src="/img/1640267736.jpg" alt=""></p>
<p>允许读取先于写入</p>
<ul>
<li>Total store ordering (TSO)<ul>
<li>处理器 P 可以在对 A 的写入被所有处理器看到之前读取 B（处理器可以将自己的读取移动到自己的写入之前）</li>
<li>在所有处理器都观察到对 A 的写入之前，其他处理器的读取无法返回 A 的新值</li>
</ul>
</li>
<li>Processor consistency (PC)<ul>
<li>任何处理器都可以在所有处理器观察到写入之前读取 A 的新值</li>
</ul>
</li>
<li>在TSO 和PC 中，只有W→R 顺序是放宽的。 W→W 约束仍然存在。同一线程的写入不会重新排序（它们按程序顺序发生）</li>
</ul>
<p><img src="/img/1640268240.jpg" alt=""></p>
<p>澄清</p>
<ul>
<li>缓存一致性问题的存在是因为优化了在多个处理器缓存中复制数据。 数据的副本必须保持一致。</li>
<li>宽松的内存一致性问题源于对内存重新排序操作的优化。（一致性与系统中是否有缓存无关。）</li>
</ul>
<p>允许重新排序写入</p>
<ul>
<li>四种内存操作顺序<ul>
<li>W→R：写入必须在后续读取之前完成（已被消除）</li>
<li>R→R：读取必须在后续读取之前完成</li>
<li>R→W：读取必须在后续写入之前完成</li>
<li>W→W：写入必须在后续写入之前完成（当前要解决的）</li>
</ul>
</li>
<li>Partial Store Ordering (PSO)<ul>
<li>执行可能与程序 1 上的顺序一致性不匹配（P2 可能在观察到 A 的更改之前观察到标志的更改）</li>
</ul>
</li>
</ul>
<p>P1<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A = 1;</span><br><span class="line">flag = 1;</span><br></pre></td></tr></table></figure></p>
<p>P2<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">while (flag == 0); </span><br><span class="line">print A;</span><br></pre></td></tr></table></figure></p>
<p>为什么允许更激进的内存操作重新排序会很有用？</p>
<ul>
<li>W→W：处理器可能会对写缓冲区中的写操作重新排序（例如，一个是缓存未命中，另一个是命中）</li>
<li>R→W，R→R：处理器可能会对指令流中的独立指令重新排序（乱序执行）</li>
<li>请记住，如果程序由单个指令流组成，这些都是有效的优化</li>
</ul>
<p>允许所有重新排序</p>
<ul>
<li>四种内存操作顺序<ul>
<li>W→R：写入必须在后续读取之前完成（已消除）</li>
<li>R→R：读取必须在后续读取之前完成（当前要解决的）</li>
<li>R→W：读取必须在后续写入之前完成（当前要解决的）</li>
<li>W→W：写入必须在后续写入之前完成（已消除）</li>
</ul>
</li>
<li>示例：<ul>
<li>Weak ordering（WO）</li>
<li>Release Consistency（RC）<ul>
<li>处理器支持特殊的同步操作</li>
<li>内存barrier指令之前的内存访问必须在barrier发出之前完成</li>
<li>在barrier指令完成之前，barrier后的内存访问不能开始</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">reorderable reads </span><br><span class="line">and writes here </span><br><span class="line">... </span><br><span class="line">MEMORY FENCE </span><br><span class="line">... </span><br><span class="line">reorderable reads </span><br><span class="line">and writes here </span><br><span class="line">... </span><br><span class="line">MEMORY FENCE</span><br></pre></td></tr></table></figure>
<p>示例：在宽松模型中表达同步</p>
<ul>
<li>Intel x86/x64 ~ total store ordering</li>
<li>如果软件需要一致性模型无法保证的特定指令顺序，则提供同步指令<ul>
<li>mm_lfence (“load fence”: wait for all loads to complete)</li>
<li>mm_sfence (“store fence”: wait for all stores to complete)</li>
<li>mm_mfence (“mem fence”: wait for all me operations to complete)</li>
</ul>
</li>
</ul>
<p>获取/释放语义</p>
<ul>
<li>具有获取语义的操作 X：防止程序顺序中在X之后的任何加载/存储与 X 重新排序<ul>
<li>其他处理器在所有后续操作的效果之前看到 X 的效果</li>
<li>示例：获取锁必须具有获取语义</li>
</ul>
</li>
<li>具有释放语义的操作 X：防止程序顺序中在X之前的任何加载/存储与 X 重新排序<ul>
<li>其他处理器在看到 X 的效果之前看到所有先前操作的效果。</li>
<li>示例：释放锁必须具有释放语义</li>
</ul>
</li>
</ul>
<p><img src="/img/1640268841.jpg" alt=""></p>
<p>C++11的<code>atomic&lt;T&gt;</code>操作</p>
<ul>
<li>提供整个对象的原子读、写、读-修改-写<ul>
<li>原子性可以由互斥体实现或由处理器支持的原子指令有效地实现（如果 T 是基本类型）</li>
</ul>
</li>
<li>为原子操作前后的操作提供内存排序语义<ul>
<li>默认：顺序一致性</li>
<li>见<code>std::memory_order</code>或更多细节</li>
</ul>
</li>
</ul>
<p>冲突的数据访问</p>
<ul>
<li>不同处理器的两次内存访问发生冲突，如果……<ul>
<li>他们访问相同的内存位置</li>
<li>至少一个是写</li>
</ul>
</li>
<li>不同步的程序<ul>
<li>未按同步排序的冲突访问（例如，栅栏、具有释放/获取语义的操作、屏障等）</li>
<li>不同步的程序包含数据竞争：程序输出取决于处理器的相对速度（非确定性程序结果）</li>
</ul>
</li>
</ul>
<p>同步程序</p>
<ul>
<li>同步程序无数据竞争</li>
<li>在实践中，你遇到的大多数程序都会通过同步库中实现的锁、屏障等被同步</li>
<li>而不是像前面的“四个示例程序”幻灯片那样通过临时读/写共享变量</li>
</ul>
<p>总结：宽松的一致性</p>
<ul>
<li>动机：通过允许记录内存操作来获得更高的性能（顺序一致性不允许重新排序）</li>
<li>一个开销是软件复杂性：程序员或编译器必须正确插入同步以确保在需要时某些特定的操作顺序<ul>
<li>但在实践中，复杂性封装在库中，提供直观的原语，如锁定/解锁、屏障（或较低级别的原语，如围栏）</li>
<li>针对常见情况进行优化：大多数内存访问都没有冲突</li>
</ul>
</li>
<li>宽松一致性模型的不同之处在于它们忽略的内存排序约束</li>
</ul>
<p>分布式系统中的最终一致性</p>
<ul>
<li>宽松的内存一致性将是在分布式环境中编写 Web 级程序的关键因素</li>
<li>“最终一致性”<ul>
<li>假设机器 A 写入共享分布式数据库中的对象 X</li>
<li>存在许多数据库副本用于性能扩展和冗余</li>
<li>最终一致性保证，如果 X 没有其他更新，系统中的所有其他节点最终都会观察到 A 的更新（注意：不保证何时，因此对对象 X 和 Y 的更新可能会以不同的方式传播到不同的客户端）</li>
</ul>
</li>
</ul>
<h1 id="lecture-14"><a href="#lecture-14" class="headerlink" title="lecture 14"></a>lecture 14</h1><p>可扩展性的网站的基础知识</p>
<p>在为多核 Web 服务器设置 进程数N 值时，您会考虑哪些因素？</p>
<ul>
<li>Parallelism：使用服务器的所有内核</li>
<li>延迟隐藏：隐藏长时间延迟的磁盘读取操作（通过工作进程之间的上下文切换）</li>
<li>并发：许多未完成的请求，想要在处理长请求的同时为快速请求提供服务（例如，大文件传输不应阻止服务 index.html）</li>
<li>占用空间：不要太多线程，以免所有线程的gather工作集导致抖动</li>
</ul>
<p>为什么将服务器划分为进程，而不是线程？</p>
<ul>
<li>保护<ul>
<li>不希望一个工作进程的崩溃导致整个网络服务器瘫痪</li>
<li>经常想在服务器操作中使用非线程安全库（例如第三方库）</li>
</ul>
</li>
<li>父进程可以定期回收子线程（对内存泄漏的鲁棒性）</li>
<li>当然，也存在多线程 Web 服务器解决方案（例如，Apache 的“worker”模块）</li>
</ul>
<p>“横向扩展”以增加吞吐量：使用多个 Web 服务器来满足站点的吞吐量目标。负载均衡器维护可用 Web 服务器的列表以及每个服务器的负载估计。将请求分发到 Web 服务器池。与会话相关的所有请求都被定向到同一服务器（又名会话亲缘关系，“粘性会话”）<br><img src="/img/1640269398.jpg" alt=""></p>
<p>站点配置</p>
<ul>
<li>站点性能监视器检测到高负载<ul>
<li>实例化新的 Web 服务器实例</li>
<li>通知负载平衡器有关新服务器的存在</li>
</ul>
</li>
<li>站点性能监视器检测到低负载<ul>
<li>卸载额外的服务器实例（以节省运营成本）</li>
<li>通知负载平衡器有关服务器卸载的信息</li>
</ul>
</li>
</ul>
<p><img src="/img/1640269521.jpg" alt=""></p>
<p>在处理请求时可能有很多重复步骤：</p>
<ul>
<li>与数据库沟通</li>
<li>执行查询</li>
<li>将数据库结果转化为脚本语言的对象模型</li>
<li>生成页面</li>
</ul>
<p>请记住，DB 可能难以扩展！所以要降低DB的负载，解决方案就是缓存。</p>
<ul>
<li>缓存经常访问的对象<ul>
<li>示例：memcached，内存键值存储（例如，大哈希表）</li>
<li>减少数据库负载（更少的查询）</li>
<li>减少网络服务器负载：</li>
<li>减少数据库响应和脚本环境之间的数据混洗</li>
<li>存储常见处理的中间结果</li>
</ul>
</li>
</ul>
<p><img src="/img/1640269774.jpg" alt=""></p>
<ul>
<li>当然，在存在写入的情况下保持缓存与数据库中的数据同步是很复杂的<ul>
<li>必须使缓存无效</li>
<li>非常简单的“第一步”解决方案：只缓存只读对象</li>
<li>更现实的解决方案提供了一定程度的一致性</li>
</ul>
</li>
</ul>
<p>CDN缓存示例图<br><img src="/img/1640269837.jpg" alt=""></p>
<h1 id="lecture-15"><a href="#lecture-15" class="headerlink" title="lecture 15"></a>lecture 15</h1><p>互连网络的用途是什么？连接！</p>
<ul>
<li>处理器内核与其他内核</li>
<li>处理器和内存</li>
<li>处理器核心和缓存</li>
<li>缓存和缓存</li>
<li>输入/输出设备</li>
</ul>
<p>为什么互连网络的设计很重要？</p>
<ul>
<li>系统可扩展性<ul>
<li>可以构建多大的系统？</li>
<li>添加更多节点（例如核心）有多容易</li>
</ul>
</li>
<li>系统性能和能源效率<ul>
<li>核心、缓存、内存的通信速度有多快</li>
<li>内存延迟有多长？</li>
<li>通信花费了多少能量？</li>
</ul>
</li>
</ul>
<p>设计问题</p>
<ul>
<li>拓扑：交换机如何通过链路连接<ul>
<li>影响路由、吞吐量、延迟、复杂性/实施成本</li>
</ul>
</li>
<li>路由：消息如何在网络中从其源头到达其目的地<ul>
<li>可以是静态的（消息采用预定路径）或基于负载自适应</li>
</ul>
</li>
<li>缓冲和流量控制<ul>
<li>网络中存储了哪些数据？ 数据包，部分数据包？ 等等。</li>
<li>网络如何管理缓冲区空间？</li>
</ul>
</li>
</ul>
<p>互连拓扑的属性</p>
<ul>
<li>路由距离<ul>
<li>沿两个节点之间路由的链接数（“跳数”）</li>
</ul>
</li>
<li>直径：最大路由距离</li>
<li>平均距离：所有有效路由的平均路由距离</li>
<li>直接与间接网络<ul>
<li>直接网络：端点位于网络“内部”</li>
<li>例如，mesh 是直接网络：每个节点既是端点又是交换机</li>
</ul>
</li>
<li>对分带宽：<ul>
<li>递归拓扑的通用性能指标</li>
<li>将网络一分为二，所有被切断链路的总带宽</li>
<li>警告：可能会产生误导，因为它没有考虑交换和路由效率</li>
</ul>
</li>
<li>阻塞与非阻塞：<ul>
<li>如果可以连接任何配对节点，则网络是非阻塞的（否则，它是阻塞的）</li>
</ul>
</li>
</ul>
<p><img src="/img/1640270829.jpg" alt=""></p>
<p>示例：阻塞与非阻塞</p>
<ul>
<li>此网络是阻塞的还是非阻塞的？<ul>
<li>考虑从 0 到 1 和 3 到 7 的同步消息。</li>
<li>考虑从 1 到 6 和 3 到 7 的同步消息。 屏蔽！！！</li>
</ul>
</li>
</ul>
<p><img src="/img/1640270958.jpg" alt=""></p>
<p>网络的负载延迟行为由以下几部分组成</p>
<ul>
<li>零负载或空闲延迟（拓扑+路由+流量控制）</li>
<li>路由算法给出的最小延迟</li>
<li>拓扑给出的最小延迟</li>
<li>饱和吞吐量（由流量控制给出）</li>
<li>路由给出的吞吐量</li>
<li>拓扑给出的吞吐量</li>
</ul>
<p><img src="/img/1640271045.jpg" alt=""></p>
<p>总线互连</p>
<ul>
<li>好：<ul>
<li>简单的设计</li>
<li>对少量节点具有成本效益</li>
<li>易于实现一致性（通过监听）</li>
</ul>
</li>
<li>差：<ul>
<li>争用：所有节点争用共享总线</li>
<li>有限的带宽：所有节点通过相同的线路（一个一次沟通）</li>
<li>高电气负载 = 低频率、高功率</li>
</ul>
</li>
</ul>
<p><img src="/img/1640271195.jpg" alt=""></p>
<p>交叉互连</p>
<ul>
<li>每个节点都连接到每个其他节点（非阻塞、间接）</li>
<li>好：<ul>
<li>O(1) 延迟和高带宽</li>
</ul>
</li>
<li>差：<ul>
<li>不可扩展：O(N^2) 个开关</li>
<li>成本高</li>
<li>难以大规模仲裁</li>
</ul>
</li>
</ul>
<p><img src="/img/1640271246.jpg" alt=""></p>
<p><img src="/img/1640271268.jpg" alt=""></p>
<p>环状</p>
<ul>
<li>好：<ul>
<li>简单的</li>
<li>便宜：O(N) 成本</li>
</ul>
</li>
<li>差：<ul>
<li>高延迟：O(N)</li>
<li>添加节点后二分带宽保持不变（可扩展性问题）</li>
</ul>
</li>
<li>用于最近的 Intel 架构<ul>
<li>酷睿 i7<ul>
<li>四环<ul>
<li>请求</li>
<li>监听</li>
<li>确认</li>
<li>数据（32 字节）</li>
</ul>
</li>
<li>六个互连节点：L3 缓存的四个“切片”+系统代理+图形</li>
<li>每组 L3 连接到环形总线两次</li>
<li>3.4 GHz 时从内核到 L3 的理论峰值带宽约为 435 GB/秒</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/img/1640271351.jpg" alt=""></p>
<p>网</p>
<ul>
<li>直接网络</li>
<li>在基于网格的应用程序中呼应局部性</li>
<li>O(N) 成本</li>
<li>平均延迟：O(sqrt(N))</li>
<li>易于在芯片上布局：固定长度的链接</li>
<li>路径多样性：消息从一个节点传播到另一个节点的多种方式</li>
<li>使用者：<ul>
<li>Tilera 处理器</li>
<li>原型英特尔芯片</li>
</ul>
</li>
</ul>
<p><img src="/img/1640271455.jpg" alt=""></p>
<p>圆环</p>
<ul>
<li>网状拓扑的特性根据节点是靠近网络边缘还是中间而有所不同（环面拓扑引入了新的链路来避免这个问题）</li>
<li>仍然是 O(N) 成本，但成本高于 2D 网格</li>
<li>比网格更高的路径多样性和二分带宽</li>
<li>更高的复杂性<ul>
<li>难以在芯片上进行布局</li>
<li>不等的链接长度</li>
</ul>
</li>
</ul>
<p><img src="/img/1640271502.jpg" alt=""></p>
<p>树型</p>
<ul>
<li>平面、分层拓扑</li>
<li>像mesh/torus，当流量具有局部性时很好</li>
<li>延迟：O(lg N)</li>
<li>使用“胖树”来缓解根带宽问题（靠近根的更高带宽链接）</li>
</ul>
<p><img src="/img/1640271547.jpg" alt=""></p>
<p>超立方体</p>
<ul>
<li>低延迟：O(lg N)</li>
<li>基数：O(lg N)</li>
<li>链接数 O(N lg N)</li>
<li>64 核中使用的 6D 超立方体</li>
</ul>
<p><img src="/img/1640271944.jpg" alt=""></p>
<p>多级结构</p>
<ul>
<li>终端间具有多个交换机的间接网络</li>
<li>成本：O(N lg N)</li>
<li>延迟：O(lg N)</li>
<li>许多变体：Omega、蝴蝶、Clos 网络等……</li>
</ul>
<p><img src="/img/1640272000.jpg" alt=""></p>
<p>电路交换与分组交换</p>
<ul>
<li>电路交换建立在发送消息之前在发送方和接收方之间完整路径（获取所有资源）<ul>
<li>建立路由（保留链接）然后发送消息的所有数据</li>
<li>更高的带宽传输（无每包链路管理开销）</li>
<li>是否会产生设置/拆除路径的开销</li>
<li>保留链接会导致利用率低</li>
</ul>
</li>
<li>数据包交换为每个数据包做出路由决定<ul>
<li>单独路由每个数据包（可能通过不同的网络链接）</li>
<li>有机会在链接空闲时为数据包使用链接</li>
<li>传输过程中动态切换逻辑导致的开销</li>
<li>没有设置/拆卸开销</li>
</ul>
</li>
</ul>
<p>通信粒度</p>
<ul>
<li>讯息<ul>
<li>网络客户端之间的传输单位（例如，内核、内存）</li>
<li>可以使用多个数据包传输</li>
</ul>
</li>
<li>数据包<ul>
<li>网络传输单位</li>
<li>可以使用多个 flit 传输（稍后讨论）</li>
</ul>
</li>
<li>Flit（流量控制位）<ul>
<li>数据包分成更小的单位，称为“flits”</li>
<li>Flit：（“流量控制位”）网络中流量控制的单位</li>
<li>Flit 成为路由/缓冲的最小粒度</li>
</ul>
</li>
</ul>
<p>数据包格式</p>
<ul>
<li>一个数据包包括：<ul>
<li>标题：<ul>
<li>包含路由和控制信息</li>
<li>在到路由器的数据包开始时可以提前开始转发</li>
</ul>
</li>
<li>Payload/body：包含要发送的数据</li>
<li>尾巴<ul>
<li>包含控制信息，例如错误代码</li>
<li>通常位于数据包的末尾，因此可以在“出路”时生成（发送方计算校验和，将其附加到数据包的末尾）</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>处理竞争</p>
<ul>
<li>两个包需要同时在同一个节点上进行路由。</li>
<li>解决办法有多个：<ul>
<li>缓存一个包，待会再发送</li>
<li>扔掉一个包</li>
<li>将一个包重新路由</li>
</ul>
</li>
</ul>
<p>电路交换路由</p>
<ul>
<li>高粒度资源分配<ul>
<li>主要思想：沿整个网络路径为消息预先分配所有资源（跨多个交换机的链接）</li>
</ul>
</li>
<li>成本<ul>
<li>需要设置阶段（“探测”）来设置路径（并在消息完成时将其拆除并释放资源）</li>
<li>较低的链接利用率。 两个消息的传输不能共享同一链路（即使在传输过程中不再使用预分配路径上的某些资源）</li>
</ul>
</li>
<li>好处<ul>
<li>由于预分配，传输过程中无争用，因此无需缓冲</li>
<li>任意消息大小（设置路径后，发送数据直到完成）</li>
</ul>
</li>
</ul>
<p>存储转发（基于数据包的路由）</p>
<ul>
<li>在移动到下一个节点之前，数据包被完全复制到网络交换机中</li>
<li>流量控制单元是一个完整的数据包<ul>
<li>来自同一消息的不同数据包可以采用不同的路由，但一个数据包中的所有数据都通过相同的路由传输</li>
</ul>
</li>
<li>需要在每个路由器中缓冲整个数据包</li>
<li>每个数据包的高延迟（延迟 = 链路上的数据包传输时间 x 网络距离）</li>
</ul>
<p><img src="/img/1640418533.jpg" alt=""></p>
<p>直通（cut-flow）流量控制（也基于数据包）</p>
<ul>
<li>一旦收到包头，交换机就开始在下一个链路上转发数据（包头决定了包需要多少链路带宽+路由到哪里）</li>
<li>结果：减少传输延迟<ul>
<li>上一张幻灯片中的存储和转发解决方案：3 跳 x 4 个时间单位在单个链路上传输数据包 = 12 个时间单位</li>
<li>直通解决方案：数据包头部到达目的地的 3 个延迟步骤 + 其余数据包的 3 个时间单位 = 6 个时间单位</li>
</ul>
</li>
</ul>
<p>直通流量控制</p>
<ul>
<li>如果输出链路被阻塞（不能传输头），传输<br>尾部可以继续<ul>
<li>最坏的情况：整个消息被吸收到交换机的缓冲区中（在这种情况下，直通流控制退化为存储转发）</li>
<li>要求交换机对整个数据包进行缓冲，就像存储转发一样</li>
</ul>
</li>
</ul>
<p>虫洞流量控制</p>
<ul>
<li>Flit（流量控制位）<ul>
<li>数据包分成更小的单位，称为“flits”</li>
<li>Flit：（“流量控制位”）网络中流量控制的单位</li>
<li>Flit 成为路由/缓冲的最小粒度</li>
<li>回想一下：到目前为止，数据包是传输和流量控制和缓冲（存储转发、直通路由）的粒度</li>
</ul>
</li>
</ul>
<p><img src="/img/1640418901.jpg" alt=""></p>
<p>虫洞流量控制</p>
<ul>
<li>路由信息仅在 head flit 中</li>
<li>身体跟随头部，尾部流向身体</li>
<li>如果 head flit 阻塞，则其余数据包停止</li>
<li>完全流水线传输<ul>
<li>对于长消息，延迟几乎完全独立于网络距离。</li>
</ul>
</li>
</ul>
<p><img src="/img/1640419258.png" alt=""></p>
<p>虚拟通道流量控制</p>
<ul>
<li>在单个物理信道上复用多个操作</li>
<li>将交换机的输入缓冲区分成共享一个物理通道的多个缓冲区</li>
<li>减少队头阻塞</li>
</ul>
<p><img src="/img/1640419307.jpg" alt=""></p>
<p>虚拟通道的其他用途</p>
<ul>
<li>死锁避免<ul>
<li>可用于打破资源的循环依赖</li>
<li>通过确保请求和响应使用不同的虚拟通道来防止循环</li>
<li>“Escape” VCs：保留至少一个使用无死锁路由的虚拟通道</li>
</ul>
</li>
<li>流量类别的优先级<ul>
<li>提供服务质量保证</li>
<li>一些虚拟通道的优先级高于其他频道</li>
</ul>
</li>
</ul>
<p>概括</p>
<ul>
<li>现代多处理器中互连网络的性能对整体系统性能至关重要<ul>
<li>总线不能扩展到许多节点</li>
</ul>
</li>
<li>网络拓扑在性能、成本、复杂性权衡方面有所不同<ul>
<li>例如，crossbar、ring、mesh、torus、multi-stage network、fat tree、hypercube</li>
</ul>
</li>
<li>挑战：通过网络高效路由数据<ul>
<li>互连是一种宝贵的资源（通信是昂贵的！）</li>
<li>基于Flit的流量控制：细粒度的流量控制，充分利用可用的链路带宽</li>
</ul>
</li>
</ul>
<h1 id="lecture-16"><a href="#lecture-16" class="headerlink" title="lecture 16"></a>lecture 16</h1><p>运行一个线程意味着什么？</p>
<ul>
<li>处理器通过在硬件执行上下文中执行其指令来运行逻辑线程。</li>
<li>如果操作系统希望进程 P 的线程 T 运行，它：<ul>
<li>选择 CPU 执行上下文</li>
<li>它将该上下文中的寄存器值设置为线程的最后状态（例如，将 PC 设置为指向线程必须运行的下一条指令，设置堆栈指针、VM 映射等）</li>
<li>然后处理器开始运行……它根据PC抓取下一条指令，并执行它：<ul>
<li>如果指令是：<code>add r0, r1, r2;</code>，处理器将 r1 和 r2 相加并将结果存储在 r0 中</li>
<li>如果指令是：<code>ld r0 mem[r1];</code>，处理器获取 r1 的内容，根据执行上下文引用的页表将其转换为物理地址，并将该地址处的值加载到 r0</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>操作系统将逻辑线程映射到执行上下文</p>
<ul>
<li>由于线程多于执行上下文，因此操作系统必须在处理器上交错执行线程。</li>
<li>操作系统将定期：<ul>
<li>中断处理器</li>
<li>将当前映射到执行上下文的线程的寄存器状态复制到内存中的OS数据结构中</li>
<li>将它现在想要运行的其他线程的寄存器状态复制到处理器执行上下文寄存器上</li>
<li>告诉处理器继续<ul>
<li>现在这些逻辑线程正在处理器上运行</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>但是如何在每个时钟只能运行一条指令的内核上运行 2 个执行上下文呢？</p>
<ul>
<li>处理器有责任（没有操作系统干预）选择如何在单个内核的资源上交错执行来自多个执行上下文的指令。</li>
</ul>
<p><img src="/img/1640420384.png" alt=""></p>
<p>同步事件的三个阶段</p>
<ul>
<li>获取方法<ul>
<li>线程如何尝试访问受保护的资源</li>
</ul>
</li>
<li>等待算法<ul>
<li>线程如何等待被授予对共享资源的访问权限</li>
</ul>
</li>
<li>释放方法<ul>
<li>当线程在同步区域中的工作完成时，线程如何使其他线程获得资源</li>
</ul>
</li>
</ul>
<p>忙等待</p>
<ul>
<li>忙着等待（又名“自旋”）</li>
<li>忙等待是不好的：为什么？</li>
</ul>
<p>“阻塞”同步</p>
<ul>
<li>思路：如果因为无法获取资源而无法取得进展，则希望为另一个线程释放执行资源（抢占正在运行的线程）</li>
<li>pthreads信号量的例子</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pthread_mutex_t mutex;</span><br><span class="line">pthread_mutex_lock(&amp;mutex);</span><br></pre></td></tr></table></figure>
<p>忙等待 vs. 阻塞</p>
<ul>
<li>在以下情况下，忙等待可能比阻塞更可取：<ul>
<li>调度开销大于预期的等待时间</li>
<li>其他任务不需要处理器的资源<ul>
<li>这在并行程序中很常见，因为在运行性能关键的并行应用程序时我们通常不会超额使用系统（例如，没有多个 CPU 密集型程序同时运行）</li>
</ul>
</li>
<li>澄清：注意不要将上述声明与多线程的价值（多线程/任务的交错执行以隐藏内存操作的长延迟）与同一应用程序中的其他工作混淆。</li>
</ul>
</li>
</ul>
<p>基于测试和设置的锁使用原子测试和设置指令：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ts R0, mem[addr] // load mem[addr] into R0</span><br><span class="line">        // if mem[addr] is 0, set mem[addr] to 1</span><br><span class="line"></span><br><span class="line">lock:</span><br><span class="line">ts   R0, mem[addr]  // load word into R0      </span><br><span class="line">bnz  R0, lock       // if 0, lock obtained</span><br><span class="line"></span><br><span class="line">unlock:</span><br><span class="line">st   mem[addr], #0  // store 0 to address</span><br></pre></td></tr></table></figure></p>
<p>考虑一致性<br><img src="/img/1640421813.jpg" alt=""></p>
<p>x86 cmpxchg用于比较和交换（与lock前缀一起使用时是原子的）<br><img src="/img/1640420386.png" alt=""></p>
<p>理想的锁性能特征</p>
<ul>
<li>低延迟<ul>
<li>如果锁是空闲的并且没有其他处理器试图获取它，则处理器应该能够快速获取锁</li>
</ul>
</li>
<li>低互连流量<ul>
<li>如果所有处理器都试图一次获取锁，它们应该以尽可能少的流量连续获取锁</li>
</ul>
</li>
<li>可扩展性<ul>
<li>延迟/流量应根据处理器数量合理扩展</li>
</ul>
</li>
<li>存储成本低</li>
<li>公平<ul>
<li>避免饥饿或严重的不公平</li>
<li>一个理想情况：处理器应该按照他们请求访问的顺序获取锁</li>
</ul>
</li>
</ul>
<p>Test-and-test-and-set lock<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">Lock</span><span class="params">(<span class="type">int</span>* lock)</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">while</span> (*lock != <span class="number">0</span>); </span><br><span class="line">        <span class="keyword">if</span>  (test_and_set(*lock) == <span class="number">0</span>) </span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">Unlock</span><span class="params">(<span class="keyword">volatile</span> <span class="type">int</span>* lock)</span> &#123; </span><br><span class="line">    *lock = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/1640422233.jpg" alt=""></p>
<p>Test-and-test-and-set特性</p>
<ul>
<li>在无竞争的情况下比测试和设置稍高的延迟<ul>
<li>必须测试…然后测试并设置</li>
</ul>
</li>
<li>产生更少的互连流量<ul>
<li>每个等待处理器、每个锁释放一个失效（O(P) 失效）</li>
<li>如果所有处理器都缓存了锁，则这是 O(P^2) 互连流量</li>
<li>每次测试时，测试和设置锁为每个等待处理器生成一个失效</li>
</ul>
</li>
<li>更具可扩展性（由于流量更少）</li>
<li>存储成本不变（一个int）</li>
<li>仍然没有公平条款</li>
</ul>
<p>带回退的test-and-set lock：获取锁失败，延迟一段时间再重试</p>
<ul>
<li>与test-and-set相同的无竞争延迟，但在争用情况下可能有更高的延迟。</li>
<li>生成的流量比 test-and-set 少（不会不断尝试获取锁）</li>
<li>提高可扩展性（由于流量减少）</li>
<li>存储成本不变（锁仍然是一个 int）</li>
<li>指数退避会导致严重的不公平<ul>
<li>较新的请求者在更短的时间间隔内退出</li>
</ul>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">Lock</span><span class="params">(<span class="keyword">volatile</span> <span class="type">int</span>* l)</span> &#123; </span><br><span class="line">    <span class="type">int</span> amount = <span class="number">1</span>; </span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123; </span><br><span class="line">        <span class="keyword">if</span> (test_and_set(*l) == <span class="number">0</span>) </span><br><span class="line">            <span class="keyword">return</span>; </span><br><span class="line">        delay(amount); </span><br><span class="line">        amount *= <span class="number">2</span>; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>test-and-set 风格锁的主要问题：释放后，所有等待的处理器尝试使用 test-and-set 获取锁。所以提出了ticket lock。<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">lock</span> &#123;</span> </span><br><span class="line">    <span class="keyword">volatile</span> <span class="type">int</span> next_ticket; </span><br><span class="line">    <span class="keyword">volatile</span> <span class="type">int</span> now_serving; </span><br><span class="line">&#125;;</span><br><span class="line"><span class="type">void</span> <span class="title function_">Lock</span><span class="params">(lock* l)</span> &#123; </span><br><span class="line">    <span class="type">int</span> my_ticket = atomic_increment(&amp;l-&gt;next_ticket);   <span class="comment">// take a “ticket”</span></span><br><span class="line">    <span class="keyword">while</span> (my_ticket != l-&gt;now_serving);                 <span class="comment">// wait for number to be called</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">unlock</span><span class="params">(lock* l)</span> &#123; </span><br><span class="line">    l-&gt;now_serving++; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>无需原子操作即可获取锁（仅读取）</p>
<ul>
<li>结果：每次锁定释放只有一次失效（O(P) 互连流量）</li>
</ul>
<p>基于数组的锁</p>
<ul>
<li>每个处理器在不同的内存地址上旋转，利用原子操作在尝试获取时分配地址。</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">lock</span> &#123;</span> </span><br><span class="line">    <span class="keyword">volatile</span> padded_int status[P];    <span class="comment">// padded to keep off same cache line</span></span><br><span class="line">    <span class="keyword">volatile</span> <span class="type">int</span> head; </span><br><span class="line">&#125;; </span><br><span class="line"></span><br><span class="line"><span class="type">int</span> my_element; </span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">Lock</span><span class="params">(lock* l)</span> &#123; </span><br><span class="line">    my_element = atomic_circ_increment(&amp;l­-&gt;head);    <span class="comment">// assume circular increment</span></span><br><span class="line">    <span class="keyword">while</span> (l-&gt;status[my_element] == <span class="number">1</span>); </span><br><span class="line">&#125; </span><br><span class="line"><span class="type">void</span> <span class="title function_">unlock</span><span class="params">(lock* l)</span> &#123;</span><br><span class="line">    l-&gt;status[my_element] = <span class="number">1</span>;</span><br><span class="line">    l-&gt;status[circ_next(my_element)] = <span class="number">0</span>; <span class="comment">// next() gives next index</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>回忆 CUDA 7 原子操作<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span>   <span class="title function_">atomicAdd</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span>;</span><br><span class="line"><span class="type">float</span> <span class="title function_">atomicAdd</span><span class="params">(<span class="type">float</span>* address, <span class="type">float</span> val)</span>;</span><br><span class="line"><span class="type">int</span>   <span class="title function_">atomicSub</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span>;</span><br><span class="line"><span class="type">int</span>   <span class="title function_">atomicExch</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span>;</span><br><span class="line"><span class="type">float</span> <span class="title function_">atomicExch</span><span class="params">(<span class="type">float</span>* address, <span class="type">float</span> val)</span>;</span><br><span class="line"><span class="type">int</span>   <span class="title function_">atomicMin</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span>;</span><br><span class="line"><span class="type">int</span>   <span class="title function_">atomicMax</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span>;</span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> <span class="title function_">atomicInc</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span>* address, <span class="type">unsigned</span> <span class="type">int</span> val)</span>;</span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> <span class="title function_">atomicDec</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span>* address, <span class="type">unsigned</span> <span class="type">int</span> val)</span>; </span><br><span class="line"><span class="type">int</span>   <span class="title function_">atomicCAS</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> compare, <span class="type">int</span> val)</span>;</span><br><span class="line"><span class="type">int</span>   <span class="title function_">atomicAnd</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span>;  <span class="comment">// bitwise </span></span><br><span class="line"><span class="type">int</span>   <span class="title function_">atomicOr</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span>;   <span class="comment">// bitwise </span></span><br><span class="line"><span class="type">int</span>   <span class="title function_">atomicXor</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span>;  <span class="comment">// bitwise </span></span><br></pre></td></tr></table></figure></p>
<p>实现原子fetch-and-op<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// atomicCAS:</span></span><br><span class="line"><span class="comment">// atomic compare and swap performs this logic atomically </span></span><br><span class="line"><span class="type">int</span> <span class="title function_">atomicCAS</span><span class="params">(<span class="type">int</span>* addr, <span class="type">int</span> compare, <span class="type">int</span> val)</span> &#123; </span><br><span class="line">   <span class="type">int</span> old = *addr; </span><br><span class="line">   *addr = (old == compare) ? val : old; </span><br><span class="line">   <span class="keyword">return</span> old; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>如何不使用<code>atomicCAS()</code>构建原子fetch-and-op？使用<code>atomic_min()</code><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">atomic_min</span><span class="params">(<span class="type">int</span>* addr, <span class="type">int</span> x)</span> &#123; </span><br><span class="line">    <span class="type">int</span> old = *addr;</span><br><span class="line">    <span class="type">int</span> new = min(old, x);</span><br><span class="line">    <span class="keyword">while</span> (atomicCAS(addr, old, new) != old) &#123; </span><br><span class="line">        old = *addr; </span><br><span class="line">        new = min(old, x); </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>C++ 11的<code>atomic&lt;T&gt;</code></p>
<ul>
<li>提供整个对象的原子读、写、读-修改-写<ul>
<li>原子性可以由互斥体实现或由处理器支持的原子指令有效地实现（如果 T 是基本类型）</li>
</ul>
</li>
<li>为原子操作前后的操作提供内存排序语义<ul>
<li>默认：顺序一致性</li>
</ul>
</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">atomic&lt;<span class="type">int</span>&gt; i;</span><br><span class="line">i++; <span class="comment">// atomically increment i </span></span><br><span class="line"><span class="type">int</span> a = i; </span><br><span class="line"><span class="comment">// do stuff </span></span><br><span class="line">i.compare_exchange_strong(a, <span class="number">10</span>); <span class="comment">// if i has same value as a, set i to 10 </span></span><br><span class="line"><span class="type">bool</span> b = i.is_lock_free();          <span class="comment">// true if implementation of atomicity is lock free</span></span><br></pre></td></tr></table></figure>
<p>实现集中式barrier（基于共享计数器）<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Barrier_t</span> &#123;</span></span><br><span class="line">    LOCK lock; </span><br><span class="line">    <span class="type">int</span> arrive_counter;   <span class="comment">// initialize to 0 (number of threads that have arrived)</span></span><br><span class="line">    <span class="type">int</span> leave_counter;    <span class="comment">// initialize to P (number of threads that have left barrier) </span></span><br><span class="line">    <span class="type">int</span> flag; </span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// barrier for p processors </span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Barrier</span><span class="params">(Barrier_t* b, <span class="type">int</span> p)</span> &#123; </span><br><span class="line">    lock(b-&gt;lock); </span><br><span class="line">    <span class="keyword">if</span> (b-&gt;arrive_counter == <span class="number">0</span>) &#123;   <span class="comment">// if first to arrive...</span></span><br><span class="line">    <span class="keyword">if</span> (b-&gt;leave_counter == P) &#123;  <span class="comment">// check to make sure no other threads “still in barrier” </span></span><br><span class="line">        b-&gt;flag = <span class="number">0</span>;               <span class="comment">// first arriving thread clears flag </span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; </span><br><span class="line">        unlock(lock); </span><br><span class="line">        <span class="keyword">while</span> (b-&gt;leave_counter != P);  <span class="comment">// wait for all threads to leave before clearing</span></span><br><span class="line">        lock(lock); </span><br><span class="line">        b-&gt;flag = <span class="number">0</span>;                <span class="comment">// first arriving thread clears flag </span></span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> num_arrived = ++(b-&gt;arrive_counter); </span><br><span class="line">    unlock(b-&gt;lock); </span><br><span class="line">    <span class="keyword">if</span> (num_arrived == p) &#123;  <span class="comment">// last arriver sets flag </span></span><br><span class="line">        b-&gt;arrive_counter = <span class="number">0</span>; </span><br><span class="line">        b-&gt;leave_counter = <span class="number">1</span>; </span><br><span class="line">        b-&gt;flag = <span class="number">1</span>; </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123; </span><br><span class="line">        <span class="keyword">while</span> (b-&gt;flag == <span class="number">0</span>);  <span class="comment">// wait for flag</span></span><br><span class="line">        lock(b-&gt;lock); </span><br><span class="line">        b-&gt;leave_counter++; </span><br><span class="line">        unlock(b-&gt;lock); </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>中心化barrier：流量</p>
<ul>
<li>每个屏障的互连上的 O(P) 流量：<ul>
<li>所有线程：2P 个写事务以获取屏障锁和更新计数器（假设锁获取以 O(1) 方式实现，则为 O(P) 流量）</li>
<li>最后一个线程：2 个写入事务以写入标志并重置计数器（O(P) 流量，因为有许多标志的共享者）</li>
<li>P-1个读取更新标志的事务</li>
</ul>
</li>
<li>但在单个共享锁上仍然存在序列化<ul>
<li>所以整个操作的跨度（延迟）是 O(P)</li>
</ul>
</li>
</ul>
<p>Barrier的树实现</p>
<ul>
<li>树可以更好地利用互连拓扑中的并行性<ul>
<li>lg(P) 跨度（延迟）</li>
<li>策略在总线上意义不大（所有流量仍然在单个共享总线上串行化）</li>
</ul>
</li>
<li>Barrier获取：当处理器到达屏障时，执行父计数器的递增<ul>
<li>进程递归到root</li>
</ul>
</li>
<li>Barrier释放：从根开始，通知孩子释放</li>
</ul>
<p><img src="/img/1640424065.jpg" alt=""></p>
<h1 id="lecture-17"><a href="#lecture-17" class="headerlink" title="lecture 17"></a>lecture 17</h1><p>当两个线程需要同时在链表上对一个节点进行插入操作时，需要对节点进行加锁。</p>
<p>解决方案1：用单锁保护列表<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span> </span><br><span class="line">    <span class="type">int</span> value; </span><br><span class="line">    Node* next; </span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">List</span> &#123;</span> </span><br><span class="line">    Node* head; </span><br><span class="line">    Lock  lock;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">insert</span><span class="params">(List* <span class="built_in">list</span>, <span class="type">int</span> value)</span> &#123; </span><br><span class="line">    Node* n = new Node; </span><br><span class="line">    n-&gt;value = value; </span><br><span class="line">    lock(<span class="built_in">list</span>-&gt;lock);</span><br><span class="line"><span class="comment">// assume case of inserting before head of </span></span><br><span class="line"><span class="comment">// of list is handled here (to keep slide simple) </span></span><br><span class="line">    Node* prev = <span class="built_in">list</span>-&gt;head; </span><br><span class="line">    Node* cur = <span class="built_in">list</span>-&gt;head-&gt;next; </span><br><span class="line">    <span class="keyword">while</span> (cur) &#123; </span><br><span class="line">        <span class="keyword">if</span> (cur-&gt;value &gt; value) </span><br><span class="line">            <span class="keyword">break</span>; </span><br><span class="line">        prev = cur; </span><br><span class="line">        cur = cur-&gt;next; </span><br><span class="line">    &#125;</span><br><span class="line">    n-&gt;next = cur;</span><br><span class="line">    prev-&gt;next = n; </span><br><span class="line">    unlock(<span class="built_in">list</span>-&gt;lock);</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">delete</span><span class="params">(List* <span class="built_in">list</span>, <span class="type">int</span> value)</span> &#123; </span><br><span class="line">    lock(<span class="built_in">list</span>-&gt;lock);</span><br><span class="line"><span class="comment">// assume case of deleting first element is </span></span><br><span class="line"><span class="comment">// handled here (to keep slide simple) </span></span><br><span class="line">    Node* prev = <span class="built_in">list</span>-&gt;head; </span><br><span class="line">    Node* cur = <span class="built_in">list</span>-&gt;head-&gt;next; </span><br><span class="line">    <span class="keyword">while</span> (cur) &#123; </span><br><span class="line">        <span class="keyword">if</span> (cur-&gt;value == value) &#123; </span><br><span class="line">            prev-&gt;next = cur-&gt;next; </span><br><span class="line">            delete cur; </span><br><span class="line">            unlock(<span class="built_in">list</span>-&gt;lock);</span><br><span class="line">            <span class="keyword">return</span>; </span><br><span class="line">        &#125; </span><br><span class="line">        prev = cur; </span><br><span class="line">        cur = cur-&gt;next; </span><br><span class="line">    &#125; </span><br><span class="line">    unlock(<span class="built_in">list</span>-&gt;lock);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>每个数据结构的单个全局锁</p>
<ul>
<li>好处：<ul>
<li>对数据结构操作实现正确的互斥相对比较简单</li>
</ul>
</li>
<li>坏处：<ul>
<li>数据结构上的操作是序列化的 - 可能会限制并行应用程序的性能</li>
</ul>
</li>
</ul>
<p>解决方案2:细粒度锁<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span> </span><br><span class="line">    <span class="type">int</span> value; </span><br><span class="line">    Node* next; </span><br><span class="line">    Lock* lock;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">List</span> &#123;</span> </span><br><span class="line">    Node* head; </span><br><span class="line">    Lock* lock;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="type">void</span> <span class="title function_">insert</span><span class="params">(List* <span class="built_in">list</span>, <span class="type">int</span> value)</span> &#123; </span><br><span class="line">    Node* n = new Node; </span><br><span class="line">    n-&gt;value = value; </span><br><span class="line"><span class="comment">// assume case of insert before head handled </span></span><br><span class="line"><span class="comment">// here (to keep slide simple) </span></span><br><span class="line">    Node* prev, *cur; </span><br><span class="line">    lock(<span class="built_in">list</span>-&gt;lock);</span><br><span class="line">    prev = <span class="built_in">list</span>-&gt;head; </span><br><span class="line">    cur = <span class="built_in">list</span>-&gt;head-&gt;next; </span><br><span class="line">    lock(prev-&gt;lock); </span><br><span class="line">    unlock(<span class="built_in">list</span>-&gt;lock); </span><br><span class="line">    <span class="keyword">if</span> (cur) lock(cur-&gt;lock);</span><br><span class="line">    <span class="keyword">while</span> (cur) &#123; </span><br><span class="line">        <span class="keyword">if</span> (cur-&gt;value &gt; value) </span><br><span class="line">            <span class="keyword">break</span>; </span><br><span class="line">        Node* old_prev = prev; </span><br><span class="line">        prev = cur; </span><br><span class="line">        cur = cur-&gt;next;</span><br><span class="line">        unlock(old_prev-&gt;lock); </span><br><span class="line">        <span class="keyword">if</span> (cur) lock(cur-&gt;lock);</span><br><span class="line">    &#125;</span><br><span class="line">    n-&gt;next = cur; </span><br><span class="line">    prev-&gt;next = n; </span><br><span class="line">    unlock(prev-&gt;lock); </span><br><span class="line">    <span class="keyword">if</span> (cur) unlock(cur-&gt;lock);</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">delete</span><span class="params">(List* <span class="built_in">list</span>, <span class="type">int</span> value)</span> &#123; </span><br><span class="line"><span class="comment">// assume case of delete head handled here </span></span><br><span class="line"><span class="comment">// (to keep slide simple) </span></span><br><span class="line">    Node* prev, *cur; </span><br><span class="line">    lock(<span class="built_in">list</span>-&gt;lock);</span><br><span class="line">    prev = <span class="built_in">list</span>-&gt;head; </span><br><span class="line">    cur = <span class="built_in">list</span>-&gt;head-&gt;next; </span><br><span class="line">    lock(prev-&gt;lock); </span><br><span class="line">    unlock(<span class="built_in">list</span>-&gt;lock); </span><br><span class="line">    <span class="keyword">if</span> (cur) lock(cur-&gt;lock)</span><br><span class="line">    <span class="keyword">while</span> (cur) &#123; </span><br><span class="line">        <span class="keyword">if</span> (cur-&gt;value == value) &#123; </span><br><span class="line">            prev-&gt;next = cur-&gt;next; </span><br><span class="line">            unlock(prev-&gt;lock); </span><br><span class="line">            unlock(cur-&gt;lock);</span><br><span class="line">            delete cur;  </span><br><span class="line">            <span class="keyword">return</span>; </span><br><span class="line">        &#125;</span><br><span class="line">        Node* old_prev = prev; </span><br><span class="line">        prev = cur; </span><br><span class="line">        cur = cur-&gt;next; </span><br><span class="line">        unlock(old_prev-&gt;lock); </span><br><span class="line">        <span class="keyword">if</span> (cur) lock(cur-&gt;lock);</span><br><span class="line">    &#125;</span><br><span class="line">    unlock(prev-&gt;lock);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>细粒度锁</p>
<ul>
<li>目标：在数据结构操作中启用并行性<ul>
<li>减少对全局数据结构锁的争用</li>
<li>在前面的链表示例中：单个单体锁过于保守（对链表不同部分的操作可以并行进行）</li>
</ul>
</li>
<li>挑战：难以确保正确性<ul>
<li>确定何时需要互斥</li>
<li>死锁/活锁？</li>
</ul>
</li>
<li>开销？<ul>
<li>每个遍历步骤锁定的开销（额外指令 + 遍历现在涉及内存写入）</li>
<li>额外的存储成本（每个节点一个锁）</li>
</ul>
</li>
</ul>
<p>阻塞算法/数据结构</p>
<ul>
<li>阻塞算法允许一个线程无限期地阻止其他线程完成对共享数据结构的操作</li>
<li>示例：<ul>
<li>线程 0 锁定我们链表中的一个节点</li>
<li>线程 0 被操作系统换出，或者崩溃，或者非常慢等。</li>
<li>现在，没有其他线程可以完成对数据结构的操作</li>
</ul>
</li>
<li>无论锁实现是使用自旋还是抢占，使用锁的算法都是阻塞的</li>
</ul>
<p>无锁算法</p>
<ul>
<li>如果保证某个线程取得进展（“系统范围的进展”），则非阻塞算法是无锁的</li>
<li>在无锁的情况下，不可能在不合时宜的时间抢占其中一个线程并阻止系统其余部分的进展</li>
<li>注意：这个定义不会阻止任何一个线程的饥饿</li>
</ul>
<p>单读、单写限界队列<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Queue</span> &#123;</span><br><span class="line">    <span class="type">int</span> data[N]; </span><br><span class="line">    <span class="type">int</span> head;   <span class="comment">// head of queue </span></span><br><span class="line">    <span class="type">int</span> tail;   <span class="comment">// next free element</span></span><br><span class="line">&#125;; </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">(Queue* q)</span> </span>&#123;</span><br><span class="line">    q-&gt;head = q-&gt;tail = <span class="number">0</span>; </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// return false if queue is full </span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">push</span><span class="params">(Queue* q, <span class="type">int</span> value)</span> </span>&#123; </span><br><span class="line">    <span class="comment">// queue is full if tail is element before head</span></span><br><span class="line">    <span class="keyword">if</span> (q-&gt;tail == <span class="built_in">MOD_N</span>(q-&gt;head - <span class="number">1</span>)) </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>; </span><br><span class="line">    q.data[q-&gt;tail] = value; </span><br><span class="line">    q-&gt;tail = <span class="built_in">MOD_N</span>(q-&gt;tail + <span class="number">1</span>); </span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>; </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// returns false if queue is empty </span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">pop</span><span class="params">(Queue* q, <span class="type">int</span>* value)</span> </span>&#123; </span><br><span class="line">    <span class="comment">// if not empty</span></span><br><span class="line">    <span class="keyword">if</span> (q-&gt;head != q-&gt;tail) &#123; </span><br><span class="line">        *value = q-&gt;data[q-&gt;head]; </span><br><span class="line">        q-&gt;head = <span class="built_in">MOD_N</span>(q-&gt;head + <span class="number">1</span>);  </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>; </span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>只有两个线程（一个生产者，一个消费者）同时访问队列</li>
<li>线程从不同步或相互等待<ul>
<li>当队列为空时（弹出失败），当队列满时（推送失败）</li>
</ul>
</li>
</ul>
<ul>
<li>目前假设一个顺序一致的内存系统（或存在适当的内存栅栏，或 C++ 11 <code>atomic&lt;&gt;</code>）</li>
</ul>
<p>单读单写无界队列<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Node</span> &#123;</span><br><span class="line">    Node* next; </span><br><span class="line">    <span class="type">int</span> value; </span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Queue</span> &#123;  </span><br><span class="line">    Node* head; </span><br><span class="line">    Node* tail; </span><br><span class="line">    Node* reclaim; </span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">(Queue* q)</span> </span>&#123; </span><br><span class="line">    q-&gt;head = q-&gt;tail = q-&gt;reclaim = <span class="keyword">new</span> Node; </span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">push</span><span class="params">(Queue* q, <span class="type">int</span> value)</span> </span>&#123; </span><br><span class="line">    Node* n = <span class="keyword">new</span> Node; </span><br><span class="line">    n-&gt;next = <span class="literal">NULL</span>; </span><br><span class="line">    n-&gt;value = value; </span><br><span class="line">    q-&gt;tail-&gt;next = n; </span><br><span class="line">    q-&gt;tail = q-&gt;tail-&gt;next; </span><br><span class="line">    <span class="keyword">while</span> (q-&gt;reclaim != q-&gt;head) &#123; </span><br><span class="line">        Node* tmp = q-&gt;reclaim; </span><br><span class="line">        q-&gt;reclaim = q-&gt;reclaim-&gt;next; </span><br><span class="line">        <span class="keyword">delete</span> tmp; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// returns false if queue is empty </span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">pop</span><span class="params">(Queue* q, <span class="type">int</span>* value)</span> </span>&#123; </span><br><span class="line">    <span class="keyword">if</span> (q-&gt;head != q-&gt;tail) &#123; </span><br><span class="line">        *value = q-&gt;head-&gt;next-&gt;value; </span><br><span class="line">        q-&gt;head = q-&gt;head-&gt;next;  </span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>; </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>; </span><br></pre></td></tr></table></figure></p>
<ul>
<li>尾部指向添加的最后一个元素</li>
<li>Head 指向 BEFORE 队列头元素</li>
<li>由同一个线程（生产者）执行的分配和删除</li>
</ul>
<p><img src="/img/1640491407.png" alt=""></p>
<p>ABA问题：线程0执行<code>pop()</code>操作时，线程B同时执行<code>pop()</code>和<code>push()</code>操作，导致栈结构破坏。<br><img src="/img/1640492001.jpg" alt=""></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span></span><br><span class="line">    Node* next; </span><br><span class="line">    <span class="type">int</span> value; </span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Stack</span> &#123;</span>  </span><br><span class="line">    Node* top; </span><br><span class="line">    <span class="type">int</span> pop_count;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="type">void</span> <span class="title function_">init</span><span class="params">(Stack* s)</span> &#123;</span><br><span class="line">    s-&gt;top = <span class="literal">NULL</span>; </span><br><span class="line">&#125; </span><br><span class="line"><span class="type">void</span> <span class="title function_">push</span><span class="params">(Stack* s, Node* n)</span> &#123; </span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123; </span><br><span class="line">        Node* old_top = s-&gt;top; </span><br><span class="line">        n-&gt;next = old_top; </span><br><span class="line">        <span class="keyword">if</span> (compare_and_swap(&amp;s-&gt;top, old_top, n) == old_top) </span><br><span class="line">        <span class="keyword">return</span>; </span><br><span class="line">    &#125;</span><br><span class="line">&#125; </span><br><span class="line">Node* <span class="title function_">pop</span><span class="params">(Stack* s)</span> &#123; </span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="type">int</span> pop_count = s-&gt;pop_count;</span><br><span class="line">        Node* top = s-&gt;top; </span><br><span class="line">        <span class="keyword">if</span> (top == <span class="literal">NULL</span>) </span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NULL</span>; </span><br><span class="line">        Node* new_top = top-&gt;next; </span><br><span class="line">        <span class="keyword">if</span> (double_compare_and_swap(&amp;s-&gt;top,       top,new_top, &amp;s-&gt;pop_count, pop_count, pop_count+<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">return</span> top; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>维护<code>pop</code>操作的计数器</li>
<li>要求机器支持“双重比较和交换”(DCAS) 或双字 CAS</li>
<li>还可以通过节点分配和/或元素重用策略解决 ABA 问题</li>
</ul>
<p>在 x86 上比较和交换</p>
<ul>
<li>x86 支持“宽”比较和交换指令<ul>
<li>不完全是上一张幻灯片代码中使用的“双重比较和交换”</li>
<li>但可以简单地确保堆栈的计数和顶部字段在内存中是连续的，以使用下面的 64 位宽单比较和交换指令。</li>
</ul>
</li>
<li>cmpxchg8b<ul>
<li>“比较和交换八个字节”</li>
<li>可用于两个 32 位值的比较和交换</li>
</ul>
</li>
<li>cmpxchg16b<ul>
<li>“比较和交换 16 个字节”</li>
<li>可用于两个 64 位值的比较和交换</li>
</ul>
</li>
</ul>
<p>另一个问题：引用释放的内存</p>
<ul>
<li>危险指针：避免释放节点，直到确定所有其他线程不持有对节点的引用</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Node</span> &#123; </span><br><span class="line">    Node* next; </span><br><span class="line">    <span class="type">int</span> value; </span><br><span class="line">&#125;; </span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Stack</span> &#123;  </span><br><span class="line">    Node* top; </span><br><span class="line">    <span class="type">int</span> pop_count;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// per thread ptr (node that cannot  </span></span><br><span class="line"><span class="comment">// be deleted since the thread is  </span></span><br><span class="line"><span class="comment">// accessing it) </span></span><br><span class="line">Node* hazard;</span><br><span class="line"><span class="comment">// per-thread list of nodes thread  </span></span><br><span class="line"><span class="comment">// must delete </span></span><br><span class="line">Node* retireList; </span><br><span class="line"><span class="type">int</span> retireListSize; </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">(Stack* s)</span> </span>&#123; </span><br><span class="line">    s-&gt;top = <span class="literal">NULL</span>; </span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">push</span><span class="params">(Stack* s, <span class="type">int</span> value)</span> </span>&#123; </span><br><span class="line">    Node* n = <span class="keyword">new</span> Node; </span><br><span class="line">    n-&gt;value = value; </span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123; </span><br><span class="line">        Node* old_top = s-&gt;top; </span><br><span class="line">        n-&gt;next = old_top; </span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">compare_and_swap</span>(&amp;s-&gt;top, old_top, n) == old_top) </span><br><span class="line">            <span class="keyword">return</span>; </span><br><span class="line">    &#125; </span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">pop</span><span class="params">(Stack* s)</span> </span>&#123; </span><br><span class="line">    <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        Stack old; </span><br><span class="line">        old.pop_count = s-&gt;pop_count; </span><br><span class="line">        old.top = s-&gt;top;</span><br><span class="line">        <span class="keyword">if</span> (old.top == <span class="literal">NULL</span>) </span><br><span class="line">            <span class="keyword">return</span> <span class="literal">NULL</span>; </span><br><span class="line">        hazard = old.top;</span><br><span class="line">        Stack new_stack; </span><br><span class="line">        new_stack.top = old.top-&gt;next; </span><br><span class="line">        new_stack.pop_count = old.pop_count<span class="number">+1</span>;  </span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">doubleword_compare_and_swap</span>(&amp;s, &amp;old, new_stack)) &#123; </span><br><span class="line">            <span class="type">int</span> value = old.top-&gt;value; </span><br><span class="line">            <span class="built_in">retire</span>(old.top);</span><br><span class="line">            <span class="keyword">return</span> value; </span><br><span class="line">        &#125;</span><br><span class="line">        hazard = <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// delete nodes if possible</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">retire</span><span class="params">(Node* ptr)</span> </span>&#123; </span><br><span class="line">    <span class="built_in">push</span>(retireList, ptr); </span><br><span class="line">    retireListSize++; </span><br><span class="line">    <span class="keyword">if</span> (retireListSize &gt; THRESHOLD) </span><br><span class="line">        <span class="keyword">for</span> (each node n in retireList) &#123; </span><br><span class="line">            <span class="keyword">if</span> (n <span class="keyword">not</span> pointed to by any thread’s hazard pointer) &#123; </span><br><span class="line">                remove n from list </span><br><span class="line">                <span class="keyword">delete</span> n; </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>无锁链表插入<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Node</span> &#123; </span><br><span class="line">    <span class="type">int</span> value; </span><br><span class="line">    Node* next; </span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">List</span> &#123; </span><br><span class="line">    Node* head; </span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// insert new node after specified node </span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">insert_after</span><span class="params">(List* list, Node* after, <span class="type">int</span> value)</span> </span>&#123; </span><br><span class="line">    Node* n = <span class="keyword">new</span> Node; </span><br><span class="line">    n-&gt;value = value; </span><br><span class="line">    <span class="comment">// assume case of insert into empty list handled </span></span><br><span class="line">    <span class="comment">// here (keep code on slide simple for class discussion) </span></span><br><span class="line">    Node* prev = list-&gt;head; </span><br><span class="line">    <span class="keyword">while</span> (prev-&gt;next) &#123; </span><br><span class="line">        <span class="keyword">if</span> (prev == after) &#123; </span><br><span class="line">            <span class="keyword">while</span> (<span class="number">1</span>) &#123; </span><br><span class="line">                Node* old_next = prev-&gt;next; </span><br><span class="line">                n-&gt;next = old_next;</span><br><span class="line">                <span class="keyword">if</span> (<span class="built_in">compare_and_swap</span>(&amp;prev-&gt;next, old_next, n) == old_next) </span><br><span class="line">                    <span class="keyword">return</span>; </span><br><span class="line">            &#125; </span><br><span class="line">        &#125; </span><br><span class="line">        prev = prev-&gt;next; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>与细粒度锁定实现相比：</p>
<ul>
<li>没有获取锁的开销 </li>
<li>没有每个节点的存储开销</li>
</ul>
<p>在实践中：为什么要无锁数据结构？</p>
<ul>
<li>在本课程中优化并行程序时，您通常假设只有您的程序在使用机器<ul>
<li>因为你关心性能</li>
<li>科学计算、图形、数据分析等中的典型假设。</li>
</ul>
</li>
<li>在这些情况下，编写良好的带锁代码可以与无锁代码一样快（或更快）</li>
<li>但在某些情况下，带锁的代码可能会遇到棘手的性能问题<ul>
<li>当线程处于临界区时可能发生页面错误、抢占等的多程序情况</li>
<li>产生 OS 类中经常讨论的问题，如优先级反转、护送、临界区崩溃等</li>
</ul>
</li>
</ul>
<p>概括</p>
<ul>
<li>使用细粒度锁定来减少共享数据结构操作中的争用（最大化并行度）<ul>
<li>但细粒度会增加代码复杂度（错误）并增加执行开销</li>
</ul>
</li>
<li>无锁数据结构：非阻塞解决方案，避免因锁造成的开销<ul>
<li>但实现起来可能很棘手（确保无锁设置的正确性有其自身的开销）</li>
<li>在现代宽松的一致性硬件上仍然需要适当的内存栅栏</li>
</ul>
</li>
<li>注意：无锁设计并不能消除争用<ul>
<li>比较和交换可能会在激烈的争用下失败，需要旋转</li>
</ul>
</li>
</ul>
<h1 id="lecture-18"><a href="#lecture-18" class="headerlink" title="lecture 18"></a>lecture 18</h1><p>你应该知道的</p>
<ul>
<li>什么是事务</li>
<li>原子代码块和锁定/解锁原语之间的区别（语义上）</li>
<li>事务内存实现的基本设计空间<ul>
<li>数据版本控制政策</li>
<li>冲突检测策略</li>
<li>检测粒度</li>
</ul>
</li>
<li>事务内存硬件实现的基础知识</li>
</ul>
<p>使用事务编程<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">deposit</span><span class="params">(Acct account, <span class="type">int</span> amount)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">lock</span>(account.lock);</span><br><span class="line">    <span class="type">int</span> tmp = bank.<span class="built_in">get</span>(account); </span><br><span class="line">    tmp += amount; </span><br><span class="line">    bank.<span class="built_in">put</span>(account, tmp); </span><br><span class="line">    <span class="built_in">unlock</span>(account.lock);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">deposit</span><span class="params">(Acct account, <span class="type">int</span> amount)</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    atomic &#123;</span><br><span class="line">        <span class="type">int</span> tmp = bank.<span class="built_in">get</span>(account); </span><br><span class="line">        tmp += amount; </span><br><span class="line">        bank.<span class="built_in">put</span>(account, tmp); </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>原子结构是声明性的<ul>
<li>程序员陈述要做什么（保持代码的原子性），而不是如何去做</li>
<li>没有明确使用或管理锁</li>
</ul>
</li>
<li>系统根据需要实现同步以确保原子性<ul>
<li>系统可以使用锁实现原子性</li>
<li>今天讨论的实现使用乐观并发：仅在真正争用（R-W 或 W-W 冲突）的情况下进行序列化</li>
</ul>
</li>
<li>声明性：程序员定义应该做什么<ul>
<li>执行所有这些独立的 1000 个任务</li>
</ul>
</li>
<li>必要的：程序员说明应该如何做<ul>
<li>产生 N 个工作线程。 通过从共享任务队列中删除工作来将工作分配给线程</li>
<li>原子地执行这组操作</li>
<li>获取锁，执行操作，释放锁</li>
</ul>
</li>
</ul>
<p>事务内存 (Transaction Memory, TM)</p>
<ul>
<li>内存事务<ul>
<li>一个原子的和隔离的内存访问序列</li>
<li>受数据库事务的启发</li>
</ul>
</li>
<li>原子性（全有或全无）<ul>
<li>事务提交后，事务中的所有内存写入立即生效</li>
<li>在事务中止时，似乎没有任何写入生效（就好像事务从未发生过一样）</li>
</ul>
</li>
<li>隔离<ul>
<li>在事务提交之前没有其他处理器可以观察写入</li>
</ul>
</li>
<li>可串行化<ul>
<li>事务似乎以单个串行顺序提交</li>
<li>但是事务的语义不能保证提交的确切顺序</li>
</ul>
</li>
<li>换句话说……我们为一致内存系统中的单个地址维护的许多属性，我们希望为事务中的读和写集维护。</li>
<li>这些内存事务要么全部被其他处理器观察到，要么都不被其他处理器观察到。（有效地全部同时发生）</li>
</ul>
<p>同步HashMap</p>
<ul>
<li>Java 1.4 解决方案：同步层<ul>
<li>将任何映射转换为线程安全变体</li>
<li>使用程序员指定的显式粗粒度锁定</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Object <span class="title function_">get</span><span class="params">(Object key)</span> &#123;</span><br><span class="line">    <span class="keyword">synchronized</span> (myHashMap) &#123;   <span class="comment">// guards all accesses to hashMap </span></span><br><span class="line">        <span class="keyword">return</span> myHashMap.get(key); </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>简单地将所有操作包含在原子块中<ul>
<li>原子块的语义：系统保证块内逻辑的原子性</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Object <span class="title function_">get</span><span class="params">(Object key)</span> &#123; </span><br><span class="line">    atomic &#123;       <span class="comment">// System guarantees atomicity </span></span><br><span class="line">        <span class="keyword">return</span> m.get(key);     </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>事务HashMap<ul>
<li>好：线程安全，易于编程</li>
<li>性能和可扩展性如何？<ul>
<li>取决于atomic的工作量和实现</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>事务的例子，两个事务执行后没有读写冲突，事务不会写在另一事务中的元素。<br><img src="/img/1640497184.jpg" alt=""></p>
<p>如果两个事务同时写入3号点，则引起冲突。事务在此时必须是串行的。<br><img src="/img/1640497265.jpg" alt=""></p>
<p>失败的原子性：锁<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">transfer</span><span class="params">(A, B, amount)</span> </span>&#123; </span><br><span class="line">    <span class="built_in">synchronized</span>(bank) </span><br><span class="line">    &#123; </span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="built_in">withdraw</span>(A, amount); </span><br><span class="line">            <span class="built_in">deposit</span>(B, amount); </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">catch</span>(exception1) &#123; <span class="comment">/* undo code 1*/</span> &#125; </span><br><span class="line">        <span class="built_in">catch</span>(exception2) &#123; <span class="comment">/* undo code 2*/</span> &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>手动捕获异常的复杂性<ul>
<li>程序员根据具体情况提供“撤消”代码</li>
<li>复杂性：必须跟踪要撤消的内容以及如何……</li>
<li>其他线程可能会看到某些副作用</li>
<li>例如，一个未捕获的case可能会导致系统死锁……</li>
</ul>
</li>
</ul>
<p>失败的原子性：事务</p>
<ul>
<li>系统现在负责处理异常<ul>
<li>所有异常（除了那些由程序员明确管理的异常）</li>
<li>事务被中止，内存更新被撤销</li>
<li>回想：事务要么提交要么不提交：其他线程看不到部分更新</li>
<li>例如，失败的线程没有持有锁……</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> <span class="title function_">transfer</span><span class="params">(A, B, amount)</span> </span><br><span class="line">&#123;</span><br><span class="line">    atomic &#123;</span><br><span class="line">        withdraw(A, amount); </span><br><span class="line">        deposit(B, amount);   </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可组合性：锁</p>
<ul>
<li>编写基于锁的代码可能很棘手<ul>
<li>需要系统范围的策略才能正确</li>
<li>系统范围的策略可以打破软件模块化</li>
</ul>
</li>
<li>可能会有额外的锁和很难实现的地方<ul>
<li>粗粒锁：低性能</li>
<li>细粒度锁：有利于性能，但会导致死锁</li>
</ul>
</li>
</ul>
<p>以下是死锁的例子：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">transfer</span><span class="params">(A, B, amount)</span> </span>&#123; </span><br><span class="line">    <span class="built_in">synchronized</span>(A) &#123; </span><br><span class="line">        <span class="built_in">synchronized</span>(B) &#123;</span><br><span class="line">            <span class="built_in">withdraw</span>(A, amount); </span><br><span class="line">            <span class="built_in">deposit</span>(B, amount); </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">transfer2</span><span class="params">(A, B, amount)</span> </span>&#123; </span><br><span class="line">    <span class="built_in">synchronized</span>(B) &#123; </span><br><span class="line">        <span class="built_in">synchronized</span>(A) &#123;</span><br><span class="line">            <span class="built_in">withdraw</span>(A, <span class="number">2</span>*amount); </span><br><span class="line">            <span class="built_in">deposit</span>(B, <span class="number">2</span>*amount); </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可组合性：事务</p>
<ul>
<li>事务优雅地组合（理论上）<ul>
<li>程序员声明全局意图（传输的原子执行）</li>
<li>无需了解全局实施策略</li>
<li>transfer中的事务包含withdraw和deposit中定义的任何内容</li>
<li>最外层事务定义原子性边界</li>
</ul>
</li>
<li>系统管理并发以及可能的序列化<ul>
<li><code>transfer(A, B, 100)</code>和<code>transfer(B, A, 200)</code>的序列化</li>
<li><code>transfer(A, B, 100)</code>和<code>transfer(B, A, 200)</code>的并发</li>
</ul>
</li>
</ul>
<p>事务内存的优点</p>
<ul>
<li>易于使用的同步结构<ul>
<li>程序员很难正确同步</li>
<li>程序员声明需要原子性，系统实现的很好</li>
<li>声明：事务与粗粒度锁一样易于使用</li>
</ul>
</li>
<li>通常与细粒度锁的性能一样好<ul>
<li>提供自动读-读并发和细粒度并发</li>
<li>性能可移植性：4 个 CPU 的锁定方案可能不是 64 个 CPU 的最佳方案</li>
</ul>
</li>
<li>故障原子性和恢复<ul>
<li>线程失败时不会丢失锁</li>
<li>故障恢复 = 事务中止 + 重启</li>
</ul>
</li>
<li>可组合性<ul>
<li>安全且可扩展的软件模块组合</li>
</ul>
</li>
</ul>
<p>与 OpenMP 的集成示例</p>
<ul>
<li>示例：OpenTM = OpenMP + TM<ul>
<li>OpenMP：主从并行模型</li>
<li>易于指定并行循环和任务</li>
<li>TM：原子和隔离执行</li>
<li>易于指定同步和推测</li>
</ul>
</li>
<li>OpenTM 特性<ul>
<li>事务、事务循环和事务部分</li>
<li>TM 的数据指令（例如，线程私有数据）</li>
<li>TM 的运行时系统提示</li>
</ul>
</li>
<li>代码示例：</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> omp transfor schedule (static, chunk=50)</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;N; i++) &#123;</span><br><span class="line">    bin[A[i]]++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>anomic&#123;&#125;</code> ≠ <code>lock()</code> + <code>unlock()</code></p>
<ul>
<li>区别<ul>
<li>Atomic：原子性的高级声明<ul>
<li>不指定原子性的实现</li>
</ul>
</li>
<li>锁：低级阻塞原语<ul>
<li>本身不提供原子性或隔离性</li>
</ul>
</li>
</ul>
</li>
<li>牢记<ul>
<li>锁可用于实现原子块</li>
<li>锁可用于原子性以外的目的<ul>
<li>不能用原子区域替换所有使用的锁</li>
</ul>
</li>
<li>Atomic 消除了许多数据竞争，但使用原子块编程仍然会受到原子性违规的影响：例如，程序员错误地将应该是原子的序列拆分为两个原子块</li>
</ul>
</li>
</ul>
<p>TM 实施基础</p>
<ul>
<li>TM 系统必须提供原子性和隔离性<ul>
<li>不牺牲并发性</li>
</ul>
</li>
<li>基本实施要求<ul>
<li>数据版本控制（允许事务中止）</li>
<li>冲突检测和解决（何时中止）</li>
</ul>
</li>
<li>实施选项<ul>
<li>硬件事务内存 (HTM)</li>
<li>软件事务存储器 (STM)</li>
<li>混合事务内存</li>
<li>例如，硬件加速的 STM</li>
</ul>
</li>
</ul>
<p>数据版本控制：管理未提交的（新的）和以前提交的（旧的）并发事务的数据版本</p>
<ol>
<li>急切的版本控制（基于撤销日志）</li>
<li>延迟版本控制（基于写缓冲区）</li>
</ol>
<p>急切的版本控制立即更新内存，维护“undo log”以防中止。当事务开始时，线程对内存进行修改， 同时将之前的值放入undo log，提交事务后，undo log被清理；当事务中断时，使用undo log将内存恢复到事务开始之前。<br><img src="/img/1640499708.jpg" alt=""></p>
<p>懒惰的版本控制：在事务写入缓冲区中记录内存更新，提交时刷新缓冲区。把事务中所有写入都放入缓冲区，当事务结束后，再把内存地址中的最终值写入内存。<br><img src="/img/1640499819.jpg" alt=""></p>
<p>数据版本控制</p>
<ul>
<li>管理未提交（新）和已提交（旧）版本的并发事务的数据</li>
<li>急切版本控制（基于撤销日志）<ul>
<li>在写入时直接更新内存位置</li>
<li>在日志中维护撤消信息（产生每个store的开销）</li>
<li>好：更快的提交（数据已经在内存中）</li>
<li>不好：中止速度较慢，容错问题（考虑在事务中间崩溃）</li>
<li>急切的版本控制理念：立即写入内存，希望事务不会中止（但在必须时处理中止）</li>
</ul>
</li>
<li>延迟版本控制（基于写缓冲区）<ul>
<li>在写入缓冲区中缓冲数据直到提交</li>
<li>在提交时更新实际内存位置</li>
<li>好：更快的中止（只是清除日志），没有容错问题</li>
<li>不好：提交速度较慢</li>
<li>懒惰的版本控制理念：仅在必须进行冲突检测时才写入内存</li>
</ul>
</li>
</ul>
<p>冲突检测</p>
<ul>
<li>必须检测和处理事务之间的冲突<ul>
<li>读写冲突：事务 A 读取地址 X，该地址由待处理事务 B 写入</li>
<li>写-写冲突：事务 A 和 B 都未决，并且都写入地址 X</li>
</ul>
</li>
<li>系统必须跟踪事务的读集和写集<ul>
<li>读取集：在事务中读取的地址</li>
<li>写集：在事务中写入的地址</li>
</ul>
</li>
</ul>
<p>悲观检测</p>
<ul>
<li>检查加载或存储期间的冲突<ul>
<li>硬件实现将通过一致性操作检查冲突</li>
<li>理念：“我怀疑可能会发生冲突，所以让我们总是在每次内存操作后检查是否发生了冲突……如果我必须回滚，不妨现在就做，以免浪费工作。”</li>
</ul>
</li>
<li>当检测到冲突时“争用管理器”决定停止或中止事务<ul>
<li>各种优先级策略，以快速处理常见情况</li>
</ul>
</li>
</ul>
<p>两个线程共同进行事务，case1中没有冲突，所以成功；case2中T1发现T0在写就直接stall；case3中包含了case2的情况。<br><img src="/img/1640500730.jpg" alt=""></p>
<p>乐观检测</p>
<ul>
<li>当事务尝试提交时检测冲突<ul>
<li>硬件：使用一致性操作验证写入集</li>
<li>获得对写集中缓存行的独占访问权限</li>
<li>直觉：“让我们抱最好的希望，只有在事务尝试提交时才能解决所有冲突”</li>
</ul>
</li>
<li>发生冲突时，优先提交事务<ul>
<li>其他事务可能会在稍后中止</li>
<li>在提交事务之间发生冲突时，使用争用管理器来决定优先级</li>
</ul>
</li>
<li>注意：可以同时使用乐观方案和悲观方案<ul>
<li>一些 STM 系统使用乐观的读取和悲观的写入</li>
</ul>
</li>
</ul>
<p>发现乐观锁是在提交的时候才检查冲突的，如果有冲突就重启事务<br><img src="/img/1640500898.jpg" alt=""></p>
<p>冲突检测权衡</p>
<ul>
<li>悲观冲突检测（又名“eager”）<ul>
<li>好：及早发现冲突（撤消较少的工作，将一些中止转为停顿）</li>
<li>不好：没有前进的保证，在某些情况下更多的中止</li>
<li>不好：细粒度的通信（检查每个加载/存储）</li>
<li>不好：关键路径上的检测</li>
</ul>
</li>
<li>乐观冲突检测（又名“懒惰”或“提交”）<ul>
<li>好：前进保证</li>
<li>好：批量通信和冲突检测</li>
<li>差：发现冲突较晚，仍可能存在公平性问题</li>
</ul>
</li>
</ul>
<p>硬件事务内存 (HTM)</p>
<ul>
<li>数据版本控制在缓存中实现<ul>
<li>缓存写缓冲区或撤消日志</li>
<li>添加新的缓存行元数据以跟踪事务读取集和写入集</li>
</ul>
</li>
<li>通过缓存一致性协议进行冲突检测<ul>
<li>一致性查找检测事务之间的冲突</li>
<li>与监听和目录一致性一起使用</li>
</ul>
</li>
<li><p>注意：</p>
<ul>
<li>还必须在事务开始时进行注册检查点（以在中止时恢复执行上下文状态）</li>
</ul>
</li>
<li><p>缓存行注释以跟踪读取集和写入集</p>
<ul>
<li>R 位：表示事务读取的数据（加载时设置）</li>
<li>W 位：表示事务写入的数据（在存储上设置）</li>
<li>R/W 位可以是字或缓存行粒度</li>
<li>R/W 位在事务提交或中止时清除</li>
<li>对于急切的版本控制，需要为撤消日志进行第二次缓存写入</li>
</ul>
</li>
<li>一致性请求检查 R/W 位以检测冲突<ul>
<li>观察到 W-word 的共享请求是读写冲突</li>
<li>观察到对 R 字的独占（意图写）请求是写-读冲突</li>
<li>观察到对 W-word 的独占（意图写）请求是写-写冲突</li>
</ul>
</li>
</ul>
<p><img src="/img/1640501501.jpg" alt=""></p>
<h1 id="lecture-19"><a href="#lecture-19" class="headerlink" title="lecture 19"></a>lecture 19</h1><p>异构并行和硬件专业化</p>
<p>更多异构：添加离散 GPU</p>
<ul>
<li>除非图形密集型应用程序需要，否则保持独立（耗电）GPU</li>
<li>将集成的低功耗图形用于基本图形/窗口管理器/UI</li>
</ul>
<p>FPGA（现场可编程门阵列）</p>
<ul>
<li>ASIC 和处理器之间的中间地带</li>
<li>FPGA 芯片提供逻辑块阵列，通过互连连接</li>
<li>由 FGPA 直接实现的程序员定义的逻辑</li>
</ul>
<p><img src="/img/1640502017.jpg" alt=""></p>
<p>异质性的挑战</p>
<ul>
<li>异构系统：每个任务的首选处理器<ul>
<li>硬件设计师面临的挑战：什么是正确的资源组合？</li>
<li>面向吞吐量的资源太少（并行工作负载的峰值吞吐量较低）</li>
<li>顺序处理资源太少（受工作负载的顺序部分限制）</li>
<li>应该为特定功能（例如视频）分配多少芯片面积？ （这些资源从通用处理中拿走）</li>
<li>必须在芯片设计时预期工作平衡</li>
<li>系统无法适应使用情况随时间、新算法等的变化。</li>
<li>对软件开发人员的挑战：如何将程序映射到异构资源集合上？</li>
<li>挑战：“为工作选择合适的工具”：设计算法可以很好地分解为组件，每个组件都可以很好地映射到机器的不同处理组件</li>
<li>异构系统上的调度问题更复杂</li>
<li>可用的资源混合可以决定算法的选择</li>
<li>软件可移植性噩梦</li>
</ul>
</li>
</ul>
<p>降低能耗</p>
<ul>
<li>理念1：使用专门的处理</li>
<li>理念2：移动更少的数据</li>
</ul>
<p>数据移动的能源成本很高</p>
<ul>
<li>移动系统设计的经验法则：始终寻求减少从内存传输的数据量<ul>
<li>在课堂早些时候，我们讨论了最小化通信以减少停顿（性能不佳）。 现在，我们希望减少通信以减少能源消耗</li>
</ul>
</li>
</ul>
<p>能源优化计算的三大趋势</p>
<ul>
<li>减少计算！<ul>
<li>计算消耗能源：即使运行速度更快，并行算法的工作量也比顺序算法多</li>
</ul>
</li>
<li>专业化计算单元：<ul>
<li>异构处理器：类 CPU 内核 + 吞吐量优化内核（类 GPU 内核）</li>
<li>固定功能单元：音频处理、“运动传感器处理”视频解码/编码、图像处理/计算机视觉？</li>
<li>专用指令：扩展AVX向量指令集，新的AES加密加速指令（AES-NI）</li>
<li>可编程软逻辑：FPGA</li>
</ul>
</li>
<li>降低带宽要求<ul>
<li>利用局部性（重构算法以尽可能多地重用片上数据）</li>
<li>积极使用压缩：在传输到内存之前执行额外的计算以压缩应用程序数据（可能会看到固定功能的硬件以减少一般数据压缩/解压缩的开销）</li>
</ul>
</li>
</ul>
<h1 id="lecture-20"><a href="#lecture-20" class="headerlink" title="lecture 20"></a>lecture 20</h1><p>领域特定编程系统</p>
<p>这是一个巨大的挑战</p>
<ul>
<li>性能特征截然不同的机器</li>
<li>更糟：同一台机器内不同规模的不同性能特征</li>
<li>为了提高效率，软件必须针对硬件特性进行优化<ul>
<li>一机一级也难</li>
<li>考虑复杂机器或不同机器时优化的组合复杂性</li>
<li>失去软件可移植性</li>
</ul>
</li>
</ul>
<p>特定领域的编程系统</p>
<ul>
<li>主要思想：提高表达程序的抽象层次</li>
<li>引入特定于应用程序域的高级编程原语<ul>
<li>高效：使用直观，跨机器移植，原语对应于经常用于解决目标领域问题的行为</li>
<li>高性能：系统使用领域知识来提供高效、优化的实现</li>
<li>给定一台机器：系统知道要使用什么算法，该领域要采用的并行化策略</li>
<li>优化超越了软件到硬件的高效映射！ 硬件平台本身也可以针对抽象进行优化</li>
</ul>
</li>
<li>成本：丧失一般性/完整性</li>
</ul>
<p>Lizst：一种在网格上求解偏微分方程的语言</p>
<ul>
<li>在网格上运行Lizst程序</li>
<li>Liszt 程序定义并计算网格上定义的字段的值 </li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val Position = FieldWithConst[Vertex,Float3](0.f, 0.f, 0.f)</span><br><span class="line">val Temperature = FieldWithConst[Vertex,Float](0.f)</span><br><span class="line">val Flux = FieldWithConst[Vertex,Float](0.f)</span><br><span class="line">val JacobiStep = FieldWithConst[Vertex,Float](0.f)</span><br></pre></td></tr></table></figure>
<p>Liszt的拓扑算子</p>
<ul>
<li>用于访问与某些输入顶点、边、面等相关的网格元素。拓扑运算符是在 Liszt 程序中访问网格数据的唯一方法</li>
<li>注意有多少运算符返回集合（例如，“这个面的所有边缘”）</li>
</ul>
<p>限制依赖分析的语言</p>
<ul>
<li>语言限制：<ul>
<li>网格元素只能通过内置的拓扑函数访问：<code>cells(mesh)</code></li>
<li>单一静态分配：<code>val va = head(e)</code></li>
<li>字段中的数据只能使用网格元素访问：<code>Pressure(b)</code></li>
<li>没有递归函数</li>
</ul>
</li>
</ul>
<p>限制允许编译器自动推断循环迭代的模板。</p>
<p>关键：确定程序依赖</p>
<ul>
<li>识别并行性<ul>
<li>没有依赖意味着代码可以并行执行</li>
</ul>
</li>
<li>识别数据局部性<ul>
<li>基于依赖的分区数据（本地化依赖计算以加快同步）</li>
</ul>
</li>
<li>需要同步的原因<ul>
<li>需要同步以尊重依赖性（必须等到计算所依赖的值已知）</li>
</ul>
</li>
</ul>
<p>在一般程序中，编译器无法在全局范围内推断依赖关系：<code>a[f(i)] += b[i]</code>（必须执行<code>f(i)</code>才能知道在循环迭代 i 中是否存在依赖关系）</p>
<p>可移植并行性：使用依赖来实现不同的并行执行策略</p>
<ul>
<li>网格分块</li>
<li>网格着色</li>
</ul>
<p>Liszt的分布式内存实现：Mesh + Stencil→Graph→Partition</p>
<p><img src="/img/1640504311.jpg" alt=""></p>
<p>考虑分布式内存实现：在集群中的每个节点上存储网格区域（注：ParMETIS 是用于划分网格的工具）</p>
<p>每个处理器还需要相邻单元的数据来执行计算（“halo单元”）。 Listz 分配halo区域存储并发出所需的通信以实现拓扑算子。<br><img src="/img/1640504396.png" alt=""></p>
<p>Liszt小结</p>
<ul>
<li>生产力：<ul>
<li>网格的抽象表示：顶点、边、面、场</li>
<li>直观的拓扑运算符</li>
</ul>
</li>
<li>可移植性<ul>
<li>相同的代码在大型 CPU (MPI) 和 GPU</li>
</ul>
</li>
<li>高性能<ul>
<li>语言被限制为允许编译器跟踪依赖项</li>
<li>用于分布式内存实现中的位置感知分区</li>
<li>用于 GPU 实现中的图形着色</li>
<li>编译器知道如何为不同平台选择不同的并行化策略</li>
<li>底层网格表示可以根据使用和平台由系统自定义（例如，如果代码不需要，则不要存储边缘指针，为每个顶点字段选择数组结构与结构数组）</li>
</ul>
</li>
</ul>
<h1 id="lecture-21"><a href="#lecture-21" class="headerlink" title="lecture 21"></a>lecture 21</h1><p>图计算的领域专门语言</p>
<p>Page Rank也是基于图算法的，node代表了网页，边代表了两个网页之间的链接<br><img src="/img/1640505321.jpg" alt=""></p>
<p>GraphLab</p>
<ul>
<li>一个描述图迭代计算的系统</li>
<li>作为 C++ 运行时实现</li>
<li>在共享内存机器上运行或分布在集群中<ul>
<li><ul>
<li>GraphLab 运行时负责并行调度工作、跨机器集群划分图、主机之间的通信等。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>GraphLab 程序：状态</p>
<ul>
<li>图用G = (V, E)表示<ul>
<li>应用程序在每个顶点和有向边上定义数据块</li>
<li>D(v) = 与顶点 v 相关的数据</li>
<li>D(u→v) = 与有向边 u→v 相关的数据</li>
</ul>
</li>
<li>只读全局数据<ul>
<li>可以将其视为每图数据，而不是每顶点或每边数据）</li>
</ul>
</li>
<li>注意：我总是先描述程序状态，然后描述哪些操作可以操作这个状态</li>
</ul>
<p>GraphLab 操作：顶点程序</p>
<ul>
<li>在顶点的本地邻域上定义每个顶点的操作</li>
<li>顶点的邻域（又名“范围”）：<ul>
<li>当前顶点</li>
<li>相邻边缘</li>
<li>相邻顶点</li>
</ul>
</li>
</ul>
<p><img src="/img/1640505590.jpg" alt=""></p>
<p>Page Rank改写程序<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">PageRank_vertex_program(vertex i) &#123; </span><br><span class="line"><span class="comment">// (Gather phase) compute the sum of my neighbors rank</span></span><br><span class="line"><span class="type">double</span> sum = <span class="number">0</span>; </span><br><span class="line">foreach(vertex j : in_neighbors(i)) &#123;</span><br><span class="line">    sum = sum + j.rank / num_out_neighbors(j); </span><br><span class="line">&#125; </span><br><span class="line"><span class="comment">// (Apply phase) Update my rank (i)</span></span><br><span class="line">i.rank = (<span class="number">1</span><span class="number">-0.85</span>)/num_graph_vertices() + <span class="number">0.85</span>*sum; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>GraphLab：数据访问</p>
<ul>
<li>应用程序的顶点程序按顶点执行</li>
<li>顶点程序定义：<ul>
<li>哪些相邻边是计算的输入</li>
<li>每条边执行什么计算</li>
<li>如何更新顶点的值</li>
<li>计算修改了哪些相邻边</li>
<li>如何更新这些输出边</li>
</ul>
</li>
<li>注意 GraphLab 如何要求程序告诉它所有将被访问的数据，以及它是读访问还是写访问</li>
</ul>
<p>PageRank：GraphLab顶点程序（C++代码）<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">web_page</span> &#123;</span><br><span class="line">    std::string pagename; </span><br><span class="line">    <span class="type">double</span> pagerank; </span><br><span class="line">    <span class="built_in">web_page</span>(): <span class="built_in">pagerank</span>(<span class="number">0.0</span>) &#123; &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">typedef</span> graphlab::distributed_graph&lt;web_page, graphlab::empty&gt; graph_type;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">pagerank_program</span>:</span><br><span class="line">    <span class="keyword">public</span> graphlab::ivertex_program&lt;graph_type, <span class="type">double</span>&gt;, </span><br><span class="line">    <span class="keyword">public</span> graphlab::IS_POD_TYPE &#123; </span><br><span class="line"><span class="keyword">public</span>: </span><br><span class="line">    <span class="comment">// we are going to gather on all the in-edges </span></span><br><span class="line">    <span class="function">edge_dir_type <span class="title">gather_edges</span><span class="params">(icontext_type&amp; context, <span class="type">const</span> vertex_type&amp; vertex)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> graphlab::IN_EDGES; </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// for each in-edge gather the weighted sum of the edge. </span></span><br><span class="line">    <span class="function"><span class="type">double</span> <span class="title">gather</span><span class="params">(icontext_type&amp; context, <span class="type">const</span> vertex_type&amp; vertex, edge_type&amp; edge)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> edge.<span class="built_in">source</span>().<span class="built_in">data</span>().pagerank / edge.<span class="built_in">source</span>().<span class="built_in">num_out_edges</span>(); </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Use the total rank of adjacent pages to update this page  </span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">apply</span><span class="params">(icontext_type&amp; context, vertex_type&amp; vertex, <span class="type">const</span> gather_type&amp; total)</span> </span>&#123;</span><br><span class="line">        <span class="type">double</span> newval = total * <span class="number">0.85</span> + <span class="number">0.15</span>; </span><br><span class="line">        vertex.<span class="built_in">data</span>().pagerank = newval; </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// No scatter needed. Return NO_EDGES  </span></span><br><span class="line">    <span class="function">edge_dir_type <span class="title">scatter_edges</span><span class="params">(icontext_type&amp; context, <span class="type">const</span> vertex_type&amp; vertex)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> graphlab::NO_EDGES; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>图的每个顶点都有 web_page 类型的记录，边上没有数据</li>
<li>定义要在“聚集阶段”聚集的边</li>
<li>计算每条边的累加值</li>
<li>更新顶点等级</li>
<li>PageRank 示例不执行分散</li>
</ul>
<p>顶点信号：GraphLab 生成新任务的机制</p>
<ul>
<li>迭代更新所有 R[i] 的 10 次</li>
<li>使用通用的“信号”原语</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">web_page</span> &#123; </span><br><span class="line">    std::string pagename; </span><br><span class="line">    <span class="type">double</span>      pagerank;</span><br><span class="line">    <span class="type">int</span>         counter;</span><br><span class="line">    <span class="built_in">web_page</span>(): <span class="built_in">pagerank</span>(<span class="number">0.0</span>),<span class="built_in">counter</span>(<span class="number">0</span>) &#123; &#125;</span><br><span class="line">&#125; </span><br><span class="line"><span class="comment">// Use the total rank of adjacent pages to update this page  </span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">apply</span><span class="params">(icontext_type&amp; context, vertex_type&amp; vertex, <span class="type">const</span> gather_type&amp; total)</span> </span>&#123;</span><br><span class="line">    <span class="type">double</span> newval = total * <span class="number">0.85</span> + <span class="number">0.15</span>; </span><br><span class="line">    vertex.<span class="built_in">data</span>().pagerank = newval;</span><br><span class="line">    vertex.<span class="built_in">data</span>().counter++; </span><br><span class="line">    <span class="keyword">if</span> (vertex.<span class="built_in">data</span>().counter &lt; <span class="number">10</span>) </span><br><span class="line">        vertex.<span class="built_in">signal</span>();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>信号：调度工作的通用原语</p>
<ul>
<li>图的一部分可能以不同的速率收敛（迭代 PageRank 直到收敛，但只针对需要它的顶点）</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">pagerank_program</span>: </span><br><span class="line"><span class="keyword">public</span> graphlab::ivertex_program&lt;graph_type, <span class="type">double</span>&gt;, </span><br><span class="line"><span class="keyword">public</span> graphlab::IS_POD_TYPE &#123; </span><br><span class="line"><span class="keyword">private</span>: </span><br><span class="line">    <span class="type">bool</span> perform_scatter; </span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="comment">// Use the total rank of adjacent pages to update this page  </span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">apply</span><span class="params">(icontext_type&amp; context, vertex_type&amp; vertex, <span class="type">const</span> gather_type&amp; total)</span> </span>&#123; </span><br><span class="line">        <span class="type">double</span> newval = total * <span class="number">0.85</span> + <span class="number">0.15</span>; </span><br><span class="line">        <span class="type">double</span> oldval = vertex.<span class="built_in">data</span>().pagerank; </span><br><span class="line">        vertex.<span class="built_in">data</span>().pagerank = newval;</span><br><span class="line">        perform_scatter = (std::<span class="built_in">fabs</span>(prevval - newval) &gt; <span class="number">1E-3</span>); </span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// Scatter now needed if algorithm has not converged</span></span><br><span class="line">    <span class="function">edge_dir_type <span class="title">scatter_edges</span><span class="params">(icontext_type&amp; context, <span class="type">const</span> vertex_type&amp; vertex)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (perform_scatter) </span><br><span class="line">            <span class="keyword">return</span> graphlab::OUT_EDGES;  </span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> graphlab::NO_EDGES; </span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// Make sure surrounding vertices are scheduled </span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">scatter</span><span class="params">(icontext_type&amp; context, <span class="type">const</span> vertex_type&amp; vertex, edge_type&amp; edge)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">        context.<span class="built_in">signal</span>(edge.<span class="built_in">target</span>()); </span><br><span class="line">    &#125; </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>同步并行执行</p>
<ul>
<li>顶点的局部邻域（顶点的“范围”）可以由顶点程序读取和写入</li>
<li>程序指定他们希望 GraphLab 运行时提供的原子性粒度（“一致性”）：这决定了可用并行性的数量<ul>
<li>“完全一致性”：实现确保在 v 的顶点程序运行时没有其他执行读取或写入 v 范围内的数据。</li>
<li>“边缘一致性”：没有其他执行读取或写入 v 中或与 v 相邻的边缘中的任何数据</li>
<li>“顶点一致性”：没有其他执行读取或写入 v …</li>
</ul>
</li>
</ul>
<p>GraphLab 实现了几种工作调度策略</p>
<ul>
<li>同步：同时更新所有顶点（顶点程序没有观察到在同一“轮”中运行在其他顶点上的程序的更新）</li>
<li>循环：顶点程序观察最近的更新</li>
<li>图形着色</li>
<li>动态：基于信号创建的新作品</li>
</ul>
<p>应用程序开发人员可以灵活选择一致性保证和调度策略</p>
<ul>
<li>含义：调度的选择会影响程序的正确性/输出</li>
</ul>
<p>大规模图的内存占用挑战</p>
<ul>
<li>挑战：对于大规模图，无法在内存中拟合所有边<br>（图形顶点可能适合）</li>
<li>考虑图形表示：<ul>
<li>每条边在图形结构中表示两次（作为输入/输出边）</li>
<li>每条边 8 个字节表示邻接</li>
</ul>
</li>
<li>可能还需要存储每条边的值（例如，每条边的权重为 4 个字节）<ul>
<li>10 亿条边（适度）：约 12 GB 内存用于边信息</li>
</ul>
</li>
<li>算法可能需要每个边结构的多个副本（当前、上一个数据等）</li>
<li>可以使用机器集群在内存中存储图形<ul>
<li>而不是在磁盘上存储图形</li>
</ul>
</li>
<li>更愿意在一台机器上处理大图<ul>
<li>管理机器集群很困难</li>
<li>分区图很昂贵（也需要大量内存）并且很困难</li>
</ul>
</li>
</ul>
<p>“流式”图形计算</p>
<ul>
<li>图操作“随机”访问图数据（与顶点 v 相邻的边可以在整个存储中任意分布）<ul>
<li>单次遍历图的边缘可能会对磁盘进行数十亿次细粒度访问</li>
</ul>
</li>
<li>流数据访问模式<ul>
<li>对慢速存储进行大型、可预测的数据访问（实现高带宽数据传输）</li>
<li>将数据从慢速存储加载到快速存储中，然后在丢弃之前尽可能多地重复使用（实现高算术强度）</li>
</ul>
</li>
</ul>
<p>分片图表示</p>
<ul>
<li>将图顶点划分为区间（调整大小以便区间的子图适合内存）</li>
<li>存储顶点并且只有这些顶点的传入边被一起存储在一个分片中</li>
<li>按源顶点 id 对分片中的边进行排序</li>
</ul>
<p><img src="/img/1640508026.jpg" alt=""></p>
<p>图压缩</p>
<ul>
<li>回忆：图操作通常受 BW 限制</li>
<li>含义：使用 CPU 指令来降低 BW 要求可以提高整体性能（无论如何处理器都在等待内存！）</li>
<li>想法：将压缩的图形存储在内存中，当操作想要读取数据时即时解压</li>
</ul>
<p>一个压缩的例子，用边与边的差压缩<br><img src="/img/1640508284.jpg" alt=""></p>
<h1 id="lecture-22"><a href="#lecture-22" class="headerlink" title="lecture 22"></a>lecture 22</h1><p>针对大量数据，让我们设计一个runMapReduceJob的实现</p>
<p>步骤1：运行mapper函数<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// called once per line in file </span></span><br><span class="line"><span class="type">void</span> <span class="title function_">mapper</span><span class="params">(<span class="built_in">string</span> line, <span class="built_in">multimap</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt;&amp; results)</span> &#123;</span><br><span class="line">    <span class="built_in">string</span> user_agent = parse_requester_user_agent(line);</span><br><span class="line">    <span class="keyword">if</span> (is_mobile_client(user_agent)) </span><br><span class="line">        results.add(user_agent, <span class="number">1</span>); </span><br><span class="line">&#125; </span><br><span class="line"><span class="comment">// called once per unique key in results </span></span><br><span class="line"><span class="type">void</span> <span class="title function_">reducer</span><span class="params">(<span class="built_in">string</span> key, <span class="built_in">list</span>&lt;<span class="built_in">string</span>&gt; values, <span class="type">int</span>&amp; result)</span> &#123; </span><br><span class="line">    <span class="type">int</span> sum = <span class="number">0</span>; </span><br><span class="line">    <span class="keyword">for</span> (v in values) </span><br><span class="line">       sum += v; </span><br><span class="line">    result = sum; </span><br><span class="line">&#125; </span><br><span class="line">LineByLineReader <span class="title function_">input</span><span class="params">(“hdfs:<span class="comment">//15418log.txt”); </span></span></span><br><span class="line"><span class="params">Writer output(“hdfs:<span class="comment">//…”); </span></span></span><br><span class="line"><span class="params">runMapReduceJob(mapper, reducer, input, output);</span></span><br></pre></td></tr></table></figure></p>
<p>步骤1：在文件的所有行上运行mapper函数</p>
<ul>
<li>问题：如何将工作分配给节点？</li>
<li>想法1：使用输入块列表的工作队列来处理动态分配：空闲节点获取下一个可用块</li>
<li>想法2：基于数据分布的分配：每个节点处理本地存储的输入文件块中的行。</li>
</ul>
<p><img src="/img/1641113089.jpg" alt=""></p>
<p>步骤2和3：收集数据，运行规约器<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// called once per line in file </span></span><br><span class="line"><span class="type">void</span> <span class="title function_">mapper</span><span class="params">(<span class="built_in">string</span> line, <span class="built_in">map</span>&lt;<span class="built_in">string</span>,<span class="built_in">string</span>&gt; results)</span> &#123;</span><br><span class="line">    <span class="built_in">string</span> user_agent = parse_requester_user_agent(line);  </span><br><span class="line">    <span class="keyword">if</span> (is_mobile_client(user_agent)) </span><br><span class="line">        results.add(user_agent, <span class="number">1</span>); </span><br><span class="line">&#125; </span><br><span class="line"><span class="comment">// called once per unique key in results </span></span><br><span class="line"><span class="type">void</span> <span class="title function_">reducer</span><span class="params">(<span class="built_in">string</span> key, <span class="built_in">list</span>&lt;<span class="built_in">string</span>&gt; values, <span class="type">int</span>&amp; result)</span> &#123; </span><br><span class="line">    <span class="type">int</span> sum = <span class="number">0</span>; </span><br><span class="line">    <span class="keyword">for</span> (v in values) </span><br><span class="line">       sum += v; </span><br><span class="line">    result = sum; </span><br><span class="line">&#125; </span><br><span class="line">LineByLineReader <span class="title function_">input</span><span class="params">(“hdfs:<span class="comment">//15418log.txt”); </span></span></span><br><span class="line"><span class="params">Writer output(“hdfs:<span class="comment">//…”); </span></span></span><br><span class="line"><span class="params">runMapReduceJob(mapper, reducer, input, output);</span></span><br></pre></td></tr></table></figure></p>
<ul>
<li>步骤2：为减速器准备中间数据</li>
<li>步骤3：在所有键上运行规约器功能<ul>
<li>问题：如何分配任务？</li>
<li>问题：如何将密钥的所有数据获取到正确的工作节点上？</li>
</ul>
</li>
</ul>
<p><img src="/img/1641113220.jpg" alt=""></p>
<p>作业调度器职责</p>
<ul>
<li>利用数据局部性：“将计算移动到数据”<ul>
<li>在包含输入文件的节点上运行mapper作业</li>
<li>在已经具有某个键的大部分数据的节点上运行reducer作业</li>
</ul>
</li>
<li>处理节点故障<ul>
<li>计划程序检测作业失败并在新计算机上重新运行作业</li>
<li>这是可能的，因为输入驻留在持久存储（分布式文件系统）中</li>
<li>调度器在多台计算机上复制作业（减少节点故障引起的总体处理延迟）</li>
</ul>
</li>
<li>处理速度慢的机器<ul>
<li>调度程序在多台计算机上复制作业</li>
</ul>
</li>
</ul>
<p>spark：内存中的容错分布式计算</p>
<ul>
<li>目标<ul>
<li>集群规模计算的编程模型，其中中间数据集的重用非常重要</li>
<li>迭代机器学习与图算法</li>
<li>交互式数据挖掘：将大型数据集加载到集群的聚合内存中，然后执行多个即时查询</li>
<li>不希望导致将中间文件写入持久分布式文件系统的效率低下（希望将其保留在内存中）</li>
</ul>
</li>
<li>挑战：高效实现大规模分布式内存计算的容错。<ul>
<li>复制所有计算<ul>
<li>昂贵的解决方案：降低峰值吞吐量</li>
</ul>
</li>
<li>检查点和回滚<ul>
<li>定期将程序状态保存到永久性存储器</li>
<li>从节点失败时的最后一个检查点重新启动</li>
</ul>
</li>
<li>维护更新日志（命令和数据）<ul>
<li>维护日志的高开销</li>
</ul>
</li>
<li>map-reduce解决方案：<ul>
<li>通过将结果写入文件系统，在每个映射/减少步骤后设置检查点</li>
<li>调度程序的未完成（但尚未完成）作业列表是一个日志</li>
<li>程序的功能结构允许以单个映射器或reducer调用的粒度重新启动（不必重新启动整个程序）</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>弹性分布式数据集（RDD）是Spark的关键编程抽象：</p>
<ul>
<li>记录的只读集合（不可变）</li>
<li>RDD只能通过对持久存储或现有RDD中的数据进行确定性转换来创建</li>
<li>RDD上的操作将数据返回到应用程序</li>
</ul>
<p>Spark样例<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create RDD from file system data </span></span><br><span class="line">var lines = spark.<span class="built_in">textFile</span>(“hdfs:<span class="comment">//15418log.txt”); </span></span><br><span class="line"><span class="comment">// create RDD using filter() transformation on lines </span></span><br><span class="line">var mobileViews = lines.<span class="built_in">filter</span>((x: String) =&gt; <span class="built_in">isMobileClient</span>(x)); </span><br><span class="line"><span class="comment">// instruct Spark runtime to try to keep mobileViews in memory </span></span><br><span class="line">mobileViews.<span class="built_in">persist</span>(); </span><br><span class="line"><span class="comment">// create a new RDD by filtering mobileViews </span></span><br><span class="line"><span class="comment">// then count number of elements in new RDD via count() action </span></span><br><span class="line">var numViews = mobileViews.<span class="built_in">filter</span>(_.<span class="built_in">contains</span>(“Safari”)).<span class="built_in">count</span>(); </span><br><span class="line"><span class="comment">// 1. create new RDD by filtering only Chrome views </span></span><br><span class="line"><span class="comment">// 2. for each element, split string and take timestamp of </span></span><br><span class="line"><span class="comment">//    page view </span></span><br><span class="line"><span class="comment">// 3. convert RDD to a scalar sequence (collect() action) </span></span><br><span class="line">var timestamps = mobileViews.<span class="built_in">filter</span>(_.<span class="built_in">contains</span>(“Chrome”)) </span><br><span class="line">                            .<span class="built_in">map</span>(_.<span class="built_in">split</span>(“ ”)(<span class="number">0</span>)) </span><br><span class="line">                            .<span class="built_in">collect</span>();</span><br></pre></td></tr></table></figure></p>
<h1 id="lecture-23"><a href="#lecture-23" class="headerlink" title="lecture 23"></a>lecture 23</h1><p>编写良好的程序利用局部性来避免CPU和内存之间的冗余数据传输（关键思想：将频繁访问的数据放在处理器附近的缓存/缓冲区中）</p>
<ul>
<li>现代处理器具有对本地内存的高带宽（低延迟）访问<ul>
<li>具有数据访问局部性的计算可以重用局部存储器中的数据</li>
</ul>
</li>
<li>软件优化技术：对计算进行重新排序，以便缓存数据在被逐出之前被多次访问</li>
<li>有性能意识的程序员努力改进程序的缓存位置</li>
</ul>
<p>示例1：为局部性重新构造循环<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Program1</span><br><span class="line"><span class="type">void</span> <span class="title function_">add</span><span class="params">(<span class="type">int</span> n, <span class="type">float</span>* A, <span class="type">float</span>* B, <span class="type">float</span>* C)</span> &#123; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;n; i++) </span><br><span class="line">       C[i] = A[i] + B[i];</span><br><span class="line">&#125; </span><br><span class="line"><span class="type">void</span> <span class="title function_">mul</span><span class="params">(<span class="type">int</span> n, <span class="type">float</span>* A, <span class="type">float</span>* B, <span class="type">float</span>* C)</span> &#123; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;n; i++) </span><br><span class="line">       C[i] = A[i] * B[i];     </span><br><span class="line">&#125; </span><br><span class="line"><span class="type">float</span>* A, *B, *C, *D, *E, *tmp1, *tmp2; </span><br><span class="line"><span class="comment">// assume arrays are allocated here </span></span><br><span class="line"><span class="comment">// compute E = D + ((A + B) * C) </span></span><br><span class="line">add(n, A, B, tmp1); </span><br><span class="line">mul(n, tmp1, C, tmp2); </span><br><span class="line">add(n, tmp2, D, E);</span><br><span class="line"></span><br><span class="line">Program2</span><br><span class="line"><span class="type">void</span> <span class="title function_">fused</span><span class="params">(<span class="type">int</span> n, <span class="type">float</span>* A, <span class="type">float</span>* B, <span class="type">float</span>* C, <span class="type">float</span>* D, <span class="type">float</span>* E)</span> &#123; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;n; i++) </span><br><span class="line">       E[i] = D[i] + (A[i] + B[i]) * C[i];     </span><br><span class="line">&#125; </span><br><span class="line"><span class="comment">// compute E = D + (A + B) * C </span></span><br><span class="line">fused(n, A, B, C, D, E);</span><br></pre></td></tr></table></figure></p>
<p>Program1两次load，一次运算，计算密集度是0.333；而Program2有4次load和3次运算，密集度为0.6。</p>
<p>下图中是内存系统示意图。<br><img src="/img/1641114333.jpg" alt=""></p>
<p>DRAM中每行有2K个bit，缓冲区有2K个bit。<br><img src="/img/1641114413.png" alt=""></p>
<p>当需要一个Byte时，首先找到这个byte所在的一行，预先充电激活这一行，将它复制到缓冲区，选出这一个byte所在的列，发送到总线。当继续需要这一行的其他byte时，可以从缓冲区中直接拿。<br><img src="/img/1641114552.jpg" alt=""></p>
<p>DRAM访问延迟不是固定的</p>
<ul>
<li>最佳情况延迟：从活动行读取<ul>
<li>列访问时间（CAS）</li>
</ul>
</li>
<li>最坏情况延迟：位线未就绪，从新行读取<ul>
<li>预充电（PRE）+行激活（RAS）+列访问（CAS）</li>
<li>预充电准备位线并将行缓冲区内容写回DRAM阵列（读取是破坏性的）</li>
</ul>
</li>
<li>问题1：何时执行预充电？<ul>
<li>每列访问之后？</li>
<li>仅当访问新行时？</li>
</ul>
</li>
<li>问题2：如何处理DRAM访问的延迟？</li>
</ul>
<p>问题：由于访问延迟，只有在数据发送到总线时才用到引脚，引脚利用率低。可以通过将多个字节合并发送提高利用率<br><img src="/img/1641114971.jpg" alt=""></p>
<p>DRAM芯片由多个存储组组成</p>
<ul>
<li>所有存储组共享相同的PIN总线</li>
<li>存储组允许内存请求的流水线<ul>
<li>预充电/激活行/向存储组发送列地址，同时从另一存储组传输数据</li>
<li>实现高数据引脚利用率</li>
</ul>
</li>
</ul>
<p><img src="/img/1641115105.jpg" alt=""></p>
<p>将多个芯片组织到一个DIMM中</p>
<ul>
<li>示例：八个DRAM芯片（64位内存总线）</li>
<li>注意：DIMM显示为内存控制器的单个、更大容量、更宽接口DRAM模块。更高的聚合带宽，但最小传输粒度现在是64位。</li>
</ul>
<p><img src="/img/1641115168.jpg" alt=""></p>
<p>读取一条64字节（512位）cache line</p>
<ul>
<li>内存控制器将物理地址转换为DRAM组、行、列</li>
<li>物理地址以字节粒度在DRAM芯片之间交错</li>
<li>DRAM芯片并行传输前64位</li>
</ul>
<p><img src="/img/1641115292.jpg" alt=""></p>
<p>DRAM控制器从新列请求数据，DRAM芯片并行传输下一个64位<br><img src="/img/1641115359.jpg" alt=""></p>
<p>内存控制器是一个内存请求调度器</p>
<ul>
<li>从Last level cache(LLC)接收加载/存储请求</li>
<li>冲突的调度目标<ul>
<li>最大化吞吐量，最小化延迟，最小化能耗</li>
<li>通用调度策略：FR-FCFS（先准备，先到先服务）</li>
<li>当前先打开行的服务请求（最大化行位置）</li>
<li>以FIFO顺序向其他行发送服务请求</li>
<li>控制器可以将多个小请求合并成大的连续请求（利用DRAM的“burst模式”）</li>
</ul>
</li>
</ul>
<p>双通道存储系统</p>
<ul>
<li>通过添加内存通道提高吞吐量（有效地拓宽总线）</li>
<li>下面：每个通道可以发出独立的命令<ul>
<li>在每个通道中读取不同的行/列</li>
<li>更简单的设置：使用单个控制器将同一命令驱动到多个通道</li>
</ul>
</li>
</ul>
<p><img src="/img/1641115849.jpg" alt=""></p>
<p>嵌入式DRAM（eDRAM）：另一个层次的内存层次结构</p>
<ul>
<li>Intel Broadwell/Skylake处理器的CPU包中包含128 MB的嵌入式DRAM（eDRAM）</li>
<li>50 GB/s读取+50 GB/s写入</li>
</ul>
<p><img src="/img/1641115940.jpg" alt=""></p>
<p>通过芯片堆叠增加带宽，降低功耗</p>
<ul>
<li>使能技术：DRAM芯片的3D堆叠<ul>
<li>DRAM通过穿过芯片的硅通孔（TSV）连接</li>
<li>TSV在逻辑层和DRAM之间提供高度并行连接</li>
<li>堆栈的底层“逻辑层”是内存控制器，管理来自处理器的请求</li>
<li>硅“插入器”用作DRAM堆栈和处理器之间的高带宽互连</li>
</ul>
</li>
</ul>
<p><img src="/img/1641116056.jpg" alt=""></p>
<p>想法：在没有处理器的情况下执行复制，修改内存系统以支持加载、存储和大容量复制。</p>
<ol>
<li>激活A行</li>
<li>传输行</li>
<li>激活B行</li>
<li>传输行</li>
</ol>
<p><img src="/img/1641116258.jpg" alt=""></p>
<p>缓存压缩</p>
<ul>
<li>想法：通过压缩驻留在缓存中的数据提高缓存的有效容量<ul>
<li>想法：扩展计算（压缩/解压缩）以节省带宽</li>
<li>缓存命中次数越多=传输次数越少</li>
</ul>
</li>
<li>必须使用硬件压缩/解压缩方案<ul>
<li>简单到可以在硬件中实现</li>
<li>快一点：解压在负载的关键路径上</li>
<li>无法显著增加缓存命中延迟</li>
</ul>
</li>
</ul>
<p>一个拟议的例子：B∆I压缩[Pekhimenko 12]</p>
<ul>
<li>观察：位于cache line的数据通常具有较低的动态范围（使用base+offset对一行中的位块进行编码）</li>
<li>如何快速找到较好的base？<ul>
<li>使用第一行中的第一个字</li>
<li>行的压缩/解压缩是数据并行的</li>
</ul>
</li>
</ul>
<p>一个0和一个包含八个1字节差异的数组。因此，整个cache line数据可以使用12个字节而不是32个字节来表示，从而节省了最初使用的20个字节的空间。<br><img src="/img/1641116650.jpg" alt=""></p>
<p>总结：内存墙正在以多种方式解决</p>
<ul>
<li>由应用程序程序员编写<ul>
<li>安排计算以最大化局部性（最小化所需的数据移动）</li>
</ul>
</li>
<li>通过新的硬件架构<ul>
<li>智能DRAM请求调度</li>
<li>使数据更接近处理器（深度缓存层次结构，eDRAM）</li>
<li>增加带宽（更宽的内存系统、3D内存堆叠）</li>
<li>在内存中或内存附近定位有限形式计算的持续研究</li>
<li>正在进行的硬件加速压缩研究</li>
</ul>
</li>
<li>一般原则<ul>
<li>在处理器附近定位数据存储器</li>
<li>将计算转移到数据存储</li>
<li>数据压缩（为减少数据传输而权衡额外计算）</li>
</ul>
</li>
</ul>
<h1 id="lecture-24"><a href="#lecture-24" class="headerlink" title="lecture 24"></a>lecture 24</h1><p>几种块状稠密矩阵乘法<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">0</span>; j&lt;BLOCKSIZE_J; j++) &#123; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;BLOCKSIZE_I; i+=SIMD_WIDTH) &#123; </span><br><span class="line">        simd_vec C_accum = <span class="built_in">vec_load</span>(&amp;C[jblock+j][iblock+i]); </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k=<span class="number">0</span>; k&lt;BLOCKSIZE_K; k++) &#123; </span><br><span class="line">            <span class="comment">// C = A*B + C </span></span><br><span class="line">            simd_vec A_val = <span class="built_in">splat</span>(&amp;A[jblock+j][kblock+k]); <span class="comment">// load a single element in vector register </span></span><br><span class="line">            <span class="built_in">simd_muladd</span>(A_val, <span class="built_in">vec_load</span>(&amp;B[kblock+k][iblock+i]), C_accum); </span><br><span class="line">        &#125; </span><br><span class="line">        <span class="built_in">vec_store</span>(&amp;C[jblock+j][iblock+i], C_accum); </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/1641118923.jpg" alt=""></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// assume blocks of A and C are pre-­‐transposed as Atrans </span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">0</span>; j&lt;BLOCKSIZE_J; j+=SIMD_WIDTH) &#123; </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;BLOCKSIZE_I; i++) &#123; </span><br><span class="line">        simd_vec C_accum = <span class="built_in">vec_load</span>(&amp;Ctrans[iblock+i][jblock+j]); </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k=<span class="number">0</span>; k&lt;BLOCKSIZE_K; k++) &#123; </span><br><span class="line">            <span class="comment">// C = A*B + C </span></span><br><span class="line">            simd_vec A_val =);  </span><br><span class="line">            <span class="built_in">simd_muladd</span>(<span class="built_in">vec_load</span>(&amp;Atrans[kblock+k][jblock+j], <span class="built_in">vec_load</span>(&amp;B[kblock+k][iblock+i]), C_accum); </span><br><span class="line">        &#125; </span><br><span class="line">        <span class="built_in">vec_store</span>(&amp;Ctrans[iblock+i][jblock+j], C_accum); </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/img/1641118955.jpg" alt=""></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> j=<span class="number">0</span>; j&lt;BLOCKSIZE_J; j++) </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;BLOCKSIZE_I; i++) &#123; </span><br><span class="line">        <span class="type">float</span> C_scalar = C[jblock+j][iblock+i]; </span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k=<span class="number">0</span>; k&lt;BLOCKSIZE_K; k+=SIMD_WIDTH) &#123; </span><br><span class="line">            <span class="comment">// C_scalar = dot(A,B) + C_scalar</span></span><br><span class="line">            C_scalar += <span class="built_in">simd_dot</span>(<span class="built_in">vec_load</span>(&amp;A[jblock+j][kblock+k]), <span class="built_in">vec_load</span>(&amp;Btrans[iblock+i][[kblock+k]); </span><br><span class="line">        &#125; </span><br><span class="line">        C[jblock+j][iblock+i] = C_scalar; </span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p><img src="/img/1641118987.jpg" alt=""></p>
<p>训练</p>
<ul>
<li>目标：学习网络参数的良好值，以便网络输出任何输入图像的正确分类结果</li>
<li>想法：尽量减少所有示例的损失（已知正确答案）</li>
<li>直觉：如果网络对各种培训示例的答案都是正确的，然后，希望它已经了解了参数值，这些参数值可以为将来提供正确的答案还有图像。</li>
</ul>
<p>梯度下降</p>
<ul>
<li>假设您有一个包含隐藏参数p1和p2的函数f</li>
<li>对于一些输入x，你的训练数据说函数应该输出0。</li>
<li>但对于p1和p2的当前值，它当前输出10。</li>
<li>假设我也给出了f的导数的表达式和P1和P2，这样你就可以计算它们在x的值。</li>
<li>如何调整值p1和p2以减少此示例的错误？</li>
</ul>
<p>基本梯度下降<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">while (loss too high): </span><br><span class="line">    for each item x_i in training set: </span><br><span class="line">        grad += evaluate_loss_gradient(f, loss_func, params, x_i) </span><br><span class="line">    params += -­‐grad * step_size;</span><br></pre></td></tr></table></figure></p>
<p>小批量随机梯度下降（Mini-batch SGD）：选择训练示例的随机（小）子集，在while循环的每次迭代中计算梯度</p>
<p>集群规模计算的挑战</p>
<ul>
<li>节点间通信速度慢<ul>
<li>集群没有超级计算机典型的高性能互连</li>
</ul>
</li>
<li>具有不同性能的节点（即使计算机相同）<ul>
<li>屏障处的工作负载不平衡（节点之间的同步点）</li>
</ul>
</li>
<li>现代解决方案：利用异步执行的SGD特性！</li>
</ul>
<p>设置参数服务器，有多个worker，将数据分块拷贝到worker，参数的拷贝复制到workers，worker自己计算自己数据集上的梯度，再把梯度合并到参数服务器上，<code>params += -subgrad * step_size</code>。<br><img src="/img/1641121643.jpg" alt=""></p>
<p>摘要：异步参数更新</p>
<ul>
<li>想法：避免每次SGD迭代之间所有参数更新的全局同步<ul>
<li>设计反映了群集计算的现实：</li>
<li>慢互连</li>
<li>不可预测的机器性能</li>
</ul>
</li>
<li>解决方案：异步（和部分）次梯度更新</li>
<li>将影响SGD的汇合<ul>
<li>在迭代i上工作的节点N可能没有导致i-1之前SGD迭代结果的参数值</li>
</ul>
</li>
</ul>
<p>切分参数服务器</p>
<ul>
<li>跨服务器的分区参数</li>
<li>Worker将子渐变块发送到所属参数服务器</li>
</ul>
<p><img src="/img/1641122330.jpg" alt=""></p>
<p>Parallelizing mini-batch on one machine<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for each item x_i in mini-­‐batch: </span><br><span class="line">    grad += evaluate_loss_gradient(f, loss_func, params, x_i) </span><br><span class="line">params += -­‐grad * step_size;</span><br></pre></td></tr></table></figure></p>
<p>Asynchronous update on one node<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for each item x_i in mini-­‐batch: </span><br><span class="line">    grad += evaluate_loss_gradient(f, loss_func, params, x_i) </span><br><span class="line">params += -­‐grad * step_size;</span><br></pre></td></tr></table></figure></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%A7%AF%E7%B4%AF/" rel="tag"># 积累</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/12/17/cs267/" rel="prev" title="CS267 并行计算应用课程笔记">
      <i class="fa fa-chevron-left"></i> CS267 并行计算应用课程笔记
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/12/19/%E5%9C%A8Linux%E4%B8%AD%E4%BD%BF%E7%94%A8%E7%BA%BF%E7%A8%8B/" rel="next" title="在Linux中使用线程">
      在Linux中使用线程 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-1"><span class="nav-number">1.</span> <span class="nav-text">lecture 1</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-2"><span class="nav-number">2.</span> <span class="nav-text">lecture 2</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-3"><span class="nav-number">3.</span> <span class="nav-text">lecture 3</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-4"><span class="nav-number">4.</span> <span class="nav-text">lecture 4</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-5"><span class="nav-number">5.</span> <span class="nav-text">lecture 5</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-6"><span class="nav-number">6.</span> <span class="nav-text">lecture 6</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-7"><span class="nav-number">7.</span> <span class="nav-text">lecture 7</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-8"><span class="nav-number">8.</span> <span class="nav-text">lecture 8</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-9"><span class="nav-number">9.</span> <span class="nav-text">lecture 9</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-10"><span class="nav-number">10.</span> <span class="nav-text">lecture 10</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-11"><span class="nav-number">11.</span> <span class="nav-text">lecture 11</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-12"><span class="nav-number">12.</span> <span class="nav-text">lecture 12</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AD%BB%E9%94%81%E6%B4%BB%E9%94%81%E5%92%8C%E9%A5%A5%E9%A5%BF"><span class="nav-number">12.1.</span> <span class="nav-text">死锁活锁和饥饿</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%91%E5%90%AC%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0%EF%BC%88%E5%81%87%E8%AE%BE%E6%98%AF%E5%8E%9F%E5%AD%90%E6%80%BB%E7%BA%BF%EF%BC%89"><span class="nav-number">12.2.</span> <span class="nav-text">监听的基本实现（假设是原子总线）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%B4%E7%BB%95%E9%9D%9E%E5%8E%9F%E5%AD%90%E6%80%BB%E7%BA%BF%E4%BA%8B%E5%8A%A1%E6%9E%84%E5%BB%BA%E7%B3%BB%E7%BB%9F"><span class="nav-number">12.3.</span> <span class="nav-text">围绕非原子总线事务构建系统</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-13"><span class="nav-number">13.</span> <span class="nav-text">lecture 13</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-14"><span class="nav-number">14.</span> <span class="nav-text">lecture 14</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-15"><span class="nav-number">15.</span> <span class="nav-text">lecture 15</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-16"><span class="nav-number">16.</span> <span class="nav-text">lecture 16</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-17"><span class="nav-number">17.</span> <span class="nav-text">lecture 17</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-18"><span class="nav-number">18.</span> <span class="nav-text">lecture 18</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-19"><span class="nav-number">19.</span> <span class="nav-text">lecture 19</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-20"><span class="nav-number">20.</span> <span class="nav-text">lecture 20</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-21"><span class="nav-number">21.</span> <span class="nav-text">lecture 21</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-22"><span class="nav-number">22.</span> <span class="nav-text">lecture 22</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-23"><span class="nav-number">23.</span> <span class="nav-text">lecture 23</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lecture-24"><span class="nav-number">24.</span> <span class="nav-text">lecture 24</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Hao Yu</p>
  <div class="site-description" itemprop="description">Introduce something interesting and recode learning process, some articles are written by others, the original link has been given as much as possible, thanks to the original author</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">128</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hao Yu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
