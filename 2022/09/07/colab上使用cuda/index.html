<!DOCTYPE html>
<html lang="zn-ch">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1-dkuFrfver70j-UCpAp0XQZszsJexHkH#scrollTo&#x3D;GJHxLzmGfb3G CUDA编程模型为应用和硬件设备之间的桥梁，所以CUDA C是编译型语言，不是解释型语言，OpenCL就有点类似于解释型语言，通过编译器和链接，给操作系统执行（操作系统包括GPU在内的系统） 首先安装插件并加">
<meta property="og:type" content="article">
<meta property="og:title" content="Colab上使用cuda">
<meta property="og:url" content="http://yoursite.com/2022/09/07/colab%E4%B8%8A%E4%BD%BF%E7%94%A8cuda/index.html">
<meta property="og:site_name" content="Hao Yu&#39;s blog">
<meta property="og:description" content="https:&#x2F;&#x2F;colab.research.google.com&#x2F;drive&#x2F;1-dkuFrfver70j-UCpAp0XQZszsJexHkH#scrollTo&#x3D;GJHxLzmGfb3G CUDA编程模型为应用和硬件设备之间的桥梁，所以CUDA C是编译型语言，不是解释型语言，OpenCL就有点类似于解释型语言，通过编译器和链接，给操作系统执行（操作系统包括GPU在内的系统） 首先安装插件并加">
<meta property="og:locale" content="zn_CH">
<meta property="og:image" content="http://yoursite.com/img/20220907141600.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141602.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141612.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141613.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141614.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141615.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141616.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141617.png">
<meta property="og:image" content="http://yoursite.com/img/20220907141618.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910105700518.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910105840559.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910110833322.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910111100141.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910112226266.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910112303270.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910113212684.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910113231769.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910113248353.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910113311093.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910113413684.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910113606985.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910113619337.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910113636979.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910113651178.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910113706123.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910144503244.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910144646230.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910151624832.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910152219591.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910152231251.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910152242868.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910152701764.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910154608530.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910154618890.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910154630133.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910154641152.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910154652275.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910154710457.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910154956112.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910155010929.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910162611009.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910162638841.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910162656480.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910164710321.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910174756825.png">
<meta property="article:published_time" content="2022-09-07T06:10:00.000Z">
<meta property="article:modified_time" content="2024-01-25T06:51:56.659Z">
<meta property="article:author" content="Hao Yu">
<meta property="article:tag" content="C++">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/img/20220907141600.png">

<link rel="canonical" href="http://yoursite.com/2022/09/07/colab%E4%B8%8A%E4%BD%BF%E7%94%A8cuda/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zn-ch'
  };
</script>

  <title>Colab上使用cuda | Hao Yu's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hao Yu's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">The program monkey was eaten by the siege lion.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/resume.pdf" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">11</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">0</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">128</span></a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/yuhao0102" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zn-ch">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/09/07/colab%E4%B8%8A%E4%BD%BF%E7%94%A8cuda/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Hao Yu">
      <meta itemprop="description" content="Introduce something interesting and recode learning process, some articles are written by others, the original link has been given as much as possible, thanks to the original author">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hao Yu's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Colab上使用cuda
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-09-07 14:10:00" itemprop="dateCreated datePublished" datetime="2022-09-07T14:10:00+08:00">2022-09-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-01-25 14:51:56" itemprop="dateModified" datetime="2024-01-25T14:51:56+08:00">2024-01-25</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a href="https://colab.research.google.com/drive/1-dkuFrfver70j-UCpAp0XQZszsJexHkH#scrollTo=GJHxLzmGfb3G">https://colab.research.google.com/drive/1-dkuFrfver70j-UCpAp0XQZszsJexHkH#scrollTo=GJHxLzmGfb3G</a></p>
<p>CUDA编程模型为应用和硬件设备之间的桥梁，所以CUDA C是编译型语言，不是解释型语言，OpenCL就有点类似于解释型语言，通过编译器和链接，给操作系统执行（操作系统包括GPU在内的系统）</p>
<p>首先安装插件并加载：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!git config --global http.sslVerify&quot;False&quot;</span><br><span class="line">!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git</span><br><span class="line">%load_ext nvcc_plugin</span><br></pre></td></tr></table></figure></p>
<p>从hello world开始：要在笔记本中运行代码，请在代码的开头添加%%cu扩展名。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">%%cu</span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="comment">//kernel function</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">helloFromGPU</span> <span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Hello World from GPU!\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  helloFromGPU &lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">  <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>__global__</code>：声明一个函数作为一个存在的kernel。这样的一个函数是：</p>
<ol>
<li>在设备上执行的，</li>
<li>仅可从主机调用。</li>
<li>其调用形式为：<code>helloFromGPU&lt;&lt;&lt;1,10&gt;&gt;&gt;();</code></li>
</ol>
<p>一个kernel是由一组线程执行，所有线程执行相同的代码。上面一行三对尖括号中的1和10 ，表示启动一个 grid 为 螺纹块 的内核。执行配置中的第一个参数1指定网格中线程块的数量，第二个参数10指定线程块中的线程数。</p>
<p>有一点需要注意的是，printf的输出是在GPU内部执行的，你若想在控制台（网页上）收到该输出，你必须添加<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaDeviceSynchronize();</span><br></pre></td></tr></table></figure></p>
<p>矩阵加法：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">%%cu</span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#<span class="keyword">define</span> VECTOR_LENGTH 10000 </span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MAX_ERR 1e-4</span></span><br><span class="line"> </span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">vector_add</span><span class="params">(<span class="type">float</span> *out, <span class="type">float</span> *a, <span class="type">float</span> *b, <span class="type">int</span> n)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        out[i] = a[i] + b[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">float</span> *a, *b, *out;</span><br><span class="line">    <span class="type">float</span> *d_a, *d_b, *d_out; </span><br><span class="line"> </span><br><span class="line">    a = (<span class="type">float</span>*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(<span class="type">float</span>) * VECTOR_LENGTH);</span><br><span class="line">    b = (<span class="type">float</span>*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(<span class="type">float</span>) * VECTOR_LENGTH);</span><br><span class="line">    out = (<span class="type">float</span>*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(<span class="type">float</span>) * VECTOR_LENGTH);</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; VECTOR_LENGTH; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        a[i] = <span class="number">3.0f</span>;</span><br><span class="line">        b[i] = <span class="number">0.14f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span>**)&amp;d_a, <span class="built_in">sizeof</span>(<span class="type">float</span>) * VECTOR_LENGTH);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span>**)&amp;d_b, <span class="built_in">sizeof</span>(<span class="type">float</span>) * VECTOR_LENGTH);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span>**)&amp;d_out, <span class="built_in">sizeof</span>(<span class="type">float</span>) * VECTOR_LENGTH);</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_a, a, <span class="built_in">sizeof</span>(<span class="type">float</span>) * VECTOR_LENGTH, cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_b, b, <span class="built_in">sizeof</span>(<span class="type">float</span>) * VECTOR_LENGTH, cudaMemcpyHostToDevice);</span><br><span class="line">    </span><br><span class="line">    vector_add&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(d_out, d_a, d_b, VECTOR_LENGTH);</span><br><span class="line"> </span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(out, d_out, <span class="built_in">sizeof</span>(<span class="type">float</span>) * VECTOR_LENGTH, cudaMemcpyDeviceToHost);</span><br><span class="line">    <span class="comment">// Test the result</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; VECTOR_LENGTH; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%f\n&quot;</span>, out[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaFree</span>(d_a);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_b);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_out);</span><br><span class="line">    <span class="built_in">free</span>(a);</span><br><span class="line">    <span class="built_in">free</span>(b);</span><br><span class="line">    <span class="built_in">free</span>(out);</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>一个异构环境，通常有多个CPU多个GPU，他们都通过PCIe总线相互通信，也是通过PCIe总线分隔开的。所以我们要区分一下两种设备的内存：</p>
<ul>
<li>主机：CPU及其内存</li>
<li>设备：GPU及其内存</li>
</ul>
<p>注意这两个内存从硬件到软件都是隔离的（CUDA6.0 以后支持统一寻址），我们目前先不研究统一寻址，我们现在还是用内存来回拷贝的方法来编写调试程序，以巩固大家对两个内存隔离这个事实的理解。</p>
<p>一个完整的CUDA应用可能的执行顺序如下图：<br><img src="/img/20220907141600.png" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>标准C函数</th>
<th>CUDA C 函数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>malloc</td>
<td>cudaMalloc</td>
<td>内存分配</td>
</tr>
<tr>
<td>memcpy</td>
<td>cudaMemcpy</td>
<td>内存复制</td>
</tr>
<tr>
<td>memset</td>
<td>cudaMemset</td>
<td>内存设置</td>
</tr>
<tr>
<td>free</td>
<td>cudaFree</td>
<td>释放内存</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaMemcpy</span><span class="params">(<span class="type">void</span> * dst,<span class="type">const</span> <span class="type">void</span> * src,<span class="type">size_t</span> count,</span></span><br><span class="line"><span class="params">  cudaMemcpyKind kind)</span></span><br></pre></td></tr></table></figure>
<p>这个函数是内存拷贝过程，可以完成以下几种过程（cudaMemcpyKind kind）</p>
<ul>
<li>cudaMemcpyHostToHost</li>
<li>cudaMemcpyHostToDevice</li>
<li>cudaMemcpyDeviceToHost</li>
<li>cudaMemcpyDeviceToDevice</li>
</ul>
<p>这四个过程的方向可以清楚的从字面上看出来，这里就不废话了，如果函数执行成功，则会返回 cudaSuccess 否则返回 cudaErrorMemoryAllocation</p>
<p>使用下面这个指令可以吧上面的错误代码翻译成详细信息：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">char</span>* <span class="title function_">cudaGetErrorString</span><span class="params">(cudaError_t error)</span></span><br></pre></td></tr></table></figure></p>
<p>内存是分层次的，下图可以简单地描述，但是不够准确，后面我们会详细介绍每一个具体的环节：<br><img src="/img/20220907141602.png" alt=""></p>
<p>共享内存（shared Memory）和全局内存（global Memory）后面我们会特别详细深入的研究，这里我们来个例子，两个向量的加法：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">%%cu</span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(a) a</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">checkResult</span><span class="params">(<span class="type">float</span> *a, <span class="type">float</span> *b, <span class="type">int</span> size)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;size;i++) &#123;</span><br><span class="line">      <span class="keyword">if</span> (a[i] != b[i]) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;the %d is different, %f, %f\n&quot;</span>, i, a[i], b[i]);</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span> </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;the %d is same, %f, %f\n&quot;</span>, i, a[i], b[i]);</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">initialData</span><span class="params">(<span class="type">float</span> *a, <span class="type">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;size;i++)</span><br><span class="line">      a[i] = <span class="number">1.0</span> * i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">sumArrays</span><span class="params">(<span class="type">float</span> * a,<span class="type">float</span> * b,<span class="type">float</span> * res,<span class="type">const</span> <span class="type">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;size;i+=<span class="number">4</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">    res[i<span class="number">+1</span>]=a[i<span class="number">+1</span>]+b[i<span class="number">+1</span>];</span><br><span class="line">    res[i<span class="number">+2</span>]=a[i<span class="number">+2</span>]+b[i<span class="number">+2</span>];</span><br><span class="line">    res[i<span class="number">+3</span>]=a[i<span class="number">+3</span>]+b[i<span class="number">+3</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="type">float</span>*a,<span class="type">float</span>*b,<span class="type">float</span>*res)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> i=threadIdx.x;</span><br><span class="line">  res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> nElem=<span class="number">32</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Vector size:%d\n&quot;</span>,nElem);</span><br><span class="line">  <span class="type">int</span> nByte=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line">  <span class="type">float</span> *a_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *b_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *res_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *res_from_gpu_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="type">float</span> *a_d,*b_d,*res_d;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;a_d,nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;b_d,nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;res_d,nByte));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">initialData</span>(a_h,nElem);</span><br><span class="line">  <span class="built_in">initialData</span>(b_h,nElem);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(a_d,a_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(b_d,b_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(nElem)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d,b_d,res_d);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n&quot;</span>,block.x,grid.x);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  <span class="built_in">sumArrays</span>(a_h,b_h,res_h,nElem);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">checkResult</span>(res_h,res_from_gpu_h,nElem);</span><br><span class="line">  <span class="built_in">cudaFree</span>(a_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(b_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(res_d);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(a_h);</span><br><span class="line">  <span class="built_in">free</span>(b_h);</span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="线程管理"><a href="#线程管理" class="headerlink" title="线程管理"></a>线程管理</h2><p>当内核函数开始执行，如何组织GPU的线程就变成了最主要的问题了，我们必须明确，一个核函数只能有一个grid，一个grid可以有很多个块，每个块可以有很多的线程，这种分层的组织结构使得我们的并行过程更加自如灵活：</p>
<p><img src="/img/20220907141612.png" alt=""></p>
<p>一个线程块block中的线程可以完成下述协作：</p>
<ul>
<li>同步</li>
<li>共享内存</li>
</ul>
<p>不同块内线程不能相互影响！他们是物理隔离的！</p>
<p>接下来就是给每个线程一个编号了，我们知道每个线程都执行同样的一段串行代码，那么怎么让这段相同的代码对应不同的数据呢？首先第一步就是让这些线程彼此区分开，才能对应到相应从线程，使得这些线程也能区分自己的数据。如果线程本身没有任何标记，那么没办法确认其行为。依靠下面两个内置结构体确定线程标号：</p>
<ul>
<li>blockIdx（线程块在线程网格内的位置索引）</li>
<li>threadIdx（线程在线程块内的位置索引）</li>
</ul>
<p>注意这里的Idx是index的缩写（我之前一直以为是identity x的缩写），这两个内置结构体基于 uint3 定义，包含三个无符号整数的结构，通过三个字段来指定：</p>
<ul>
<li><code>blockIdx.x</code></li>
<li><code>blockIdx.y</code></li>
<li><code>blockIdx.z</code></li>
<li><code>threadIdx.x</code></li>
<li><code>threadIdx.y</code></li>
<li><code>threadIdx.z</code></li>
</ul>
<p>上面这两个是坐标，当然我们要有同样对应的两个结构体来保存其范围，也就是blockIdx中三个字段的范围threadIdx中三个字段的范围：</p>
<ul>
<li>blockDim</li>
<li>gridDim</li>
</ul>
<p>他们是dim3类型(基于uint3定义的数据结构)的变量，也包含三个字段x,y,z.</p>
<ul>
<li><code>blockDim.x</code></li>
<li><code>blockDim.y</code></li>
<li><code>blockDim.z</code></li>
</ul>
<p>网格和块的维度一般是二维和三维的，也就是说一个网格通常被分成二维的块，而每个块常被分成三维的线程。</p>
<p>注意：dim3是手工定义的，主机端可见。uint3是设备端在执行的时候可见的，不可以在核函数运行时修改，初始化完成后uint3值就不变了。他们是有区别的！这一点必须要注意。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">%%cu</span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">checkIndex</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;threadIdx:(%d,%d,%d) </span></span><br><span class="line"><span class="string">          blockIdx:(%d,%d,%d) </span></span><br><span class="line"><span class="string">          blockDim:(%d,%d,%d)</span></span><br><span class="line"><span class="string">          gridDim(%d,%d,%d)\n&quot;</span>,</span><br><span class="line">                       threadIdx.x,threadIdx.y,threadIdx.z,</span><br><span class="line">                       blockIdx.x,blockIdx.y,blockIdx.z,</span><br><span class="line">                       blockDim.x,blockDim.y,blockDim.z,</span><br><span class="line">                       gridDim.x,gridDim.y,gridDim.z);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> nElem=<span class="number">6</span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">3</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">((nElem+block.x<span class="number">-1</span>)/block.x)</span></span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;grid.x %d grid.y %d grid.z %d\n&quot;</span>,grid.x,grid.y,grid.z);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;block.x %d block.y %d block.z %d\n&quot;</span>,block.x,block.y,block.z);</span><br><span class="line">  checkIndex&lt;&lt;&lt;grid,block&gt;&gt;&gt;();</span><br><span class="line">  <span class="built_in">cudaDeviceReset</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>输出：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">grid.x 2 grid.y 1 grid.z 1</span><br><span class="line">block.x 3 block.y 1 block.z 1</span><br><span class="line">threadIdx:(0,0,0) blockIdx:(0,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span><br><span class="line">threadIdx:(1,0,0) blockIdx:(0,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span><br><span class="line">threadIdx:(2,0,0) blockIdx:(0,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span><br><span class="line">threadIdx:(0,0,0) blockIdx:(1,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span><br><span class="line">threadIdx:(1,0,0) blockIdx:(1,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span><br><span class="line">threadIdx:(2,0,0) blockIdx:(1,0,0) blockDim:(3,1,1)  gridDim(2,1,1)</span><br></pre></td></tr></table></figure></p>
<h2 id="核函数概述"><a href="#核函数概述" class="headerlink" title="核函数概述"></a>核函数概述</h2><p>核函数就是在CUDA模型上诸多线程中运行的那段串行代码，这段代码在设备上运行，用NVCC编译，产生的机器码是GPU的机器码，所以我们写CUDA程序就是写核函数。</p>
<p>启动核函数，通过的以下的ANSI C 扩展出的CUDA C指令：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;grid,block&gt;&gt;&gt;(argument <span class="built_in">list</span>);</span><br></pre></td></tr></table></figure></p>
<p>其标准C的原型就是C语言函数调用<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">function_name(argument <span class="built_in">list</span>);</span><br></pre></td></tr></table></figure></p>
<p>这个三个尖括号<code>&lt;&lt;&lt;grid,block&gt;&gt;&gt;</code>内是对设备代码执行的线程结构的配置（或者简称为对内核进行配置），也就是我们上一篇中提到的线程结构中的网格，块。回忆一下上文，我们通过CUDA C内置的数据类型dim3类型的变量来配置grid和block。通过指定grid和block的维度，我们可以配置：</p>
<ul>
<li>内核中线程的数目</li>
<li>内核中使用的线程布局</li>
</ul>
<p>我们可以使用dim3类型的grid维度和block维度配置内核，也可以使用int类型的变量，或者常量直接初始化：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;4,8&gt;&gt;&gt;(argument list);</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/20220907141613.png" alt=""></p>
<p>我们的核函数是同时复制到多个线程执行的，上文我们说过一个对应问题，多个计算执行在一个数据，肯定是浪费时间，所以为了让多线程按照我们的意愿对应到不同的数据，就要给线程一个唯一的标识，由于设备内存是线性的（基本市面上的内存硬件都是线性形式存储数据的）我们观察上图，可以用threadIdx.x 和blockIdx.x 来组合获得对应的线程的唯一标识</p>
<p>接下来我们就是修改代码的时间了，改变核函数的配置，产生运行出结果一样，但效率不同的代码：</p>
<p>一个块：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;<span class="number">1</span>,<span class="number">32</span>&gt;&gt;&gt;(argument <span class="built_in">list</span>);</span><br></pre></td></tr></table></figure></p>
<p>32个块<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;<span class="number">32</span>,<span class="number">1</span>&gt;&gt;&gt;(argument <span class="built_in">list</span>);</span><br></pre></td></tr></table></figure></p>
<p>上述代码如果没有特殊结构在核函数中，执行结果应该一致，但是有些效率会一直比较低。</p>
<p>上面这些是启动部分，当主机启动了核函数，控制权马上回到主机，而不是主机等待设备完成核函数的运行，这一点我们上一篇文章也有提到过（就是等待hello world输出的那段代码后面要加一句）</p>
<p>想要主机等待设备端执行可以用下面这个指令：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaDeviceSynchronize</span><span class="params">(<span class="type">void</span>)</span>;</span><br></pre></td></tr></table></figure></p>
<p>这是一个显示的方法，对应的也有隐式方法，隐式方法就是不明确说明主机要等待设备端，而是设备端不执行完，主机没办法进行，比如内存拷贝函数：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaMemcpy</span><span class="params">(<span class="type">void</span>* dst,<span class="type">const</span> <span class="type">void</span> * src,</span></span><br><span class="line"><span class="params">  <span class="type">size_t</span> count,cudaMemcpyKind kind)</span>;</span><br></pre></td></tr></table></figure></p>
<p>这个函数上文已经介绍过了，当核函数启动后的下一条指令就是从设备复制数据回主机端，那么主机端必须要等待设备端计算完成。</p>
<p><strong>所有CUDA核函数的启动都是异步的</strong>，这点与C语言是完全不同的</p>
<h2 id="编写核函数"><a href="#编写核函数" class="headerlink" title="编写核函数"></a>编写核函数</h2><p>我们会启动核函数了，但是核函数哪里来的？当然我们写的，核函数也是一个函数，但是声明核函数有一个比较模板化的方法：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">kernel_name</span><span class="params">(argument <span class="built_in">list</span>)</span>;</span><br></pre></td></tr></table></figure></p>
<p>注意：声明和定义是不同的，这点CUDA与C语言是一致的</p>
<p>在C语言函数前没有的限定符global，CUDA C中还有一些其他我们在C中没有的限定符，如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>限定符</th>
<th>执行</th>
<th>调用</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>global</strong></td>
<td>设备端执行</td>
<td>可以从主机调用也可以从计算能力3以上的设备调用</td>
<td>必须有一个void的返回类型</td>
</tr>
<tr>
<td><strong>device</strong></td>
<td>设备端执行</td>
<td>设备端调用</td>
</tr>
<tr>
<td><strong>host</strong></td>
<td>主机端执行</td>
<td>主机调用</td>
<td>可以省略</td>
</tr>
</tbody>
</table>
</div>
<p>而且这里有个特殊的情况就是有些函数可以同时定义为 device 和 host ，这种函数可以同时被设备和主机端的代码调用，主机端代码调用函数很正常，设备端调用函数与C语言一致，但是要声明成设备端代码，告诉nvcc编译成设备机器码，同时声明主机端设备端函数，那么就要告诉编译器，生成两份不同设备的机器码。</p>
<p>Kernel核函数编写有以下限制</p>
<ul>
<li>只能访问设备内存</li>
<li>必须有void返回类型</li>
<li>不支持可变数量的参数</li>
<li>不支持静态变量</li>
<li>显示异步行为</li>
</ul>
<p>并行程序中经常的一种现象：把串行代码并行化时对串行代码块for的操作，也就是把for并行化。例如：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sumArraysOnGPU</span><span class="params">(<span class="type">float</span> *A, <span class="type">float</span> *B, <span class="type">float</span> *C)</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> i = threadIdx.x;</span><br><span class="line">  C[i] = A[i] + B[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这两个简单的段不能执行，但是我们可以大致的看一下for展开并行化的样子。</p>
<h2 id="验证核函数"><a href="#验证核函数" class="headerlink" title="验证核函数"></a>验证核函数</h2><p>验证核函数就是验证其正确性，下面这段代码上文出现过，但是同样包含验证核函数的方法：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 3_sum_arrays</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">sumArrays</span><span class="params">(<span class="type">float</span> * a,<span class="type">float</span> * b,<span class="type">float</span> * res,<span class="type">const</span> <span class="type">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;size;i+=<span class="number">4</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">    res[i<span class="number">+1</span>]=a[i<span class="number">+1</span>]+b[i<span class="number">+1</span>];</span><br><span class="line">    res[i<span class="number">+2</span>]=a[i<span class="number">+2</span>]+b[i<span class="number">+2</span>];</span><br><span class="line">    res[i<span class="number">+3</span>]=a[i<span class="number">+3</span>]+b[i<span class="number">+3</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="type">float</span>*a,<span class="type">float</span>*b,<span class="type">float</span>*res)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> i=threadIdx.x;</span><br><span class="line">  res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> nElem=<span class="number">32</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Vector size:%d\n&quot;</span>,nElem);</span><br><span class="line">  <span class="type">int</span> nByte=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line">  <span class="type">float</span> *a_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *b_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *res_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *res_from_gpu_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="type">float</span> *a_d,*b_d,*res_d;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;a_d,nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;b_d,nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;res_d,nByte));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">initialData</span>(a_h,nElem);</span><br><span class="line">  <span class="built_in">initialData</span>(b_h,nElem);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(a_d,a_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(b_d,b_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(nElem)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d,b_d,res_d);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n&quot;</span>,block.x,grid.x);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  <span class="built_in">sumArrays</span>(a_h,b_h,res_h,nElem);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">checkResult</span>(res_h,res_from_gpu_h,nElem);</span><br><span class="line">  <span class="built_in">cudaFree</span>(a_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(b_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(res_d);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(a_h);</span><br><span class="line">  <span class="built_in">free</span>(b_h);</span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>CUDA小技巧，当我们进行调试的时候可以把核函数配置成单线程的：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(argument <span class="built_in">list</span>)</span><br></pre></td></tr></table></figure></p>
<p>当错误出现的时候，不一定是哪一条指令触发的，这一点非常头疼；这时候我们就需要对错误进行防御性处理了，例如我们代码库头文件里面的这个宏：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK(call)\</span></span><br><span class="line"><span class="meta">&#123;\</span></span><br><span class="line"><span class="meta">  const cudaError_t <span class="keyword">error</span>=call;\</span></span><br><span class="line"><span class="meta">  <span class="keyword">if</span>(<span class="keyword">error</span>!=cudaSuccess)\</span></span><br><span class="line"><span class="meta">  &#123;\</span></span><br><span class="line"><span class="meta">      printf(<span class="string">&quot;ERROR: %s:%d,&quot;</span>,__FILE__,__LINE__);\</span></span><br><span class="line"><span class="meta">      printf(<span class="string">&quot;code:%d,reason:%s\n&quot;</span>,<span class="keyword">error</span>,cudaGetErrorString(<span class="keyword">error</span>));\</span></span><br><span class="line"><span class="meta">      exit(1);\</span></span><br><span class="line"><span class="meta">  &#125;\</span></span><br><span class="line"><span class="meta">&#125;</span></span><br></pre></td></tr></table></figure></p>
<p>就是获得每个函数执行后的返回结果，然后对不成功的信息加以处理，CUDA C 的API每个调用都会返回一个错误代码，这个代码我们就可以好好利用了，当然在release版本中可以去除这部分，但是开发的时候一定要有的。</p>
<h1 id="编程接口"><a href="#编程接口" class="headerlink" title="编程接口"></a>编程接口</h1><p>CUDA的编程接口由一系列C语言的扩展和运行库(runtime library)组成。</p>
<p>C语言的扩展在第二章“编程模型”中有所提及，如内核函数、线程网格和线程块等；<br>运行库则是在CUDA Driver API的基础上建立的。用户可以直接在应用程序中跳过CUDA，直接调用CUDA Driver API，以便更底层地操作GPU，如操作GPU的上下文。不过对于大多数应用来说，使用CUDA提供的运行库就足够了。</p>
<p>本章讲首先讲解CUDA程序的编译过程，之后会介绍CUDA运行库，最后会介绍程序兼容性等问题。</p>
<h2 id="使用NVCC编译CUDA程序"><a href="#使用NVCC编译CUDA程序" class="headerlink" title="使用NVCC编译CUDA程序"></a>使用NVCC编译CUDA程序</h2><p>CUDA程序使用NVCC编译器。<br>NVCC提供了简单方便的接口，能够很好的同时处理主机端和设备端代码。这里将简要介绍NVCC编译CUDA程序的流程，更多信息请参考nvcc user manual。</p>
<h3 id="编译流程"><a href="#编译流程" class="headerlink" title="编译流程"></a>编译流程</h3><h4 id="离线编译"><a href="#离线编译" class="headerlink" title="离线编译"></a>离线编译</h4><p>NVCC进行离线编译的操作流程是：</p>
<ul>
<li>分离CUDA程序中的主机端代码(host code)和设备端代码(device code)</li>
<li>将设备端代码编译成一种虚拟汇编文件(名为PTX)，再接着编译成二进制代码(名为cubin) </li>
<li>将主机端代码中含有”&lt;&lt;&lt;&gt;&gt;&gt;”的代码(即内核调用)替换为CUDA运行库中的函数调用代码</li>
<li>NVCC会借助其他编译器(如gcc)将主机端代码编译出来</li>
<li>主机端代码和设备端代码被编译好后，nvcc会将两段代码链接起来</li>
</ul>
<h4 id="在线编译-JIT-Compilation"><a href="#在线编译-JIT-Compilation" class="headerlink" title="在线编译(JIT Compilation)"></a>在线编译(JIT Compilation)</h4><p>PTX是一个虚拟汇编文件。其形式虽然很像汇编，但里面的每一条指令实际上是一个虚拟的指令，与机器码无法对应。需要编译器或设备驱动程序将其翻译成对应平台的汇编/机器码才能运行。</p>
<p>如果在编译过程中，NVCC不将设备端代码编译为cubin文件，即二进制代码，而是停在PTX代码上。设备驱动(device driver)会负责在运行时，使用PTX代码生成二进制代码。这个过程被称作在线编译(JIT Compilation, Just-In-Time Compilation)。</p>
<p>在线编译必然会使得程序启动的时间延长，不过设备驱动程序会自动缓存编译出来的二进制代码(也被称作compute cache)。</p>
<p>在线编译一方面的优势在于兼容性。另一方面的优势在于，当设备驱动程序有关编译的部分得到优化时，同样的PTX编出来的cubin文件同样会得到优化。也就是说，一段祖传的PTX代码，很有可能因为驱动程序不断的优化，而躺着得到了优化。而如果直接离线编译得到了cubin文件的话，则无法享受到这一优化。</p>
<h3 id="二进制代码的兼容性"><a href="#二进制代码的兼容性" class="headerlink" title="二进制代码的兼容性"></a>二进制代码的兼容性</h3><p>二进制代码cubin是受到GPU计算能力的限制的。在编译时，需要使用<code>-code</code>来指定将代码编译到哪个计算能力平台上，如<code>-code=sm_35</code>代表生成的cubin代码是运行在计算能力为3.5的平台上的。</p>
<p>二进制代码若要兼容，首先架构得一致。不同架构上的二进制代码不能互相兼容，如在Maxwell架构上编译出来的代码，不能在其他架构上运行。<br>其次，若执行平台的次版本号版本比编译时指定的的次版本号高，则可以运行。例如如果在编译时指定<code>-code=sm_35</code>，则在计算能力3.7的平台上也可以运行。反之则不可以。</p>
<p>另外需要说明的是，上述二进制代码的兼容性原则只限于桌面款显卡。</p>
<h3 id="PTX代码的兼容性"><a href="#PTX代码的兼容性" class="headerlink" title="PTX代码的兼容性"></a>PTX代码的兼容性</h3><p>PTX代码的兼容性远强于二进制代码。只要不涉及到不同架构上的特性差异，PTX可以在任何架构上运行。</p>
<p>不过PTX代码在两种情况下其兼容性会受限：</p>
<ol>
<li>若PTX代码使用了较高级别架构的特有特性，则无法在较低架构上运行。例如若PTX代码用到了计算能力3.0以上才能使用的Warp Shuffle特性，则无法在2.x或1.x平台上运行。 </li>
<li>若PTX在较低架构上生成，则虽然能够在所有更高级别的架构上运行，但无法充分利用这些架构的硬件特性，造成性能无法最大化的问题。</li>
</ol>
<p>在编译时，可以通过<code>-arch</code>来指定生成的PTX代码的版本，如<code>-arch=compute_30</code>。</p>
<h3 id="应用程序兼容性"><a href="#应用程序兼容性" class="headerlink" title="应用程序兼容性"></a>应用程序兼容性</h3><p>为了保证应用程序的兼容性，最好是将代码编译成PTX代码，然后依靠各个计算能力的驱动程序在线编译成对应平台的二进制代码cubin。</p>
<p>除了使用<code>-arch</code>和<code>-code</code>来分别指定C-&gt;PTX和PTX-&gt;cubin的计算能力外，还可以用<code>-gencode</code>关键字来操作，如下例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nvcc x.cu</span><br><span class="line">    -gencode arch=compute_35,code=sm_35</span><br><span class="line">    -gencode arch=compute_50,code=sm_50</span><br><span class="line">    -gencode arch=compute_60,code=\&#x27;compute_60,sm_60\&#x27;</span><br></pre></td></tr></table></figure>
<p>使用上述编译指令后，会生成3.5/5.0/6.0的cubin文件，以及6.0的PTX代码。</p>
<p>对于主机端代码，会自动编译，并在运行时决定调用哪一个版本的执行。对于上例，主机端代码会编译为：3.5/5.0/6.0的二进制文件，以及7.0的PTX文件。</p>
<p>另外，在程序中可以使用<code>__CUDA_ARCH__</code>宏来指定计算能力(只能用于修饰设备端代码)。计算能力3.5在程序中对应的<code>__CUDA_ARCH__</code>为350。</p>
<p>有一点需要注意的是，7.0以前，都是以线程束为单位在调度，线程束内指令永远是同步的，被成为<strong>锁步</strong>。而Volta架构(计算能力7.x)引入了Independent Thread Scheduling，破坏了线程束内的隐式同步。因此，如果老版本的代码里面有默认锁步的代码，在Volta架构下运行时可能会因为锁步的消失而出问题，可以指定<code>-arch=compute_60 -code=sm_70</code>，即将PTX编到Pascal架构下以禁用Independent Thread Scheduling特性。（当然，也可以修改代码来显示同步）</p>
<p>另外，版本相关编译指令有缩写的情况，具体看手册。</p>
<h3 id="C-C-兼容性"><a href="#C-C-兼容性" class="headerlink" title="C/C++兼容性"></a>C/C++兼容性</h3><p>对于主机端代码，nvcc支持C++的全部特性；而对于设备端代码，只支持C++的部分特性。具体查阅手册。</p>
<h3 id="32-64位兼容性"><a href="#32-64位兼容性" class="headerlink" title="32/64位兼容性"></a>32/64位兼容性</h3><p>当且仅当主机端代码按照64位编译时，设备端代码才能编译为64位。当主机端代码编译为32位时，设备端代码只能编译成32位。即设备端代码的位数和主机端永远保持一致。</p>
<p>具体编译成32/64位的哪一种，取决于nvcc本身的版本。32位nvcc会自动编出32位的代码，不过可以使用<code>-m64</code>来编出64位代码。对于64位编译器亦然。</p>
<h2 id="CUDA-C-运行库"><a href="#CUDA-C-运行库" class="headerlink" title="CUDA C 运行库"></a>CUDA C 运行库</h2><p>运行库实际上在cudart库内，可以使静态链接库<code>cudart.lib/libcudart.a</code>，或者动态链接库<code>cudart.dll/cudart.so</code>。</p>
<p>所有程序的入口都是<code>cuda</code>。</p>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>CUDA运行库没有显式的初始化函数，在调用第一个函数时会自动初始化(设备和版本管理函数不行)。初始化时，会产生一个全局可见的设备上下文(device context)。</p>
<p>当主机端代码调用了<code>cudaDeviceReset()</code>函数，则会销毁掉这个上下文。注意，销毁的上下文是主机端正在操纵的设备。如要更换，需要使用<code>cudaSetDevice()</code>来进行切换。</p>
<h3 id="设备内存"><a href="#设备内存" class="headerlink" title="设备内存"></a>设备内存</h3><p>CUDA运行库提供了函数以分配/释放设备端的内存，以及与主机端内存传输数据。</p>
<p>这里的设备内存，指的是全局内存+常量内存+纹理内存。</p>
<p>设备内存有两种分配模式：线性存储(linear memory)、CUDA arrays。 其中CUDA arrays与纹理内存有关，本导读略去不谈。</p>
<p>线性内存是我们常用的内存方式，在GPU上用40位的地址线寻址。线性内存可以用<code>cudaMalloc()</code>分配，用<code>cudaFree()</code>释放，用<code>cudaMemcpy()</code>复制数据，用<code>cudaMemset()</code>赋值。</p>
<p>对于2D或3D数组，可以使用<code>cudaMallocPitch()</code>和<code>cudaMalloc3D()</code>来分配内存。这两个函数会自动padding，以满足内存对齐的要求，提高内存读写效率。内存对齐的问题，会在第五章里详细阐述。</p>
<p>另外，如果要在设备内存中定义全局变量，则需要使用使用<code>__constant__</code>或<code>__device__</code>来修饰，并使用<code>cudaMemcpyToSymbol()</code>和<code>cudaMemcpyFromSymbol()</code>来读写。如下例：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">__constant__ <span class="type">float</span> constData[<span class="number">256</span>];</span><br><span class="line"><span class="type">float</span> data[<span class="number">256</span>];</span><br><span class="line"><span class="built_in">cudaMemcpyToSymbol</span>(constData, data, <span class="built_in">sizeof</span>(data));</span><br><span class="line"><span class="built_in">cudaMemcpyFromSymbol</span>(data, constData, <span class="built_in">sizeof</span>(data));</span><br><span class="line"></span><br><span class="line">__device__ <span class="type">float</span> devData;</span><br><span class="line"><span class="type">float</span> value = <span class="number">3.14f</span>;</span><br><span class="line"><span class="built_in">cudaMemcpyToSymbol</span>(devData, &amp;value, <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line"></span><br><span class="line">__device__ <span class="type">float</span>* devPointer;</span><br><span class="line"><span class="type">float</span>* ptr;</span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;ptr, <span class="number">256</span> * <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line"><span class="built_in">cudaMemcpyToSymbol</span>(devPointer, &amp;ptr, <span class="built_in">sizeof</span>(ptr));</span><br></pre></td></tr></table></figure>
<p>实际上，当使用<code>__constant__</code>关键字时，是申请了一块常量内存；而使用<code>__device__</code>时，是普通的全局内存。因此<code>__device__</code>申请的内存需要申请，而<code>__constant__</code>不用。不管是全局内存，还是常量内存，需要用带有<code>Symbol</code>的函数拷贝。</p>
<h3 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h3><p>不管是全局变量还是局部变量，都需要使用<code>__shared__</code>来修饰。不过需要注意的是，即使定义为全局变量，共享内存依旧只能被同一线程块内的线程可见。</p>
<p>举个例子，对于如下代码，虽然是定义了一个全局的共享内存hist_shared，但实际上，在每一个线程块被调度到SM上时，都会在SM的共享内存区开一块内存。因此，每一个线程块都有一个hist_shared，且之间无法互相访问。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">__shared__ <span class="type">unsigned</span> <span class="type">int</span> hist_shared[<span class="number">256</span>];   <span class="comment">//共享内存仅在线程块内共享</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">getGrayHistincuda_usesharemem</span><span class="params">(<span class="type">unsigned</span> <span class="type">char</span> * <span class="type">const</span> grayData, </span></span></span><br><span class="line"><span class="params"><span class="function">                                            <span class="type">unsigned</span> <span class="type">int</span> * <span class="type">const</span> hist,</span></span></span><br><span class="line"><span class="params"><span class="function">                                            uint imgheight,</span></span></span><br><span class="line"><span class="params"><span class="function">                                            uint imgwidth)</span>  <span class="comment">//使用共享内存加速</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">unsigned</span> <span class="type">int</span> idx = blockDim.x * blockIdx.x + threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">unsigned</span> <span class="type">int</span> idy = blockDim.y * blockIdx.y + threadIdx.y;  </span><br><span class="line">    <span class="type">const</span> <span class="type">unsigned</span> <span class="type">char</span> inner_idx = threadIdx.y * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">    hist_shared[inner_idx%<span class="number">256</span>] = <span class="number">0</span>;   <span class="comment">//清空数据，由于每个块的inner_idx可以超过256，所以这样可以保证hist_shared被全部清零</span></span><br><span class="line"></span><br><span class="line">    __syncthreads();    <span class="comment">//等待其他线程完成</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(idx &lt; imgwidth &amp;&amp; idy &lt; imgheight)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">const</span> <span class="type">unsigned</span> <span class="type">long</span> pid = imgwidth * idy + idx;</span><br><span class="line">        <span class="type">const</span> <span class="type">unsigned</span> <span class="type">char</span> value = grayData[pid];</span><br><span class="line">        <span class="built_in">atomicAdd</span>(&amp;(hist_shared[value]), <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(threadIdx.y &lt; <span class="number">8</span>) <span class="comment">//每个线程块将自己共享内存中的值合并到全局内存中去</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">atomicAdd</span>(&amp;(hist[inner_idx]), hist_shared[inner_idx]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当然，共享内存的声明放在内核函数里面也是可以的，效果一致。</p>
<p>使用共享内存，可以获得等同于L1 cache的访存速度，其速度远快于全局内存。</p>
<p>但是注意，并不是什么时候都可以使用共享内存来获取加速的。例如内核函数计算出来结果后，如果这个结果只需要传输回主机端，而不需要再次被用到时，直接写回全局内存会比较快。如果先写回共享内存，再写回全局内存，反而会比较缓慢。<br>一般来讲，当需要频繁读写，或是有原子操作时，使用共享内存替代全局内存，会取得比较大的增益。</p>
<p>强调一下，共享内存只能为线程块内的线程共享。如果需要整个线程网格中线程都能访问，则需要全局内存或常量内存。</p>
<p>另外，共享内存是一个稀缺资源。有些架构可以通过配置，分配L1 cache和共享内存的比例。</p>
<h3 id="锁页内存-Page-Locked-Host-Memory-Pinned-Memory"><a href="#锁页内存-Page-Locked-Host-Memory-Pinned-Memory" class="headerlink" title="锁页内存(Page-Locked Host Memory/Pinned Memory)"></a>锁页内存(Page-Locked Host Memory/Pinned Memory)</h3><p>锁页内存指的是主机端上不会被换出到虚拟内存(位于硬盘)上的内存。</p>
<p>锁页内存的分配与释放：在CUDA程序中，使用<code>cudaHostAlloc()</code>，可以分配锁页内存，使用<code>cudaFreeHost()</code>来释放锁页内存，或者使用<code>cudaHostRegister()</code>来将<code>malloc()</code>分配的内存指定为锁页内存</p>
<p>NVIDIA官方给出的锁页内存相对于普通的内存的的好处是：</p>
<ul>
<li>使用锁页内存后，锁页内存与设备内存之间的数据传输，可以使用流的方式，和内核函数执行并行。</li>
<li>使用锁页内存后，可以将锁页内存映射到设备内存上。 </li>
<li>对于使用<em>前端总线</em>的系统，使用锁页内存可以提升主机端到设备端传输的带宽；如果将锁页内存指定为合并写(write_combining)，则可以进一步提高带宽。</li>
</ul>
<p>另一本书对于锁页内存之所以快的解释是：</p>
<ul>
<li>如果主机端将数据放在锁页内存，则可以使用PCI-E的DMA与设备内存进行数据传输，而不需要CPU来搬运数据。 这也是为何使用了锁页内存后，可以使用流和内存映射，来让CPU程序、数据传输和内核执行并行。</li>
<li>如果主机端将数据放在普通内存，则CUDA会先申请一块锁页内存，然后将数据拷贝到锁页内存，再做后面的操作。 拷贝的过程浪费了一定时间。</li>
</ul>
<p>注意，锁页内存在 <em>non I/O coherent Tegra</em> 设备上不支持</p>
<h4 id="Portable-Memory"><a href="#Portable-Memory" class="headerlink" title="Portable Memory"></a>Portable Memory</h4><p>NVIDIA官方文档表示：上述所说的锁页内存的优点，只有在使用<code>cudaHostAlloc()</code>时，传入<code>cudaHostAllocPortable</code> flag，或者在使用<code>cudaHostRegister()</code>时传入<code>cudaHostRegisterPortable</code> flag，才能体现。否则锁页内存并不会有上述优点。</p>
<p>《GPU编程指南》一书中是这么描述的：如果传入了<code>cudaHostAllocPortable</code> flag，则锁页内存在所有的CUDA上下文中变成锁页的和可见的。如果需要在CUDA上下文之间或者主机处理器的线程之间传递指针，则必须使用这个标志。</p>
<h4 id="合并写内存-Write-Combining-Memory"><a href="#合并写内存-Write-Combining-Memory" class="headerlink" title="合并写内存(Write-Combining Memory)"></a>合并写内存(Write-Combining Memory)</h4><p>锁页内存默认是使用缓存的。如果将flag <code>cudaHostAllocWriteCombined</code> 传入到 <code>cudaHostAlloc()</code>，则可以将这块锁页内存指定为合并写内存。</p>
<p>合并写内存不再使用主机端的L1&amp;L2 cache，使得更多的cache可以供其他任务使用。</p>
<p>另外，对于通过PCI-E传输数据的情景，使用合并写内存不会被snooped <em>(是不是指的是不会被缓存管？不理解这个snooped什么意思)</em>，可以提升40%的传输性能。</p>
<p>此外需要注意的是，由于合并写内存不使用缓存，因此读入CPU核的操作会<strong>非常的慢</strong>。因此合并写内存最好只用作向GPU传数据的内存，而不是传回数据的内存。</p>
<h4 id="内存映射-Mapped-Memory"><a href="#内存映射-Mapped-Memory" class="headerlink" title="内存映射(Mapped Memory)"></a>内存映射(Mapped Memory)</h4><p>CUDA中的内存映射，指的是将CPU端的锁页内存，映射到GPU端。</p>
<p>通过向<code>cudaHostAlloc()</code>传入<code>cudaHostAllocMapped</code> flag，或向<code>cudaHostRegister()</code>传入<code>cudaHostAllocMapped</code> flag，来将一块内存指定为向GPU映射的内存。</p>
<p>映射的内存有两个地址，一个是CPU端访问的地址，一个是GPU端访问的地址。CPU端的地址在调用<code>malloc()</code>或<code>cudaHostAlloc()</code>时就已经返回； GPU端的地址使用<code>cudaHostGetDevicePointer()</code>函数来获取。</p>
<p>使用内存映射有以下好处：</p>
<ul>
<li>使用内存映射，可以让CPU/GPU之间的数据传输隐式执行，而不需要显示的分配GPU内存并传输数据。</li>
<li>当设备端执行内核函数需要某一块数据时，如果数据实际上在CPU端，会给出一个PCI-E传输请求(比全局内存还慢)，从主机端内存获取数据。此时给出数据请求的线程会被换出，直到数据就位后再被换入。因此如果使用内存映射，需要使用足够多的线程来隐藏PCI-E的传输延迟。</li>
<li>内存映射可以替代流，实现数据传输和内核执行的并行 有一点不是很确定：内存映射是否会在GPU端缓存数据；据我的记忆是不会缓存的，因此多次请求同一块数据的话，会启动多个PCI-E传输，效率很低 </li>
</ul>
<p>使用内存映射必须要注意的几点：</p>
<ul>
<li>由于映射的内存会被CPU和GPU两方共享，因此程序需要注意数据同步问题</li>
<li>如果要使用内存映射，必须在其他CUDA函数执行前，执行<code>cudaSetDeviceFlags()</code>并传入<code>cudaDeviceMapHost</code>，来使能设备的内存映射功能。否则<code>cudaHostGetDevicePointer()</code>函数会返回error。 </li>
<li>如果设备本身不支持内存映射，则使用<code>cudaHostGetDevicePointer()</code>一定会返回error。可以通过查看设备的<code>canMapHostMemory</code>信息来确认。</li>
<li>如果使用原子操作(atomicXXX)，需要注意，主机端和设备端的同时操作是不原子的。</li>
</ul>
<h3 id="异步并行执行"><a href="#异步并行执行" class="headerlink" title="异步并行执行"></a>异步并行执行</h3><p>CUDA允许以下操作互相并行：</p>
<ul>
<li>主机端计算</li>
<li>设备端计算(内核执行) </li>
<li>主机端to设备端传数据</li>
<li>设备端to主机端传数据 </li>
<li>设备端内部传数据</li>
<li>设备间传数据(可通过PCI-E直接传输，不需要先传到主机端再转发，<em>不过这一操作跟使用的操作系统有关</em>)</li>
</ul>
<h4 id="主机端-设备端并行"><a href="#主机端-设备端并行" class="headerlink" title="主机端/设备端并行"></a>主机端/设备端并行</h4><p>设备端的如下操作，可以跟主机端并行：</p>
<ul>
<li><em>内核启动与执行(可以通过将<code>CUDA_LAUNCH_BLOCKING</code>设为1，来disable内核执行并行，debug使用)</em></li>
<li>设备端内部传输数据 <em>64KB及以下的 host-to-device数据传输</em></li>
<li>使用流(带有<code>Async</code>前缀的内存传输函数)或内存映射传输数据（不再受64KB的限制）</li>
<li>设备端memset函数(<code>cudaMemset()</code>)</li>
</ul>
<p>其中第3、4条说明，在使用<code>cudaMemcpy()</code>时，如果数据小于等于64KB，其实传输相对于CPU是异步的。 如果数据多于64KB，则CPU会阻塞<em>到数据传输完成</em>。 这时使用带<code>Async</code>的内存传输函数，会释放CPU资源。使用<code>Async</code>传输函数，不仅可以和CPU并行，而且可以和内核执行并行。</p>
<p>需要注意的是，如果没有使用锁页内存，即使使用了<code>Async</code>函数，内存传输也不是并行的(<em>和CPU？还是GPU？</em>)。</p>
<h4 id="内核并行执行"><a href="#内核并行执行" class="headerlink" title="内核并行执行"></a>内核并行执行</h4><p>计算能力2.x及以上的设备，支持多个内核函数同时执行。(可以通过检查<code>concurrentKernels</code>来确定)</p>
<p><strong>执行多个内核函数，需要主机端不同的线程启动。如果一个线程依次启动多个内核，则这些内核会串行执行。同一线程的内核函数返回时会触发隐式的同步。</strong></p>
<p>另外，多个内核函数必须位于同一个CUDA上下文(CUDA context)上。不同CUDA上下文上的内核不能并行。这意味着，启动多个内核的多个线程必须使用相同的CUDA上下文。(<em>如何传递CUDA上下文？</em>)</p>
<h4 id="数据传输和内核执行并行-需要使用锁页内存"><a href="#数据传输和内核执行并行-需要使用锁页内存" class="headerlink" title="数据传输和内核执行并行(需要使用锁页内存)"></a>数据传输和内核执行并行(需要使用锁页内存)</h4><p>一些设备支持数据传输(主机端/设备端、设备端/设备端)和内核执行并行，可通过检查<code>asyncEngineCount</code>来确认。</p>
<p>一些设备支持设备端内部数据传输和内核执行/数据传输并行，可通过检查<code>concurrentKernels</code>来确认。</p>
<p>这一特性需要使用锁页内存。</p>
<h4 id="数据并行传输-需要使用锁页内存"><a href="#数据并行传输-需要使用锁页内存" class="headerlink" title="数据并行传输(需要使用锁页内存)"></a>数据并行传输(需要使用锁页内存)</h4><p>计算能力2.x及以上的设备，支持数据传入和传出并行。</p>
<p>必须使用锁页内存。</p>
<h4 id="流-streams"><a href="#流-streams" class="headerlink" title="流(streams)"></a>流(streams)</h4><p>在CUDA中，流(streams)指的是在GPU上一连串执行的命令。</p>
<p>不同的线程，可以向同一个流填入任务。</p>
<p>同一个流内的任务会按顺序执行；同一设备上不同的流有可能并行，其执行顺序不会有保证。</p>
<h5 id="流的创建和销毁"><a href="#流的创建和销毁" class="headerlink" title="流的创建和销毁"></a>流的创建和销毁</h5><p>下述代码是一个流的创建和销毁的例子。该程序创建了两个流，分配了两个锁页内存传输数据，依次启动了两个内核，最后销毁了这两个流。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cudaStream_t stream[<span class="number">2</span>];</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)</span><br><span class="line">    <span class="built_in">cudaStreamCreate</span>(&amp;stream[i]);</span><br><span class="line"><span class="type">float</span>* hostPtr;</span><br><span class="line"><span class="built_in">cudaMallocHost</span>(&amp;hostPtr, <span class="number">2</span> * size);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i) &#123;</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(inputDevPtr + i * size, hostPtr + i * size,</span><br><span class="line">                    size, cudaMemcpyHostToDevice, stream[i]);</span><br><span class="line">    MyKernel &lt;&lt;&lt;<span class="number">100</span>, <span class="number">512</span>, <span class="number">0</span>, stream[i]&gt;&gt;&gt;</span><br><span class="line">        (outputDevPtr + i * size, inputDevPtr + i * size, size);</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(hostPtr + i * size, outputDevPtr + i * size,</span><br><span class="line">                    size, cudaMemcpyDeviceToHost, stream[i]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)</span><br><span class="line">    <span class="built_in">cudaStreamDestroy</span>(stream[i]);</span><br></pre></td></tr></table></figure>
<p>从上例中可以看到，流的创建需要定义<code>cudaStream_t</code>结构，并调用<code>cudaStreamCreate()</code>来初始化。<br>流的销毁需要调用<code>cudaStreamDestroy()</code>来实现。</p>
<p>当向流中添加内核函数任务时，<code>&lt;&lt;&lt;...&gt;&gt;&gt;</code>不再是<code>&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;</code>，而是<code>&lt;&lt;&lt;blocksPerGrid, threadsPerBlock, dynamic_shared_memory, stream&gt;&gt;&gt;</code>。<br>其中dynamic_shared_memory指的是动态共享内存的大小(<em>回去翻书</em>)； stream就是<code>cudaStream_t</code>结构。</p>
<p>当设备还在执行流中的任务，而用户调用<code>cudaStreamDestroy()</code>函数时，函数会立刻执行(不会阻塞)。之后，当流中的任务完成后，与流相关的资源会自动释放。</p>
<p><strong>另外需要注意的是，上例中主机端线程、数据拷贝和内核执行完全异步，因此在”拷贝回主机端”这一操作完成之前，主机端的内存数据是不正确的。必须在数据返回的一步做同步操作，方能保证数据是正确的。</strong></p>
<h5 id="默认流-Default-Stream"><a href="#默认流-Default-Stream" class="headerlink" title="默认流(Default Stream)"></a>默认流(Default Stream)</h5><p>在调用内核函数时，不指定流或者将流指定为0，则代表使用了默认流(default stream)。</p>
<p>如果在编译时使用了<code>--default-stream per-thread</code>，或是在include任何cuda头文件前<code>#define CUDA_API_PER_THREAD_DEFAULT_STREAM</code>，则主机端的每一个线程都有自己专属的默认流。</p>
<p>而如果在编译时未指定相关flag，或指定<code>--default-stream legacy</code>，则默认流是一个特殊的流，称作NULL stream。主机端的所有线程会共享这个NULL stream。NULL stream是一个同步流，所有命令会产生隐式的同步。</p>
<h5 id="显式同步-Explicit-Synchronization"><a href="#显式同步-Explicit-Synchronization" class="headerlink" title="显式同步(Explicit Synchronization)"></a>显式同步(Explicit Synchronization)</h5><p>可以使用如下函数进行显式同步：</p>
<ul>
<li><code>cudaDeviceSynchronize()</code>：直到<strong>所有线程</strong>向设备端的<strong>所有流</strong>的<strong>所有已送入指令</strong>完成，才会退出阻塞。</li>
<li><code>cudaStreamSynchronize()</code>：直到<strong>指定流</strong>的<strong>之前所有已送入指令</strong>完成，才会退出阻塞。此函数可以用作同步指定流，而其他流可以不受干扰地继续运行。 </li>
<li><code>cudaStreamWaitEvent()</code>：需要stream和event作为输入参数。在调用该函数之后的命令，需要等待该函数等待的事件(Event)发生后，才能执行。如果stream指定为0，则对于向所有stream加入的命令来说，只要加在了该函数之后，都会阻塞直到等待的时间发生方可执行。</li>
</ul>
<p>注意，同步函数慎用，因为有可能会产生速度的下降。</p>
<h5 id="隐式同步-Implicit-Synchronization"><a href="#隐式同步-Implicit-Synchronization" class="headerlink" title="隐式同步(Implicit Synchronization)"></a>隐式同步(Implicit Synchronization)</h5><p>一般来讲，不同流内的命令可以并行。但是当任何一个流执行如下的命令时，情况例外，不能并行：<br><em>锁页内存的分配</em> 设备端内存分配 <em>设备端内存设置(memset)</em> 设备内部拷贝 <em>NULL stream内的命令</em> L1 cache/共享内存空间的重新分配</p>
<h5 id="操作重叠-Overlapping-Behavior"><a href="#操作重叠-Overlapping-Behavior" class="headerlink" title="操作重叠(Overlapping Behavior)"></a>操作重叠(Overlapping Behavior)</h5><p>操作的重叠程度，一方面取决于各个操作的顺序，另一方面取决于设备支持重叠的程度(是否支持内核执行并行/数据传输与内核执行并行/数据传输并行)</p>
<h5 id="回调函数-Callbacks"><a href="#回调函数-Callbacks" class="headerlink" title="回调函数(Callbacks)"></a>回调函数(Callbacks)</h5><p>可以使用<code>cudaStreamAddCallback()</code>函数，向流中添加callback。该callback会在流中之前所有的任务完成后被调用。如果stream参数设为0，则代表之前的所有stream的任务执行完后就调用该callback。</p>
<p>回调函数和<code>cudaStreamWaitEvent()</code>一样，对于在加在callback之后的指令，必须等待callback<em>执行完成</em>后，才会继续执行。</p>
<p>下例是一个使用回调的例子。该例中，两个stream将数据拷回主机端后，会调用回调函数。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> CUDART_CB <span class="title">MyCallback</span><span class="params">(cudaStream_t stream, cudaError_t status, <span class="type">void</span> *data)</span></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Inside callback %d\n&quot;</span>, (<span class="type">size_t</span>)data);</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i) &#123;</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(devPtrIn[i], hostPtr[i], size, cudaMemcpyHostToDevice, stream[i]);</span><br><span class="line">    MyKernel&lt;&lt;&lt;<span class="number">100</span>, <span class="number">512</span>, <span class="number">0</span>, stream[i]&gt;&gt;&gt;(devPtrOut[i], devPtrIn[i], size);</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(hostPtr[i], devPtrOut[i], size, cudaMemcpyDeviceToHost, stream[i]);</span><br><span class="line">    <span class="built_in">cudaStreamAddCallback</span>(stream[i], MyCallback, (<span class="type">void</span>*)i, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>回调函数中不能直接或间接的执行CUDA函数，否则会因为等待自己完成而造成死锁。 <em>(原因尚不太明白)</em></p>
<h5 id="流的优先级-Stream-Priorities"><a href="#流的优先级-Stream-Priorities" class="headerlink" title="流的优先级(Stream Priorities)"></a>流的优先级(Stream Priorities)</h5><p>可以通过<code>cudaStreamCreateWithPriority()</code>来在创建流时指定流的优先级。可以指定的优先级可由<code>cudaDeviceGetStreamPriorityRange()</code>来获得。</p>
<p>运行时，高优先级stream中的线程块不能打断正在执行的低优先级stream的线程块(即不是抢占式的)。但是当低优先级stream的线程块退出SM时，高优先级stream中的线程块会被优先调度进SM。</p>
<h4 id="事件-Event"><a href="#事件-Event" class="headerlink" title="事件(Event)"></a>事件(Event)</h4><p>事件(Event)可以被压入流中以监视流的运行情况，或者用于精确计时。</p>
<p>如果向stream 0压入事件，则当压入事件前向所有流压入的任务完成后，事件才被触发。</p>
<h5 id="事件的创建和销毁"><a href="#事件的创建和销毁" class="headerlink" title="事件的创建和销毁"></a>事件的创建和销毁</h5><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t start, stop;    <span class="comment">//创建</span></span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line">...</span><br><span class="line"><span class="built_in">cudaEventDestroy</span>(start);    <span class="comment">//销毁</span></span><br><span class="line"><span class="built_in">cudaEventDestroy</span>(stop);</span><br></pre></td></tr></table></figure>
<h5 id="计算时间"><a href="#计算时间" class="headerlink" title="计算时间"></a>计算时间</h5><p>下例是一个使用Event计算时间的例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);  <span class="comment">//记录事件(将事件压入流)，流0则代表所有流完成任务后事件才会被触发</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i) &#123;</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(inputDev + i * size, inputHost + i * size, size, cudaMemcpyHostToDevice, stream[i]);</span><br><span class="line">    MyKernel&lt;&lt;&lt;<span class="number">100</span>, <span class="number">512</span>, <span class="number">0</span>, stream[i]&gt;&gt;&gt;(outputDev + i * size, inputDev + i * size, size);</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(outputHost + i * size, outputDev + i * size, size, cudaMemcpyDeviceToHost, stream[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line"><span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line"><span class="type">float</span> elapsedTime;</span><br><span class="line"><span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);    <span class="comment">//获取两个事件发生的时间差(ms)</span></span><br></pre></td></tr></table></figure>
<h3 id="3-2-6-多设备系统-Multi-Device-System"><a href="#3-2-6-多设备系统-Multi-Device-System" class="headerlink" title="3.2.6 多设备系统(Multi-Device System)"></a>3.2.6 多设备系统(Multi-Device System)</h3><h4 id="设备枚举-Device-Enumeration"><a href="#设备枚举-Device-Enumeration" class="headerlink" title="设备枚举(Device Enumeration)"></a>设备枚举(Device Enumeration)</h4><p>下例是如何枚举设备，并获取设备信息的例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> deviceCount;</span><br><span class="line"><span class="built_in">cudaGetDeviceCount</span>(&amp;deviceCount);   <span class="comment">//获取设备数量</span></span><br><span class="line"><span class="type">int</span> device;</span><br><span class="line"><span class="keyword">for</span> (device = <span class="number">0</span>; device &lt; deviceCount; ++device) &#123;</span><br><span class="line">    cudaDeviceProp deviceProp;</span><br><span class="line">    <span class="built_in">cudaGetDeviceProperties</span>(&amp;deviceProp, device);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Device %d has compute capability %d.%d.\n&quot;</span>, device, deviceProp.major, deviceProp.minor);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-2-6-2-设备选择-Device-Selection"><a href="#3-2-6-2-设备选择-Device-Selection" class="headerlink" title="3.2.6.2 设备选择(Device Selection)"></a>3.2.6.2 设备选择(Device Selection)</h4><p>使用<code>cudaSetDevice()</code>选择设备，当不选择时，默认使用设备0。</p>
<p>注意，所有的内存分配、内核函数启动、流和事件的创建等，都是针对当前选择的设备的。</p>
<p>下例是一个设备选择的例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">size_t</span> size = <span class="number">1024</span> * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">0</span>);   <span class="comment">// Set device 0 as current</span></span><br><span class="line"><span class="type">float</span>* p0;</span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;p0, size);  <span class="comment">// Allocate memory on device 0</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p0);    <span class="comment">// Launch kernel on device 0</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">1</span>);   <span class="comment">// Set device 1 as current</span></span><br><span class="line"><span class="type">float</span>* p1;</span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;p1, size);  <span class="comment">// Allocate memory on device 1</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p1); <span class="comment">// Launch kernel on device 1</span></span><br></pre></td></tr></table></figure>
<h4 id="多设备下-流和事件的执行情况"><a href="#多设备下-流和事件的执行情况" class="headerlink" title="(多设备下)流和事件的执行情况"></a>(多设备下)流和事件的执行情况</h4><p>下面将讨论，如果对一个不属于当前设备的流或事件进行操作，哪些操作会成功，哪些操作会失败：</p>
<ul>
<li><strong>内核启动</strong>(will fail)：如果将内核压入不属于当前设备的流中，则内核会启动失败。也就是说，如果要向一个流中压入内核，必须先切换到流所在的设备：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">0</span>);   <span class="comment">// Set device 0 as current</span></span><br><span class="line">cudaStream_t s0;</span><br><span class="line"><span class="built_in">cudaStreamCreate</span>(&amp;s0);  <span class="comment">// Create stream s0 on device 0</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">100</span>, <span class="number">64</span>, <span class="number">0</span>, s0&gt;&gt;&gt;(); <span class="comment">// Launch kernel on device 0 in s0</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">1</span>);   <span class="comment">// Set device 1 as current</span></span><br><span class="line">cudaStream_t s1;</span><br><span class="line"><span class="built_in">cudaStreamCreate</span>(&amp;s1);  <span class="comment">// Create stream s1 on device 1</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">100</span>, <span class="number">64</span>, <span class="number">0</span>, s1&gt;&gt;&gt;(); <span class="comment">// Launch kernel on device 1 in s1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// This kernel launch will fail:</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">100</span>, <span class="number">64</span>, <span class="number">0</span>, s0&gt;&gt;&gt;(); <span class="comment">// Launch kernel on device 1 in s0</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>内存拷贝</strong>(will success)：如果对一个不属于当前设备的流进行内存拷贝工作，内存拷贝会成功。</li>
<li><strong>cudaEventRecord()</strong>(will fail)：必须现将设备上下文切换过去，再向流压入事件。</li>
<li><strong>cudaEventElapsedTime()</strong>(will fail)：计算时间差前，必须先切换设备。</li>
<li><strong>cudaEventSynchronize() and cudaEventQuery()</strong>(will success)：即使处于不同的设备，事件同步和事件查询依然有效。</li>
<li><strong>cudaStreamWaitEvent()</strong>(will success)：比较特殊，即使函数输入的流和事件不在同一个设备上，也能成功执行。也就是说，可以让流等待另一个设备上(当然当前设备也可以)的事件。这个函数可以用作多个设备间的同步。</li>
</ul>
<p>另外需要注意，<strong>每个设备都有自己的默认流</strong>。因此在没有指定流的情况下，向不同设备分派的任务，实际上是压入了各个设备的默认流，他们之间是并行执行的。</p>
<h4 id="3-2-6-4-设备间-对等内存访问-Peer-to-Peer-Memory-Access"><a href="#3-2-6-4-设备间-对等内存访问-Peer-to-Peer-Memory-Access" class="headerlink" title="3.2.6.4 (设备间)对等内存访问(Peer-to-Peer Memory Access)"></a>3.2.6.4 (设备间)对等内存访问(Peer-to-Peer Memory Access)</h4><p>计算能力2.0及以上的设备支持设备间对等内存访问，这意味着两个GPU之间的传输和访问可以不经过主机端中转，速度会有提升。查询<code>cudaDeviceCanAccessPeer()</code>可以得知设备是否支持这一特性。<em>(官方文档说还需要一个条件：64位程序，存疑)</em></p>
<p>需要使用<code>cudaDeviceEnablePeerAccess()</code>来使能这一特性。</p>
<p>对等设备的的地址是统一编址的，可用同一个指针访问，如下例：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">0</span>);   <span class="comment">// Set device 0 as current</span></span><br><span class="line"><span class="type">float</span>* p0;</span><br><span class="line"><span class="type">size_t</span> size = <span class="number">1024</span> * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;p0, size);      <span class="comment">// Allocate memory on device 0</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p0);    <span class="comment">// Launch kernel on device 0</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">1</span>);               <span class="comment">// Set device 1 as current</span></span><br><span class="line"><span class="built_in">cudaDeviceEnablePeerAccess</span>(<span class="number">0</span>, <span class="number">0</span>);   <span class="comment">// Enable peer-to-peer access with device 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Launch kernel on device 1</span></span><br><span class="line"><span class="comment">// This kernel launch can access memory on device 0 at address p0</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p0);</span><br></pre></td></tr></table></figure>
<h4 id="设备间-对等内存拷贝-Peer-to-Peer-Memory-Copy"><a href="#设备间-对等内存拷贝-Peer-to-Peer-Memory-Copy" class="headerlink" title="(设备间)对等内存拷贝(Peer-to-Peer Memory Copy)"></a>(设备间)对等内存拷贝(Peer-to-Peer Memory Copy)</h4><p>对等设备的地址是统一编址的，可以使用<code>cudaMemcpyPeer()、cudaMemcpyPeerAsync()、cudaMemcpy3DPeer、cudaMemcpy3DPeerAsync()</code>来进行直接拷贝。无需先拷贝会主机端内存，再转到另一块卡上。如下例：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">0</span>);   <span class="comment">// Set device 0 as current</span></span><br><span class="line"><span class="type">float</span>* p0;</span><br><span class="line"><span class="type">size_t</span> size = <span class="number">1024</span> * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;p0, size);  <span class="comment">// Allocate memory on device 0</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">1</span>);</span><br><span class="line"><span class="type">float</span>* p1;</span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;p1, size);  <span class="comment">// Allocate memory on device 1</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">0</span>);       <span class="comment">// Set Device 0 as Current</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p0);    <span class="comment">// Launch Kernel on Device 0</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">1</span>);               <span class="comment">// Set Device 1 as Current</span></span><br><span class="line"><span class="built_in">cudaMemcpyPeer</span>(p1, <span class="number">1</span>, p0, <span class="number">0</span>, size); <span class="comment">// Copy p0 to p1</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p1);        <span class="comment">// Launch Kernel on Device 1</span></span><br></pre></td></tr></table></figure>
<p>关于设备间的对等拷贝，如果使用的是NULL stream，则有如下性质：<br><em>如果拷贝的双方中的任何一方，在设备拷贝前有任务未完成，则拷贝会被阻塞，直至任务完成。</em> 只有拷贝结束后，两者的后续任务才能继续执行。</p>
<p><em>(使用的如果不是NULL Stream，又会怎样呢？)</em></p>
<h3 id="统一虚拟地址空间-Unified-Virtual-Address-Space"><a href="#统一虚拟地址空间-Unified-Virtual-Address-Space" class="headerlink" title="统一虚拟地址空间(Unified Virtual Address Space)"></a>统一虚拟地址空间(Unified Virtual Address Space)</h3><p>当程序是64位程序时，所有主机端内存，以及计算能力≥2.0的设备的内存是统一编址的。所有通过CUDA API分配的主机内存和设备内存，都在统一编址的范围内，有自己的虚拟地址。因此：</p>
<ul>
<li>可以通过<code>cudaPointerGetAttributes()</code>，来确定指针所指的内存处在主机端还是设备端。</li>
<li>进行拷贝时，可以将<code>cudaMemcpy***()</code>中的<code>cudaMemcpyKind</code>参数设置为<code>cudaMemcpyDefault</code>，去让函数根据指针所处的位置自行判断应该是从哪里拷到哪里。</li>
<li>使用<code>cudaHostAlloc()</code>分配的锁页内存，自动是<em>Portable</em>的，所有支持统一虚拟编址的设备均可访问。<code>cudaHostAlloc()</code>返回的指针，无需通过<code>cudaHostGetDevicePointer()</code>，就可以直接被设备端使用。</li>
</ul>
<p>可以通过查询<code>unifiedAddressing</code>来查看设备是否支持统一虚拟编址。</p>
<h3 id="进程间通讯-Interprocess-Communication"><a href="#进程间通讯-Interprocess-Communication" class="headerlink" title="进程间通讯(Interprocess Communication)"></a>进程间通讯(Interprocess Communication)</h3><p>线程间通讯，可以很方便的通过共享的变量来实现。然而进程间通讯不行。</p>
<p>为了在进程间共享设备端内存的指针或者事件，必须使用IPC(Inter Process Communication) API。IPC API只支持64位程序，并且要求设备计算能力≥2.0。</p>
<p>通过IPC中的<code>cudaIpcGetMemHandle()</code>，可以得到设备内存指针的IPC句柄。该句柄可以通过标准的IPC机制(interprocess shared memory or files)传递到另一个进程，再使用<code>cudaIpcOpenMemHandle()</code>解码得到该进程可以使用的设备内存指针。<br>事件的共享也是如此。</p>
<h3 id="错误检查-Error-Checking"><a href="#错误检查-Error-Checking" class="headerlink" title="错误检查(Error Checking)"></a>错误检查(Error Checking)</h3><p>所有的runtime function都会返回一个error code，可通过检查error code判断是否出错。</p>
<p>但是对于异步函数，由于在执行前就会返回，因此返回的error code仅仅代表函数启动时的错误(如参数校验)；异步函数不会返回运行时出现的错误。如果运行时出了错，会被后面的某个函数捕获并返回。</p>
<p>检查异步函数是否出错的唯一方式，就是在异步函数启动后，进行同步。 如在异步函数后，调用<code>cudaDeviceSynchronize()</code>，则异步函数的错误会被<code>cudaDeviceSynchronize()</code>捕获到。</p>
<p>事实上，除了runtime function会返回error code之外，每一个主机端线程都会有一个初始化为<code>cudaSuccess</code>的变量，用于指示错误。一旦发生了错误，该变量也会被设置为相应的error code。</p>
<p>该变量不会被直接调用，但可以被<code>cudaPeekAtLastError()</code>和<code>cudaGetLastError()</code>访问到。不同的是，<code>cudaGetLastError()</code>在返回这一变量的同时，会把它重置为<code>cudaSuccess</code>。</p>
<p>内核函数不会返回值，因此只能通过<code>cudaPeekAtLastError()</code>或<code>cudaGetLastError()</code>来知悉<strong>调用内核</strong>是否有错误。<br>当然，为了排除错误出现在调用内核之前就有错误，可以先检验之前的错误变量是否为<code>cudaSuccess</code>。</p>
<p>另外需要注意的是，<code>cudaStreamQuery()</code>和<code>cudaEventQuery()</code>这类函数，有可能会返回<code>cudaErrorNotReady</code>。但这不被认为是错误，因此不会被<code>cudaPeekAtLastError()</code>和<code>cudaGetLastError()</code>捕获到。</p>
<h2 id="计算模式-Compute-Mode"><a href="#计算模式-Compute-Mode" class="headerlink" title="计算模式(Compute Mode)"></a>计算模式(Compute Mode)</h2><p>NVIDIA的设备可以设置三种计算模式：</p>
<ul>
<li>默认模式(Default Compute Mode)：多个主机端线程可以同时使用一个设备(通过调用<code>cudaSetDevice()</code>)</li>
<li>专属进程模式(Exclusive-Process Compute Mode)：对于一个设备，只能由一个进程创建设备上下文。一旦创建成功后，该进程的所有线程都可以使用该设备，而其他进程则不行。 </li>
<li>禁止模式(Prohibited Compute Mode)：无法对设备建立CUDA上下文。</li>
</ul>
<p>正常情况下，如果程序没有调用<code>cudaSetDevice()</code>，则会默认使用0号设备。但是如果0号设备被置成禁止模式，亦或是被其他进程所专属，则会在其他设备上创建上下文并使用。 可以向<code>cudaSetValidDevices()</code>函数输入一个设备列表，函数会在第一个可以使用的设备上创建上下文。</p>
<h1 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h1><h2 id="性能优化概述"><a href="#性能优化概述" class="headerlink" title="性能优化概述"></a>性能优化概述</h2><p>CUDA程序性能优化有三个原则：</p>
<ul>
<li><em>最大化并行，以提升资源利用率</em></li>
<li>优化内存排布，以最大化内存吞吐</li>
<li>最大化指令吞吐</li>
</ul>
<p>在性能优化前，需要先分析程序性能的瓶颈，再针对瓶颈优化，否则收益会很低。分析程序瓶颈，可以使用CUDA profiler等工具。</p>
<h2 id="最大化利用率-Maximize-Utilization"><a href="#最大化利用率-Maximize-Utilization" class="headerlink" title="最大化利用率(Maximize Utilization)"></a>最大化利用率(Maximize Utilization)</h2><p>最大化利用率的方法就是并行。</p>
<h3 id="应用级别并行-Application-Level"><a href="#应用级别并行-Application-Level" class="headerlink" title="应用级别并行(Application Level)"></a>应用级别并行(Application Level)</h3><p>从程序最高层来看，应该尽可能让主机端、设备端、PCI-E总线并行工作。对此可以使用异步CUDA函数，以及流(Stream)来实现。</p>
<p>同步操作，以及内存的共享会影响程序的并行性。因此需要仔细设计算法流程，尽量减少同步和内存共享。 如果一定需要同步和内存共享，尽量在线程块内完成(线程块同步——使用<code>__syncthreads()</code>涉及到的线程少，且可以通过SM内的共享内存共享数据。如果需要线程网格内同步，则需要两个内核调用，且共享数据只能通过全局内存，速度慢)。</p>
<h3 id="设备级别并行-Device-Level"><a href="#设备级别并行-Device-Level" class="headerlink" title="设备级别并行(Device Level)"></a>设备级别并行(Device Level)</h3><p>可以通过流的方式，尽可能的让多个内核并行，提升利用率。</p>
<h3 id="处理器级别并行-Multiprocessor-Level"><a href="#处理器级别并行-Multiprocessor-Level" class="headerlink" title="处理器级别并行(Multiprocessor Level)"></a>处理器级别并行(Multiprocessor Level)</h3><p>延迟(latency)指的是线程束(从上一个动作开始)到它处于ready状态的时钟数。 例如线程束先提交了一个内存访问请求，然后等了400个时钟周期，内存管理系统才返回数据，线程束可以继续执行。这400个时钟周期称为延迟。</p>
<p>当一个线程束发生延迟时，线程束调度器(warp scheduler)会将其他处于ready状态的线程束调度到SP上。等到延迟结束后，再将该线程调度回SP继续执行。这样一来，前一个线程束的延迟，就被另一个线程束的执行所隐藏了。 这一过程被称作延迟的隐藏(hidden latency)。</p>
<p>隐藏延迟是GPU编程的核心概念。由于GPU具有巨大的寄存器空间，线程的切换不存在损耗。因此，通过向GPU上分配足够多的线程，可以让这些线程延迟互相交错，以起到隐藏延迟的作用，提高硬件利用率。</p>
<p>造成线程(束)产生延迟的原因有：</p>
<ul>
<li><em>指令执行：不同指令有不同的执行延迟</em></li>
<li>内存请求：共享内存、全局内存、PCI-E(Mapped Memory)的读写请求</li>
<li>同步操作：如使用<code>__syncthreads()</code>后，先完成的线程(束)，会等待线程块中其他线程(束)达到同步点。</li>
</ul>
<p>通过配置线程网格、线程块、寄存器和共享内存用量，让SM可以运行尽可能多的线程束，以隐藏延迟。例如对于计算能力3.x的设备，为了完全隐藏全局内存读取的延迟(200-400时钟)，需要大概40个线程束。</p>
<p>举个例子，设SM有32KB共享內存空间。程序每个线程需要32B共享內存，即一个线程束需要1KB共享內存，考虑下述两种方案：</p>
<ul>
<li>方案1：每个线程块有16个线程束，则每个线程块需要16KB共享內存。可以调度两个线程块到SM上。</li>
<li>方案2：每个线程块有18个线程束，则每个线程块需要18KB共享內存，则只能调度一个线程块到SM上。</li>
</ul>
<p>虽然方案2在一个线程块上，有更多的线程束，但是实际上SM上运行的线程束减少了(32-&gt;18)。因此方案2隐藏延迟的能力弱于方案1，资源利用率较低。</p>
<p>此外，如果寄存器使用过多，超过了SM上的寄存器空间，则会使用本地内存作为寄存器。本地内存是存在在全局内存上的，速度很慢，会严重影响程序速度。因此需要严格考虑寄存器使用数量。</p>
<p>最后强调一点，线程块中的线程数量，最好是32的整数倍。这样，就不会有为了补齐线程束，而出现的永远不会激活的线程。这些不激活的线程也会占用SM的资源，降低资源利用率。</p>
<p>CUDA具有Occupancy Calculator，帮助程序员设计。</p>
<h2 id="最大化内存吞吐-Maximize-Memory-Throughput"><a href="#最大化内存吞吐-Maximize-Memory-Throughput" class="headerlink" title="最大化内存吞吐(Maximize Memory Throughput)"></a>最大化内存吞吐(Maximize Memory Throughput)</h2><p>最大化内存吞吐，主要手段就是少用低带宽的内存。这意味着首先要尽可能减少主机端和设备端间的设备传输(PCI-E，特别慢)，其次要尽可能减少全局内存的读写(快于PCI-E，但是相对于片内内存来说，还是挺慢的)；尽可能的使用片内的内存(寄存器、cache、共享内存)。</p>
<p>这里需要强调一下cache和共享内存的事情。</p>
<p>共享内存是程序可控的高速缓存。一般情况下，共享内存的使用流程为：</p>
<ul>
<li>将数据从全局内存拷贝到共享内存，或初始化共享内存*</li>
<li>进行一个同步操作，确保共享内存全部被赋值</li>
<li>利用共享内存的数据，运行程序*</li>
<li>如果出现了共享内存的写操作，一般需要进行一个同步操作，确保写操作全部完成后再进行下面的操作</li>
<li>将数据写回全局内存</li>
</ul>
<p>这里有一点要强调，只有在数据需要反复读写的时候，共享内存才有意义。如果数据只会被读一次，处理完后又写回并不再处理。则直接从全局内存读出-&gt;寄存器运行-&gt;写回全局内存是最快的。在共享内存中转反而是慢的。</p>
<p>缓存(L1/L2 cache)是程序员无法显式编程的。但是如果了解缓存的特性的话，可以通过合适的程序设计，增加缓存命中率。</p>
<h3 id="主机端和设备端间数据传输"><a href="#主机端和设备端间数据传输" class="headerlink" title="主机端和设备端间数据传输"></a>主机端和设备端间数据传输</h3><p>由于PCI-E传输并不快，因此要尽量减少主机端和设备端间的数据传输： <em>一种方式是让中间结果尽可能的在设备端产生，在设备端使用。</em> 另一种方式是将很多小的数据，打包传输。 <em>还有可以通过分配锁页内存来加快前端总线</em>系统的带宽。</p>
<p>当使用内存映射时，需要注意，每次内存访问都会启动一次PCI-E传输。因此，尽量保证数据只被读写一次，且尽可能合并访问以提升有效内存带宽。</p>
<p>有些GPU设备，主机端和设备端内存，在物理上就是同一块。这种情况下，主机端和设备端传输是不存在的。可通过标志<code>integrated</code>来查看。</p>
<h3 id="设备内存访问"><a href="#设备内存访问" class="headerlink" title="设备内存访问"></a>设备内存访问</h3><h4 id="全局内存-global-memory"><a href="#全局内存-global-memory" class="headerlink" title="全局内存(global memory)"></a>全局内存(global memory)</h4><p>全局内存支持合并访问，可以一次性传输连续的32、 64、 128字节的数据。因此，在设计内核时，线程束内的线程尽量连续的访问内存。</p>
<p>考虑如下两个内核：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//假设gpuData是一个二维数组，尺寸为32x32</span></span><br><span class="line"><span class="type">int</span> gpuData[<span class="number">32</span>][<span class="number">32</span>];  <span class="comment">//这样是不合法的，因为这么定义实际上是在主机端，还需要拷贝到设备端，这里只是为了方便说明问题</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">Kernel1</span><span class="params">(<span class="type">int</span> gpuData[][<span class="number">32</span>])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">32</span>; i++)</span><br><span class="line">        sum += gpuData[i][tid]; <span class="comment">//行访问</span></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">Kernel2</span><span class="params">(<span class="type">int</span> gpu[][<span class="number">32</span>])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">32</span>; i++)</span><br><span class="line">        sum += gpuData[tid][i]; <span class="comment">//列访问</span></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上例中，执行Kernel1的线程束中的线程，在一次循环中，32个线程依次访问<code>gpuData[0][0]</code>, <code>gpuData[0][1]</code>,<code>gpuData[0][2]</code>, …, <code>gpuData[0][31]</code>。在内存中，这32个变量是连续存储的，因此可以被合并访问。这种访问被称为行访问。</p>
<p>而Kernel2在一次循环中，读取的变量为<code>gpuData[0][0]</code>, <code>gpuData[1][0]</code>, <code>gpuData[2][0]</code>, …, <code>gpuData[31][0]</code>。这32个变量是不连续的，需要进行32次内存请求。这种访问被称为列访问。</p>
<p>上例中，列访问之所以效率低，原因有二：</p>
<ul>
<li>对于执行一次循环，行访问只需要一个内存请求指令，而列访问需要32个内存请求指令。从指令角度来讲，行访问的内存请求指令带宽是列访问的1/32。</li>
<li>全局内存的最大带宽为一次取128Byte，但是内核每次只需要4个Byte的数据。这使得列访问的内存带宽为峰值带宽的1/32。事实上，即使内核只需要4Byte，GPU也会取连续的<em>32Byte</em>，然后丢掉后面的28Byte，造成资源的浪费。但是缓存的引入(自计算能力2.x开始)，这一问题得到了缓解，28Byte会先放到缓存中，下次会命中。</li>
</ul>
<p>因此，从上例中可以看到，好好安排内存排布，尽量使得内存访问可以合并，可以加速全局内存的读写。</p>
<h4 id="对齐-Alignment"><a href="#对齐-Alignment" class="headerlink" title="对齐(Alignment)"></a>对齐(Alignment)</h4><p>当变量的尺寸为1/2/4/8/16字节时，变量会对齐。但如果不是的话，变量无法对齐，会产生额外的内存访问。</p>
<p>C/C++内建的变量(int/float等)，以及CUDA支持的向量(float2/float4等)，是对齐的。</p>
<p>一些结构体可能会产生不对齐的情况，看下例：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">struct1</span>&#123;</span><br><span class="line">    <span class="type">float</span> x;</span><br><span class="line">    <span class="type">float</span> y;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">struct2</span>&#123;</span><br><span class="line">    <span class="type">float</span> x;</span><br><span class="line">    <span class="type">float</span> y;</span><br><span class="line">    <span class="type">float</span> z;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">struct3</span> __align__(<span class="number">16</span>)&#123;</span><br><span class="line">    <span class="type">float</span> x;</span><br><span class="line">    <span class="type">float</span> y;</span><br><span class="line">    <span class="type">float</span> z;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>上例中，struct1是8字节的结构体，自动会对齐； struct2具有12个字节，无法对齐； struct3使用了<code>__align__(16)</code>关键字，显式指定对齐到16。</p>
<p>使用各类malloc分配的设备内存，一定是256字节对齐的。</p>
<h4 id="本地内存-local-memory"><a href="#本地内存-local-memory" class="headerlink" title="本地内存(local memory)"></a>本地内存(local memory)</h4><p>通过看PTX代码，可以看到标记为<code>.local</code>的变量，就是本地内存。<br>即使PTX代码里没有使用本地内存，在编译到cubin代码的过程中，仍然会使用本地内存，编译器会报告<code>lmem</code>的使用情况。</p>
<p>前面多次强调过了，一旦使用了本地内存，其速度会非常慢。不过本地内存在存储的时候，是按照32个线程连续存储的，因此可以合并访问。<br>对于计算能力3.x的设备，本地内存会被缓存在L1/L2 cahce；对于计算能力5.x和6.x设备，本地内存会被缓存到L2 cache。即便如此，其速度还是慢于寄存器。</p>
<h4 id="共享内存-shared-memory"><a href="#共享内存-shared-memory" class="headerlink" title="共享内存(shared memory)"></a>共享内存(shared memory)</h4><p>共享内存实际上是被分为多个存储体(memory bank)。多个线程访问同一个存储体会造成串行化。<br><em>(存疑：存储体其实是可以广播的，因此多个线程读同一个存储体是不存在冲突的，只是写会存在串行化问题)</em></p>
<p>因此，编写内核时，需要认真设计，以避免存储体访问的冲突。</p>
<h2 id="最大化指令吞吐-Maximize-Instruction-Throughput"><a href="#最大化指令吞吐-Maximize-Instruction-Throughput" class="headerlink" title="最大化指令吞吐(Maximize Instruction Throughput)"></a>最大化指令吞吐(Maximize Instruction Throughput)</h2><p>可以使用如下方法来最大化指令吞吐：</p>
<ul>
<li><em>尽量少使用吞吐率低的算数指令</em></li>
<li>尽量减少线程束内的分支</li>
<li>尽量减少指令数，如少用<code>__syncthreads()</code>，或者在合适的时候使用<code>__restrict__</code></li>
</ul>
<p>指令吞吐的定义：<strong>每个SP</strong>在每个时钟周期内执行的操作数。如果一个线程束在一个时钟周期内执行了N个操作，则指令吞吐为N/32。</p>
<h3 id="算数指令-Arithmetic-Instructions"><a href="#算数指令-Arithmetic-Instructions" class="headerlink" title="算数指令(Arithmetic Instructions)"></a>算数指令(Arithmetic Instructions)</h3><p>官方文档这里比较混乱，但主要有如下几点： <em>不同架构的设备，不同指令有不同的指令吞吐，可以查表</em> 有一些快速的内联(inline)函数，如使用<code>__fdividef()</code>(快速浮点数除法)来代替普通的除法来加速 <em>整形的除法和取余会比较慢，可能需要20个机器周期；因此对于n为2的幂次的情况，使用<code>i&gt;&gt;log2(n)</code>代替<code>i/n</code>，使用<code>i&amp;(n-1)</code>来代替<code>i%n</code></em> 半精度(浮点数)运算(Half Precision Arithmetic)：可以使用<code>half2</code>数据类型，并使用对应的运算指令(如<code>__hadd2, __hsub2, __hmul2, __hfma2</code>等)，来让一个周期内执行两次运算，以节省指令带宽。可以通过<code>__halves2half2</code>将两个半精度浮点数合并为<code>half2</code>数据类型。 <em>(半精度又是咋定义的？)</em> * 数据类型转换：当使用char或short，亦或是双精度常量与单精度变量相互操作时，会触发数据类型转换，需要一定执行时间(实际上，char和short，不管是存储在寄存器中，还是在运算时，都是以int型进行的)</p>
<h3 id="控制流指令-Control-Flow-Instructions"><a href="#控制流指令-Control-Flow-Instructions" class="headerlink" title="控制流指令(Control Flow Instructions)"></a>控制流指令(Control Flow Instructions)</h3><p>尽量避免向线程束中引入分支。</p>
<p>此外，可以使用<code>#pragma unroll</code>宏，来进行循环展开，减少控制指令。</p>
<h3 id="同步指令-Synchronization-Instruction"><a href="#同步指令-Synchronization-Instruction" class="headerlink" title="同步指令(Synchronization Instruction)"></a>同步指令(Synchronization Instruction)</h3><p>下表为不同计算能力的设备，同步指令<code>__syncthreads()</code>需要消耗的指令周期为：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">计算能力</th>
<th style="text-align:center"><code>__syncthreads()消耗的指令周期</code></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">3.x</td>
<td style="text-align:center">128</td>
</tr>
<tr>
<td style="text-align:center">5.x,6.1,6.2</td>
<td style="text-align:center">64</td>
</tr>
<tr>
<td style="text-align:center">6.0</td>
<td style="text-align:center">32</td>
</tr>
<tr>
<td style="text-align:center">7.x</td>
<td style="text-align:center">16</td>
</tr>
</tbody>
</table>
</div>
<p>注意，<code>__syncthreads()</code>会造成线程块中的线程等待，影响内核执行效率。</p>
<h1 id="给核函数计时"><a href="#给核函数计时" class="headerlink" title="给核函数计时"></a>给核函数计时</h1><p>gettimeofday是linux下的一个库函数，创建一个cpu计时器，从1970年1月1日0点以来到现在的秒数，需要头文件<code>sys/time.h</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;freshman.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="type">float</span>*a,<span class="type">float</span>*b,<span class="type">float</span>*res,<span class="type">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span>(i &lt; N)</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">// set up device.....</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// init data ......</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//timer</span></span><br><span class="line">  <span class="type">double</span> iStart,iElaps;</span><br><span class="line">  iStart=<span class="built_in">cpuSecond</span>();</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d,b_d,res_d,nElem);</span><br><span class="line">  <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">  iElaps=<span class="built_in">cpuSecond</span>()-iStart;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ......</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>主要分析计时这段，首先iStart是cpuSecond返回一个秒数，接着执行核函数，核函数开始执行后马上返回主机线程，所以我们必须要加一个同步函数等待核函数执行完毕，如果不加这个同步函数，那么测试的时间是从调用核函数，到核函数返回给主机线程的时间段，而不是核函数的执行时间，加上了<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaDeviceSynchronize();</span><br></pre></td></tr></table></figure></p>
<p>函数后，计时是从调用核函数开始，到核函数执行完并返回给主机的时间段，下面图大致描述了执行过程的不同时间节点：<br><img src="/img/20220907141614.png" alt=""></p>
<p>我们可以大概分析下核函数启动到结束的过程：</p>
<ul>
<li>主机线程启动核函数</li>
<li>核函数启动成功</li>
<li>控制返回主机线程</li>
<li>核函数执行完成</li>
<li>主机同步函数侦测到核函数执行完</li>
</ul>
<p>我们要测试的是2~4的时间，但是用CPU计时方法，只能测试1~5的时间，所以测试得到的时间偏长。</p>
<h2 id="用nvprof计时"><a href="#用nvprof计时" class="headerlink" title="用nvprof计时"></a>用nvprof计时</h2><p>CUDA 5.0后有一个工具叫做nvprof的命令行分析工具，后面还要介绍一个图形化的工具，现在我们来学习一下nvprof，学习工具主要技巧是学习工具的功能，当你掌握了一个工具的全部功能，那就是学习成功了。<br>nvprof的用法如下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvprof [nvprof_args] &lt;application&gt;[application_args]</span><br></pre></td></tr></table></figure></p>
<p>工具不仅给出了kernel执行的时间，比例，还有其他cuda函数的执行时间，可以看出核函数执行时间只有4%左右，其他内存分配，内存拷贝占了大部分事件。</p>
<h1 id="组织并行线程"><a href="#组织并行线程" class="headerlink" title="组织并行线程"></a>组织并行线程</h1><h2 id="使用块和线程建立矩阵索引"><a href="#使用块和线程建立矩阵索引" class="headerlink" title="使用块和线程建立矩阵索引"></a>使用块和线程建立矩阵索引</h2><p>多线程的优点就是每个线程处理不同的数据计算，那么怎么分配好每个线程处理不同的数据，而不至于多个不同的线程处理同一个数据。下图可以非常形象的反应线程模型：<br><img src="/img/20220907141615.png" alt=""></p>
<p>这里(ix,iy)就是整个线程模型中任意一个线程的索引，或者叫做全局地址，局部地址当然就是(threadIdx.x,threadIdx.y)了，当然这个局部地址目前还没有什么用处，他只能索引线程块内的线程，不同线程块中有相同的局部索引值，比如同一个小区，A栋有16楼，B栋也有16楼，A栋和B栋就是blockIdx，而16就是threadIdx啦。图中的横坐标就是：<code>ix=threadIdx.x+blockIdx.x×blockDim.x</code>，纵坐标是：<code>iy=threadIdx.y+blockIdx.y×blockDim.y</code></p>
<p>这样我们就得到了每个线程的唯一标号，并且在运行时kernel是可以访问这个标号的。前面讲过CUDA每一个线程执行相同的代码，也就是异构计算中说的多线程单指令，如果每个不同的线程执行同样的代码，又处理同一组数据，将会得到多个相同的结果，显然这是没意义的，为了让不同线程处理不同的数据，CUDA常用的做法是让不同的线程对应不同的数据，也就是用线程的全局标号对应不同组的数据。</p>
<p>设备内存或者主机内存都是线性存在的，我们要做管理的就是：</p>
<ul>
<li>线程和块索引（来计算线程的全局索引）</li>
<li>矩阵中给定点的坐标（ix,iy）</li>
<li>(ix,iy)对应的线性内存的位置</li>
</ul>
<p>线性位置的计算方法是：<code>idx=ix+iy∗nx</code></p>
<p>我们上面已经计算出了线程的全局坐标，用线程的全局坐标对应矩阵的坐标，也就是说，线程的坐标(ix,iy)对应矩阵中(ix,iy)的元素，这样就形成了一一对应，不同的线程处理矩阵中不同的数据，举个具体的例子，ix=10,iy=10的线程去处理矩阵中(10,10)的数据，当然你也可以设计别的对应模式，但是这种方法是最简单出错可能最低的。我们接下来的代码来输出每个线程的标号信息：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">printThreadIndex</span><span class="params">(<span class="type">float</span> *A,<span class="type">const</span> <span class="type">int</span> nx,<span class="type">const</span> <span class="type">int</span> ny)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> ix=threadIdx.x+blockIdx.x*blockDim.x;</span><br><span class="line">  <span class="type">int</span> iy=threadIdx.y+blockIdx.y*blockDim.y;</span><br><span class="line">  <span class="type">unsigned</span> <span class="type">int</span> idx=iy*nx+ix;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;thread_id(%d,%d) block_id(%d,%d) coordinate(%d,%d)&quot;</span></span><br><span class="line">          <span class="string">&quot;global index %2d ival %2d\n&quot;</span>,threadIdx.x,threadIdx.y,</span><br><span class="line">          blockIdx.x,blockIdx.y,ix,iy,idx,A[idx]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">initDevice</span>(<span class="number">0</span>);</span><br><span class="line">  <span class="type">int</span> nx=<span class="number">8</span>,ny=<span class="number">6</span>;</span><br><span class="line">  <span class="type">int</span> nxy=nx*ny;</span><br><span class="line">  <span class="type">int</span> nBytes=nxy*<span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//Malloc</span></span><br><span class="line">  <span class="type">float</span>* A_host=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">  <span class="built_in">initialData</span>(A_host,nxy);</span><br><span class="line">  <span class="built_in">printMatrix</span>(A_host,nx,ny);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//cudaMalloc</span></span><br><span class="line">  <span class="type">float</span> *A_dev=<span class="literal">NULL</span>;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">void</span>**)&amp;A_dev,nBytes));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">cudaMemcpy</span>(A_dev,A_host,nBytes,cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">4</span>,<span class="number">2</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">((nx<span class="number">-1</span>)/block.x<span class="number">+1</span>,(ny<span class="number">-1</span>)/block.y<span class="number">+1</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">  printThreadIndex&lt;&lt;&lt;grid,block&gt;&gt;&gt;(A_dev,nx,ny);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaDeviceSynchronize</span>());</span><br><span class="line">  <span class="built_in">cudaFree</span>(A_dev);</span><br><span class="line">  <span class="built_in">free</span>(A_host);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">cudaDeviceReset</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="二维矩阵加法"><a href="#二维矩阵加法" class="headerlink" title="二维矩阵加法"></a>二维矩阵加法</h2><p>我们利用上面的线程与数据的对应完成了下面的核函数：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sumMatrix</span><span class="params">(<span class="type">float</span> * MatA,<span class="type">float</span> * MatB,<span class="type">float</span> * MatC,<span class="type">int</span> nx,<span class="type">int</span> ny)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> ix=threadIdx.x+blockDim.x*blockIdx.x;</span><br><span class="line">    <span class="type">int</span> iy=threadIdx.y+blockDim.y*blockIdx.y;</span><br><span class="line">    <span class="type">int</span> idx=ix+iy*ny;</span><br><span class="line">    <span class="keyword">if</span> (ix&lt;nx &amp;&amp; iy&lt;ny)</span><br><span class="line">    &#123;</span><br><span class="line">      MatC[idx]=MatA[idx]+MatB[idx];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="二维网格和二维块"><a href="#二维网格和二维块" class="headerlink" title="二维网格和二维块"></a>二维网格和二维块</h2><p>首先来看二维网格二维模块的代码：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2d block and 2d grid</span></span><br><span class="line"><span class="function">dim3 <span class="title">block_0</span><span class="params">(dimx,dimy)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">grid_0</span><span class="params">((nx<span class="number">-1</span>)/block_<span class="number">0.</span>x<span class="number">+1</span>,(ny<span class="number">-1</span>)/block_<span class="number">0.</span>y<span class="number">+1</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">iStart=<span class="built_in">cpuSecond</span>();</span><br><span class="line"></span><br><span class="line">sumMatrix&lt;&lt;&lt;grid_0,block_0&gt;&gt;&gt;(A_dev,B_dev,C_dev,nx,ny);</span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaDeviceSynchronize</span>());</span><br><span class="line"></span><br><span class="line">iElaps=<span class="built_in">cpuSecond</span>()-iStart;</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;GPU Execution configuration&lt;&lt;&lt;(%d,%d),(%d,%d)&gt;&gt;&gt; Time elapsed %f sec\n&quot;</span>,</span><br><span class="line">      grid_<span class="number">0.</span>x,grid_<span class="number">0.</span>y,block_<span class="number">0.</span>x,block_<span class="number">0.</span>y,iElaps);</span><br><span class="line"></span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(C_from_gpu,C_dev,nBytes,cudaMemcpyDeviceToHost));</span><br><span class="line"><span class="built_in">checkResult</span>(C_host,C_from_gpu,nxy);</span><br></pre></td></tr></table></figure></p>
<h2 id="一维网格和一维块"><a href="#一维网格和一维块" class="headerlink" title="一维网格和一维块"></a>一维网格和一维块</h2><p>接着我们使用一维网格一维块：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1d block and 1d grid</span></span><br><span class="line">dimx=<span class="number">32</span>;</span><br><span class="line"><span class="function">dim3 <span class="title">block_1</span><span class="params">(dimx)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">grid_1</span><span class="params">((nxy<span class="number">-1</span>)/block_<span class="number">1.</span>x<span class="number">+1</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">iStart=<span class="built_in">cpuSecond</span>();</span><br><span class="line">sumMatrix&lt;&lt;&lt;grid_1,block_1&gt;&gt;&gt;(A_dev,B_dev,C_dev,nx*ny ,<span class="number">1</span>);</span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaDeviceSynchronize</span>());</span><br><span class="line">iElaps=<span class="built_in">cpuSecond</span>()-iStart;</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;GPU Execution configuration&lt;&lt;&lt;(%d,%d),(%d,%d)&gt;&gt;&gt; Time elapsed %f sec\n&quot;</span>,</span><br><span class="line">      grid_<span class="number">1.</span>x,grid_<span class="number">1.</span>y,block_<span class="number">1.</span>x,block_<span class="number">1.</span>y,iElaps);</span><br><span class="line"></span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(C_from_gpu,C_dev,nBytes,cudaMemcpyDeviceToHost));</span><br><span class="line"><span class="built_in">checkResult</span>(C_host,C_from_gpu,nxy);</span><br></pre></td></tr></table></figure></p>
<h1 id="GPU设备信息"><a href="#GPU设备信息" class="headerlink" title="GPU设备信息"></a>GPU设备信息</h1><p>在软件内查询信息，用到如下代码：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%s Starting ...\n&quot;</span>,argv[<span class="number">0</span>]);</span><br><span class="line">    <span class="type">int</span> deviceCount = <span class="number">0</span>;</span><br><span class="line">    cudaError_t error_id = <span class="built_in">cudaGetDeviceCount</span>(&amp;deviceCount);</span><br><span class="line">    <span class="keyword">if</span>(error_id!=cudaSuccess)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;cudaGetDeviceCount returned %d\n -&gt;%s\n&quot;</span>,</span><br><span class="line">              (<span class="type">int</span>)error_id,<span class="built_in">cudaGetErrorString</span>(error_id));</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Result = FAIL\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(deviceCount==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;There are no available device(s) that support CUDA\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Detected %d CUDA Capable device(s)\n&quot;</span>,deviceCount);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> dev=<span class="number">0</span>,driverVersion=<span class="number">0</span>,runtimeVersion=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line">    cudaDeviceProp deviceProp;</span><br><span class="line">    <span class="built_in">cudaGetDeviceProperties</span>(&amp;deviceProp,dev);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Device %d:\&quot;%s\&quot;\n&quot;</span>,dev,deviceProp.name);</span><br><span class="line">    <span class="built_in">cudaDriverGetVersion</span>(&amp;driverVersion);</span><br><span class="line">    <span class="built_in">cudaRuntimeGetVersion</span>(&amp;runtimeVersion);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  CUDA Driver Version / Runtime Version         %d.%d  /  %d.%d\n&quot;</span>,</span><br><span class="line">        driverVersion/<span class="number">1000</span>,(driverVersion%<span class="number">100</span>)/<span class="number">10</span>,</span><br><span class="line">        runtimeVersion/<span class="number">1000</span>,(runtimeVersion%<span class="number">100</span>)/<span class="number">10</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  CUDA Capability Major/Minor version number:   %d.%d\n&quot;</span>,</span><br><span class="line">        deviceProp.major,deviceProp.minor);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  Total amount of global memory:                %.2f MBytes (%llu bytes)\n&quot;</span>,</span><br><span class="line">            (<span class="type">float</span>)deviceProp.totalGlobalMem/<span class="built_in">pow</span>(<span class="number">1024.0</span>,<span class="number">3</span>));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  GPU Clock rate:                               %.0f MHz (%0.2f GHz)\n&quot;</span>,</span><br><span class="line">            deviceProp.clockRate*<span class="number">1e-3f</span>,deviceProp.clockRate*<span class="number">1e-6f</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  Memory Bus width:                             %d-bits\n&quot;</span>,</span><br><span class="line">            deviceProp.memoryBusWidth);</span><br><span class="line">    <span class="keyword">if</span> (deviceProp.l2CacheSize)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;  L2 Cache Size:                            	%d bytes\n&quot;</span>,</span><br><span class="line">                deviceProp.l2CacheSize);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  Max Texture Dimension Size (x,y,z)            1D=(%d),2D=(%d,%d),3D=(%d,%d,%d)\n&quot;</span>,</span><br><span class="line">            deviceProp.maxTexture1D,deviceProp.maxTexture2D[<span class="number">0</span>],deviceProp.maxTexture2D[<span class="number">1</span>]</span><br><span class="line">            ,deviceProp.maxTexture3D[<span class="number">0</span>],deviceProp.maxTexture3D[<span class="number">1</span>],deviceProp.maxTexture3D[<span class="number">2</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  Max Layered Texture Size (dim) x layers       1D=(%d) x %d,2D=(%d,%d) x %d\n&quot;</span>,</span><br><span class="line">            deviceProp.maxTexture1DLayered[<span class="number">0</span>],deviceProp.maxTexture1DLayered[<span class="number">1</span>],</span><br><span class="line">            deviceProp.maxTexture2DLayered[<span class="number">0</span>],deviceProp.maxTexture2DLayered[<span class="number">1</span>],</span><br><span class="line">            deviceProp.maxTexture2DLayered[<span class="number">2</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  Total amount of constant memory               %lu bytes\n&quot;</span>,</span><br><span class="line">            deviceProp.totalConstMem);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  Total amount of shared memory per block:      %lu bytes\n&quot;</span>,</span><br><span class="line">            deviceProp.sharedMemPerBlock);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  Total number of registers available per block:%d\n&quot;</span>,</span><br><span class="line">            deviceProp.regsPerBlock);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  Wrap size:                                    %d\n&quot;</span>,deviceProp.warpSize);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  Maximun number of thread per multiprocesser:  %d\n&quot;</span>,</span><br><span class="line">            deviceProp.maxThreadsPerMultiProcessor);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  Maximun number of thread per block:           %d\n&quot;</span>,</span><br><span class="line">            deviceProp.maxThreadsPerBlock);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  Maximun size of each dimension of a block:    %d x %d x %d\n&quot;</span>,</span><br><span class="line">            deviceProp.maxThreadsDim[<span class="number">0</span>],deviceProp.maxThreadsDim[<span class="number">1</span>],deviceProp.maxThreadsDim[<span class="number">2</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  Maximun size of each dimension of a grid:     %d x %d x %d\n&quot;</span>,</span><br><span class="line">            deviceProp.maxGridSize[<span class="number">0</span>],</span><br><span class="line">            deviceProp.maxGridSize[<span class="number">1</span>],</span><br><span class="line">            deviceProp.maxGridSize[<span class="number">2</span>]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;  Maximu memory pitch                           %lu bytes\n&quot;</span>,deviceProp.memPitch);</span><br><span class="line">    <span class="built_in">exit</span>(EXIT_SUCCESS);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>输出：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Detected 1 CUDA Capable device(s)</span><br><span class="line">Device 0:&quot;Tesla T4&quot;</span><br><span class="line">  CUDA Driver Version / Runtime Version         11.2  /  11.1</span><br><span class="line">  CUDA Capability Major/Minor version number:   7.5</span><br><span class="line">  Total amount of global memory:                14.76 MBytes (140518271855200 bytes)</span><br><span class="line">  GPU Clock rate:                               1590 MHz (1.59 GHz)</span><br><span class="line">  Memory Bus width:                             256-bits</span><br><span class="line">  L2 Cache Size:                                4194304 bytes</span><br><span class="line">  Max Texture Dimension Size (x,y,z)            1D=(131072),2D=(131072,65536),3D=(16384,16384,16384)</span><br><span class="line">  Max Layered Texture Size (dim) x layers       1D=(32768) x 2048,2D=(32768,32768) x 2048</span><br><span class="line">  Total amount of constant memory               65536 bytes</span><br><span class="line">  Total amount of shared memory per block:      49152 bytes</span><br><span class="line">  Total number of registers available per block:65536</span><br><span class="line">  Wrap size:                                    32</span><br><span class="line">  Maximun number of thread per multiprocesser:  1024</span><br><span class="line">  Maximun number of thread per block:           1024</span><br><span class="line">  Maximun size of each dimension of a block:    1024 x 1024 x 64</span><br><span class="line">  Maximun size of each dimension of a grid:     2147483647 x 65535 x 65535</span><br><span class="line">  Maximu memory pitch                           2147483647 bytes</span><br></pre></td></tr></table></figure></p>
<p>这里面很多参数是我们后面要介绍的，而且每一个都对性能有影响：</p>
<ul>
<li>CUDA驱动版本</li>
<li>设备计算能力编号</li>
<li>全局内存大小（1.95G,原文有错误，写成MBytes了）</li>
<li>GPU主频</li>
<li>GPU带宽</li>
<li>L2缓存大小</li>
<li>纹理维度最大值，不同维度下的</li>
<li>层叠纹理维度最大值</li>
<li>常量内存大小</li>
<li>块内共享内存大小</li>
<li>块内寄存器大小</li>
<li>线程束大小</li>
<li>每个处理器硬件处理的最大线程数</li>
<li>每个块处理的最大线程数</li>
<li>块的最大尺寸</li>
<li>网格的最大尺寸</li>
<li>最大连续线性内存</li>
</ul>
<h1 id="CUDA执行模型概述"><a href="#CUDA执行模型概述" class="headerlink" title="CUDA执行模型概述"></a>CUDA执行模型概述</h1><p>CUDA执行模型揭示了GPU并行架构的抽象视图，再设计硬件的时候，其功能和特性都已经被设计好了，然后去开发硬件，如果这个过程模型特性或功能与硬件设计有冲突，双方就会进行商讨妥协，知道最后产品定型量产，功能和特性算是全部定型，而这些功能和特性就是变成模型的设计基础，而编程模型又直接反应了硬件设计，从而反映了设备的硬件特性。</p>
<p>比如最直观的一个就是内存，线程的层次结构帮助我们控制大规模并行，这个特性就是硬件设计最初设计好，然后集成电路工程师拿去设计，定型后程序员开发驱动，然后在上层可以直接使用这种执行模型来控制硬件。<br>所以了解CUDA的执行模型，可以帮助我们优化指令吞吐量，和内存使用来获得极限速度。</p>
<h2 id="GPU架构概述"><a href="#GPU架构概述" class="headerlink" title="GPU架构概述"></a>GPU架构概述</h2><p>GPU架构是围绕一个流式多处理器（SM）的扩展阵列搭建的。通过复制这种结构来实现GPU的硬件并行。</p>
<p><img src="/img/20220907141616.png" alt=""></p>
<p>上图包括关键组件：</p>
<ul>
<li>CUDA核心</li>
<li>共享内存/一级缓存</li>
<li>寄存器文件</li>
<li>加载/存储单元</li>
<li>特殊功能单元</li>
<li>线程束调度器</li>
</ul>
<p>GPU中每个SM都能支持数百个线程并发执行，每个GPU通常有多个SM，当一个核函数的网格被启动的时候，多个block会被同时分配给可用的SM上执行。</p>
<blockquote>
<p>注意: 当一个blcok被分配给一个SM后，他就只能在这个SM上执行了，不可能重新分配到其他SM上了，多个线程块可以被分配到同一个SM上。</p>
</blockquote>
<p>在SM上同一个块内的多个线程进行线程级别并行，而同一线程内，指令利用指令级并行将单个线程处理成流水线。</p>
<h3 id="线程束"><a href="#线程束" class="headerlink" title="线程束"></a>线程束</h3><p>CUDA 采用单指令多线程SIMT架构管理执行线程，不同设备有不同的线程束大小，但是到目前为止基本所有设备都是维持在32，也就是说每个SM上有多个block，一个block有多个线程（可以是几百个，但不会超过某个最大值），但是从机器的角度，在某时刻T，SM上只执行一个线程束，也就是32个线程在同时同步执行，线程束中的每个线程执行同一条指令，包括有分支的部分，这个我们后面会讲到，</p>
<h3 id="SIMD-vs-SIMT"><a href="#SIMD-vs-SIMT" class="headerlink" title="SIMD vs SIMT"></a>SIMD vs SIMT</h3><p>单指令多数据的执行属于向量机，比如我们有四个数字要加上四个数字，那么我们可以用这种单指令多数据的指令来一次完成本来要做四次的运算。这种机制的问题就是过于死板，不允许每个分支有不同的操作，所有分支必须同时执行相同的指令，必须执行没有例外。</p>
<p>相比之下单指令多线程SIMT就更加灵活了，虽然两者都是将相同指令广播给多个执行单元，但是SIMT的某些线程可以选择不执行，也就是说同一时刻所有线程被分配给相同的指令，SIMD规定所有人必须执行，而SIMT则规定有些人可以根据需要不执行，这样SIMT就保证了线程级别的并行，而SIMD更像是指令级别的并行。</p>
<p>SIMT包括以下SIMD不具有的关键特性：</p>
<ul>
<li>每个线程都有自己的指令地址计数器</li>
<li>每个线程都有自己的寄存器状态</li>
<li>每个线程可以有一个独立的执行路径</li>
</ul>
<p>而上面这三个特性在编程模型可用的方式就是给每个线程一个唯一的标号（blckIdx,threadIdx），并且这三个特性保证了各线程之间的独立</p>
<h3 id="32"><a href="#32" class="headerlink" title="32"></a>32</h3><p>32是个神奇数字，他的产生是硬件系统设计的结果，也就是集成电路工程师搞出来的，所以软件工程师只能接受。</p>
<p>从概念上讲，32是SM以SIMD方式同时处理的工作粒度，这句话这么理解，可能学过后面的会更深刻的明白，一个SM上在某一个时刻，有32个线程在执行同一条指令，这32个线程可以选择性执行，虽然有些可以不执行，但是他也不能执行别的指令，需要另外需要执行这条指令的线程执行完</p>
<h2 id="CUDA编程的组件与逻辑"><a href="#CUDA编程的组件与逻辑" class="headerlink" title="CUDA编程的组件与逻辑"></a>CUDA编程的组件与逻辑</h2><p>下图从逻辑角度和硬件角度描述了CUDA编程模型对应的组件。</p>
<p><img src="/img/20220907141617.png" alt=""></p>
<p>SM中共享内存，和寄存器是关键的资源，线程块中线程通过共享内存和寄存器相互通信协调。寄存器和共享内存的分配可以严重影响性能！</p>
<p>因为SM有限，虽然我们的编程模型层面看所有线程都是并行执行的，但是在微观上看，所有线程块也是分批次的在物理层面的机器上执行，线程块里不同的线程可能进度都不一样，但是同一个线程束内的线程拥有相同的进度。</p>
<p>并行就会引起竞争，多线程以未定义的顺序访问同一个数据，就导致了不可预测的行为，CUDA只提供了一种块内同步的方式，块之间没办法同步！同一个SM上可以有不止一个常驻的线程束，有些在执行，有些在等待，他们之间状态的转换是不需要开销的。</p>
<h2 id="理解线程束执行的本质"><a href="#理解线程束执行的本质" class="headerlink" title="理解线程束执行的本质"></a>理解线程束执行的本质</h2><p>从外表来看，CUDA执行所有的线程，并行的，没有先后次序的，但实际上硬件资源是有限的，不可能同时执行百万个线程，所以从硬件角度来看，物理层面上执行的也只是线程的一部分，而每次执行的这一部分，就是我们前面提到的线程束。</p>
<p>线程束是SM中基本的执行单元，当一个网格被启动（网格被启动，等价于一个内核被启动，每个内核对应于自己的网格），网格中包含线程块，线程块被分配到某一个SM上以后，将分为多个线程束，每个线程束一般是32个线程（目前的GPU都是32个线程，但不保证未来还是32个）在一个线程束中，所有线程按照单指令多线程SIMT的方式执行，每一步执行相同的指令，但是处理的数据为私有的数据。</p>
<p>在块中，每个线程有唯一的编号（可能是个三维的编号），threadIdx。网格中，每个线程块也有唯一的编号(可能是个三维的编号)，blockIdx。那么每个线程就有在网格中的唯一编号。当一个线程块中有128个线程的时候，其分配到SM上执行时，会分成4个块：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">warp0: thread  0,........thread31</span><br><span class="line">warp1: thread 32,........thread63</span><br><span class="line">warp2: thread 64,........thread95</span><br><span class="line">warp3: thread 96,........thread127</span><br></pre></td></tr></table></figure></p>
<p>当编号使用三维编号时，x位于最内层，y位于中层，z位于最外层，想象下c语言的数组，如果把上面这句话写成c语言，假设三维数组t保存了所有的线程，那么(threadIdx.x,threadIdx.y,threadIdx.z)表示为<code>t[z][y][x];</code></p>
<p>计算出三维对应的线性地址是：<code>tid=threadIdx.x+threadIdx.y×blockDim.x+threadIdx.z×blockDim.x×blockDim.y</code>。上面的公式可以借助c语言的三维数组计算相对地址的方法</p>
<p>因为线程束分化导致的性能下降就应该用线程束的方法解决，根本思路是避免同一个线程束内的线程分化，而让我们能控制线程束内线程行为的原因是线程块中线程分配到线程束是有规律的而不是随机的。这就使得我们根据线程编号来设计分支是可以的，补充说明下，当一个线程束中所有的线程都执行if或者，都执行else时，不存在性能下降；只有当线程束内有分歧产生分支的时候，性能才会急剧下降。</p>
<p>线程束内的线程是可以被我们控制的，那么我们就把都执行if的线程塞到一个线程束中，或者让一个线程束中的线程都执行if，另外线程都执行else的这种方式可以将效率提高很多。下面这个kernel可以产生一个比较低效的分支：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">mathKernel1</span><span class="params">(<span class="type">float</span> *c)</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="type">int</span> tid = blockIdx.x* blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">	<span class="type">float</span> a = <span class="number">0.0</span>;</span><br><span class="line">	<span class="type">float</span> b = <span class="number">0.0</span>;</span><br><span class="line">	<span class="keyword">if</span> (tid % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		a = <span class="number">100.0f</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">	&#123;</span><br><span class="line">		b = <span class="number">200.0f</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	c[tid] = a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这种情况下我们假设只配置一个x=64的一维线程块，那么只有两个个线程束，线程束内奇数线程（threadIdx.x为奇数）会执行else，偶数线程执行if，分化很严重。</p>
<p>但是如果我们换一种方法，得到相同但是错乱的结果C，这个顺序其实是无所谓的，因为我们可以后期调整。那么下面代码就会很高效<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">mathKernel2</span><span class="params">(<span class="type">float</span> *c)</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="type">int</span> tid = blockIdx.x* blockDim.x + threadIdx.x;</span><br><span class="line">	<span class="type">float</span> a = <span class="number">0.0</span>;</span><br><span class="line">	<span class="type">float</span> b = <span class="number">0.0</span>;</span><br><span class="line">	<span class="keyword">if</span> ((tid/warpSize) % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		a = <span class="number">100.0f</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">	&#123;</span><br><span class="line">		b = <span class="number">200.0f</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	c[tid] = a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>第一个线程束内的线程编号tid从0到31，tid/warpSize都等于0，那么就都执行if语句。第二个线程束内的线程编号tid从32到63，tid/warpSize都等于1，执行else。线程束内没有分支，效率较高。</p>
<h2 id="延迟隐藏"><a href="#延迟隐藏" class="headerlink" title="延迟隐藏"></a>延迟隐藏</h2><p>与其他类型的编程相比，GPU的延迟隐藏及其重要。对于指令的延迟，通常分为两种：</p>
<ul>
<li>算术指令</li>
<li>内存指令</li>
</ul>
<p>算数指令延迟是一个算术操作从开始，到产生结果之间的时间，这个时间段内只有某些计算单元处于工作状态，而其他逻辑计算单元处于空闲。内存指令延迟很好理解，当产生内存访问的时候，计算单元要等数据从内存拿到寄存器，这个周期是非常长的。</p>
<p>延迟：</p>
<ul>
<li>算术延迟 10~20 个时钟周期</li>
<li>内存延迟 400~800 个时钟周期</li>
</ul>
<h2 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h2><p>并发程序对同步非常有用，比如pthread中的锁，openmp中的同步机制，主要目的是避免内存竞争。CUDA同步这里只讲两种：</p>
<ul>
<li>线程块内同步</li>
<li>系统级别</li>
</ul>
<p>块级别的就是同一个块内的线程会同时停止在某个设定的位置，用<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__syncthread();</span><br></pre></td></tr></table></figure></p>
<p>这个函数完成，这个函数只能同步同一个块内的线程，不能同步不同块内的线程，想要同步不同块内的线程，就只能让核函数执行完成，控制程序交换主机，这种方式来同步所有线程。</p>
<p>内存竞争是非常危险的，一定要非常小心，这里经常出错。</p>
<h2 id="并行性表现"><a href="#并行性表现" class="headerlink" title="并行性表现"></a>并行性表现</h2><p>本文的主要内容就是进一步理解线程束在硬件上执行的本质过程，结合上几篇关于执行模型的学习，本文相对简单，通过修改核函数的配置，来观察核函数的执行速度，以及分析硬件利用数据，分析性能，调整核函数配置是CUDA开发人员必须掌握的技能，本篇只研究对核函数的配置是如何影响效率的（也就是通过网格，块的配置来获得不同的执行效率。）本文全文只用到下面的核函数<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sumMatrix</span><span class="params">(<span class="type">float</span> * MatA,<span class="type">float</span> * MatB,<span class="type">float</span> * MatC,<span class="type">int</span> nx,<span class="type">int</span> ny)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> ix=threadIdx.x+blockDim.x*blockIdx.x;</span><br><span class="line">    <span class="type">int</span> iy=threadIdx.y+blockDim.y*blockIdx.y;</span><br><span class="line">    <span class="type">int</span> idx=ix+iy*ny;</span><br><span class="line">    <span class="keyword">if</span> (ix&lt;nx &amp;&amp; iy&lt;ny)</span><br><span class="line">    &#123;</span><br><span class="line">      MatC[idx]=MatA[idx]+MatB[idx];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>没有任何优化的最简单的二维矩阵加法。</p>
<p>全部代码：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//printf(&quot;strating...\n&quot;);</span></span><br><span class="line">  <span class="comment">//initDevice(0);</span></span><br><span class="line">  <span class="type">int</span> nx=<span class="number">1</span>&lt;&lt;<span class="number">13</span>;</span><br><span class="line">  <span class="type">int</span> ny=<span class="number">1</span>&lt;&lt;<span class="number">13</span>;</span><br><span class="line">  <span class="type">int</span> nxy=nx*ny;</span><br><span class="line">  <span class="type">int</span> nBytes=nxy*<span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//Malloc</span></span><br><span class="line">  <span class="type">float</span>* A_host=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">  <span class="type">float</span>* B_host=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">  <span class="type">float</span>* C_host=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">  <span class="type">float</span>* C_from_gpu=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">  <span class="built_in">initialData</span>(A_host,nxy);</span><br><span class="line">  <span class="built_in">initialData</span>(B_host,nxy);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//cudaMalloc</span></span><br><span class="line">  <span class="type">float</span> *A_dev=<span class="literal">NULL</span>;</span><br><span class="line">  <span class="type">float</span> *B_dev=<span class="literal">NULL</span>;</span><br><span class="line">  <span class="type">float</span> *C_dev=<span class="literal">NULL</span>;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">void</span>**)&amp;A_dev,nBytes));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">void</span>**)&amp;B_dev,nBytes));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">void</span>**)&amp;C_dev,nBytes));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(A_dev,A_host,nBytes,cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(B_dev,B_host,nBytes,cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> dimx=argc&gt;<span class="number">2</span>?<span class="built_in">atoi</span>(argv[<span class="number">1</span>]):<span class="number">32</span>;</span><br><span class="line">  <span class="type">int</span> dimy=argc&gt;<span class="number">2</span>?<span class="built_in">atoi</span>(argv[<span class="number">2</span>]):<span class="number">32</span>;</span><br><span class="line"></span><br><span class="line">  <span class="type">double</span> iStart,iElaps;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2d block and 2d grid</span></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(dimx,dimy)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">((nx<span class="number">-1</span>)/block.x<span class="number">+1</span>,(ny<span class="number">-1</span>)/block.y<span class="number">+1</span>)</span></span>;</span><br><span class="line">  iStart=<span class="built_in">cpuSecond</span>();</span><br><span class="line">  sumMatrix&lt;&lt;&lt;grid,block&gt;&gt;&gt;(A_dev,B_dev,C_dev,nx,ny);</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaDeviceSynchronize</span>());</span><br><span class="line">  iElaps=<span class="built_in">cpuSecond</span>()-iStart;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;GPU Execution configuration&lt;&lt;&lt;(%d,%d),(%d,%d)|%f sec\n&quot;</span>,</span><br><span class="line">        grid.x,grid.y,block.x,block.y,iElaps);</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(C_from_gpu,C_dev,nBytes,cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">cudaFree</span>(A_dev);</span><br><span class="line">  <span class="built_in">cudaFree</span>(B_dev);</span><br><span class="line">  <span class="built_in">cudaFree</span>(C_dev);</span><br><span class="line">  <span class="built_in">free</span>(A_host);</span><br><span class="line">  <span class="built_in">free</span>(B_host);</span><br><span class="line">  <span class="built_in">free</span>(C_host);</span><br><span class="line">  <span class="built_in">free</span>(C_from_gpu);</span><br><span class="line">  <span class="built_in">cudaDeviceReset</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可见我们用两个 8192×8192 的矩阵相加来测试我们效率。</p>
<h2 id="避免分支分化"><a href="#避免分支分化" class="headerlink" title="避免分支分化"></a>避免分支分化</h2><h3 id="并行规约问题"><a href="#并行规约问题" class="headerlink" title="并行规约问题"></a>并行规约问题</h3><p>在串行编程中，我们最最最常见的一个问题就是一组特别多数字通过计算变成一个数字，比如加法，也就是求这一组数据的和，或者乘法，对应的加法或者乘法就是交换律和结合律。归约的方式基本包括如下几个步骤：</p>
<ul>
<li>将输入向量划分到更小的数据块中</li>
<li>用一个线程计算一个数据块的部分和</li>
<li>对每个数据块的部分和再求和得到最终的结果。</li>
<li>数据分块保证我们可以用一个线程块来处理一个数据块。</li>
<li>一个线程处理更小的块，所以一个线程块可以处理一个较大的块，然后多个块完成整个数据集的处理。</li>
<li>最后将所有线程块得到的结果相加，就是结果，这一步一般在cpu上完成。</li>
</ul>
<p>归约问题最常见的加法计算是把向量的数据分成对，然后用不同线程计算每一对元素，得到的结果作为输入继续分成对，迭代的进行，直到最后一个元素。成对的划分常见的方法有以下两种：</p>
<ol>
<li>相邻配对：元素与他们相邻的元素配对</li>
<li>交错配对：元素与一定距离的元素配对</li>
</ol>
<p>首先是cpu版本实现交错配对归约计算的代码：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">recursiveReduce</span><span class="params">(<span class="type">int</span> *data, <span class="type">int</span> <span class="type">const</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">// terminate check</span></span><br><span class="line">	<span class="keyword">if</span> (size == <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> data[<span class="number">0</span>];</span><br><span class="line">	<span class="comment">// renew the stride</span></span><br><span class="line">	<span class="type">int</span> <span class="type">const</span> stride = size / <span class="number">2</span>;</span><br><span class="line">	<span class="keyword">if</span> (size % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; stride; i++)</span><br><span class="line">		&#123;</span><br><span class="line">			data[i] += data[i + stride];</span><br><span class="line">		&#125;</span><br><span class="line">		data[<span class="number">0</span>] += data[size - <span class="number">1</span>];</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; stride; i++)</span><br><span class="line">		&#123;</span><br><span class="line">			data[i] += data[i + stride];</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// call</span></span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">recursiveReduce</span>(data, stride);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="并行规约中的分化"><a href="#并行规约中的分化" class="headerlink" title="并行规约中的分化"></a>并行规约中的分化</h3><p>线程束分化已经明确说明了，有判断条件的地方就会产生分支，比如if 和 for这类关键词。</p>
<p>第一步：是把这个一个数组分块，每一块只包含部分数据，如上图那样（图中数据较少，但是我们假设一块上只有这么多。），我们假定这是线程块的全部数据</p>
<p>第二步：就是每个线程要做的事，橙色圆圈就是每个线程做的操作，可见线程threadIdx.x=0 的线程进行了三次计算，奇数线程一致在陪跑，没做过任何计算，但是根据3.2中介绍，这些线程虽然什么都不干，但是不可以执行别的指令，4号线程做了两步计算，2号和6号只做了一次计算。</p>
<p>第三步：将所有块得到的结果相加，就是最终结果</p>
<p>这个计算划分就是最简单的并行规约算法，完全符合上面我们提到的三步走的套路</p>
<p>值得注意的是，我们每次进行一轮计算（黄色框，这些操作同时并行）的时候，部分全局内存要进行一次修改，但只有部分被替换，而不被替换的，也不会在后面被使用到，如蓝色框里标注的内存，就被读了一次，后面就完全没有人管了。</p>
<p>我们现在把我们的内核代码贴出来<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduceNeighbored</span><span class="params">(<span class="type">int</span> * g_idata,<span class="type">int</span> * g_odata,<span class="type">unsigned</span> <span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">//set thread ID</span></span><br><span class="line">	<span class="type">unsigned</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">	<span class="comment">//boundary check</span></span><br><span class="line">	<span class="keyword">if</span> (tid &gt;= n) <span class="keyword">return</span>;</span><br><span class="line">	<span class="comment">//convert global data pointer to the</span></span><br><span class="line">	<span class="type">int</span> *idata = g_idata + blockIdx.x*blockDim.x;</span><br><span class="line">	<span class="comment">//in-place reduction in global memory</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="type">int</span> stride = <span class="number">1</span>; stride &lt; blockDim.x; stride *= <span class="number">2</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">if</span> ((tid % (<span class="number">2</span> * stride)) == <span class="number">0</span>)</span><br><span class="line">		&#123;</span><br><span class="line">			idata[tid] += idata[tid + stride];</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//synchronize within block</span></span><br><span class="line">		__syncthreads();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">//write result for this block to global mem</span></span><br><span class="line">	<span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">		g_odata[blockIdx.x] = idata[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里面唯一要注意的地方就是同步指令<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__syncthreads();</span><br></pre></td></tr></table></figure></p>
<p>原因还是能从图上找到，我们的每一轮操作都是并行的，但是不保证所有线程能同时执行完毕，所以需要等待，执行的快的等待慢的，这样就能避免块内的线程竞争内存了。</p>
<p>被操作的两个对象之间的距离叫做跨度，也就是变量stride，</p>
<h2 id="展开循环"><a href="#展开循环" class="headerlink" title="展开循环"></a>展开循环</h2><p>目前CUDA的编译器还不能帮我们做这种优化，人为的展开核函数内的循环，能够非常大的提升内核性能。在CUDA中展开循环的目的还是那两个：</p>
<ul>
<li>减少指令消耗</li>
<li>增加更多的独立调度指令来提高性能</li>
</ul>
<p>如果这种指令<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a[i<span class="number">+0</span>]=b[i<span class="number">+0</span>]+c[i<span class="number">+0</span>];</span><br><span class="line">a[i<span class="number">+1</span>]=b[i<span class="number">+1</span>]+c[i<span class="number">+1</span>];</span><br><span class="line">a[i<span class="number">+2</span>]=b[i<span class="number">+2</span>]+c[i<span class="number">+2</span>];</span><br><span class="line">a[i<span class="number">+3</span>]=b[i<span class="number">+3</span>]+c[i<span class="number">+3</span>];</span><br></pre></td></tr></table></figure></p>
<p>被添加到CUDA流水线上，是非常受欢迎的，因为其能最大限度的提高指令和内存带宽。下面我们就在前面归约的例子上继续挖掘性能，看看是否能得到更高的效率。</p>
<h1 id="cuda内存模型"><a href="#cuda内存模型" class="headerlink" title="cuda内存模型"></a>cuda内存模型</h1><p>CUDA内存模型相对于CPU来说那是相当丰富了，GPU上的内存设备有：</p>
<ul>
<li>寄存器</li>
<li>共享内存</li>
<li>本地内存</li>
<li>常量内存</li>
<li>纹理内存</li>
<li>全局内存</li>
</ul>
<p>上述各种都有自己的作用域，生命周期和缓存行为。CUDA中每个线程都有自己的私有的本地内存；线程块有自己的共享内存，对线程块内所有线程可见；所有线程都能访问读取常量内存和纹理内存，但是不能写，因为他们是只读的；全局内存，常量内存和纹理内存空间有不同的用途。对于一个应用来说，全局内存，常量内存和纹理内存有相同的生命周期。下图总结了上面这段话，后面的大篇幅文章就是挨个介绍这些内存的性质和使用的。</p>
<p><img src="/img/20220907141618.png" alt=""></p>
<h2 id="寄存器"><a href="#寄存器" class="headerlink" title="寄存器"></a>寄存器</h2><p>寄存器无论是在CPU还是在GPU都是速度最快的内存空间，但是和CPU不同的是GPU的寄存器储量要多一些，而且当我们在核函数内不加修饰的声明一个变量，此变量就存储在寄存器中，但是CPU运行的程序有些不同，只有当前在计算的变量存储在寄存器中，其余在主存中，使用时传输至寄存器。在核函数中定义的有常数长度的数组也是在寄存器中分配地址的。</p>
<p>寄存器对于每个线程是私有的，寄存器通常保存被频繁使用的私有变量，注意这里的变量也一定不能使共有的，不然的话彼此之间不可见，就会导致大家同时改变一个变量而互相不知道，寄存器变量的声明周期和核函数一致，从开始运行到运行结束，执行完毕后，寄存器就不能访问了。</p>
<p>寄存器是SM中的稀缺资源，Fermi架构中每个线程最多63个寄存器。Kepler结构扩展到255个寄存器，一个线程如果使用更少的寄存器，那么就会有更多的常驻线程块，SM上并发的线程块越多，效率越高，性能和使用率也就越高。</p>
<p>那么问题就来了，如果一个线程里面的变量太多，以至于寄存器完全不够呢？这时候寄存器发生溢出，本地内存就会过来帮忙存储多出来的变量，这种情况会对效率产生非常负面的影响，所以，不到万不得已，一定要避免此种情况发生。</p>
<p>为了避免寄存器溢出，可以在核函数的代码中配置额外的信息来辅助编译器优化，比如：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span></span><br><span class="line">__lauch_bounds__(maxThreadaPerBlock,minBlocksPerMultiprocessor)</span><br><span class="line">kernel(...) &#123;</span><br><span class="line">    <span class="comment">/* kernel code */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里面在核函数定义前加了一个 关键字 lauch_bounds，然后他后面对应了两个变量：</p>
<ol>
<li>maxThreadaPerBlock：线程块内包含的最大线程数，线程块由核函数来启动</li>
<li>minBlocksPerMultiprocessor：可选参数，每个SM中预期的最小的常驻内存块参数。</li>
</ol>
<p>注意，对于一定的核函数，优化的启动边界会因为不同的结构而不同。也可以在编译选项中加入<code>-maxrregcount=32</code>来控制一个编译单元里所有核函数使用的最大数量。</p>
<h2 id="本地内存"><a href="#本地内存" class="headerlink" title="本地内存"></a>本地内存</h2><p>核函数中符合存储在寄存器中但不能进入被核函数分配的寄存器空间中的变量将存储在本地内存中，编译器可能存放在本地内存中的变量有以下几种：</p>
<ul>
<li>使用未知索引引用的本地数组</li>
<li>可能会占用大量寄存器空间的较大本地数组或者结构体</li>
<li>任何不满足核函数寄存器限定条件的变量</li>
</ul>
<p>本地内存实质上是和全局内存一样在同一块存储区域当中的，其访问特点——高延迟，低带宽。对于2.0以上的设备，本地内存存储在每个SM的一级缓存，或者设备的二级缓存上。</p>
<h2 id="共享内存-1"><a href="#共享内存-1" class="headerlink" title="共享内存"></a>共享内存</h2><p>在核函数中使用如下修饰符的内存，称为共享内存：<code>__share__</code>。</p>
<p>每个SM都有一定数量的由线程块分配的共享内存，共享内存是片上内存，跟主存相比，速度要快很多，也即是延迟低，带宽高。其类似于一级缓存，但是可以被编程。使用共享内存的时候一定要注意，不要因为过度使用共享内存，而导致SM上活跃的线程束减少，也就是说，一个线程块使用的共享内存过多，导致更过的线程块没办法被SM启动，这样影响活跃的线程束数量。</p>
<p>共享内存在核函数内声明，生命周期和线程块一致，线程块运行开始，此块的共享内存被分配，当此块结束，则共享内存被释放。因为共享内存是块内线程可见的，所以就有竞争问题的存在，也可以通过共享内存进行通信，当然，为了避免内存竞争，可以使用同步语句：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncthreads();</span><br></pre></td></tr></table></figure></p>
<p>此语句相当于在线程块执行时各个线程的一个障碍点，当块内所有线程都执行到本障碍点的时候才能进行下一步的计算，这样可以设计出避免内存竞争的共享内存使用程序。</p>
<p>注意，<code>__syncthreads();</code>频繁使用会影响内核执行效率。</p>
<p>SM中的一级缓存，和共享内存共享一个64k的片上内存（不知道现在的设备有没有提高），他们通过静态划分，划分彼此的容量，运行时可以通过下面语句进行设置：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaFuncSetCacheConfig</span><span class="params">(<span class="type">const</span> <span class="type">void</span> * func,<span class="keyword">enum</span> cudaFuncCache)</span>;</span><br></pre></td></tr></table></figure></p>
<p>这个函数可以设置内核的共享内存和一级缓存之间的比例。cudaFuncCache参数可选如下配置：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cudaFuncCachePreferNone<span class="comment">//无参考值，默认设置</span></span><br><span class="line">cudaFuncCachePreferShared<span class="comment">//48k共享内存，16k一级缓存</span></span><br><span class="line">cudaFuncCachePreferL1<span class="comment">// 48k一级缓存，16k共享内存</span></span><br><span class="line">cudaFuncCachePreferEqual<span class="comment">// 32k一级缓存，32k共享内存</span></span><br></pre></td></tr></table></figure></p>
<h2 id="常量内存"><a href="#常量内存" class="headerlink" title="常量内存"></a>常量内存</h2><p>常量内存驻留在设备内存中，每个SM都有专用的常量内存缓存，常量内存使用：<code>__constant__</code>修饰，常量内存在核函数外，全局范围内声明，对于所有设备，只可以声明64k的常量内存，常量内存静态声明，并对同一编译单元中的所有核函数可见。</p>
<p>常量内存，显然是不能被修改的，这里不能被修改指的是被核函数修改，主机端代码是可以初始化常量内存的，不然这个内存谁都不能改就没有什么使用意义了，常量内存，被主机端初始化后不能被核函数修改，初始化函数如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaMemcpyToSymbol</span><span class="params">(<span class="type">const</span> <span class="type">void</span>* symbol,<span class="type">const</span> <span class="type">void</span> *src,<span class="type">size_t</span> count)</span>;</span><br></pre></td></tr></table></figure></p>
<p>同 cudaMemcpy的参数列表相似，从src复制count个字节的内存到symbol里面，也就是设备端的常量内存。多数情况下此函数是同步的，也就是会马上被执行。</p>
<p>当线程束中所有线程都从相同的地址取数据时，常量内存表现较好，比如执行某一个多项式计算，系数都存在常量内存里效率会非常高，但是如果不同的线程取不同地址的数据，常量内存就不那么好了，因为常量内存的读取机制是：一次读取会广播给所有线程束内的线程。</p>
<h2 id="纹理内存"><a href="#纹理内存" class="headerlink" title="纹理内存"></a>纹理内存</h2><p>纹理内存驻留在设备内存中，在每个SM的只读缓存中缓存，纹理内存是通过指定的缓存访问的全局内存，只读缓存包括硬件滤波的支持，它可以将浮点插入作为读取过程中的一部分来执行，纹理内存是对二维空间局部性的优化。总的来说纹理内存设计目的应该是为了GPU本职工作显示设计的，但是对于某些特定的程序可能效果更好，比如需要滤波的程序，可以直接通过硬件完成。</p>
<h2 id="全局内存"><a href="#全局内存" class="headerlink" title="全局内存"></a>全局内存</h2><p>GPU上最大的内存空间，延迟最高，使用最常见的内存，global指的是作用域和生命周期，一般在主机端代码里定义，也可以在设备端定义，不过需要加修饰符，只要不销毁，是和应用程序同生命周期的。全局内存对应于设备内存，一个是逻辑表示，一个是硬件表示。</p>
<p>全局内存可以动态声明，或者静态声明，可以用下面的修饰符在设备代码中静态的声明一个变量：<code>__device__</code>。我们前面声明的所有的在GPU上访问的内存都是全局内存，或者说到目前为止我们还没对内存进行任何优化。因为全局内存的性质，当有多个核函数同时执行的时候，如果使用到了同一全局变量，应注意内存竞争。</p>
<p>全局内存访问是对齐，也就是一次要读取指定大小（32，64，128）整数倍字节的内存，所以当线程束执行内存加载/存储时，需要满足的传输数量通常取决与以下两个因素：</p>
<ul>
<li>跨线程的内存地址分布</li>
<li>内存事务的对齐方式。</li>
</ul>
<p>一般情况下满足内存请求的事务越多，未使用的字节被传输的可能性越大，数据吞吐量就会降低，换句话说，对齐的读写模式使得不需要的数据也被传输，所以，利用率低到时吞吐量下降。1.1以下的设备对内存访问要求非常严格（为了达到高效，访问受到限制）因为当时还没有缓存，现在的设备都有缓存了，所以宽松了一些。</p>
<h2 id="GPU缓存"><a href="#GPU缓存" class="headerlink" title="GPU缓存"></a>GPU缓存</h2><p>与CPU缓存类似，GPU缓存不可编程，其行为出厂是时已经设定好了。GPU上有4种缓存：</p>
<ul>
<li>一级缓存</li>
<li>二级缓存</li>
<li>只读常量缓存</li>
<li>只读纹理缓存</li>
</ul>
<p>每个SM都有一个一级缓存，所有SM公用一个二级缓存。一级二级缓存的作用都是被用来存储本地内存和全局内存中的数据，也包括寄存器溢出的部分。Fermi，Kepler以及以后的设备，CUDA允许我们配置读操作的数据是使用一级缓存和二级缓存，还是只使用二级缓存。</p>
<p>与CPU不同的是，CPU读写过程都有可能被缓存，但是GPU写的过程不被缓存，只有加载会被缓存！</p>
<p>每个SM有一个只读常量缓存，只读纹理缓存，它们用于设备内存中提高来自于各自内存空间内的读取性能。</p>
<p>CUDA变量声明总结<br>用表格进行总结：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>修饰符</th>
<th>变量名称</th>
<th>存储器</th>
<th>作用域</th>
<th>生命周期</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>float var</td>
<td>寄存器</td>
<td>线程</td>
<td>线程</td>
</tr>
<tr>
<td></td>
<td>float var[100]</td>
<td>本地</td>
<td>线程</td>
<td>线程</td>
</tr>
<tr>
<td><strong>share</strong></td>
<td>float var*</td>
<td>共享</td>
<td>块</td>
<td>块</td>
</tr>
<tr>
<td><strong>device</strong></td>
<td>float var*</td>
<td>全局</td>
<td>全局</td>
<td>应用程序</td>
</tr>
<tr>
<td>__constant</td>
<td>float var*</td>
<td>常量</td>
<td>全局</td>
<td>应用程序</td>
</tr>
</tbody>
</table>
</div>
<p>设备存储器的重要特征：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>存储器</th>
<th>片上/片外</th>
<th>缓存</th>
<th>存取</th>
<th>范围</th>
<th>生命周期</th>
</tr>
</thead>
<tbody>
<tr>
<td>寄存器</td>
<td>片上</td>
<td>n/a</td>
<td>R/W</td>
<td>一个线程</td>
<td>线程</td>
</tr>
<tr>
<td>本地</td>
<td>片外</td>
<td>1.0以上有</td>
<td>R/W</td>
<td>一个线程</td>
<td>线程</td>
</tr>
<tr>
<td>共享</td>
<td>片上</td>
<td>n/a</td>
<td>R/W</td>
<td>块内所有线程</td>
<td>块</td>
</tr>
<tr>
<td>全局</td>
<td>片外</td>
<td>1.0以上有</td>
<td>R/W</td>
<td>所有线程+主机</td>
<td>主机配置</td>
</tr>
<tr>
<td>常量</td>
<td>片外</td>
<td>Yes</td>
<td>R</td>
<td>所有线程+主机</td>
<td>主机配置</td>
</tr>
<tr>
<td>纹理</td>
<td>片外</td>
<td>Yes</td>
<td>R</td>
<td>所有线程+主机</td>
<td>主机配置</td>
</tr>
</tbody>
</table>
</div>
<h2 id="静态全局内存"><a href="#静态全局内存" class="headerlink" title="静态全局内存"></a>静态全局内存</h2><p>CPU内存有动态分配和静态分配两种类型，从内存位置来说，动态分配在堆上进行，静态分配在栈上进行，在代码上的表现是一个需要new，malloc等类似的函数动态分配空间，并用delete和free来释放。在CUDA中也有类似的动态静态之分，我们前面用的都是要cudaMalloc的，所以对比来说就是动态分配，我们今天来个静态分配的，不过与动态分配相同是，也需要显式的将内存copy到设备端，我们用下面代码来看一下程序的运行结果:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line">__device__ <span class="type">float</span> devData;</span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">checkGlobalVariable</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Device: The value of the global variable is %f\n&quot;</span>,devData);</span><br><span class="line">    devData+=<span class="number">2.0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> value=<span class="number">3.14f</span>;</span><br><span class="line">    cudaMemcpyToSymbol(devData,&amp;value,<span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Host: copy %f to the global variable\n&quot;</span>,value);</span><br><span class="line">    checkGlobalVariable&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">    cudaMemcpyFromSymbol(&amp;value,devData,<span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Host: the value changed by the kernel to %f \n&quot;</span>,value);</span><br><span class="line">    cudaDeviceReset();</span><br><span class="line">    <span class="keyword">return</span> EXIT_SUCCESS;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个唯一要注意的就是，这一句<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpyToSymbol(devData,&amp;value,sizeof(float));</span><br></pre></td></tr></table></figure></p>
<p>函数原型说的是第一个应该是个void*，但是这里写了一个<code>device float devData;</code>变量，这个说到底还是设备上的变量定义和主机变量定义的不同，设备变量在代码中定义的时候其实就是一个指针，这个指针指向何处，主机端是不知道的，指向的内容也不知道，想知道指向的内容，唯一的办法还是通过显式的办法传输过来：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpyFromSymbol(&amp;value,devData,sizeof(float));</span><br></pre></td></tr></table></figure></p>
<p>这里需要注意的只有这点：在主机端，devData只是一个标识符，不是设备全局内存的变量地址<br>在核函数中，devData就是一个全局内存中的变量。主机代码不能直接访问设备变量，设备也不能访问主机变量，这就是CUDA编程与CPU多核最大的不同之处<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpy(&amp;value,devData,sizeof(float));</span><br></pre></td></tr></table></figure></p>
<p>是不可以的！这个函数是无效的！就是你不能用动态copy的方法给静态变量赋值！</p>
<p>如果你死活都要用cudaMemcpy，只能用下面的方式：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> *dptr=<span class="literal">NULL</span>;</span><br><span class="line"><span class="built_in">cudaGetSymbolAddress</span>((<span class="type">void</span>**)&amp;dptr,devData);</span><br><span class="line"><span class="built_in">cudaMemcpy</span>(dptr,&amp;value,<span class="built_in">sizeof</span>(<span class="type">float</span>),cudaMemcpyHostToDevice);</span><br></pre></td></tr></table></figure></p>
<p>主机端不可以对设备变量进行取地址操作！这是非法的！</p>
<p>想要得到devData的地址可以用下面方法：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> *dptr=<span class="literal">NULL</span>;</span><br><span class="line"><span class="built_in">cudaGetSymbolAddress</span>((<span class="type">void</span>**)&amp;dptr,devData);</span><br></pre></td></tr></table></figure></p>
<p>当然也有一个例外，可以直接从主机引用GPU内存——CUDA固定内存。后面我们会研究这部分。</p>
<p>CUDA运行时API能访问主机和设备变量，但这取决于你给正确的函数是否提供了正确的参数，使用运行时API，如果参数填错，尤其是主机和设备上的指针，结果是无法预测的。</p>
<h1 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h1><p>CUDA是C语言的扩展，内存方面基本集成了C语言的方式，由程序员控制CUDA内存，当然，这些内存的物理设备是在GPU上的，而且与CPU内存分配不同，CPU内存分配完就完事了，GPU还涉及到数据传输，主机和设备之间的传输。接下来我们要了解的是：</p>
<ul>
<li>分配释放设备内存</li>
<li>在主机和设备间传输内存</li>
</ul>
<p>为达到最优性能，CUDA提供了在主机端准备设备内存的函数，并且显式地向设备传递数据，显式的从设备取回数据。</p>
<h2 id="内存分配和释放"><a href="#内存分配和释放" class="headerlink" title="内存分配和释放"></a>内存分配和释放</h2><p>内存的分配和释放我们在前面已经用过很多次了，前面所有的要计算的例子都包含这一步：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span><span class="params">(<span class="type">void</span> ** devPtr,<span class="type">size_t</span> count)</span></span></span><br></pre></td></tr></table></figure>
<p>这个函数用过很多次了，唯一要注意的是第一个参数，是指针的指针，一般的用法是首先我们生命一个指针变量，然后调用这个函数：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> * devMem=<span class="literal">NULL</span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span><span class="params">((<span class="type">float</span>**) devMem, count)</span></span></span><br></pre></td></tr></table></figure>
<p>这里是这样的，devMem是一个指针，定义时初始化指向NULL，这样做是安全的，避免出现野指针，cudaMalloc函数要修改devMem的值，所以必须把他的指针传递给函数，如果把devMem当做参数传递，经过函数后，指针的内容还是NULL。</p>
<p>内存分配支持所有的数据类型，什么int，float。。。这些都无所谓，因为他是按照字节分配的，只要是正数字节的变量都能分配，当然我们根本没有半个字节的东西。函数执行失败返回：cudaErrorMemoryAllocation。</p>
<p>当分配完地址后，可以使用下面函数进行初始化：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemset</span><span class="params">(<span class="type">void</span> * devPtr,<span class="type">int</span> value,<span class="type">size_t</span> count)</span></span></span><br></pre></td></tr></table></figure>
<p>用法和Memset类似，但是注意，这些被我们操作的内存对应的物理内存都在GPU上。</p>
<p>当分配的内存不被使用时，使用下面语句释放程序。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaFree</span><span class="params">(<span class="type">void</span> * devPtr)</span></span></span><br></pre></td></tr></table></figure>
<p>注意这个参数一定是前面cudaMalloc类的函数（还有其他分配函数）分配到空间，如果输入非法指针参数，会返回 cudaErrorInvalidDevicePointer 错误，如果重复释放一个空间，也会报错。</p>
<h2 id="内存传输"><a href="#内存传输" class="headerlink" title="内存传输"></a>内存传输</h2><p>下面介绍点C语言没有的，C语言的内存分配完成后就可以直接读写了，但是对于异构计算，这样是不行的，因为主机线程不能访问设备内存，设备线程也不能访问主机内存，这时候我们要传送数据了：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t cudaMemcpy(void *dst,const void * src,size_t count,enum cudaMemcpyKind kind)</span><br></pre></td></tr></table></figure>
<p>这个函数我们前面也反复用到，注意这里的参数是指针，而不是指针的指针，第一个参数dst是目标地址，第二个参数src是原始地址，然后是拷贝的内存大小，最后是传输类型，传输类型包括以下几种：</p>
<ul>
<li>cudaMemcpyHostToHost</li>
<li>cudaMemcpyHostToDevice</li>
<li>cudaMemcpyDeviceToHost</li>
<li>cudaMemcpyDeviceToDevice</li>
</ul>
<p>这个例子也不用说了，前面随便找个有数据传输的都有这两步：从主机到设备，然后计算，最后从设备到主机。</p>
<p><img src="/img/image-20220910105700518.png" alt="image-20220910105700518"></p>
<p>GPU的内存理论峰值带宽非常高，对于Fermi C2050 有144GB/s，这个值估计现在的GPU应该都超过了，CPU和GPU之间通信要经过PCIe总线，总线的理论峰值要低很多——8GB/s左右，也就是说所，管理不当，算到半路需要从主机读数据，那效率瞬间全挂在PCIe上了。</p>
<p><strong>CUDA编程需要大家减少主机和设备之间的内存传输</strong>。</p>
<h2 id="固定内存"><a href="#固定内存" class="headerlink" title="固定内存"></a>固定内存</h2><p>主机内存采用分页式管理，通俗的说法就是操作系统把物理内存分成一些“页”，然后给一个应用程序一大块内存，而操作系统可能随时更换物理地址的页，但是从主机传输到设备上的时候，如果此时发生了页面移动，对于传输操作来说是致命的，所以在数据传输之前，CUDA驱动会锁定页面，或者直接分配固定的主机内存，将主机源数据复制到固定内存上，然后从固定内存传输数据到设备上：</p>
<p><img src="/img/image-20220910105840559.png" alt="image-20220910105840559"></p>
<p>上图左边是正常分配内存，传输过程是：锁页-复制到固定内存-复制到设备。右边时分配时就是固定内存，直接传输到设备上。</p>
<p>下面函数用来分配固定内存：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMallocHost</span><span class="params">(<span class="type">void</span> ** devPtr,<span class="type">size_t</span> count)</span></span></span><br></pre></td></tr></table></figure>
<p>分配count字节的固定内存，这些内存是页面锁定的，可以直接传输到设备的。这样就是的传输带宽变得高很多。</p>
<p>固定的主机内存释放使用：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaFreeHost</span><span class="params">(<span class="type">void</span> *ptr)</span></span></span><br></pre></td></tr></table></figure>
<p>我们可以测试一下固定内存和分页内存的传输效率，代码如下</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;freshman.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">sumArrays</span><span class="params">(<span class="type">float</span> * a,<span class="type">float</span> * b,<span class="type">float</span> * res,<span class="type">const</span> <span class="type">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;size;i+=<span class="number">4</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">    res[i<span class="number">+1</span>]=a[i<span class="number">+1</span>]+b[i<span class="number">+1</span>];</span><br><span class="line">    res[i<span class="number">+2</span>]=a[i<span class="number">+2</span>]+b[i<span class="number">+2</span>];</span><br><span class="line">    res[i<span class="number">+3</span>]=a[i<span class="number">+3</span>]+b[i<span class="number">+3</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="type">float</span>*a,<span class="type">float</span>*b,<span class="type">float</span>*res)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">  res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">14</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Vector size:%d\n&quot;</span>,nElem);</span><br><span class="line">  <span class="type">int</span> nByte=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line">  <span class="type">float</span> *a_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *b_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *res_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *res_from_gpu_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="type">float</span> *a_d,*b_d,*res_d;</span><br><span class="line">  <span class="comment">// pine memory malloc</span></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="type">float</span>**)&amp;a_d,nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="type">float</span>**)&amp;b_d,nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMallocHost</span>((<span class="type">float</span>**)&amp;res_d,nByte));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">initialData</span>(a_h,nElem);</span><br><span class="line">  <span class="built_in">initialData</span>(b_h,nElem);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(a_d,a_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(b_d,b_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d,b_d,res_d);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n&quot;</span>,grid.x,block.x);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  <span class="built_in">sumArrays</span>(a_h,b_h,res_h,nElem);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">checkResult</span>(res_h,res_from_gpu_h,nElem);</span><br><span class="line">  <span class="built_in">cudaFreeHost</span>(a_d);</span><br><span class="line">  <span class="built_in">cudaFreeHost</span>(b_d);</span><br><span class="line">  <span class="built_in">cudaFreeHost</span>(res_d);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(a_h);</span><br><span class="line">  <span class="built_in">free</span>(b_h);</span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvprof ./pine_memory</span><br></pre></td></tr></table></figure>
<p>固定内存的释放和分配成本比可分页内存要高很多，但是传输速度更快，所以对于大规模数据，固定内存效率更高。</p>
<h2 id="零拷贝内存"><a href="#零拷贝内存" class="headerlink" title="零拷贝内存"></a>零拷贝内存</h2><p>截止到目前，我们所接触到的内存知识的基础都是：主机直接不能访问设备内存，设备不能直接访问主机内存。对于早期设备，这是肯定的，但是后来，一个例外出现了——零拷贝内存。GPU线程可以直接访问零拷贝内存，这部分内存在主机内存里面，CUDA核函数使用零拷贝内存有以下几种情况：</p>
<ul>
<li>当设备内存不足的时候可以利用主机内存</li>
<li>避免主机和设备之间的显式内存传输</li>
<li>提高PCIe传输率</li>
</ul>
<p>前面我们讲，注意线程之间的内存竞争，因为他们可以同时访问同一个内存地址，现在设备和主机可以同时访问同一个设备地址了，所以，我们要注意主机和设备的内存竞争——当使用零拷贝内存的时候。</p>
<p>零拷贝内存是固定内存，不可分页。可以通过以下函数创建零拷贝内存：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostAlloc</span><span class="params">(<span class="type">void</span> ** pHost,<span class="type">size_t</span> count,<span class="type">unsigned</span> <span class="type">int</span> flags)</span></span></span><br></pre></td></tr></table></figure>
<p>最后一个标志参数，可以选择以下值：</p>
<ul>
<li>cudaHostAllocDefalt</li>
<li>cudaHostAllocPortable</li>
<li>cudaHostAllocWriteCombined</li>
<li>cudaHostAllocMapped</li>
</ul>
<p><code>cudaHostAllocDefalt</code>和<code>cudaMallocHost</code>函数一致，<code>cudaHostAllocPortable</code>函数返回能被所有CUDA上下文使用的固定内存，<code>cudaHostAllocWriteCombined</code>返回写结合内存，在某些设备上这种内存传输效率更高。<code>cudaHostAllocMapped</code>产生零拷贝内存。</p>
<p>注意，零拷贝内存虽然不需要显式的传递到设备上，但是设备还不能通过pHost直接访问对应的内存地址，设备需要访问主机上的零拷贝内存，需要先获得另一个地址，这个地址帮助设备访问到主机对应的内存，方法是：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostGetDevicePointer</span><span class="params">(<span class="type">void</span> ** pDevice,<span class="type">void</span> * pHost,<span class="type">unsigned</span> flags)</span></span>;</span><br></pre></td></tr></table></figure>
<p><code>pDevice</code>就是设备上访问主机零拷贝内存的指针了！零拷贝内存可以当做比设备主存储器更慢的一个设备。</p>
<p>频繁的读写，零拷贝内存效率极低，这个非常容易理解，因为每次都要经过PCIe。</p>
<p>我们下面进行一个小实验，数组加法，改编自前面的代码，然后我们看看效果：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line">  <span class="type">int</span> power=<span class="number">10</span>;</span><br><span class="line">  <span class="keyword">if</span>(argc&gt;=<span class="number">2</span>)</span><br><span class="line">    power=<span class="built_in">atoi</span>(argv[<span class="number">1</span>]);</span><br><span class="line">  <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;power;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Vector size:%d\n&quot;</span>,nElem);</span><br><span class="line">  <span class="type">int</span> nByte=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line">  <span class="type">float</span> *res_from_gpu_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *res_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="type">float</span> *a_host,*b_host,*res_d;</span><br><span class="line">  <span class="type">double</span> iStart,iElaps;</span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">  res_from_gpu_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *a_dev,*b_dev;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;a_host,nByte,cudaHostAllocMapped));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;b_host,nByte,cudaHostAllocMapped));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;res_d,nByte));</span><br><span class="line">  <span class="built_in">initialData</span>(a_host,nElem);</span><br><span class="line">  <span class="built_in">initialData</span>(b_host,nElem);</span><br><span class="line"></span><br><span class="line"> <span class="comment">//=============================================================//</span></span><br><span class="line">  iStart = <span class="built_in">cpuSecond</span>();</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaHostGetDevicePointer</span>((<span class="type">void</span>**)&amp;a_dev,(<span class="type">void</span>*) a_host,<span class="number">0</span>));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaHostGetDevicePointer</span>((<span class="type">void</span>**)&amp;b_dev,(<span class="type">void</span>*) b_host,<span class="number">0</span>));</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_dev,b_dev,res_d);</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  iElaps = <span class="built_in">cpuSecond</span>() - iStart;</span><br><span class="line"> <span class="comment">//=============================================================//</span></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;zero copy memory elapsed %lf ms \n&quot;</span>, iElaps);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n&quot;</span>,grid.x,block.x);</span><br><span class="line"><span class="comment">//-----------------------normal memory---------------------------</span></span><br><span class="line">  <span class="type">float</span> *a_h_n=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *b_h_n=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *res_h_n=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *res_from_gpu_h_n=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h_n,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h_n,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="type">float</span> *a_d_n,*b_d_n,*res_d_n;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;a_d_n,nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;b_d_n,nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;res_d_n,nByte));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">initialData</span>(a_h_n,nElem);</span><br><span class="line">  <span class="built_in">initialData</span>(b_h_n,nElem);</span><br><span class="line"><span class="comment">//=============================================================//</span></span><br><span class="line">  iStart = <span class="built_in">cpuSecond</span>();</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(a_d_n,a_h_n,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(b_d_n,b_h_n,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d_n,b_d_n,res_d_n);</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  iElaps = <span class="built_in">cpuSecond</span>() - iStart;</span><br><span class="line"><span class="comment">//=============================================================//</span></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;device memory elapsed %lf ms \n&quot;</span>, iElaps);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt;\n&quot;</span>,grid.x,block.x);</span><br><span class="line"><span class="comment">//--------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">sumArrays</span>(a_host,b_host,res_h,nElem);</span><br><span class="line">  <span class="built_in">checkResult</span>(res_h,res_from_gpu_h,nElem);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">cudaFreeHost</span>(a_host);</span><br><span class="line">  <span class="built_in">cudaFreeHost</span>(b_host);</span><br><span class="line">  <span class="built_in">cudaFree</span>(res_d);</span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">cudaFree</span>(a_d_n);</span><br><span class="line">  <span class="built_in">cudaFree</span>(b_d_n);</span><br><span class="line">  <span class="built_in">cudaFree</span>(res_d_n);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(a_h_n);</span><br><span class="line">  <span class="built_in">free</span>(b_h_n);</span><br><span class="line">  <span class="built_in">free</span>(res_h_n);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h_n);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们把结果写在一个表里面：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">数据规模n( 2^n )</th>
<th style="text-align:center">常规内存（us）</th>
<th style="text-align:center">零拷贝内存（us）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">2.5</td>
<td style="text-align:center">3.0</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">4.1</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:center">7.8</td>
<td style="text-align:center">8.6</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">23.1</td>
<td style="text-align:center">25.8</td>
</tr>
<tr>
<td style="text-align:center">18</td>
<td style="text-align:center">86.5</td>
<td style="text-align:center">98.2</td>
</tr>
<tr>
<td style="text-align:center">20</td>
<td style="text-align:center">290.9</td>
<td style="text-align:center">310.5</td>
</tr>
</tbody>
</table>
</div>
<p>这是通过观察运行时间得到的，当然也可以通过我们上面的nvprof得到内核执行时间：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">数据规模n( 2^n )</th>
<th style="text-align:center">常规内存（us）</th>
<th style="text-align:center">零拷贝内存（us）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">1.088</td>
<td style="text-align:center">4.257</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">1.056</td>
<td style="text-align:center">8.00</td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:center">1.920</td>
<td style="text-align:center">24.578</td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">4.544</td>
<td style="text-align:center">86.63</td>
</tr>
</tbody>
</table>
</div>
<h2 id="统一虚拟寻址"><a href="#统一虚拟寻址" class="headerlink" title="统一虚拟寻址"></a>统一虚拟寻址</h2><p>设备架构2.0以后，Nvida又有新创意，他们搞了一套称为同一寻址方式（UVA）的内存机制，这样，设备内存和主机内存被映射到同一虚拟内存地址中。如图</p>
<p><img src="/img/image-20220910110833322.png" alt="image-20220910110833322"></p>
<p>UVA之前，我们要管理所有的设备和主机内存，尤其是他们的指针。通过UVA，<code>cudaHostAlloc</code>函数分配的固定主机内存具有相同的主机和设备地址，可以直接将返回的地址传递给核函数。</p>
<p>前面的零拷贝内存，可以知道以下几个方面：</p>
<ul>
<li>分配映射的固定主机内存</li>
<li>使用CUDA运行时函数获取映射到固定内存的设备指针</li>
<li>将设备指针传递给核函数</li>
</ul>
<p>有了UVA，可以不用上面的那个获得设备上访问零拷贝内存的函数了：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostGetDevicePointer</span><span class="params">(<span class="type">void</span> ** pDevice,<span class="type">void</span> * pHost,<span class="type">unsigned</span> flags)</span></span>;</span><br></pre></td></tr></table></figure>
<p>UVA来了以后，此函数基本失业了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">  float *a_host,*b_host,*res_d;</span><br><span class="line">  CHECK(cudaHostAlloc((float**)&amp;a_host,nByte,cudaHostAllocMapped));</span><br><span class="line">  CHECK(cudaHostAlloc((float**)&amp;b_host,nByte,cudaHostAllocMapped));</span><br><span class="line">  CHECK(cudaMalloc((float**)&amp;res_d,nByte));</span><br><span class="line">  res_from_gpu_h=(float*)malloc(nByte);</span><br><span class="line"></span><br><span class="line">  initialData(a_host,nElem);</span><br><span class="line">  initialData(b_host,nElem);</span><br><span class="line"></span><br><span class="line">  dim3 block(1024);</span><br><span class="line">  dim3 grid(nElem/block.x);</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_host,b_host,res_d);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>UVA代码主要就是差个获取指针，UVA可以直接使用主机端的地址。</p>
<h1 id="内存访问模式"><a href="#内存访问模式" class="headerlink" title="内存访问模式"></a>内存访问模式</h1><p>多数GPU程序容易受到内存带宽的限制，所以最大程度的利用全局内存带宽，提高全局加载效率，是调控内核函数性能的基本条件。</p>
<p>CUDA执行模型告诉我们，CUDA执行的基本单位是线程束，所以，内存访问也是以线程束为基本单位发布和执行的，存储也一致。</p>
<h2 id="对齐与合并访问"><a href="#对齐与合并访问" class="headerlink" title="对齐与合并访问"></a>对齐与合并访问</h2><p>全局内存通过缓存实现加载和存储的过程如下图</p>
<p><img src="/img/image-20220910111100141.png" alt="image-20220910111100141"></p>
<p>全局内存是一个逻辑层面的模型，我们编程的时候有两种模型考虑：一种是逻辑层面的，也就是我们在写程序的时候（包括串行程序和并行程序），写的一维（多维）数组，结构体，定义的变量，这些都是在逻辑层面的；一种是硬件角度，就是一块DRAM上的电信号，以及最底层内存驱动代码所完成数字信号的处理。</p>
<p>L1表示一级缓存，每个SM都有自己L1，但是L2是所有SM公用的，除了L1缓存外，还有只读缓存和常量缓存。</p>
<p>核函数运行时需要从全局内存（DRAM）中读取数据，只有两种粒度，这个是关键的：</p>
<ul>
<li>128字节</li>
<li>32字节</li>
</ul>
<p>解释下“粒度”，可以理解为最小单位，也就是核函数运行时每次读内存，哪怕是读一个字节的变量，也要读128字节，或者32字节，而具体是到底是32还是128还是要看访问方式：</p>
<ul>
<li>使用一级缓存</li>
<li>不使用一级缓存</li>
</ul>
<p>对于CPU来说，一级缓存或者二级缓存是不能被编程的，但是CUDA是支持通过编译指令停用一级缓存的。如果启用一级缓存，那么每次从DRAM上加载数据的粒度是128字节，如果不适用一级缓存，只是用二级缓存，那么粒度是32字节。</p>
<p>还要强调一下CUDA内存模型的内存读写，我们现在讨论的都是单个SM上的情况，多个SM只是下面我们描述的情形的复制：SM执行的基础是线程束，也就是说，当一个SM中正在被执行的某个线程需要访问内存，那么，和它同线程束的其他31个线程也要访问内存，这个基础就表示，即使每个线程只访问一个字节，那么在执行的时候，只要有内存请求，至少是32个字节，所以不使用一级缓存的内存加载，一次粒度是32字节而不是更小。</p>
<p>在优化内存的时候，我们要最关注的是以下两个特性</p>
<ul>
<li>对齐内存访问</li>
<li>合并内存访问</li>
</ul>
<p>我们把一次内存请求——也就是从内核函数发起请求，到硬件响应返回数据这个过程称为一个内存事务（加载和存储都行）。</p>
<p>当一个内存事务的首个访问地址是缓存粒度（32或128字节）的偶数倍的时候：比如二级缓存32字节的偶数倍64，128字节的偶数倍256的时候，这个时候被称为对齐内存访问，非对齐访问就是除上述的其他情况，非对齐的内存访问会造成带宽浪费。</p>
<p>当一个线程束内的线程访问的内存都在一个内存块里的时候，就会出现合并访问。</p>
<p>对齐合并访问的状态是理想化的，也是最高速的访问方式，当线程束内的所有线程访问的数据在一个内存块，并且数据是从内存块的首地址开始被需要的，那么对齐合并访问出现了。为了最大化全局内存访问的理想状态，尽量将线程束访问内存组织成对齐合并的方式，这样的效率是最高的。下面看一个例子。</p>
<ul>
<li>一个线程束加载数据，使用一级缓存，并且这个事务所请求的所有数据在一个128字节的对齐的地址段上（对齐的地址段是我自己发明的名字，就是首地址是粒度的偶数倍，那么上面这句话的意思是，所有请求的数据在某个首地址是粒度偶数倍的后128个字节里），具体形式如下图，这里请求的数据是连续的，其实可以不连续，但是不要越界就好。</li>
</ul>
<p><img src="/img/image-20220910112226266.png" alt="image-20220910112226266"></p>
<p>上面蓝色表示全局内存，下面橙色是线程束要的数据，绿色就是我称为对齐的地址段。</p>
<ul>
<li>如果一个事务加载的数据分布在不一个对齐的地址段上，就会有以下两种情况：<ul>
<li>连续的，但是不在一个对齐的段上，比如，请求访问的数据分布在内存地址1~128，那么0~127和128~255这两段数据要传递两次到SM</li>
<li>不连续的，也不在一个对齐的段上，比如，请求访问的数据分布在内存地址0~63和128~191上，明显这也需要两次加载。</li>
</ul>
</li>
</ul>
<p><img src="/img/image-20220910112303270.png" alt="image-20220910112303270"></p>
<p>上图就是典型的一个线程束，数据分散开了，thread0的请求在128之前，后面还有请求在256之后，所以需要三个内存事务，而利用率，也就是从主存取回来的数据被使用到的比例，只有 1/3 的比例。这个比例低会造成带宽的浪费，最极端的表现，就是如果每个线程的请求都在不同的段，也就是一个128字节的事务只有1个字节是有用的，那么利用率只有 1/128</p>
<h2 id="全局内存读取"><a href="#全局内存读取" class="headerlink" title="全局内存读取"></a>全局内存读取</h2><p>注意我们说的都是读取，也就是加载过程，写或者叫做存储是另外一回事！SM加载数据，根据不同的设备和类型分为三种路径：</p>
<ol>
<li>一级和二级缓存</li>
<li>常量缓存</li>
<li>只读缓存</li>
</ol>
<p>常规的路径是一级和二级缓存，需要使用常量和只读缓存的需要在代码中显式声明。但是提高性能，主要还是要取决于访问模式。</p>
<p>控制全局加载操作是否通过一级缓存可以通过编译选项来控制，当然比较老的设备可能就没有一级缓存。</p>
<p>编译器禁用一级缓存的选项是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Xptxas -dlcm=cg</span><br></pre></td></tr></table></figure>
<p>编译器启用一级缓存的选项是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Xptxas -dlcm=ca</span><br></pre></td></tr></table></figure>
<p>当一级缓存被禁用的时候，对全局内存的加载请求直接进入二级缓存，如果二级缓存缺失，则由DRAM完成请求。</p>
<p>每次内存事务可由一个两个或者四个部分执行，每个部分有32个字节，也就是32，64或者128字节一次（注意前面我们讲到是否使用一级缓存决定了读取粒度是128还是32字节，这里增加的64并不在此情况，所以需要注意）。</p>
<p>启用一级缓存后，当SM有全局加载请求会首先通过尝试一级缓存，如果一级缓存缺失，则尝试二级缓存，如果二级缓存也没有，那么直接DRAM。</p>
<p>在有些设备上一级缓存不用来缓存全局内存访问，而是只用来存储寄存器溢出的本地数据，比如Kepler 的K10,K20。</p>
<p>内存加载可以分为两类：</p>
<ul>
<li>缓存加载</li>
<li>没有缓存的加载</li>
</ul>
<p>内存访问有以下特点：</p>
<ul>
<li>是否使用缓存：一级缓存是否介入加载过程</li>
<li>对齐与非对齐的：如果访问的第一个地址是32的倍数（前面说是32或者128的偶数倍，这里似乎产生了矛盾，为什么我现在也很迷惑）</li>
<li>合并与非合并，访问连续数据块则是合并的</li>
</ul>
<h3 id="缓存加载"><a href="#缓存加载" class="headerlink" title="缓存加载"></a>缓存加载</h3><p>下面是使用一级缓存的加载过程，图片表达很清楚，我们只用少量文字进行说明：</p>
<ol>
<li>对齐合并的访问，利用率100%</li>
</ol>
<p><img src="/img/image-20220910113212684.png" alt="image-20220910113212684"></p>
<ol>
<li>对齐的，但是不是连续的，每个线程访问的数据都在一个块内，但是位置是交叉的，利用率100%</li>
</ol>
<p><img src="/img/image-20220910113231769.png" alt="image-20220910113231769"></p>
<ol>
<li>连续非对齐的，线程束请求一个连续的非对齐的，32个4字节数据，那么会出现，数据横跨两个块，但是没有对齐，当启用一级缓存的时候，就要两个128字节的事务来完成</li>
</ol>
<p><img src="/img/image-20220910113248353.png" alt="image-20220910113248353"></p>
<ol>
<li>线程束所有线程请求同一个地址，那么肯定落在一个缓存行范围（缓存行的概念没提到过，就是主存上一个可以被一次读到缓存中的一段数据。），那么如果按照请求的是4字节数据来说，使用一级缓存的利用率是 4/128=3.125%</li>
</ol>
<p><img src="/img/image-20220910113311093.png" alt="image-20220910113311093"></p>
<ol>
<li>比较坏的情况，前面提到过最坏的，就是每个线程束内的线程请求的都是不同的缓存行内，这里比较坏的情况就是，所有数据分布在 N 个缓存行上，其中 1≤N≤32，那么请求32个4字节的数据，就需要 N 个事务来完成，利用率也是 1/N</li>
</ol>
<p><img src="/img/image-20220910113413684.png" alt="image-20220910113413684"></p>
<p>CPU和GPU的一级缓存有显著的差异，GPU的一级缓存可以通过编译选项等控制，CPU不可以，而且CPU的一级缓存是的替换算法是有使用频率和时间局部性的，GPU则没有。</p>
<h3 id="没有缓存的加载"><a href="#没有缓存的加载" class="headerlink" title="没有缓存的加载"></a>没有缓存的加载</h3><p>没有缓存的加载是指的没有通过一级缓存，二级缓存则是不得不经过的。</p>
<p>当不使用一级缓存的时候，内存事务的粒度变为32字节，更细粒度的好处是提高利用律。</p>
<ol>
<li><p>对齐合并访问128字节，不用说，还是最理想的情况，使用4个段，利用率 100%</p>
<p> <img src="/img/image-20220910113606985.png" alt="image-20220910113606985"></p>
</li>
<li><p>对齐不连续访问128字节，都在四个段内，且互不相同，这样的利用率也是 100%<br> <img src="/img/image-20220910113619337.png" alt="image-20220910113619337"></p>
</li>
<li><p>连续不对齐，一个段32字节，所以，一个连续的128字节的请求，即使不对齐，最多也不会超过五个段，所以利用率是 45=80%45=80% ,如果不明白为啥不能超过5个段，请注意前提是连续的，这个时候不可能超过五段<br> <img src="/img/image-20220910113636979.png" alt="image-20220910113636979"></p>
</li>
<li><p>所有线程访问一个4字节的数据，那么此时的利用率是 432=12.5%432=12.5%<br> <img src="/img/image-20220910113651178.png" alt="image-20220910113651178"></p>
</li>
<li><p>最坏的情况，所有目标数据分散在内存的各个角落，那么需要 N 个内存段， 此时与使用一级缓存的作比较也是有优势的因为 N×128 还是要比 N×32 大不少，这里假设 N 不会因为 128 还是 32 而变的，而实际情况，当使用大粒度的缓存行的时候， N 有可能会减小<br> <img src="/img/image-20220910113706123.png" alt="image-20220910113706123"></p>
</li>
</ol>
<h3 id="非对齐读取示例"><a href="#非对齐读取示例" class="headerlink" title="非对齐读取示例"></a>非对齐读取示例</h3><p>下面就非对齐读取进行演示，<br>代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;freshman.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">sumArrays</span><span class="params">(<span class="type">float</span> * a,<span class="type">float</span> * b,<span class="type">float</span> * res,<span class="type">int</span> offset,<span class="type">const</span> <span class="type">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>,k=offset;k&lt;size;i++,k++)</span><br><span class="line">    &#123;</span><br><span class="line">        res[i]=a[k]+b[k];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="type">float</span>*a,<span class="type">float</span>*b,<span class="type">float</span>*res,<span class="type">int</span> offset,<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">//int i=threadIdx.x;</span></span><br><span class="line">  <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">  <span class="type">int</span> k=i+offset;</span><br><span class="line">  <span class="keyword">if</span>(k&lt;n)</span><br><span class="line">    res[i]=a[k]+b[k];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">  <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">18</span>;</span><br><span class="line">  <span class="type">int</span> offset=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span>(argc&gt;=<span class="number">2</span>)</span><br><span class="line">    offset=<span class="built_in">atoi</span>(argv[<span class="number">1</span>]);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Vector size:%d\n&quot;</span>,nElem);</span><br><span class="line">  <span class="type">int</span> nByte=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line">  <span class="type">float</span> *a_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *b_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *res_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="type">float</span> *res_from_gpu_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="type">float</span> *a_d,*b_d,*res_d;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;a_d,nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;b_d,nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;res_d,nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemset</span>(res_d,<span class="number">0</span>,nByte));</span><br><span class="line">  <span class="built_in">initialData</span>(a_h,nElem);</span><br><span class="line">  <span class="built_in">initialData</span>(b_h,nElem);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(a_d,a_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(b_d,b_h,nByte,cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">  <span class="type">double</span> iStart,iElaps;</span><br><span class="line">  iStart=<span class="built_in">cpuSecond</span>();</span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d,b_d,res_d,offset,nElem);</span><br><span class="line">  <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">  iElaps=<span class="built_in">cpuSecond</span>()-iStart;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Execution configuration&lt;&lt;&lt;%d,%d&gt;&gt;&gt; Time elapsed %f sec --offset:%d \n&quot;</span>,grid.x,block.x,iElaps,offset);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="built_in">sumArrays</span>(a_h,b_h,res_h,offset,nElem);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">checkResult</span>(res_h,res_from_gpu_h,nElem);</span><br><span class="line">  <span class="built_in">cudaFree</span>(a_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(b_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(res_d);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(a_h);</span><br><span class="line">  <span class="built_in">free</span>(b_h);</span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line">  <span class="built_in">free</span>(res_from_gpu_h);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>编译指令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tony@tony-Lenovo:~/Project/CUDA_Freshman/18_sum_array_offset$ nvcc -O3 -arch=sm_35 -Xptxas -dlcm=cg -I ../include/ sum_array_offset.cu -o sum_array_offset</span><br></pre></td></tr></table></figure>
<h3 id="只读缓存"><a href="#只读缓存" class="headerlink" title="只读缓存"></a>只读缓存</h3><p>只读缓存最初是留给纹理内存加载用的，在3.5以上的设备，只读缓存也支持使用全局内存加载代替一级缓存。也就是说3.5以后的设备，可以通过只读缓存从全局内存中读数据了。</p>
<p>只读缓存粒度32字节，对于分散读取，细粒度优于一级缓存</p>
<p>有两种方法指导内存从只读缓存读取：</p>
<ol>
<li>使用函数 _ldg</li>
<li>在间接引用的指针上使用修饰符</li>
</ol>
<p>代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">copyKernel</span><span class="params">(<span class="type">float</span> * in,<span class="type">float</span>* out)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> idx=blockDim*blockIdx.x+threadIdx.x;</span><br><span class="line">    out[idx]=__ldg(&amp;in[idx]);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意函数参数，然后就能强制使用只读缓存了。</p>
<h1 id="核函数可达到的带宽"><a href="#核函数可达到的带宽" class="headerlink" title="核函数可达到的带宽"></a>核函数可达到的带宽</h1><p>内存延迟是影响核函数的一大关键，内存延迟，也就是从你发起内存请求到数据进入SM的寄存器的整个时间。内存带宽，也就是SM访问内存的速度，它以单位时间内传输的字节数进行测量。上一节我们用了两种方法改善内核性能：</p>
<ul>
<li>最大化线程束的数量来隐藏内存延迟，维持更多的正在执行的内存访问达到更好的总线利用率</li>
<li>通过适当的对齐和合并访问，提高带宽效率</li>
</ul>
<h2 id="内存带宽"><a href="#内存带宽" class="headerlink" title="内存带宽"></a>内存带宽</h2><p>多数内核对带宽敏感，也就是说，工人们生产效率特别高，而原料来的很慢，这限制了生产速度。去哪聚内存中数据的安排方式和线程束的访问方式都对带宽有显著影响。一般有如下两种带宽</p>
<ul>
<li>理论带宽</li>
<li>有效带宽</li>
</ul>
<p>理论带宽就是硬件设计的绝对最大值，硬件限制了这个最大值为多少，比如对于不使用ECC的Fermi M2090来说，理论峰值 117.6 GB/s。有效带宽是核函数实际达到的带宽，是测量带宽，可以用下面公式计算:</p>
<p>有效带宽=(读字节数+写字节数)×10−9运行时间(1)(1)有效带宽=(读字节数+写字节数)×10−9运行时间</p>
<p>注意吞吐量和带宽的区别，吞吐量是衡量计算核心效率的，用的单位是每秒多少十亿次浮点运算(gflops)，有效吞吐量其不止和有效带宽有关，还和带宽的利用率等因素有关，当然最主要的还是设备的运算核心。</p>
<p>当然，也有内存吞吐量这种说法这种说法就是单位时间上内存访问的总量，用单位 GB/s 表示，这个值越大表示读取到的数据越多，但是这些数据不一定是有用的。</p>
<h2 id="矩阵转置问题"><a href="#矩阵转置问题" class="headerlink" title="矩阵转置问题"></a>矩阵转置问题</h2><p>矩阵转置就是交换矩阵的坐标，我们本文研究有二维矩阵，转置结果如下：</p>
<p>使用串行编程很容易实现：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">transformMatrix2D_CPU</span><span class="params">(<span class="type">float</span> * MatA,<span class="type">float</span> * MatB,<span class="type">int</span> nx,<span class="type">int</span> ny)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;ny;j++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;nx;i++)</span><br><span class="line">    &#123;</span><br><span class="line">      MatB[i*nx+j]=MatA[j*nx+i];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码应该比较容易懂，这是串行解决的方法，必须要注意的是，我们所有的数据，结构体也好，数组也好，多维数组也好，所有的数据，在内存硬件层面都是一维排布的，所以我们这里也是使用一维的数组作为输入输出，那么从真实的角度看内存中的数据就是下面这样的：</p>
<p><img src="/img/image-20220910144503244.png" alt="image-20220910144503244"></p>
<p>转置操作：</p>
<ul>
<li>读：原矩阵行进行读取，请求的内存是连续的，可以进行合并访问</li>
<li>写：写到转置矩阵的列中，访问是交叉的</li>
</ul>
<p>图中的颜色需要大家注意一下，读的过程同一颜色可以看成是合并读取的，但是转置发生后写入的过程，是交叉的。</p>
<p>如果按照我们上文的观点，如果按照下面两种方法进行读</p>
<p><img src="/img/image-20220910144646230.png" alt="image-20220910144646230"></p>
<p>最初的想法肯定是：按照图一合并读更有效率，因为写的时候不需要经过一级缓存，所以对于有一级缓存的程序，合并的读取应该是更有效率的。如果你这么想，恭喜你，你想的不对（我当时也是这么想的）。</p>
<p>我们需要补充下关于一级缓存的作用，上文我们讲到合并，可能第一印象就是一级缓存是缓冲从全局内存里过来的数据一样，但是我们忽略了一些东西，就是内存发起加载请求的时候，会现在一级缓存里看看有没有这个数据，如果有，这个就是一个命中，这和CPU的缓存运行原理是一样的，如果命中了，就不需要再去全局内存读了，如果用在上面这个例子，虽然按照列读是不合并的，但是使用一级缓存加载过来的数据在后面会被使用，我们必须要注意虽然，一级缓存一次读取128字节的数据，其中只有一个单位是有用的，但是剩下的并不会被马上覆盖，粒度是128字节，但是一级缓存的大小有几k或是更大，这些数据很有可能不会被替换，所以，我们按列读取数据，虽然第一行只用了一个，但是下一列的时候，理想情况是所有需要读取的元素都在一级缓存中，这时候，数据直接从缓存里面读取</p>
<h3 id="为转置核函数设置上限和下限"><a href="#为转置核函数设置上限和下限" class="headerlink" title="为转置核函数设置上限和下限"></a>为转置核函数设置上限和下限</h3><p>我们本例子中的瓶颈在交叉访问，所以我们假设没有交叉访问，和全是交叉访问的情况，来给出上限和下限：</p>
<ul>
<li>行读取，行存储来复制矩阵(上限)</li>
<li>列读取，列存储来复制矩阵(下限)</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">copyRow</span><span class="params">(<span class="type">float</span> * MatA,<span class="type">float</span> * MatB,<span class="type">int</span> nx,<span class="type">int</span> ny)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> ix=threadIdx.x+blockDim.x*blockIdx.x;</span><br><span class="line">    <span class="type">int</span> iy=threadIdx.y+blockDim.y*blockIdx.y;</span><br><span class="line">    <span class="type">int</span> idx=ix+iy*nx;</span><br><span class="line">    <span class="keyword">if</span> (ix&lt;nx &amp;&amp; iy&lt;ny)</span><br><span class="line">    &#123;</span><br><span class="line">      MatB[idx]=MatA[idx];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">copyCol</span><span class="params">(<span class="type">float</span> * MatA,<span class="type">float</span> * MatB,<span class="type">int</span> nx,<span class="type">int</span> ny)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> ix=threadIdx.x+blockDim.x*blockIdx.x;</span><br><span class="line">    <span class="type">int</span> iy=threadIdx.y+blockDim.y*blockIdx.y;</span><br><span class="line">    <span class="type">int</span> idx=ix*ny+iy;</span><br><span class="line">    <span class="keyword">if</span> (ix&lt;nx &amp;&amp; iy&lt;ny)</span><br><span class="line">    &#123;</span><br><span class="line">      MatB[idx]=MatA[idx];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们使用命令行编译，开启一级缓存：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -O3 -arch=sm_35 -Xptxas -dlcm=ca -I ../include/ transform_matrix2D.cu -o transform_matrix2D</span><br></pre></td></tr></table></figure>
<p>可以得到：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">核函数</th>
<th style="text-align:center">试验1</th>
<th style="text-align:center">试验2</th>
<th style="text-align:center">试验3</th>
<th style="text-align:center">平均值</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">上限</td>
<td style="text-align:center">0.001611</td>
<td style="text-align:center">0.001614</td>
<td style="text-align:center">0.001606</td>
<td style="text-align:center">0.001610</td>
</tr>
<tr>
<td style="text-align:center">下限</td>
<td style="text-align:center">0.004191</td>
<td style="text-align:center">0.004210</td>
<td style="text-align:center">0.004205</td>
<td style="text-align:center">0.004202</td>
</tr>
</tbody>
</table>
</div>
<p>这个时间是三次测试出来的平均值，基本可以肯定在当前数据规模下，上限在0.001610s，下限在0.004202s</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;strating...\n&quot;</span>);</span><br><span class="line">  <span class="built_in">initDevice</span>(<span class="number">0</span>);</span><br><span class="line">  <span class="type">int</span> nx=<span class="number">1</span>&lt;&lt;<span class="number">12</span>;</span><br><span class="line">  <span class="type">int</span> ny=<span class="number">1</span>&lt;&lt;<span class="number">12</span>;</span><br><span class="line">  <span class="type">int</span> nxy=nx*ny;</span><br><span class="line">  <span class="type">int</span> nBytes=nxy*<span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">  <span class="type">int</span> transform_kernel=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span>(argc&gt;=<span class="number">2</span>)</span><br><span class="line">    transform_kernel=<span class="built_in">atoi</span>(argv[<span class="number">1</span>]);</span><br><span class="line">  <span class="comment">//Malloc</span></span><br><span class="line">  <span class="type">float</span>* A_host=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">  <span class="type">float</span>* B_host=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">  <span class="built_in">initialData</span>(A_host,nxy);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//cudaMalloc</span></span><br><span class="line">  <span class="type">float</span> *A_dev=<span class="literal">NULL</span>;</span><br><span class="line">  <span class="type">float</span> *B_dev=<span class="literal">NULL</span>;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">void</span>**)&amp;A_dev,nBytes));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>((<span class="type">void</span>**)&amp;B_dev,nBytes));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(A_dev,A_host,nBytes,cudaMemcpyHostToDevice));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemset</span>(B_dev,<span class="number">0</span>,nBytes));</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> dimx=<span class="number">32</span>;</span><br><span class="line">  <span class="type">int</span> dimy=<span class="number">32</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// cpu compute</span></span><br><span class="line">  <span class="type">double</span> iStart=<span class="built_in">cpuSecond</span>();</span><br><span class="line">  <span class="built_in">transformMatrix2D_CPU</span>(A_host,B_host,nx,ny);</span><br><span class="line">  <span class="type">double</span> iElaps=<span class="built_in">cpuSecond</span>()-iStart;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;CPU Execution Time elapsed %f sec\n&quot;</span>,iElaps);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2d block and 2d grid</span></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(dimx,dimy)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">((nx<span class="number">-1</span>)/block.x<span class="number">+1</span>,(ny<span class="number">-1</span>)/block.y<span class="number">+1</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">block_1</span><span class="params">(dimx,dimy)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid_1</span><span class="params">((nx<span class="number">-1</span>)/(block_<span class="number">1.</span>x*<span class="number">4</span>)<span class="number">+1</span>,(ny<span class="number">-1</span>)/block_<span class="number">1.</span>y<span class="number">+1</span>)</span></span>;</span><br><span class="line">  iStart=<span class="built_in">cpuSecond</span>();</span><br><span class="line">  <span class="keyword">switch</span>(transform_kernel)</span><br><span class="line">  &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">    copyRow&lt;&lt;&lt;grid,block&gt;&gt;&gt;(A_dev,B_dev,nx,ny);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">    copyCol&lt;&lt;&lt;grid,block&gt;&gt;&gt;(A_dev,B_dev,nx,ny);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">    transformNaiveRow&lt;&lt;&lt;grid,block&gt;&gt;&gt;(A_dev,B_dev,nx,ny);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">        transformNaiveCol&lt;&lt;&lt;grid,block&gt;&gt;&gt;(A_dev,B_dev,nx,ny);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> <span class="number">4</span>:</span><br><span class="line">        transformNaiveColUnroll&lt;&lt;&lt;grid_1,block_1&gt;&gt;&gt;(A_dev,B_dev,nx,ny);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> <span class="number">5</span>:</span><br><span class="line">        transformNaiveColUnroll&lt;&lt;&lt;grid_1,block_1&gt;&gt;&gt;(A_dev,B_dev,nx,ny);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> <span class="number">6</span>:</span><br><span class="line">        transformNaiveRowDiagonal&lt;&lt;&lt;grid,block&gt;&gt;&gt;(A_dev,B_dev,nx,ny);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> <span class="number">7</span>:</span><br><span class="line">        transformNaiveColDiagonal&lt;&lt;&lt;grid,block&gt;&gt;&gt;(A_dev,B_dev,nx,ny);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">default</span>:</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaDeviceSynchronize</span>());</span><br><span class="line">  iElaps=<span class="built_in">cpuSecond</span>()-iStart;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot; Time elapsed %f sec\n&quot;</span>,iElaps);</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(B_host,B_dev,nBytes,cudaMemcpyDeviceToHost));</span><br><span class="line">  <span class="built_in">checkResult</span>(B_host,B_host,nxy);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">cudaFree</span>(A_dev);</span><br><span class="line">  <span class="built_in">cudaFree</span>(B_dev);</span><br><span class="line">  <span class="built_in">free</span>(A_host);</span><br><span class="line">  <span class="built_in">free</span>(B_host);</span><br><span class="line">  <span class="built_in">cudaDeviceReset</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="展开转置：读取行与读取列"><a href="#展开转置：读取行与读取列" class="headerlink" title="展开转置：读取行与读取列"></a>展开转置：读取行与读取列</h3><p>接下来这个是老套路了，有效地隐藏延迟，从展开操作开始：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">transformNaiveRowUnroll</span><span class="params">(<span class="type">float</span> * MatA,<span class="type">float</span> * MatB,<span class="type">int</span> nx,<span class="type">int</span> ny)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> ix=threadIdx.x+blockDim.x*blockIdx.x*<span class="number">4</span>;</span><br><span class="line">    <span class="type">int</span> iy=threadIdx.y+blockDim.y*blockIdx.y;</span><br><span class="line">    <span class="type">int</span> idx_row=ix+iy*nx;</span><br><span class="line">    <span class="type">int</span> idx_col=ix*ny+iy;</span><br><span class="line">    <span class="keyword">if</span> (ix&lt;nx &amp;&amp; iy&lt;ny)</span><br><span class="line">    &#123;</span><br><span class="line">      MatB[idx_col]=MatA[idx_row];</span><br><span class="line">      MatB[idx_col+ny*<span class="number">1</span>*blockDim.x]=MatA[idx_row<span class="number">+1</span>*blockDim.x];</span><br><span class="line">      MatB[idx_col+ny*<span class="number">2</span>*blockDim.x]=MatA[idx_row<span class="number">+2</span>*blockDim.x];</span><br><span class="line">      MatB[idx_col+ny*<span class="number">3</span>*blockDim.x]=MatA[idx_row<span class="number">+3</span>*blockDim.x];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">transformNaiveColUnroll</span><span class="params">(<span class="type">float</span> * MatA,<span class="type">float</span> * MatB,<span class="type">int</span> nx,<span class="type">int</span> ny)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> ix=threadIdx.x+blockDim.x*blockIdx.x*<span class="number">4</span>;</span><br><span class="line">    <span class="type">int</span> iy=threadIdx.y+blockDim.y*blockIdx.y;</span><br><span class="line">    <span class="type">int</span> idx_row=ix+iy*nx;</span><br><span class="line">    <span class="type">int</span> idx_col=ix*ny+iy;</span><br><span class="line">    <span class="keyword">if</span> (ix&lt;nx &amp;&amp; iy&lt;ny)</span><br><span class="line">    &#123;</span><br><span class="line">        MatB[idx_row]=MatA[idx_col];</span><br><span class="line">        MatB[idx_row<span class="number">+1</span>*blockDim.x]=MatA[idx_col+ny*<span class="number">1</span>*blockDim.x];</span><br><span class="line">        MatB[idx_row<span class="number">+2</span>*blockDim.x]=MatA[idx_col+ny*<span class="number">2</span>*blockDim.x];</span><br><span class="line">        MatB[idx_row<span class="number">+3</span>*blockDim.x]=MatA[idx_col+ny*<span class="number">3</span>*blockDim.x];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="使用统一内存的向量加法"><a href="#使用统一内存的向量加法" class="headerlink" title="使用统一内存的向量加法"></a>使用统一内存的向量加法</h1><h2 id="统一内存矩阵加法"><a href="#统一内存矩阵加法" class="headerlink" title="统一内存矩阵加法"></a>统一内存矩阵加法</h2><p>统一内存的基本思路就是减少指向同一个地址的指针，比如我们经常见到的，在本地分配内存，然后传输到设备，然后在从设备传输回来，使用统一内存，就没有这些显式的需求了，而是驱动程序帮我们完成。具体的做法就是:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CHECK(cudaMallocManaged((float**)&amp;a_d,nByte));</span><br><span class="line">CHECK(cudaMallocManaged((float**)&amp;b_d,nByte));</span><br><span class="line">CHECK(cudaMallocManaged((float**)&amp;res_d,nByte));</span><br></pre></td></tr></table></figure>
<p>使用<code>cudaMallocManaged</code>来分配内存，这种内存在表面上看在设备和主机端都能访问，但是内部过程和我们前面手动copy过来copy过去是一样的，也就是<code>memcopy</code>是本质，而这个只是封装了一下。</p>
<p>我们来看看完整的代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">sumArrays</span><span class="params">(<span class="type">float</span> * a,<span class="type">float</span> * b,<span class="type">float</span> * res,<span class="type">const</span> <span class="type">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;size;i+=<span class="number">4</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">    res[i<span class="number">+1</span>]=a[i<span class="number">+1</span>]+b[i<span class="number">+1</span>];</span><br><span class="line">    res[i<span class="number">+2</span>]=a[i<span class="number">+2</span>]+b[i<span class="number">+2</span>];</span><br><span class="line">    res[i<span class="number">+3</span>]=a[i<span class="number">+3</span>]+b[i<span class="number">+3</span>];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="type">float</span>*a,<span class="type">float</span>*b,<span class="type">float</span>*res,<span class="type">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">  <span class="keyword">if</span>(i &lt; N)</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">// set up device</span></span><br><span class="line">  <span class="built_in">initDevice</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">24</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Vector size:%d\n&quot;</span>,nElem);</span><br><span class="line">  <span class="type">int</span> nByte=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line">  <span class="type">float</span> *res_h=(<span class="type">float</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_h,<span class="number">0</span>,nByte);</span><br><span class="line">  <span class="built_in">memset</span>(res_from_gpu_h,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">  <span class="type">float</span> *a_d,*b_d,*res_d;</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;a_d,nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;b_d,nByte));</span><br><span class="line">  <span class="built_in">CHECK</span>(<span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;res_d,nByte));</span><br><span class="line"></span><br><span class="line">  <span class="built_in">initialData</span>(a_d,nElem);</span><br><span class="line">  <span class="built_in">initialData</span>(b_d,nElem);</span><br><span class="line"></span><br><span class="line">  <span class="comment">//CHECK(cudaMemcpy(a_d,a_h,nByte,cudaMemcpyHostToDevice));</span></span><br><span class="line">  <span class="comment">//CHECK(cudaMemcpy(b_d,b_h,nByte,cudaMemcpyHostToDevice));</span></span><br><span class="line"></span><br><span class="line">  <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">512</span>)</span></span>;</span><br><span class="line">  <span class="function">dim3 <span class="title">grid</span><span class="params">((nElem<span class="number">-1</span>)/block.x<span class="number">+1</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">  sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_d,b_d,res_d,nElem);</span><br><span class="line">  <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">//CHECK(cudaMemcpy(res_from_gpu_h,res_d,nByte,cudaMemcpyDeviceToHost));</span></span><br><span class="line">  <span class="built_in">sumArrays</span>(b_d,b_d,res_h,nElem);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">checkResult</span>(res_h,res_d,nElem);</span><br><span class="line">  <span class="built_in">cudaFree</span>(a_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(b_d);</span><br><span class="line">  <span class="built_in">cudaFree</span>(res_d);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">free</span>(res_h);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意我们注释掉的，这就是省去的代码部分。</p>
<h1 id="共享内存和常量内存"><a href="#共享内存和常量内存" class="headerlink" title="共享内存和常量内存"></a>共享内存和常量内存</h1><h2 id="共享内存-2"><a href="#共享内存-2" class="headerlink" title="共享内存"></a>共享内存</h2><p>共享内存（shared memory，SMEM）是GPU的一个关键部分，物理层面，每个SM都有一个小的内存池，这个线程池被次SM上执行的线程块中的所有线程所共享。共享内存使同一个线程块中可以相互协同，便于片上的内存可以被最大化的利用，降低回到全局内存读取的延迟。<br>共享内存是被我们用代码控制的，这也是是他称为我们手中最灵活的优化武器。</p>
<p>结合我们前面学习的一级缓存，二级缓存，今天的共享内存，以及后面的只读和常量缓存，他们的关系如下图：</p>
<p><img src="/img/image-20220910151624832.png" alt="image-20220910151624832"></p>
<p>SM上有共享内存，L1一级缓存，ReadOnly 只读缓存，Constant常量缓存。所有从Dram全局内存中过来的数据都要经过二级缓存，相比之下，更接近SM计算核心的SMEM，L1，ReadOnly，Constant拥有更快的读取速度，SMEM和L1相比于L2延迟低大概20~30倍，带宽大约是10倍。</p>
<p>共享内存是在他所属的线程块被执行时建立，线程块执行完毕后共享内存释放，线程块和他的共享内存有相同的生命周期。</p>
<p>对于每个线程对共享内存的访问请求</p>
<ol>
<li>最好的情况是当前线程束中的每个线程都访问一个不冲突的共享内存，具体是什么样的我们后面再说，这种情况，大家互不干扰，一个事务完成整个线程束的访问，效率最高</li>
<li>当有访问冲突的时候，具体怎么冲突也要后面详细说，这时候一个线程束32个线程，需要32个事务。</li>
<li>如果线程束内32个线程访问同一个地址，那么一个线程访问完后以广播的形式告诉大家</li>
</ol>
<p>注意我们刚才说的共享内存的生命周期是和其所属的线程块相同的，这个共享内存是编程模型层面上的。物理层面上，一个SM上的所有的正在执行的线程块共同使用物理的共享内存，所以共享内存也成为了活跃线程块的限制，共享内存越大，或者块使用的共享内存越小，那么线程块级别的并行度就越高。</p>
<h2 id="共享内存分配"><a href="#共享内存分配" class="headerlink" title="共享内存分配"></a>共享内存分配</h2><p>分配和定义共享内存的方法有多种，动态的声明，静态的声明都是可以的。可以在核函数内，也可以在核函数外（也就是本地的和全局的，这里是说变量的作用域，在一个文件中），CUDA支持1，2，3维的共享内存声明。</p>
<p>声明共享内存通过关键字：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__shared__</span><br></pre></td></tr></table></figure>
<p>声明一个二维浮点数共享内存数组的方法是：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__shared__ <span class="type">float</span> a[size_x][size_y];</span><br></pre></td></tr></table></figure>
<p>这里的<code>size_x</code>，<code>size_y</code>和声明c++数组一样，要是一个编译时确定的数字，不能是变量。如果想动态声明一个共享内存数组，可以使用extern关键字，并在核函数启动时添加第三个参数。声明:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> __shared__ <span class="type">int</span> tile[];</span><br></pre></td></tr></table></figure>
<p>在执行上面这个声明的核函数时，使用下面这种配置：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel&lt;&lt;&lt;grid,block,<span class="function">isize*<span class="title">sizeof</span><span class="params">(<span class="type">int</span>)</span>&gt;&gt;&gt;<span class="params">(...)</span></span>;</span><br></pre></td></tr></table></figure>
<p><code>isize</code>就是共享内存要存储的数组的大小。比如一个十个元素的int数组，isize就是10。</p>
<h2 id="共享内存存储体和访问模式"><a href="#共享内存存储体和访问模式" class="headerlink" title="共享内存存储体和访问模式"></a>共享内存存储体和访问模式</h2><h3 id="内存存储体"><a href="#内存存储体" class="headerlink" title="内存存储体"></a>内存存储体</h3><p>共享内存是一个一维的地址空间，注意这句话的意思是，共享内存的地址是一维的，也就是和所有我们前面提到过的内存一样，都是线性的，二维三维更多维的地址都要转换成一维的来对应物理上的内存地址。</p>
<p>共享内存有个特殊的形式是，分为32个同样大小的内存模型，称为存储体，可以同时访问。32个存储体的目的是对应一个线程束中有32个线程，这些线程在访问共享内存的时候，如果都访问不同存储体（无冲突），那么一个事务就能够完成，否则（有冲突）需要多个内存事务了，这样带宽利用率降低。</p>
<h3 id="存储体冲突"><a href="#存储体冲突" class="headerlink" title="存储体冲突"></a>存储体冲突</h3><p>当多个线程要访问一个存储体的时候，冲突就发生了，注意这里是说访问同一个存储体，而不是同一个地址，访问同一个地址不存在冲突（广播形式）。当发生冲突就会有等待和更多的事务产生，这是严重影响效率的。线程束访问共享内存的时候有下面3种经典模式：</p>
<ol>
<li>并行访问，多地址访问多存储体</li>
<li>串行访问，多地址访问同一存储体</li>
<li>广播访问，单一地址读取单一存储体</li>
</ol>
<p>并行访问是最常见，也是效率较高的一种，但是也可以分为完全无冲突，和小部分冲突的情况，完全无冲突是理想模式，线程束中所有线程通过一个内存事务完成自己的需求，互不干扰，效率最高，当有小部分冲突的时候，大部分不冲突的部分可以通过一个内存事务完成，冲突的被分割成另外的不冲突的事务被执行，这样效率稍低。</p>
<p>上面的小部分冲突变成完全冲突就是串行模式了，这是最糟糕的形式，所有线程访问同一个存储体，注意不是同一个地址，是同一个存储体，一个存储体有很多地址。这时就是串行访问。</p>
<p>广播访问是所有线程访问一个地址，这时候，一个内存事务执行完毕后，一个线程得到了这个地址的数据，他会通过广播的形式告诉其他所有线程，虽然这个延迟相比于完全的并行访问并不慢，但是他只读取了一个数据，带宽利用率很差。</p>
<p>最优访问模式（并行不冲突）：</p>
<p><img src="/img/image-20220910152219591.png" alt="image-20220910152219591"></p>
<p>不规则的访问模式（并行不冲突）：</p>
<p><img src="/img/image-20220910152231251.png" alt="image-20220910152231251"></p>
<p>不规则的访问模式（并行可能冲突，也可能不冲突）</p>
<p><img src="/img/image-20220910152242868.png" alt="image-20220910152242868"></p>
<p>这时候又两种可能</p>
<ol>
<li>冲突：这时候就要等待了</li>
<li>不冲突：访问同一个存储体的线程都要访问同一个地址，通过广播解决问题。</li>
</ol>
<p>以上就是产生冲突的根本原因，我们通过调整数据，代码，算法，最好规避冲突，提高性能。</p>
<h3 id="访问模式"><a href="#访问模式" class="headerlink" title="访问模式"></a>访问模式</h3><p>共享内存的存储体和地址有什么关系呢？这个关系决定了访问模式。内存存储体的宽度随设备计算能力不同而变化，有以下两种情况：</p>
<ol>
<li>2.x计算能力的设备，为4字节（32位）</li>
<li>3.x计算能力的设备，为8字节（64位）</li>
</ol>
<p><img src="/img/image-20220910152701764.png" alt="image-20220910152701764"></p>
<p>同一个线程束中的两个线程访问同一个地址不会发生冲突，一个线程读取后广播告诉有相同需求的线程。但是对于写入，这个就不确定了，结果不可预料。</p>
<p>我们之前一次只能取四个西瓜，现在可以取八个西瓜了，这时候如果有两个线程访问同一个存储体，按照我们前面的解释，一种是访问同一个地址，这时候通过广播来解决冲突，还有一种冲突是需要用等待解决的，当桶变宽了，如果一个线程想要桶里左边的西瓜，而一个线程想要右边的西瓜，这时候是不冲突的，因为桶是够宽的。</p>
<p>或者我们可以理解为更宽的桶，在桶中间又进行了一次间隔，左右两边各一个空间，读取不影响，如果两个线程都要左边的西瓜则等待，如果一个要左边的一个要右边的，这时候可以同时进行不冲突。</p>
<p>把桶换成存储体就是</p>
<p><img src="/img/image-20220910154608530.png" alt="image-20220910154608530"></p>
<p>下图显示64位宽的存储体无冲突访问的一种情况，每个bank被划分成了两部分</p>
<p><img src="/img/image-20220910154618890.png" alt="image-20220910154618890"></p>
<p>下图是另一种无冲突方式：</p>
<p><img src="/img/image-20220910154630133.png" alt="image-20220910154630133"></p>
<p>一种冲突方式，两个线程访问同一个小桶：</p>
<p><img src="/img/image-20220910154641152.png" alt="image-20220910154641152"></p>
<p>另一种冲突方式，三个线程访问同一个小桶</p>
<p><img src="/img/image-20220910154652275.png" alt="image-20220910154652275"></p>
<h3 id="内存填充"><a href="#内存填充" class="headerlink" title="内存填充"></a>内存填充</h3><p>存储体冲突会严重影响共享内存的效率，那么当我们遇到严重冲突的情况下，可以使用填充的办法让数据错位，来降低冲突。假如我们当前存储体内的数据罗列如下，这里假设共4个存储体，实际是32个</p>
<p><img src="/img/image-20220910154710457.png" alt="image-20220910154710457"></p>
<p>当我们的线程束访问bank0中的不同数据的时候就会发生一个5线程的冲突，这时候我们假如我们分配内存时候的声明是：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__shared__ <span class="type">int</span> a[<span class="number">5</span>][<span class="number">4</span>];</span><br></pre></td></tr></table></figure>
<p>这时候我们的就会得到上面的图中的这种内存布局，但是当我们声明的时候改成</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__shared__ <span class="type">int</span> a[<span class="number">5</span>][<span class="number">5</span>];</span><br></pre></td></tr></table></figure>
<p>就会产生这个效果，在编程时候加入一行填充物</p>
<p><img src="/img/image-20220910154956112.png" alt="image-20220910154956112"></p>
<p>然后编译器会将这个二维数组重新分配到存储体，因为存储体一共就4个，我们每一行有5个元素，所以有一个元素进入存储体的下一行，这样，所有元素都错开了，就不会出现冲突了。</p>
<p><img src="/img/image-20220910155010929.png" alt="image-20220910155010929"></p>
<p>共享内存在确定大小的时候，比如编译的时候，就已经被确定好每个地址在哪个存储体中了，想要改变分布，就在声明共享内存的时候调整就行，跟将要存储到共享内存中的数据没有关系。</p>
<p>注意：共享内存声明时，就决定了每个地址所在的存储体，想要调整每个地址对应的存储体，就要扩大声明的共享内存的大小，至于扩大多少，就要根据我们前面的公式好好计算了。这段是本文较难理解的一段。</p>
<h3 id="访问模式配置"><a href="#访问模式配置" class="headerlink" title="访问模式配置"></a>访问模式配置</h3><p>访问模式查询：可以通过以下语句，查询是4字节还是8字节：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaDeviceGetSharedMemConfig</span><span class="params">(cudaSharedMemConfig * pConfig)</span></span>;</span><br></pre></td></tr></table></figure>
<p>返回的pConfig可以是下面的结果：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cudaSharedMemBankSizeFourByte</span><br><span class="line">cudaSharedMemBankSizeEightByte</span><br></pre></td></tr></table></figure>
<p>在可以配置的设备上，可以用下面函数来配置新的存储体大小：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaDeviceSetShareMemConfig</span><span class="params">(cudaSharedMemConfig config)</span></span>;</span><br></pre></td></tr></table></figure>
<p>其中 config可以是：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cudaSharedMemBankSizeDefault</span><br><span class="line">cudaSharedMemBankSizeFourByte</span><br><span class="line">cudaSharedMemBankSizeEightByte</span><br></pre></td></tr></table></figure>
<p>不同的核函数启动之间，更改共享内存的配置，可能需要一个隐式的设备同步点，更改共享内存存储体的大小不会增加共享内存的使用，也不会影响内核函数的占用率，但其对性能可能有重大的影响。大的存储体可能有更高的带宽，大可能导致更多的冲突，要根据具体情况进行分析。</p>
<h2 id="配置共享内存"><a href="#配置共享内存" class="headerlink" title="配置共享内存"></a>配置共享内存</h2><p>每个SM上有64KB的片上内存，共享内存和L1共享这64KB，并且可以配置。CUDA为配置一级缓存和共享内存提供以下两种方法：</p>
<ol>
<li>按设备进行配置</li>
<li>按核函数进行配置</li>
</ol>
<p>配置函数：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaDeviceSetCacheConfig</span><span class="params">(cudaFuncCache cacheConfig)</span></span>;</span><br></pre></td></tr></table></figure>
<p>其中配置参数如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cudaFuncCachePreferNone: <span class="function">no <span class="title">preference</span><span class="params">(<span class="keyword">default</span>)</span></span></span><br><span class="line"><span class="function">cudaFuncCachePreferShared: prefer <span class="number">48</span>KB shared memory and <span class="number">16</span> KB L1 cache</span></span><br><span class="line"><span class="function">cudaFuncCachePreferL1: prefer <span class="number">48</span>KB L1 cache and <span class="number">16</span> KB shared memory</span></span><br><span class="line"><span class="function">cudaFuncCachePreferEqual: prefer <span class="number">32</span>KB L1 cache and <span class="number">32</span> KB shared memory</span></span><br></pre></td></tr></table></figure>
<p>那种更好全看核函数：</p>
<ol>
<li>共享内存使用较多，那么更多的共享内存更好</li>
<li>更多的寄存器使用，L1更多更好。</li>
</ol>
<p>另一个函数是通过不同核函数自动配置的。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaFuncSetCacheConfig</span><span class="params">(<span class="type">const</span> <span class="type">void</span>* func,<span class="keyword">enum</span> cudaFuncCacheca cheConfig)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这里的func是核函数指针，当我们调用某个核函数时，次核函数已经配置了对应的L1和共享内存，那么其如果和当前配置不同，则会重新配置，否则直接执行。<br>一级缓存和共享内存都在同一个片上，但是行为大不相同，共享内存靠的的是存储体来管理数据，而L1则是通过缓存行进行访问。我们对共享内存有绝对的控制权，但是L1的删除工作是硬件完成的。</p>
<p>GPU缓存比CPU的更难理解，GPU使用启发式算法删除数据，由于GPU使用缓存的线程更多，所以数据删除更频繁而且不可预知。共享内存则可以很好的被控制，减少不必要的误删造成的低效，保证SM的局部性。</p>
<h2 id="同步-1"><a href="#同步-1" class="headerlink" title="同步"></a>同步</h2><p>同步是并行的重要机制，其主要目的就是防止冲突。同步基本方法：</p>
<ol>
<li>障碍</li>
<li>内存栅栏</li>
</ol>
<p>障碍是所有调用线程等待其余调用线程达到障碍点。内存栅栏，所有调用线程必须等到全部内存修改对其余线程可见时才继续进行。</p>
<h3 id="弱排序内存模型"><a href="#弱排序内存模型" class="headerlink" title="弱排序内存模型"></a>弱排序内存模型</h3><p>CUDA采用宽松的内存模型，也就是内存访问不一定按照他们在程序中出现的位置进行的。宽松的内存模型，导致了更激进的编译器。</p>
<blockquote>
<p>GPU线程在不同的内存，比如SMEM，全局内存，锁页内存或对等设备内存中，写入数据的顺序是不一定和这些数据在源代码中访问的顺序相同，当一个线程的写入顺序对其他线程可见的时候，他可能和写操作被执行的实际顺序不一致。指令之间相互独立，线程从不同内存中读取数据的顺序和读指令在程序中的顺序不一定相同。换句话说，核函数内连续两个内存访问指令，如果独立，其不一定哪个先被执行。</p>
</blockquote>
<h3 id="显示障碍"><a href="#显示障碍" class="headerlink" title="显示障碍"></a>显示障碍</h3><p>CUDA中，障碍点设置在核函数中，注意这个指令只能在核函数中调用，并只对同一线程块内线程有效。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncthreads();</span><br></pre></td></tr></table></figure>
<ol>
<li><p>__syncthreads()作为一个障碍点，他保证在同一线程块内所有线程没到达此障碍点时，不能继续向下执行。</p>
</li>
<li><p>同一线程块内此障碍点之前的所有全局内存，共享内存操作，对后面的线程都是可见的。</p>
</li>
<li><p>这个也就能解决同一线程块内，内存竞争的问题，同步，保证先后顺序，不会混乱。</p>
</li>
<li><p>避免死锁情况出现，比如下面这种情况，就会导致内核死锁：</p>
</li>
<li><p>只能解决一个块内的线程同步，想做块之间的，只能通过核函数的执行和结束来进行块之间的同步。（把要同步的地方作为核函数的结束，来隐式的同步线程块）</p>
</li>
</ol>
<pre><code><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (threadID % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</code></pre><h3 id="内存栅栏"><a href="#内存栅栏" class="headerlink" title="内存栅栏"></a>内存栅栏</h3><p>内存栅栏能保证栅栏前的内核内存写操作对栅栏后的其他线程都是可见的，有以下三种栅栏：块，网格，系统。</p>
<ol>
<li><p>线程块内：</p>
 <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence_block();</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>保证同一块中的其他线程对于栅栏前的内存写操作可见</p>
<ol>
<li><p>网格级内存栅栏</p>
 <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence();</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>挂起调用线程，直到全局内存中所有写操作对相同的网格内的所有线程可见</p>
<ol>
<li><p>系统级栅栏，夸系统，包括主机和设备，</p>
 <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence_system();</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>挂起调用线程，以保证该线程对全局内存，锁页主机内存和其他设备内存中的所有写操作对全部设备中的线程和主机线程可见。</p>
<h3 id="Volatile修饰符"><a href="#Volatile修饰符" class="headerlink" title="Volatile修饰符"></a>Volatile修饰符</h3><p>volatile声明一个变量，防止编译器优化，防止这个变量存入缓存，如果恰好此时被其他线程改写，那就会造成内存缓存不一致的错误，所以volatile声明的变量始终在全局内存中。</p>
<h1 id="减少全局内存访问"><a href="#减少全局内存访问" class="headerlink" title="减少全局内存访问"></a>减少全局内存访问</h1><h2 id="使用共享内存的并行归约"><a href="#使用共享内存的并行归约" class="headerlink" title="使用共享内存的并行归约"></a>使用共享内存的并行归约</h2><p>我们首先来回忆全局内存下的，完全展开的归约计算：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduceGmem</span><span class="params">(<span class="type">int</span> * g_idata,<span class="type">int</span> * g_odata,<span class="type">unsigned</span> <span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">//set thread ID</span></span><br><span class="line">	<span class="type">unsigned</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">	<span class="type">unsigned</span> <span class="type">int</span> idx = blockDim.x*blockIdx.x+threadIdx.x;</span><br><span class="line">	<span class="comment">//boundary check</span></span><br><span class="line">	<span class="keyword">if</span> (tid &gt;= n) <span class="keyword">return</span>;</span><br><span class="line">	<span class="comment">//convert global data pointer to the</span></span><br><span class="line">	<span class="type">int</span> *idata = g_idata + blockIdx.x*blockDim.x;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//in-place reduction in global memory</span></span><br><span class="line">	<span class="keyword">if</span>(blockDim.x&gt;=<span class="number">1024</span> &amp;&amp; tid &lt;<span class="number">512</span>)</span><br><span class="line">		idata[tid]+=idata[tid<span class="number">+512</span>];</span><br><span class="line">	__syncthreads();</span><br><span class="line">	<span class="keyword">if</span>(blockDim.x&gt;=<span class="number">512</span> &amp;&amp; tid &lt;<span class="number">256</span>)</span><br><span class="line">		idata[tid]+=idata[tid<span class="number">+256</span>];</span><br><span class="line">	__syncthreads();</span><br><span class="line">	<span class="keyword">if</span>(blockDim.x&gt;=<span class="number">256</span> &amp;&amp; tid &lt;<span class="number">128</span>)</span><br><span class="line">		idata[tid]+=idata[tid<span class="number">+128</span>];</span><br><span class="line">	__syncthreads();</span><br><span class="line">	<span class="keyword">if</span>(blockDim.x&gt;=<span class="number">128</span> &amp;&amp; tid &lt;<span class="number">64</span>)</span><br><span class="line">		idata[tid]+=idata[tid<span class="number">+64</span>];</span><br><span class="line">	__syncthreads();</span><br><span class="line">	<span class="comment">//write result for this block to global mem</span></span><br><span class="line">	<span class="keyword">if</span>(tid&lt;<span class="number">32</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">volatile</span> <span class="type">int</span> *vmem = idata;</span><br><span class="line">		vmem[tid]+=vmem[tid<span class="number">+32</span>];</span><br><span class="line">		vmem[tid]+=vmem[tid<span class="number">+16</span>];</span><br><span class="line">		vmem[tid]+=vmem[tid<span class="number">+8</span>];</span><br><span class="line">		vmem[tid]+=vmem[tid<span class="number">+4</span>];</span><br><span class="line">		vmem[tid]+=vmem[tid<span class="number">+2</span>];</span><br><span class="line">		vmem[tid]+=vmem[tid<span class="number">+1</span>];</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">		g_odata[blockIdx.x] = idata[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下面这步是计算当前线程的索引位置：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> idx = blockDim.x*blockIdx.x+threadIdx.x;</span><br></pre></td></tr></table></figure>
<p>当前线程块对应的数据块首地址</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> *idata = g_idata + blockIdx.x*blockDim.x;</span><br></pre></td></tr></table></figure>
<p>然后是展开循环的部分，tid是当前线程块中线程的标号，主要区别于全局编号idx：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(blockDim.x&gt;=<span class="number">1024</span> &amp;&amp; tid &lt;<span class="number">512</span>)</span><br><span class="line">    idata[tid]+=idata[tid<span class="number">+512</span>];</span><br><span class="line">__syncthreads();</span><br><span class="line"><span class="keyword">if</span>(blockDim.x&gt;=<span class="number">512</span> &amp;&amp; tid &lt;<span class="number">256</span>)</span><br><span class="line">    idata[tid]+=idata[tid<span class="number">+256</span>];</span><br><span class="line">__syncthreads();</span><br><span class="line"><span class="keyword">if</span>(blockDim.x&gt;=<span class="number">256</span> &amp;&amp; tid &lt;<span class="number">128</span>)</span><br><span class="line">    idata[tid]+=idata[tid<span class="number">+128</span>];</span><br><span class="line">__syncthreads();</span><br><span class="line"><span class="keyword">if</span>(blockDim.x&gt;=<span class="number">128</span> &amp;&amp; tid &lt;<span class="number">64</span>)</span><br><span class="line">    idata[tid]+=idata[tid<span class="number">+64</span>];</span><br><span class="line">__syncthreads();</span><br></pre></td></tr></table></figure>
<p>这一步把是当前线程块中的所有数据归约到前64个元素中，接着使用如下代码，将最后64个元素归约成一个</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(tid&lt;<span class="number">32</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">volatile</span> <span class="type">int</span> *vmem = idata;</span><br><span class="line">    vmem[tid]+=vmem[tid<span class="number">+32</span>];</span><br><span class="line">    vmem[tid]+=vmem[tid<span class="number">+16</span>];</span><br><span class="line">    vmem[tid]+=vmem[tid<span class="number">+8</span>];</span><br><span class="line">    vmem[tid]+=vmem[tid<span class="number">+4</span>];</span><br><span class="line">    vmem[tid]+=vmem[tid<span class="number">+2</span>];</span><br><span class="line">    vmem[tid]+=vmem[tid<span class="number">+1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意这里声明了一个volatile变量，如果我们不这么做，编译器不能保证这些数据读写操作按照代码中的顺序执行，所以必须要这么做。</p>
<p>然后我们对上面的代码进行改写，改写成共享内存的版本，来看代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduceSmem</span><span class="params">(<span class="type">int</span> * g_idata,<span class="type">int</span> * g_odata,<span class="type">unsigned</span> <span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">//set thread ID</span></span><br><span class="line">    __shared__ <span class="type">int</span> smem[DIM];</span><br><span class="line">	<span class="type">unsigned</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">	<span class="comment">//unsigned int idx = blockDim.x*blockIdx.x+threadIdx.x;</span></span><br><span class="line">	<span class="comment">//boundary check</span></span><br><span class="line">	<span class="keyword">if</span> (tid &gt;= n) <span class="keyword">return</span>;</span><br><span class="line">	<span class="comment">//convert global data pointer to the</span></span><br><span class="line">	<span class="type">int</span> *idata = g_idata + blockIdx.x*blockDim.x;</span><br><span class="line"></span><br><span class="line">    smem[tid]=idata[tid];</span><br><span class="line">	__syncthreads();</span><br><span class="line">	<span class="comment">//in-place reduction in global memory</span></span><br><span class="line">	<span class="keyword">if</span>(blockDim.x&gt;=<span class="number">1024</span> &amp;&amp; tid &lt;<span class="number">512</span>)</span><br><span class="line">		smem[tid]+=smem[tid<span class="number">+512</span>];</span><br><span class="line">	__syncthreads();</span><br><span class="line">	<span class="keyword">if</span>(blockDim.x&gt;=<span class="number">512</span> &amp;&amp; tid &lt;<span class="number">256</span>)</span><br><span class="line">		smem[tid]+=smem[tid<span class="number">+256</span>];</span><br><span class="line">	__syncthreads();</span><br><span class="line">	<span class="keyword">if</span>(blockDim.x&gt;=<span class="number">256</span> &amp;&amp; tid &lt;<span class="number">128</span>)</span><br><span class="line">		smem[tid]+=smem[tid<span class="number">+128</span>];</span><br><span class="line">	__syncthreads();</span><br><span class="line">	<span class="keyword">if</span>(blockDim.x&gt;=<span class="number">128</span> &amp;&amp; tid &lt;<span class="number">64</span>)</span><br><span class="line">		smem[tid]+=smem[tid<span class="number">+64</span>];</span><br><span class="line">	__syncthreads();</span><br><span class="line">	<span class="comment">//write result for this block to global mem</span></span><br><span class="line">	<span class="keyword">if</span>(tid&lt;<span class="number">32</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">volatile</span> <span class="type">int</span> *vsmem = smem;</span><br><span class="line">		vsmem[tid]+=vsmem[tid<span class="number">+32</span>];</span><br><span class="line">		vsmem[tid]+=vsmem[tid<span class="number">+16</span>];</span><br><span class="line">		vsmem[tid]+=vsmem[tid<span class="number">+8</span>];</span><br><span class="line">		vsmem[tid]+=vsmem[tid<span class="number">+4</span>];</span><br><span class="line">		vsmem[tid]+=vsmem[tid<span class="number">+2</span>];</span><br><span class="line">		vsmem[tid]+=vsmem[tid<span class="number">+1</span>];</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">		g_odata[blockIdx.x] = smem[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>唯一的不同就是多了一个共享内存的声明，以及各线程将全局写入共享内存，以及后面的同步指令：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">smem[tid]=idata[tid];</span><br><span class="line">__syncthreads();</span><br></pre></td></tr></table></figure>
<p>这一步过后同步保证该线程块内的所有线程，都执行到此处后继续向下进行，这是可以理解的，因为我们的归约只针对本块内，当然如果想跨几个块执行，可能同步这里就有问题了，这个是上一节课要讨论的，这里就不过多解释了，我们接着就看到一个volatile类型的指针，指向共享内存，对最后64个归约结果进行归约，整个过程和全局内存一毛一样，只不过一个在全局内存操作，一个在共享内存操作。</p>
<h2 id="使用展开的并行归约"><a href="#使用展开的并行归约" class="headerlink" title="使用展开的并行归约"></a>使用展开的并行归约</h2><p>可能看到上面的截图你已经知道我接下来要并行4块了，对于前面说的，使用共享内存不能并行四块，是因为没办法同步读四个块，这里我们还是用老方法进行并行四个块，就是在写入共享内存之前进行归约，4个块变成一个，然后把这一个存入共享内存，进行常规的共享内存归约:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduceUnroll4Smem</span><span class="params">(<span class="type">int</span> * g_idata,<span class="type">int</span> * g_odata,<span class="type">unsigned</span> <span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="comment">//set thread ID</span></span><br><span class="line">    __shared__ <span class="type">int</span> smem[DIM];</span><br><span class="line">	<span class="type">unsigned</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">	<span class="type">unsigned</span> <span class="type">int</span> idx = blockDim.x*blockIdx.x*<span class="number">4</span>+threadIdx.x;</span><br><span class="line">	<span class="comment">//boundary check</span></span><br><span class="line">	<span class="keyword">if</span> (tid &gt;= n) <span class="keyword">return</span>;</span><br><span class="line">	<span class="comment">//convert global data pointer to the</span></span><br><span class="line">    <span class="type">int</span> tempSum=<span class="number">0</span>;</span><br><span class="line">	<span class="keyword">if</span>(idx<span class="number">+3</span> * blockDim.x&lt;=n)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="type">int</span> a1=g_idata[idx];</span><br><span class="line">		<span class="type">int</span> a2=g_idata[idx+blockDim.x];</span><br><span class="line">		<span class="type">int</span> a3=g_idata[idx<span class="number">+2</span>*blockDim.x];</span><br><span class="line">		<span class="type">int</span> a4=g_idata[idx<span class="number">+3</span>*blockDim.x];</span><br><span class="line">		tempSum=a1+a2+a3+a4;</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line">    smem[tid]=tempSum;</span><br><span class="line">	__syncthreads();</span><br><span class="line">	<span class="comment">//in-place reduction in global memory</span></span><br><span class="line">	<span class="keyword">if</span>(blockDim.x&gt;=<span class="number">1024</span> &amp;&amp; tid &lt;<span class="number">512</span>)</span><br><span class="line">		smem[tid]+=smem[tid<span class="number">+512</span>];</span><br><span class="line">	__syncthreads();</span><br><span class="line">	<span class="keyword">if</span>(blockDim.x&gt;=<span class="number">512</span> &amp;&amp; tid &lt;<span class="number">256</span>)</span><br><span class="line">		smem[tid]+=smem[tid<span class="number">+256</span>];</span><br><span class="line">	__syncthreads();</span><br><span class="line">	<span class="keyword">if</span>(blockDim.x&gt;=<span class="number">256</span> &amp;&amp; tid &lt;<span class="number">128</span>)</span><br><span class="line">		smem[tid]+=smem[tid<span class="number">+128</span>];</span><br><span class="line">	__syncthreads();</span><br><span class="line">	<span class="keyword">if</span>(blockDim.x&gt;=<span class="number">128</span> &amp;&amp; tid &lt;<span class="number">64</span>)</span><br><span class="line">		smem[tid]+=smem[tid<span class="number">+64</span>];</span><br><span class="line">	__syncthreads();</span><br><span class="line">	<span class="comment">//write result for this block to global mem</span></span><br><span class="line">	<span class="keyword">if</span>(tid&lt;<span class="number">32</span>)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">volatile</span> <span class="type">int</span> *vsmem = smem;</span><br><span class="line">		vsmem[tid]+=vsmem[tid<span class="number">+32</span>];</span><br><span class="line">		vsmem[tid]+=vsmem[tid<span class="number">+16</span>];</span><br><span class="line">		vsmem[tid]+=vsmem[tid<span class="number">+8</span>];</span><br><span class="line">		vsmem[tid]+=vsmem[tid<span class="number">+4</span>];</span><br><span class="line">		vsmem[tid]+=vsmem[tid<span class="number">+2</span>];</span><br><span class="line">		vsmem[tid]+=vsmem[tid<span class="number">+1</span>];</span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (tid == <span class="number">0</span>)</span><br><span class="line">		g_odata[blockIdx.x] = smem[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这段代码就是多了其他三块的求和：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> idx = blockDim.x*blockIdx.x*<span class="number">4</span>+threadIdx.x;</span><br><span class="line"><span class="comment">//boundary check</span></span><br><span class="line"><span class="keyword">if</span> (tid &gt;= n) <span class="keyword">return</span>;</span><br><span class="line"><span class="comment">//convert global data pointer to the</span></span><br><span class="line"><span class="type">int</span> tempSum=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">if</span>(idx<span class="number">+3</span> * blockDim.x&lt;=n)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> a1=g_idata[idx];</span><br><span class="line">    <span class="type">int</span> a2=g_idata[idx+blockDim.x];</span><br><span class="line">    <span class="type">int</span> a3=g_idata[idx<span class="number">+2</span>*blockDim.x];</span><br><span class="line">    <span class="type">int</span> a4=g_idata[idx<span class="number">+3</span>*blockDim.x];</span><br><span class="line">    tempSum=a1+a2+a3+a4;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这一步在3.5中已经介绍过了为什么能加速了，因为可以通过增加三步计算而减少之前的3个线程块的计算，这是非常大的减少。同时多步内存加载也可以使内存带宽达到更好的使用。</p>
<h1 id="流和并发"><a href="#流和并发" class="headerlink" title="流和并发"></a>流和并发</h1><h2 id="流和事件概述"><a href="#流和事件概述" class="headerlink" title="流和事件概述"></a>流和事件概述</h2><p>CUDA流：一系列异步CUDA操作，比如我们常见的套路，在主机端分配设备主存（cudaMalloc），主机向设备传输数据（cudaMemcpy），核函数启动，复制数据回主机（Memcpy）这些操作中有些是异步的，执行顺序也是按照主机代码中的顺序执行的（但是异步操作的结束不一定是按照代码中的顺序的）。</p>
<p>流能封装这些异步操作，并保持操作顺序，允许操作在流中排队。保证其在前面所有操作启动之后启动，有了流，我们就能查询排队状态了。</p>
<p>我们上面举得一般情况下的操作基本可以分为以下三种：</p>
<ul>
<li>主机与设备间的数据传输</li>
<li>核函数启动</li>
<li>其他的由主机发出的设备执行的命令</li>
</ul>
<p>流中的操作相对于主机来说总是异步的，CUDA运行时决定何时可以在设备上执行操作。我们要做的就是控制这些操作在其结果出来之前，不启动需要调用这个结果的操作。</p>
<p>一个流中的不同操作有着严格的顺序。但是不同流之间是没有任何限制的。多个流同时启动多个内核，就形成了网格级别的并行。CUDA流中排队的操作和主机都是异步的，所以排队的过程中并不耽误主机运行其他指令，所以这就隐藏了执行这些操作的开销。CUDA编程的一个典型模式是，也就是我们上面讲到的一般套路：</p>
<ol>
<li>将输入数据从主机复制到设备上</li>
<li>在设备上执行一个内核</li>
<li>将结果从设备移回主机</li>
</ol>
<p>一般的生产情况下，内核执行的时间要长于数据传输，所以我们前面的例子大多是数据传输更耗时，这是不实际的。当重叠核函数执行和数据传输操作，可以屏蔽数据移动造成的时间消耗，当然正在执行的内核的数据需要提前复制到设备上的，这里说的数据传输和内核执行是同时操作的是指当前传输的数据是接下来流中的内核需要的。这样总的执行时间就被缩减了。流在CUDA的API调用可以实现流水线和双缓冲技术。</p>
<p>CUDA的API也分为同步和异步的两种：</p>
<ul>
<li>同步行为的函数会阻塞主机端线程直到其完成</li>
<li>异步行为的函数在调用后会立刻把控制权返还给主机。</li>
</ul>
<p>异步行为和流式构建网格级并行的支柱。</p>
<p>虽然我们从软件模型上提出了流，网格级并行的概念，但是说来说去我们能用的就那么一个设备，如果设备空闲当然可以同时执行多个核，但是如果设备已经跑满了，那么我们认为并行的指令也必须排队等待——PCIe总线和SM数量是有限的，当他们被完全占用，流是没办法做什么的，除了等待</p>
<p>我们接下来就要研究多种计算能力的设备上的流是如何运行的。</p>
<h2 id="CUDA流"><a href="#CUDA流" class="headerlink" title="CUDA流"></a>CUDA流</h2><p>我们的所有CUDA操作都是在流中进行的，虽然我们可能没发现，但是有我们前面的例子中的指令，内核启动，都是在CUDA流中进行的，只是这种操作是隐式的，所以肯定还有显式的，所以，流分为：</p>
<ul>
<li>隐式声明的流，我们叫做空流</li>
<li>显式声明的流，我们叫做非空流</li>
</ul>
<p>如果我们没有特别声明一个流，那么我们的所有操作是在默认的空流中完成的，我们前面的所有例子都是在默认的空流中进行的。<br>空流是没办法管理的，因为他连个名字都没有，似乎也没有默认名，所以当我们想控制流，非空流是非常必要的。<br>基于流的异步内核启动和数据传输支持以下类型的粗粒度并发</p>
<ul>
<li>重叠主机和设备计算</li>
<li>重叠主机计算和主机设备数据传输</li>
<li>重叠主机设备数据传输和设备计算</li>
<li>并发设备计算（多个设备）</li>
</ul>
<p>CUDA编程和普通的C++不同的就是，我们有两个“可运算的设备”也就是CPU和GPU这两个东西，这种情况下，他们之间的同步并不是每一步指令都互相通信执行进度的，设备不知道主机在干啥，主机也不是完全知道设备在干啥。但是数据传输是同步的，也就是主机要等设备接收完数据才干别的。内核启动就是异步的。异步操作，可以重叠主机计算和设备计算。</p>
<p>前面用的cudaMemcpy就是个同步操作，我们还提到过隐式同步——从设备复制结果数据回主机，要等设备执行完。当然数据传输有异步版本：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpyAsync</span><span class="params">(<span class="type">void</span>* dst, <span class="type">const</span> <span class="type">void</span>* src, <span class="type">size_t</span> count,cudaMemcpyKind kind, cudaStream_t stream = <span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure>
<p>值得注意的就是最后一个参数，stream表示流，一般情况设置为默认流，这个函数和主机是异步的，执行后控制权立刻归还主机，当然我们需要声明一个非空流：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaStreamCreate</span><span class="params">(cudaStream_t* pStream)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这样我们就有一个可以被管理的流了，这段代码是创建了一个流，有C++经验的人能看出来，这个是为一个流分配必要资源的函数，给流命名声明流的操作应该是：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaStream_t a;</span><br></pre></td></tr></table></figure>
<p>定义了一个叫a的流，但是这个流没法用，相当于只有了名字，资源还是要用<code>cudaStreamCreate</code>分配的。</p>
<p>接下来必须要特别注意：执行异步数据传输时，主机端的内存必须是固定的，非分页的！！</p>
<p>讲内存模型的时候我们说到过，分配方式：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMallocHost</span><span class="params">(<span class="type">void</span> **ptr, <span class="type">size_t</span> size)</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaHostAlloc</span><span class="params">(<span class="type">void</span> **pHost, <span class="type">size_t</span> size, <span class="type">unsigned</span> <span class="type">int</span> flags)</span></span>;</span><br></pre></td></tr></table></figure>
<p>主机虚拟内存中分配的数据在物理内存中是随时可能被移动的，我们必须确保其在整个生存周期中位置不变，这样在异步操作中才能准确的转移数据，否则如果操作系统移动了数据的物理地址，那么我们的设备可能还是回到之前的物理地址取数据，这就会出现未定义的错误。</p>
<p>在非空流中执行内核需要在启动核函数的时候加入一个附加的启动配置：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;grid, block, sharedMemSize, stream&gt;&gt;&gt;(argument list);</span><br></pre></td></tr></table></figure>
<p>pStream参数就是附加的参数，使用目标流的名字作为参数，比如想把核函数加入到a流中，那么这个stream就变成a。前面我们为一个流分配资源，当然后面就要回收资源，回收方式：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaStreamDestroy</span><span class="params">(cudaStream_t stream)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这个回收函数很有意思，由于流和主机端是异步的，你在使用上面指令回收流的资源的时候，很有可能流还在执行，这时候，这条指令会正常执行，但是不会立刻停止流，而是等待流执行完成后，立刻回收该流中的资源。这样做是合理的也是安全的。</p>
<p>当然，我们可以查询流执行的怎么样了，下面两个函数就是帮我们查查我们的流到哪了：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaStreamSynchronize</span><span class="params">(cudaStream_t stream)</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaStreamQuery</span><span class="params">(cudaStream_t stream)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这两条执行的行为非常不同，<code>cudaStreamSynchronize</code>会阻塞主机，直到流完成。<code>cudaStreamQuery</code>则是立即返回，如果查询的流执行完了，那么返回cudaSuccess否则返回cudaErrorNotReady。</p>
<p>下面这段示例代码就是典型多个流中调度CUDA操作的常见模式：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; i++) &#123;</span><br><span class="line">    <span class="type">int</span> offset = i * bytesPerStream;</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(&amp;d_a[offset], &amp;a[offset], bytePerStream, streams[i]);</span><br><span class="line">    kernel&lt;&lt;grid, block, <span class="number">0</span>, streams[i]&gt;&gt;(&amp;d_a[offset]);</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(&amp;a[offset], &amp;d_a[offset], bytesPerStream, streams[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; i++) &#123;</span><br><span class="line">    <span class="built_in">cudaStreamSynchronize</span>(streams[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第一个for中循环执行了nStreams个流，每个流中都是“复制数据，执行核函数，最后将结果复制回主机”这一系列操作。</p>
<p>下面的图就是一个简单的时间轴示意图，假设nStreams=3，所有传输和核启动都是并发的：</p>
<p><img src="/img/image-20220910162611009.png" alt="image-20220910162611009"></p>
<p>H2D是主机到设备的内存传输，D2H是设备到主机的内存传输。显然这些操作没有并发执行，而是错开的，原因是PCIe总线是共享的，当第一个流占据了主线，后来的就一定要等待，等待主线空闲。编程模型和硬件的实际执行时有差距了。</p>
<p>上面同时从主机到设备涉及硬件竞争要等待，如果是从主机到设备和从设备到主机同时发生，这时候不会产生等待，而是同时进行。</p>
<p>内核并发最大数量也是有极限的，不同计算能力的设备不同，Fermi设备支持16路并发，Kepler支持32路并发。设备上的所有资源都是限制并发数量的原因，比如共享内存，寄存器，本地内存，这些资源都会限制最大并发数。</p>
<h2 id="流调度"><a href="#流调度" class="headerlink" title="流调度"></a>流调度</h2><h3 id="虚假的依赖关系"><a href="#虚假的依赖关系" class="headerlink" title="虚假的依赖关系"></a>虚假的依赖关系</h3><p>在Fermi架构上16路流并发执行但是所有流最终都是在单一硬件上执行的，Fermi只有一个硬件工作队列，所以他们虽然在编程模型上式并行的，但是在硬件执行过程中是在一个队列中（像串行一样）。当要执行某个网格的时候CUDA会检测任务依赖关系，如果其依赖于其他结果，那么要等结果出来后才能继续执行。单一流水线可能会导致虚假依赖关系：</p>
<p><img src="/img/image-20220910162638841.png" alt="image-20220910162638841"></p>
<p>这个图就是虚假依赖的最准确的描述，我们有三个流，流中的操作相互依赖，比如B要等待A的结果，Z要等待Y的结果，当我们把三个流塞到一个队列中，那么我们就会得到紫色箭头的样子，这个硬件队列中的任务可以并行执行，但是要考虑依赖关系，所以，我们按照顺序会这样执行：</p>
<ol>
<li>执行A，同时检查B是否有依赖关系，当然此时B依赖于A而A没执行完，所以整个队列阻塞</li>
<li>A执行完成后执行B，同时检查C，发现依赖，等待</li>
<li>B执行完后，执行C同时检查，发现P没有依赖，如果此时硬件有多于资源P开始执行</li>
<li>P执行时检查Q，发现Q依赖P，所以等待</li>
</ol>
<p>这种一个队列的模式，会产生一种，虽然P依赖B的感觉，虽然不依赖，但是B不执行完，P没办法执行，而所谓并行，只有一个依赖链的头和尾有可能并行，也就是红圈中任务可能并行，而我们的编程模型中设想的并不是这样的。</p>
<h3 id="Hyper-Q技术"><a href="#Hyper-Q技术" class="headerlink" title="Hyper-Q技术"></a>Hyper-Q技术</h3><p>解决上面虚假依赖的最好办法就是多个工作队列，这样就从根本上解决了虚假依赖关系，Hyper-Q就是这种技术，32个硬件工作队列同时执行多个流，这就可以实现所有流的并发，最小化虚假依赖：</p>
<p><img src="/img/image-20220910162656480.png" alt="image-20220910162656480"></p>
<h2 id="流的优先级"><a href="#流的优先级" class="headerlink" title="流的优先级"></a>流的优先级</h2><p>3.5以上的设备可以给流优先级，也就是优先级高的（数字上更小的，类似于C++运算符优先级）。优先级只影响核函数，不影响数据传输，高优先级的流可以占用低优先级的工作。下面函数创建一个有指定优先级的流</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaStreamCreateWithPriority</span><span class="params">(cudaStream_t* pStream, <span class="type">unsigned</span> <span class="type">int</span> flags,<span class="type">int</span> priority)</span></span>;</span><br></pre></td></tr></table></figure>
<p>不同的设备有不同的优先级等级，下面函数可以查询当前设备的优先级分布情况：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaDeviceGetStreamPriorityRange</span><span class="params">(<span class="type">int</span> *leastPriority, <span class="type">int</span> *greatestPriority)</span></span>;</span><br></pre></td></tr></table></figure>
<p>leastPriority表示最低优先级（整数，远离0）；greatestPriority表示最高优先级（整数，数字较接近0）；如果设备不支持优先级返回0。</p>
<h2 id="CUDA事件"><a href="#CUDA事件" class="headerlink" title="CUDA事件"></a>CUDA事件</h2><p>CUDA事件不同于我们前面介绍的内存事务，不要搞混，事件也是软件层面上的概念。事件的本质就是一个标记，它与其所在的流内的特定点相关联。可以使用时间来执行以下两个基本任务：</p>
<ul>
<li><p>同步流执行</p>
</li>
<li><p>监控设备的进展</p>
</li>
</ul>
<p>流中的任意点都可以通过API插入事件以及查询事件完成的函数，只有事件所在流中其之前的操作都完成后才能触发事件完成。默认流中设置事件，那么其前面的所有操作都完成时，事件才出发完成。</p>
<p>事件就像一个个路标，其本身不执行什么功能，就像我们最原始测试c语言程序的时候插入的无数多个printf一样。</p>
<h3 id="创建和销毁"><a href="#创建和销毁" class="headerlink" title="创建和销毁"></a>创建和销毁</h3><p>事件的声明如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t event;</span><br></pre></td></tr></table></figure></p>
<p>同样声明完后要分配资源：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaEventCreate</span><span class="params">(cudaEvent_t* event)</span></span>;</span><br></pre></td></tr></table></figure>
<p>回收事件的资源</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaEventDestroy</span><span class="params">(cudaEvent_t event)</span></span>;</span><br></pre></td></tr></table></figure>
<p>如果回收指令执行的时候事件还没有完成，那么回收指令立即完成，当事件完成后，资源马上被回收。</p>
<h3 id="记录事件和计算运行时间"><a href="#记录事件和计算运行时间" class="headerlink" title="记录事件和计算运行时间"></a>记录事件和计算运行时间</h3><p>事件的一个主要用途就是记录事件之间的时间间隔。<br>事件通过下面指令添加到CUDA流：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaEventRecord</span><span class="params">(cudaEvent_t event, cudaStream_t stream = <span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure>
<p>在流中的事件主要左右就是等待前面的操作完成，或者测试指定流中操作完成情况，下面和流类似的事件测试指令（是否出发完成）会阻塞主机线程知道事件被完成。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaEventSynchronize</span><span class="params">(cudaEvent_t event)</span></span>;</span><br></pre></td></tr></table></figure>
<p>同样，也有异步版本：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaEventQuery</span><span class="params">(cudaEvent_t event)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这个不会阻塞主机线程，而是直接返回结果和stream版本的类似。另一个函数用在事件上的是记录两个事件之间的时间间隔：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaEventElapsedTime</span><span class="params">(<span class="type">float</span>* ms, cudaEvent_t start, cudaEvent_t stop)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这个函数记录两个事件start和stop之间的时间间隔，单位毫秒，两个事件不一定是同一个流中。这个时间间隔可能会比实际大一些，因为cudaEventRecord这个函数是异步的，所以加入时间完全不可控，不能保证两个事件之间的间隔刚好是两个事件之间的。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create two events</span></span><br><span class="line">cudaEvent_t start, stop;</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line"><span class="comment">// record start event on the default stream</span></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(start);</span><br><span class="line"><span class="comment">// execute kernel</span></span><br><span class="line">kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(arguments);</span><br><span class="line"><span class="comment">// record stop event on the default stream</span></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(stop);</span><br><span class="line"><span class="comment">// wait until the stop event completes</span></span><br><span class="line"><span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line"><span class="comment">// calculate the elapsed time between two events</span></span><br><span class="line"><span class="type">float</span> time;</span><br><span class="line"><span class="built_in">cudaEventElapsedTime</span>(&amp;time, start, stop);</span><br><span class="line"><span class="comment">// clean up the two events</span></span><br><span class="line"><span class="built_in">cudaEventDestroy</span>(start);</span><br><span class="line"><span class="built_in">cudaEventDestroy</span>(stop);</span><br></pre></td></tr></table></figure>
<p>这段代码显示，我们的事件被插入到空流中，设置两个事件作为标记，然后记录他们之间的时间间隔。cudaEventRecord是异步的，所以间隔不准，这是特别要注意的。</p>
<h2 id="流同步"><a href="#流同步" class="headerlink" title="流同步"></a>流同步</h2><p>流分成阻塞流和非阻塞流，在非空流中所有操作都是非阻塞的，所以流启动以后，主机还要完成自己的任务，有时候就可能需要同步主机和流之间的进度，或者同步流和流之间的进度。从主机的角度，CUDA操作可以分为两类：</p>
<ul>
<li>内存相关操作</li>
<li>内核启动</li>
</ul>
<p>内核启动总是异步的，虽然某些内存是同步的，但是他们也有异步版本。</p>
<p>前面我们提到了流的两种类型：</p>
<ul>
<li>异步流（非空流）</li>
<li>同步流（空流/默认流）</li>
</ul>
<p>没有显式声明的流式默认同步流，程序员声明的流都是异步流，异步流通常不会阻塞主机，同步流中部分操作会造成阻塞，主机等待，什么都不做，直到某操作完成。</p>
<p>非空流并不都是非阻塞的，其也可以分为两种类型：</p>
<ul>
<li>阻塞流</li>
<li>非阻塞流</li>
</ul>
<p>虽然正常来讲，非空流都是异步操作，不存在阻塞主机的情况，但是有时候可能被空流中的操作阻塞。如果一个非空流被声明为非阻塞的，那么没人能阻塞他，如果声明为阻塞流，则会被空流阻塞。</p>
<p>有点晕，就是非空流有时候可能需要在运行到一半和主机通信，这时候我们更希望他能被阻塞，而不是不受控制，这样我们就可以自己设定这个流到底受不受控制，也就是是否能被阻塞，下面我们研究如何使用这两种流。</p>
<h3 id="阻塞流和非阻塞流"><a href="#阻塞流和非阻塞流" class="headerlink" title="阻塞流和非阻塞流"></a>阻塞流和非阻塞流</h3><p>cudaStreamCreate创建的是阻塞流，意味着里面有些操作会被阻塞，直到空流中默写操作完成。空流不需要显式声明，而是隐式的，他是阻塞的，跟所有阻塞流同步。</p>
<p>下面这个过程很重要：当操作A发布到空流中，A执行之前，CUDA会等待A之前的全部操作都发布到阻塞流中，所有发布到阻塞流中的操作都会挂起，等待，直到在此操作指令之前的操作都完成，才开始执行。</p>
<p>有点复杂，因为这涉及到代码编写的过程和执行的过程，两个过程混在一起说，肯定有点乱，我们来个例子压压惊就好了：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kernel_1&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, stream_1&gt;&gt;&gt;();</span><br><span class="line">kernel_2&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">kernel_3&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, stream_2&gt;&gt;&gt;();</span><br></pre></td></tr></table></figure>
<p>上面这段代码，有三个流，两个有名字的，一个空流，我们认为stream_1和stream_2是阻塞流，空流是阻塞的，这三个核函数都在阻塞流上执行，具体过程是，kernel_1被启动，控制权返回主机，然后启动kernel_2，但是此时kernel_2 不会并不会马上执行，他会等到kernel_1执行完毕，同理启动完kernel_2 控制权立刻返回给主机，主机继续启动kernel_3,这时候kernel_3 也要等待，直到kernel_2执行完，但是从主机的角度，这三个核都是异步的，启动后控制权马上还给主机。<br>然后我们就想创建一个非阻塞流，因为我们默认创建的是阻塞版本：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaStreamCreateWithFlags</span><span class="params">(cudaStream_t* pStream, <span class="type">unsigned</span> <span class="type">int</span> flags)</span></span>;</span><br></pre></td></tr></table></figure>
<p>第二个参数就是选择阻塞还是非阻塞版本：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cudaStreamDefault;<span class="comment">// 默认阻塞流</span></span><br><span class="line">cudaStreamNonBlocking: <span class="comment">//非阻塞流，对空流的阻塞行为失效。</span></span><br></pre></td></tr></table></figure>
<p>如果前面的stream_1和stream_2声明为非阻塞的，那么上面的调用方法的结果是三个核函数同时执行。</p>
<h3 id="隐式同步"><a href="#隐式同步" class="headerlink" title="隐式同步"></a>隐式同步</h3><p>前面几章核函数计时的时候，我们说过要同步，并且提到过cudaMemcpy 可以隐式同步，也介绍了</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cudaDeviceSynchronize;</span><br><span class="line">cudaStreamSynchronize;</span><br><span class="line">cudaEventSynchronize;</span><br></pre></td></tr></table></figure>
<p>这几个也是同步指令，可以用来同步不同的对象，这些是显式的调用的；与上面的隐式不同。<br>隐式同步的指令其最原始的函数功能并不是同步，所以同步效果是隐式的，这个我们需要非常注意，忽略隐式同步会造成性能下降。所谓同步就是阻塞的意思，被忽视的隐式同步就是被忽略的阻塞，隐式操作常出现在内存操作上，比如：</p>
<ul>
<li>锁页主机内存分布</li>
<li>设备内存分配</li>
<li>设备内存初始化</li>
<li>同一设备两地址之间的内存复制</li>
<li>一级缓存，共享内存配置修改</li>
</ul>
<p>这些操作都要时刻小心，因为他们带来的阻塞非常不容易察觉</p>
<h3 id="显式同步"><a href="#显式同步" class="headerlink" title="显式同步"></a>显式同步</h3><p>显式同步相比就更加光明磊落了，因为一条指令就一个作用，没啥副作用，常见的同步有：</p>
<ul>
<li>同步设备</li>
<li>同步流</li>
<li>同步流中的事件</li>
<li>使用事件跨流同步</li>
</ul>
<p>下面的函数就可以阻塞主机线程，直到设备完成所有操作：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaDeviceSynchronize</span><span class="params">(<span class="type">void</span>)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这个函数我们前面常用，但是尽量少用，这个会拖慢效率。然后是流版本的，我们可以同步流，使用下面两个函数：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaStreamSynchronize</span><span class="params">(cudaStream_t stream)</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaStreamQuery</span><span class="params">(cudaStream_t stream)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这两个函数，第一个是同步流的，阻塞主机直到完成，第二个可以完成非阻塞流测试。也就是测试一下这个流是否完成。</p>
<p>我们提到事件，事件的作用就是在流中设定一些标记用来同步，和检查是否执行到关键点位（事件位置），也是用类似的函数</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaEventSynchronize</span><span class="params">(cudaEvent_t event)</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaEventQuery</span><span class="params">(cudaEvent_t event)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这两个函数的性质和上面的非常类似。</p>
<p>事件提供了一个流之间同步的方法：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaStreamWaitEvent</span><span class="params">(cudaStream_t stream, cudaEvent_t event)</span></span>;</span><br></pre></td></tr></table></figure>
<p>这条命令的含义是，指定的流要等待指定的事件，事件完成后流才能继续，这个事件可以在这个流中，也可以不在，当在不同的流的时候，这个就是实现了跨流同步。</p>
<p><img src="/img/image-20220910164710321.png" alt="image-20220910164710321"></p>
<h3 id="可配置事件"><a href="#可配置事件" class="headerlink" title="可配置事件"></a>可配置事件</h3><p>CDUA提供了一种控制事件行为和性能的函数：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaEventCreateWithFlags</span><span class="params">(cudaEvent_t* event, <span class="type">unsigned</span> <span class="type">int</span> flags)</span></span>;</span><br></pre></td></tr></table></figure>
<p>其中参数是：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cudaEventDefault</span><br><span class="line">cudaEventBlockingSync</span><br><span class="line">cudaEventDisableTiming</span><br><span class="line">cudaEventInterprocess</span><br></pre></td></tr></table></figure>
<p>其中<code>cudaEventBlockingSync</code>指定使用<code>cudaEventSynchronize</code>同步会造成阻塞调用线程。<code>cudaEventSynchronize</code>默认是使用cpu周期不断重复查询事件状态，而当指定了事件是<code>cudaEventBlockingSync</code>的时候，会将查询放在另一个线程中，而原始线程继续执行，直到事件满足条件，才会通知原始线程，这样可以减少CPU的浪费，但是由于通讯的时间，会造成一定的延迟。</p>
<p><code>cudaEventDisableTiming</code>表示事件不用于计时，可以减少系统不必要的开支也能提升<code>cudaStreamWaitEvent</code>和<code>cudaEventQuery</code>的效率，<code>cudaEventInterprocess</code>表明可能被用于进程之间的事件</p>
<h2 id="并发内核执行"><a href="#并发内核执行" class="headerlink" title="并发内核执行"></a>并发内核执行</h2><h3 id="非空流中的并发内核"><a href="#非空流中的并发内核" class="headerlink" title="非空流中的并发内核"></a>非空流中的并发内核</h3><p>我们的核函数是：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel_1</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">double</span> sum=<span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N;i++)</span><br><span class="line">        sum=sum+<span class="built_in">tan</span>(<span class="number">0.1</span>)*<span class="built_in">tan</span>(<span class="number">0.1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel_2</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">double</span> sum=<span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N;i++)</span><br><span class="line">        sum=sum+<span class="built_in">tan</span>(<span class="number">0.1</span>)*<span class="built_in">tan</span>(<span class="number">0.1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel_3</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">double</span> sum=<span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N;i++)</span><br><span class="line">        sum=sum+<span class="built_in">tan</span>(<span class="number">0.1</span>)*<span class="built_in">tan</span>(<span class="number">0.1</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel_4</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">double</span> sum=<span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N;i++)</span><br><span class="line">        sum=sum+<span class="built_in">tan</span>(<span class="number">0.1</span>)*<span class="built_in">tan</span>(<span class="number">0.1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>四个核函数，N是100，tan计算在GPU中应该有优化过的高速版本，但是就算优化，这个也是相对耗时的，足够我们进行观察了。</p>
<p>我们本章主要关注主机代码，下面是创建流的代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cudaStream_t *stream=(cudaStream_t*)<span class="built_in">malloc</span>(n_stream*<span class="built_in">sizeof</span>(cudaStream_t));</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n_stream;i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">cudaStreamCreate</span>(&amp;stream[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先声明一个流的头结构，是malloc的注意后面要free掉</p>
<p>然后为每个流的头结构分配资源，也就是Create的过程，这样我们就有n_stream个流可以使用了，接着，我们添加核函数到流，并观察运行效果</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">1</span>)</span></span>;</span><br><span class="line">cudaEvent_t start,stop;</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line"><span class="built_in">cudaEventRecord</span>(start);</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n_stream;i++)</span><br><span class="line">&#123;</span><br><span class="line">    kernel_1&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    kernel_2&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    kernel_3&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    kernel_4&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cudaEventRecord</span>(stop);</span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaEventSynchronize</span>(stop));</span><br><span class="line"><span class="type">float</span> elapsed_time;</span><br><span class="line"><span class="built_in">cudaEventElapsedTime</span>(&amp;elapsed_time,start,stop);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;elapsed time:%f ms\n&quot;</span>,elapsed_time);</span><br></pre></td></tr></table></figure>
<p>这不是完整的代码，这个循环是将每个核函数都放入不同的流之中，也就是假设我们有10个流，那么这10个流中每个流都要按照上面的顺序执行这4个核函数。<br>注意如果没有</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaEventSynchronize</span>(stop)</span><br></pre></td></tr></table></figure>
<p>nvvp将会无法运行，因为所有这些都是异步操作，不会等到操作完再返回，而是启动后自动把控制权返回主机，如果没有一个阻塞指令，主机进程就会执行完毕推出，这样就跟设备失联了，nvvp也会相应的报错。</p>
<h3 id="使用OpenMP的调度操作"><a href="#使用OpenMP的调度操作" class="headerlink" title="使用OpenMP的调度操作"></a>使用OpenMP的调度操作</h3><p>OpenMP是一种非常好用的并行工具，比pthread更加好用，但是没有pthread那么灵活，这里我们不光要让核函数或者设备操作用多个流处理，同时也让主机在多线程下工作，我们尝试使用每个线程来操作一个流：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">omp_set_num_thread</span>(n_stream);</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> omp parallel</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> i=<span class="built_in">omp_get_thread_num</span>();</span><br><span class="line">        kernel_1&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">        kernel_2&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">        kernel_3&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">        kernel_4&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>解释下代码</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">omp_set_num_thread</span>(n_stream);</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> omp parallel</span></span><br></pre></td></tr></table></figure>
<p>调用OpenMP的API创建n_stream个线程，然后宏指令告诉编译器下面大括号中的部分就是每个线程都要执行的部分，有点类似于核函数，或者叫做并行单元。</p>
<h2 id="重叠内核执行和数据传输"><a href="#重叠内核执行和数据传输" class="headerlink" title="重叠内核执行和数据传输"></a>重叠内核执行和数据传输</h2><h3 id="使用深度优先调度重叠"><a href="#使用深度优先调度重叠" class="headerlink" title="使用深度优先调度重叠"></a>使用深度优先调度重叠</h3><p>向量加法的内核我们很熟悉了</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sumArraysGPU</span><span class="params">(<span class="type">float</span>*a,<span class="type">float</span>*b,<span class="type">float</span>*res,<span class="type">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> idx=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span>(idx &lt; N)</span><br><span class="line">    <span class="comment">//for delay</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;N_REPEAT;j++)</span><br><span class="line">            res[idx]=a[idx]+b[idx];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们这一章的重点都不是在核函数上，所以，我们使用这种非常简单的内核函数。但是不同的是，我们使用N_REPEAT进行多次冗余计算，原因是为了延长线程的执行时间，方便nvvp捕捉运行数据。</p>
<p>向量加法的过程是：</p>
<ol>
<li>两个输入向量从主机传入内核</li>
<li>内核运算，计算加法结果</li>
<li>将结果（一个向量）从设备回传到主机</li>
</ol>
<p>由于这个问题就是一个一步问题，我们没办法让内核和数据传输重叠，因为内核需要全部的数据，但是，我们如果思考一下，向量加法之所以能够并发执行，因为每一位都互不干扰，那么我们可以把向量分块，然后每一个块都是一个上面的过程，并且A块中的数据只用于A块的内核，而跟B，C，D内核没有关系，于是我们来把整个过程分成 N_SEGMENT 份，也就是 N_SEGMENT 个流分别执行，在主机代码中流的使用如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cudaStream_t stream[N_SEGMENT];</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N_SEGMENT;i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaStreamCreate</span>(&amp;stream[i]));</span><br><span class="line">&#125;</span><br><span class="line">cudaEvent_t start,stop;</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line"><span class="built_in">cudaEventRecord</span>(start,<span class="number">0</span>);</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N_SEGMENT;i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> ioffset=i*iElem;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(&amp;a_d[ioffset],&amp;a_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(&amp;b_d[ioffset],&amp;b_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]));</span><br><span class="line">    sumArraysGPU&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;(&amp;a_d[ioffset],&amp;b_d[ioffset],&amp;res_d[ioffset],iElem);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(&amp;res_from_gpu_h[ioffset],&amp;res_d[ioffset],nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i]));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//timer</span></span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>));</span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaEventSynchronize</span>(stop));</span><br></pre></td></tr></table></figure>
<p>其中和前面唯一有区别的就是</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N_SEGMENT;i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> ioffset=i*iElem;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(&amp;a_d[ioffset],&amp;a_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(&amp;b_d[ioffset],&amp;b_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]));</span><br><span class="line">    sumArraysGPU&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;(&amp;a_d[ioffset],&amp;b_d[ioffset],&amp;res_d[ioffset],iElem);</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(&amp;res_from_gpu_h[ioffset],&amp;res_d[ioffset],nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i]));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>数据传输使用异步方式，注意异步处理的数据要声明称为固定内存，不能是分页的，如果是分页的可能会出现未知错误。</p>
<h3 id="使用广度优先调度重叠"><a href="#使用广度优先调度重叠" class="headerlink" title="使用广度优先调度重叠"></a>使用广度优先调度重叠</h3><p>同样的，我们看完深度优先之后看一下广度优先<br>代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N_SEGMENT;i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> ioffset=i*iElem;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(&amp;a_d[ioffset],&amp;a_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(&amp;b_d[ioffset],&amp;b_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N_SEGMENT;i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> ioffset=i*iElem;</span><br><span class="line">    sumArraysGPU&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;(&amp;a_d[ioffset],&amp;b_d[ioffset],&amp;res_d[ioffset],iElem);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N_SEGMENT;i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> ioffset=i*iElem;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(&amp;res_from_gpu_h[ioffset],&amp;res_d[ioffset],nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i]));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="矩阵乘法实例"><a href="#矩阵乘法实例" class="headerlink" title="矩阵乘法实例"></a>矩阵乘法实例</h1><p>我们再实现一个稍微复杂一些的例子，就是两个矩阵的乘法，设输入矩阵为 A 和 B ，要得到 C=A×B 。实现思路是每个线程计算 C 的一个元素值 Ci,j ，对于矩阵运算，应该选用grid和block为2-D的。首先定义矩阵的结构体：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 矩阵类型，行优先，M(row, col) = *(M.elements + row * M.width + col)</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Matrix</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="type">int</span> width;</span><br><span class="line">    <span class="type">int</span> height;</span><br><span class="line">    <span class="type">float</span> *elements;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p><img src="/img/image-20220910174756825.png" alt="image-20220910174756825"></p>
<p>然后实现矩阵乘法的核函数，这里我们定义了两个辅助的<code>__device__</code>函数分别用于获取矩阵的元素值和为矩阵元素赋值，具体代码如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取矩阵A的(row, col)元素</span></span><br><span class="line">__device__ <span class="type">float</span> <span class="title function_">getElement</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col)</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="keyword">return</span> A-&gt;elements[row * A-&gt;width + col];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 为矩阵A的(row, col)元素赋值</span></span><br><span class="line">__device__ <span class="type">void</span> <span class="title function_">setElement</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col, <span class="type">float</span> value)</span></span><br><span class="line">&#123;</span><br><span class="line">	A-&gt;elements[row * A-&gt;width + col] = value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 矩阵相乘kernel，2-D，每个线程计算一个元素</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">matMulKernel</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="type">float</span> Cvalue = <span class="number">0.0</span>;</span><br><span class="line">	<span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">	<span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">	<span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; A-&gt;width; ++i)</span><br><span class="line">	&#123;</span><br><span class="line">		Cvalue += getElement(A, row, i) * getElement(B, i, col);</span><br><span class="line">	&#125;</span><br><span class="line">	setElement(C, row, col, Cvalue);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后我们采用统一内存编写矩阵相乘的测试实例。CUDA 6.0引入统一内存，使用一个托管内存来共同管理host和device中的内存，并且自动在host和device中进行数据传输。CUDA中使用cudaMallocManaged函数分配托管内存：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t <span class="title function_">cudaMallocManaged</span><span class="params">(<span class="type">void</span> **devPtr, <span class="type">size_t</span> size, <span class="type">unsigned</span> <span class="type">int</span> flag=<span class="number">0</span>)</span>;</span><br></pre></td></tr></table></figure>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> width = <span class="number">1</span> &lt;&lt; <span class="number">10</span>;</span><br><span class="line">    <span class="type">int</span> height = <span class="number">1</span> &lt;&lt; <span class="number">10</span>;</span><br><span class="line">    Matrix *A, *B, *C;</span><br><span class="line">    <span class="comment">// 申请托管内存</span></span><br><span class="line">    cudaMallocManaged((<span class="type">void</span>**)&amp;A, <span class="keyword">sizeof</span>(Matrix));</span><br><span class="line">    cudaMallocManaged((<span class="type">void</span>**)&amp;B, <span class="keyword">sizeof</span>(Matrix));</span><br><span class="line">    cudaMallocManaged((<span class="type">void</span>**)&amp;C, <span class="keyword">sizeof</span>(Matrix));</span><br><span class="line">    <span class="type">int</span> nBytes = width * height * <span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    cudaMallocManaged((<span class="type">void</span>**)&amp;A-&gt;elements, nBytes);</span><br><span class="line">    cudaMallocManaged((<span class="type">void</span>**)&amp;B-&gt;elements, nBytes);</span><br><span class="line">    cudaMallocManaged((<span class="type">void</span>**)&amp;C-&gt;elements, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化数据</span></span><br><span class="line">    A-&gt;height = height;</span><br><span class="line">    A-&gt;width = width;</span><br><span class="line">    B-&gt;height = height;</span><br><span class="line">    B-&gt;width = width;</span><br><span class="line">    C-&gt;height = height;</span><br><span class="line">    C-&gt;width = width;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; width * height; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        A-&gt;elements[i] = <span class="number">1.0</span>;</span><br><span class="line">        B-&gt;elements[i] = <span class="number">2.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义kernel的执行配置</span></span><br><span class="line">    dim3 <span class="title function_">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span>;</span><br><span class="line">    dim3 <span class="title function_">gridSize</span><span class="params">((width + blockSize.x - <span class="number">1</span>) / blockSize.x, </span></span><br><span class="line"><span class="params">        (height + blockSize.y - <span class="number">1</span>) / blockSize.y)</span>;</span><br><span class="line">    <span class="comment">// 执行kernel</span></span><br><span class="line">    matMulKernel &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A, B, C);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 同步device 保证结果能正确访问</span></span><br><span class="line">    cudaDeviceSynchronize();</span><br><span class="line">    <span class="comment">// 检查执行结果</span></span><br><span class="line">    <span class="type">float</span> maxError = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; width * height; ++i)</span><br><span class="line">        maxError = fmax(maxError, <span class="built_in">fabs</span>(C-&gt;elements[i] - <span class="number">2</span> * width));</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;最大误差: &quot;</span> &lt;&lt; maxError &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里矩阵大小为，设计的线程的block大小为(32, 32)，那么grid大小为(32, 32)，最终测试结果如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">nvprof cuda9.exe</span><br><span class="line">==16304== NVPROF is profiling process 16304, command: cuda9.exe</span><br><span class="line">最大误差: 0</span><br><span class="line">==16304== Profiling application: cuda9.exe</span><br><span class="line">==16304== Profiling result:</span><br><span class="line">            Type  Time(%)      Time     Calls       Avg       Min       Max  Name</span><br><span class="line"> GPU activities:  100.00%  1.32752s         1  1.32752s  1.32752s  1.32752s  matMulKernel(Matrix*, Matrix*, Matrix*)</span><br><span class="line">      API calls:   83.11%  1.32762s         1  1.32762s  1.32762s  1.32762s  cudaDeviceSynchronize</span><br><span class="line">                   13.99%  223.40ms         6  37.233ms  37.341us  217.66ms  cudaMallocManaged</span><br><span class="line">                    2.81%  44.810ms         1  44.810ms  44.810ms  44.810ms  cudaLaunch</span><br><span class="line">                    0.08%  1.3300ms        94  14.149us       0ns  884.64us  cuDeviceGetAttribute</span><br><span class="line">                    0.01%  199.03us         1  199.03us  199.03us  199.03us  cuDeviceGetName</span><br><span class="line">                    0.00%  10.009us         1  10.009us  10.009us  10.009us  cuDeviceTotalMem</span><br><span class="line">                    0.00%  6.5440us         1  6.5440us  6.5440us  6.5440us  cudaConfigureCall</span><br><span class="line">                    0.00%  3.0800us         3  1.0260us     385ns  1.5400us  cudaSetupArgument</span><br><span class="line">                    0.00%  2.6940us         3     898ns     385ns  1.5390us  cuDeviceGetCount</span><br><span class="line">                    0.00%  1.9250us         2     962ns     385ns  1.5400us  cuDeviceGet</span><br><span class="line"></span><br><span class="line">==16304== Unified Memory profiling result:</span><br><span class="line">Device &quot;GeForce GT 730 (0)&quot;</span><br><span class="line">   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name</span><br><span class="line">    2051  4.0000KB  4.0000KB  4.0000KB  8.011719MB  21.20721ms  Host To Device</span><br><span class="line">     270  45.570KB  4.0000KB  1.0000MB  12.01563MB  7.032508ms  Device To Host</span><br></pre></td></tr></table></figure>
<p>当然，这不是最高效的实现，后面可以继续优化…</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/C/" rel="tag"># C++</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/08/18/cpp%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/" rel="prev" title="C++并发编程">
      <i class="fa fa-chevron-left"></i> C++并发编程
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/09/27/%E4%B9%B0%E6%88%BF%E8%A6%81%E7%82%B9/" rel="next" title="买房要点">
      买房要点 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E7%AE%A1%E7%90%86"><span class="nav-number">1.1.</span> <span class="nav-text">线程管理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%E6%A6%82%E8%BF%B0"><span class="nav-number">1.2.</span> <span class="nav-text">核函数概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BC%96%E5%86%99%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-number">1.3.</span> <span class="nav-text">编写核函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-number">1.4.</span> <span class="nav-text">验证核函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BC%96%E7%A8%8B%E6%8E%A5%E5%8F%A3"><span class="nav-number">2.</span> <span class="nav-text">编程接口</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8NVCC%E7%BC%96%E8%AF%91CUDA%E7%A8%8B%E5%BA%8F"><span class="nav-number">2.1.</span> <span class="nav-text">使用NVCC编译CUDA程序</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%96%E8%AF%91%E6%B5%81%E7%A8%8B"><span class="nav-number">2.1.1.</span> <span class="nav-text">编译流程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A6%BB%E7%BA%BF%E7%BC%96%E8%AF%91"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">离线编译</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9C%A8%E7%BA%BF%E7%BC%96%E8%AF%91-JIT-Compilation"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">在线编译(JIT Compilation)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%BB%A3%E7%A0%81%E7%9A%84%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="nav-number">2.1.2.</span> <span class="nav-text">二进制代码的兼容性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PTX%E4%BB%A3%E7%A0%81%E7%9A%84%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="nav-number">2.1.3.</span> <span class="nav-text">PTX代码的兼容性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="nav-number">2.1.4.</span> <span class="nav-text">应用程序兼容性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-C-%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="nav-number">2.1.5.</span> <span class="nav-text">C&#x2F;C++兼容性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#32-64%E4%BD%8D%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="nav-number">2.1.6.</span> <span class="nav-text">32&#x2F;64位兼容性</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA-C-%E8%BF%90%E8%A1%8C%E5%BA%93"><span class="nav-number">2.2.</span> <span class="nav-text">CUDA C 运行库</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">2.2.1.</span> <span class="nav-text">初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%BE%E5%A4%87%E5%86%85%E5%AD%98"><span class="nav-number">2.2.2.</span> <span class="nav-text">设备内存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98"><span class="nav-number">2.2.3.</span> <span class="nav-text">共享内存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%94%81%E9%A1%B5%E5%86%85%E5%AD%98-Page-Locked-Host-Memory-Pinned-Memory"><span class="nav-number">2.2.4.</span> <span class="nav-text">锁页内存(Page-Locked Host Memory&#x2F;Pinned Memory)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Portable-Memory"><span class="nav-number">2.2.4.1.</span> <span class="nav-text">Portable Memory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%88%E5%B9%B6%E5%86%99%E5%86%85%E5%AD%98-Write-Combining-Memory"><span class="nav-number">2.2.4.2.</span> <span class="nav-text">合并写内存(Write-Combining Memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E6%98%A0%E5%B0%84-Mapped-Memory"><span class="nav-number">2.2.4.3.</span> <span class="nav-text">内存映射(Mapped Memory)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%82%E6%AD%A5%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C"><span class="nav-number">2.2.5.</span> <span class="nav-text">异步并行执行</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E6%9C%BA%E7%AB%AF-%E8%AE%BE%E5%A4%87%E7%AB%AF%E5%B9%B6%E8%A1%8C"><span class="nav-number">2.2.5.1.</span> <span class="nav-text">主机端&#x2F;设备端并行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E6%A0%B8%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C"><span class="nav-number">2.2.5.2.</span> <span class="nav-text">内核并行执行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93%E5%92%8C%E5%86%85%E6%A0%B8%E6%89%A7%E8%A1%8C%E5%B9%B6%E8%A1%8C-%E9%9C%80%E8%A6%81%E4%BD%BF%E7%94%A8%E9%94%81%E9%A1%B5%E5%86%85%E5%AD%98"><span class="nav-number">2.2.5.3.</span> <span class="nav-text">数据传输和内核执行并行(需要使用锁页内存)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E4%BC%A0%E8%BE%93-%E9%9C%80%E8%A6%81%E4%BD%BF%E7%94%A8%E9%94%81%E9%A1%B5%E5%86%85%E5%AD%98"><span class="nav-number">2.2.5.4.</span> <span class="nav-text">数据并行传输(需要使用锁页内存)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B5%81-streams"><span class="nav-number">2.2.5.5.</span> <span class="nav-text">流(streams)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B5%81%E7%9A%84%E5%88%9B%E5%BB%BA%E5%92%8C%E9%94%80%E6%AF%81"><span class="nav-number">2.2.5.5.1.</span> <span class="nav-text">流的创建和销毁</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%BB%98%E8%AE%A4%E6%B5%81-Default-Stream"><span class="nav-number">2.2.5.5.2.</span> <span class="nav-text">默认流(Default Stream)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%98%BE%E5%BC%8F%E5%90%8C%E6%AD%A5-Explicit-Synchronization"><span class="nav-number">2.2.5.5.3.</span> <span class="nav-text">显式同步(Explicit Synchronization)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9A%90%E5%BC%8F%E5%90%8C%E6%AD%A5-Implicit-Synchronization"><span class="nav-number">2.2.5.5.4.</span> <span class="nav-text">隐式同步(Implicit Synchronization)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%93%8D%E4%BD%9C%E9%87%8D%E5%8F%A0-Overlapping-Behavior"><span class="nav-number">2.2.5.5.5.</span> <span class="nav-text">操作重叠(Overlapping Behavior)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0-Callbacks"><span class="nav-number">2.2.5.5.6.</span> <span class="nav-text">回调函数(Callbacks)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B5%81%E7%9A%84%E4%BC%98%E5%85%88%E7%BA%A7-Stream-Priorities"><span class="nav-number">2.2.5.5.7.</span> <span class="nav-text">流的优先级(Stream Priorities)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%8B%E4%BB%B6-Event"><span class="nav-number">2.2.5.6.</span> <span class="nav-text">事件(Event)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BA%8B%E4%BB%B6%E7%9A%84%E5%88%9B%E5%BB%BA%E5%92%8C%E9%94%80%E6%AF%81"><span class="nav-number">2.2.5.6.1.</span> <span class="nav-text">事件的创建和销毁</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%97%B6%E9%97%B4"><span class="nav-number">2.2.5.6.2.</span> <span class="nav-text">计算时间</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-6-%E5%A4%9A%E8%AE%BE%E5%A4%87%E7%B3%BB%E7%BB%9F-Multi-Device-System"><span class="nav-number">2.2.6.</span> <span class="nav-text">3.2.6 多设备系统(Multi-Device System)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E5%A4%87%E6%9E%9A%E4%B8%BE-Device-Enumeration"><span class="nav-number">2.2.6.1.</span> <span class="nav-text">设备枚举(Device Enumeration)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-6-2-%E8%AE%BE%E5%A4%87%E9%80%89%E6%8B%A9-Device-Selection"><span class="nav-number">2.2.6.2.</span> <span class="nav-text">3.2.6.2 设备选择(Device Selection)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E8%AE%BE%E5%A4%87%E4%B8%8B-%E6%B5%81%E5%92%8C%E4%BA%8B%E4%BB%B6%E7%9A%84%E6%89%A7%E8%A1%8C%E6%83%85%E5%86%B5"><span class="nav-number">2.2.6.3.</span> <span class="nav-text">(多设备下)流和事件的执行情况</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-6-4-%E8%AE%BE%E5%A4%87%E9%97%B4-%E5%AF%B9%E7%AD%89%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE-Peer-to-Peer-Memory-Access"><span class="nav-number">2.2.6.4.</span> <span class="nav-text">3.2.6.4 (设备间)对等内存访问(Peer-to-Peer Memory Access)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%BE%E5%A4%87%E9%97%B4-%E5%AF%B9%E7%AD%89%E5%86%85%E5%AD%98%E6%8B%B7%E8%B4%9D-Peer-to-Peer-Memory-Copy"><span class="nav-number">2.2.6.5.</span> <span class="nav-text">(设备间)对等内存拷贝(Peer-to-Peer Memory Copy)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%9F%E4%B8%80%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4-Unified-Virtual-Address-Space"><span class="nav-number">2.2.7.</span> <span class="nav-text">统一虚拟地址空间(Unified Virtual Address Space)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E8%AE%AF-Interprocess-Communication"><span class="nav-number">2.2.8.</span> <span class="nav-text">进程间通讯(Interprocess Communication)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%94%99%E8%AF%AF%E6%A3%80%E6%9F%A5-Error-Checking"><span class="nav-number">2.2.9.</span> <span class="nav-text">错误检查(Error Checking)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%BC%8F-Compute-Mode"><span class="nav-number">2.3.</span> <span class="nav-text">计算模式(Compute Mode)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96"><span class="nav-number">3.</span> <span class="nav-text">性能优化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%A6%82%E8%BF%B0"><span class="nav-number">3.1.</span> <span class="nav-text">性能优化概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E5%8C%96%E5%88%A9%E7%94%A8%E7%8E%87-Maximize-Utilization"><span class="nav-number">3.2.</span> <span class="nav-text">最大化利用率(Maximize Utilization)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%94%E7%94%A8%E7%BA%A7%E5%88%AB%E5%B9%B6%E8%A1%8C-Application-Level"><span class="nav-number">3.2.1.</span> <span class="nav-text">应用级别并行(Application Level)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%BE%E5%A4%87%E7%BA%A7%E5%88%AB%E5%B9%B6%E8%A1%8C-Device-Level"><span class="nav-number">3.2.2.</span> <span class="nav-text">设备级别并行(Device Level)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E5%99%A8%E7%BA%A7%E5%88%AB%E5%B9%B6%E8%A1%8C-Multiprocessor-Level"><span class="nav-number">3.2.3.</span> <span class="nav-text">处理器级别并行(Multiprocessor Level)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E5%8C%96%E5%86%85%E5%AD%98%E5%90%9E%E5%90%90-Maximize-Memory-Throughput"><span class="nav-number">3.3.</span> <span class="nav-text">最大化内存吞吐(Maximize Memory Throughput)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E6%9C%BA%E7%AB%AF%E5%92%8C%E8%AE%BE%E5%A4%87%E7%AB%AF%E9%97%B4%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93"><span class="nav-number">3.3.1.</span> <span class="nav-text">主机端和设备端间数据传输</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%BE%E5%A4%87%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE"><span class="nav-number">3.3.2.</span> <span class="nav-text">设备内存访问</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98-global-memory"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">全局内存(global memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%B9%E9%BD%90-Alignment"><span class="nav-number">3.3.2.2.</span> <span class="nav-text">对齐(Alignment)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%AC%E5%9C%B0%E5%86%85%E5%AD%98-local-memory"><span class="nav-number">3.3.2.3.</span> <span class="nav-text">本地内存(local memory)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98-shared-memory"><span class="nav-number">3.3.2.4.</span> <span class="nav-text">共享内存(shared memory)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E5%8C%96%E6%8C%87%E4%BB%A4%E5%90%9E%E5%90%90-Maximize-Instruction-Throughput"><span class="nav-number">3.4.</span> <span class="nav-text">最大化指令吞吐(Maximize Instruction Throughput)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%95%B0%E6%8C%87%E4%BB%A4-Arithmetic-Instructions"><span class="nav-number">3.4.1.</span> <span class="nav-text">算数指令(Arithmetic Instructions)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A7%E5%88%B6%E6%B5%81%E6%8C%87%E4%BB%A4-Control-Flow-Instructions"><span class="nav-number">3.4.2.</span> <span class="nav-text">控制流指令(Control Flow Instructions)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%8C%E6%AD%A5%E6%8C%87%E4%BB%A4-Synchronization-Instruction"><span class="nav-number">3.4.3.</span> <span class="nav-text">同步指令(Synchronization Instruction)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%99%E6%A0%B8%E5%87%BD%E6%95%B0%E8%AE%A1%E6%97%B6"><span class="nav-number">4.</span> <span class="nav-text">给核函数计时</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8nvprof%E8%AE%A1%E6%97%B6"><span class="nav-number">4.1.</span> <span class="nav-text">用nvprof计时</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B"><span class="nav-number">5.</span> <span class="nav-text">组织并行线程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%9D%97%E5%92%8C%E7%BA%BF%E7%A8%8B%E5%BB%BA%E7%AB%8B%E7%9F%A9%E9%98%B5%E7%B4%A2%E5%BC%95"><span class="nav-number">5.1.</span> <span class="nav-text">使用块和线程建立矩阵索引</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E7%BB%B4%E7%9F%A9%E9%98%B5%E5%8A%A0%E6%B3%95"><span class="nav-number">5.2.</span> <span class="nav-text">二维矩阵加法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E7%BB%B4%E7%BD%91%E6%A0%BC%E5%92%8C%E4%BA%8C%E7%BB%B4%E5%9D%97"><span class="nav-number">5.3.</span> <span class="nav-text">二维网格和二维块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E7%BB%B4%E7%BD%91%E6%A0%BC%E5%92%8C%E4%B8%80%E7%BB%B4%E5%9D%97"><span class="nav-number">5.4.</span> <span class="nav-text">一维网格和一维块</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GPU%E8%AE%BE%E5%A4%87%E4%BF%A1%E6%81%AF"><span class="nav-number">6.</span> <span class="nav-text">GPU设备信息</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CUDA%E6%89%A7%E8%A1%8C%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0"><span class="nav-number">7.</span> <span class="nav-text">CUDA执行模型概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GPU%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0"><span class="nav-number">7.1.</span> <span class="nav-text">GPU架构概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E7%A8%8B%E6%9D%9F"><span class="nav-number">7.1.1.</span> <span class="nav-text">线程束</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SIMD-vs-SIMT"><span class="nav-number">7.1.2.</span> <span class="nav-text">SIMD vs SIMT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#32"><span class="nav-number">7.1.3.</span> <span class="nav-text">32</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA%E7%BC%96%E7%A8%8B%E7%9A%84%E7%BB%84%E4%BB%B6%E4%B8%8E%E9%80%BB%E8%BE%91"><span class="nav-number">7.2.</span> <span class="nav-text">CUDA编程的组件与逻辑</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%89%A7%E8%A1%8C%E7%9A%84%E6%9C%AC%E8%B4%A8"><span class="nav-number">7.3.</span> <span class="nav-text">理解线程束执行的本质</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BB%B6%E8%BF%9F%E9%9A%90%E8%97%8F"><span class="nav-number">7.4.</span> <span class="nav-text">延迟隐藏</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%8C%E6%AD%A5"><span class="nav-number">7.5.</span> <span class="nav-text">同步</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E6%80%A7%E8%A1%A8%E7%8E%B0"><span class="nav-number">7.6.</span> <span class="nav-text">并行性表现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%81%BF%E5%85%8D%E5%88%86%E6%94%AF%E5%88%86%E5%8C%96"><span class="nav-number">7.7.</span> <span class="nav-text">避免分支分化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E8%A7%84%E7%BA%A6%E9%97%AE%E9%A2%98"><span class="nav-number">7.7.1.</span> <span class="nav-text">并行规约问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E8%A7%84%E7%BA%A6%E4%B8%AD%E7%9A%84%E5%88%86%E5%8C%96"><span class="nav-number">7.7.2.</span> <span class="nav-text">并行规约中的分化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%95%E5%BC%80%E5%BE%AA%E7%8E%AF"><span class="nav-number">7.8.</span> <span class="nav-text">展开循环</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#cuda%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="nav-number">8.</span> <span class="nav-text">cuda内存模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%84%E5%AD%98%E5%99%A8"><span class="nav-number">8.1.</span> <span class="nav-text">寄存器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%AC%E5%9C%B0%E5%86%85%E5%AD%98"><span class="nav-number">8.2.</span> <span class="nav-text">本地内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98-1"><span class="nav-number">8.3.</span> <span class="nav-text">共享内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E9%87%8F%E5%86%85%E5%AD%98"><span class="nav-number">8.4.</span> <span class="nav-text">常量内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%B9%E7%90%86%E5%86%85%E5%AD%98"><span class="nav-number">8.5.</span> <span class="nav-text">纹理内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98"><span class="nav-number">8.6.</span> <span class="nav-text">全局内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GPU%E7%BC%93%E5%AD%98"><span class="nav-number">8.7.</span> <span class="nav-text">GPU缓存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9D%99%E6%80%81%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98"><span class="nav-number">8.8.</span> <span class="nav-text">静态全局内存</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="nav-number">9.</span> <span class="nav-text">内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%92%8C%E9%87%8A%E6%94%BE"><span class="nav-number">9.1.</span> <span class="nav-text">内存分配和释放</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E4%BC%A0%E8%BE%93"><span class="nav-number">9.2.</span> <span class="nav-text">内存传输</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BA%E5%AE%9A%E5%86%85%E5%AD%98"><span class="nav-number">9.3.</span> <span class="nav-text">固定内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%B6%E6%8B%B7%E8%B4%9D%E5%86%85%E5%AD%98"><span class="nav-number">9.4.</span> <span class="nav-text">零拷贝内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%9F%E4%B8%80%E8%99%9A%E6%8B%9F%E5%AF%BB%E5%9D%80"><span class="nav-number">9.5.</span> <span class="nav-text">统一虚拟寻址</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F"><span class="nav-number">10.</span> <span class="nav-text">内存访问模式</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E9%BD%90%E4%B8%8E%E5%90%88%E5%B9%B6%E8%AE%BF%E9%97%AE"><span class="nav-number">10.1.</span> <span class="nav-text">对齐与合并访问</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98%E8%AF%BB%E5%8F%96"><span class="nav-number">10.2.</span> <span class="nav-text">全局内存读取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%93%E5%AD%98%E5%8A%A0%E8%BD%BD"><span class="nav-number">10.2.1.</span> <span class="nav-text">缓存加载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B2%A1%E6%9C%89%E7%BC%93%E5%AD%98%E7%9A%84%E5%8A%A0%E8%BD%BD"><span class="nav-number">10.2.2.</span> <span class="nav-text">没有缓存的加载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9D%9E%E5%AF%B9%E9%BD%90%E8%AF%BB%E5%8F%96%E7%A4%BA%E4%BE%8B"><span class="nav-number">10.2.3.</span> <span class="nav-text">非对齐读取示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AA%E8%AF%BB%E7%BC%93%E5%AD%98"><span class="nav-number">10.2.4.</span> <span class="nav-text">只读缓存</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0%E5%8F%AF%E8%BE%BE%E5%88%B0%E7%9A%84%E5%B8%A6%E5%AE%BD"><span class="nav-number">11.</span> <span class="nav-text">核函数可达到的带宽</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E5%B8%A6%E5%AE%BD"><span class="nav-number">11.1.</span> <span class="nav-text">内存带宽</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E8%BD%AC%E7%BD%AE%E9%97%AE%E9%A2%98"><span class="nav-number">11.2.</span> <span class="nav-text">矩阵转置问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E8%BD%AC%E7%BD%AE%E6%A0%B8%E5%87%BD%E6%95%B0%E8%AE%BE%E7%BD%AE%E4%B8%8A%E9%99%90%E5%92%8C%E4%B8%8B%E9%99%90"><span class="nav-number">11.2.1.</span> <span class="nav-text">为转置核函数设置上限和下限</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%95%E5%BC%80%E8%BD%AC%E7%BD%AE%EF%BC%9A%E8%AF%BB%E5%8F%96%E8%A1%8C%E4%B8%8E%E8%AF%BB%E5%8F%96%E5%88%97"><span class="nav-number">11.2.2.</span> <span class="nav-text">展开转置：读取行与读取列</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%9A%84%E5%90%91%E9%87%8F%E5%8A%A0%E6%B3%95"><span class="nav-number">12.</span> <span class="nav-text">使用统一内存的向量加法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%9F%A9%E9%98%B5%E5%8A%A0%E6%B3%95"><span class="nav-number">12.1.</span> <span class="nav-text">统一内存矩阵加法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E5%92%8C%E5%B8%B8%E9%87%8F%E5%86%85%E5%AD%98"><span class="nav-number">13.</span> <span class="nav-text">共享内存和常量内存</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98-2"><span class="nav-number">13.1.</span> <span class="nav-text">共享内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D"><span class="nav-number">13.2.</span> <span class="nav-text">共享内存分配</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E5%AD%98%E5%82%A8%E4%BD%93%E5%92%8C%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F"><span class="nav-number">13.3.</span> <span class="nav-text">共享内存存储体和访问模式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E5%AD%98%E5%82%A8%E4%BD%93"><span class="nav-number">13.3.1.</span> <span class="nav-text">内存存储体</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%98%E5%82%A8%E4%BD%93%E5%86%B2%E7%AA%81"><span class="nav-number">13.3.2.</span> <span class="nav-text">存储体冲突</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F"><span class="nav-number">13.3.3.</span> <span class="nav-text">访问模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E5%A1%AB%E5%85%85"><span class="nav-number">13.3.4.</span> <span class="nav-text">内存填充</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE"><span class="nav-number">13.3.5.</span> <span class="nav-text">访问模式配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98"><span class="nav-number">13.4.</span> <span class="nav-text">配置共享内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%8C%E6%AD%A5-1"><span class="nav-number">13.5.</span> <span class="nav-text">同步</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%B1%E6%8E%92%E5%BA%8F%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="nav-number">13.5.1.</span> <span class="nav-text">弱排序内存模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%98%BE%E7%A4%BA%E9%9A%9C%E7%A2%8D"><span class="nav-number">13.5.2.</span> <span class="nav-text">显示障碍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E6%A0%85%E6%A0%8F"><span class="nav-number">13.5.3.</span> <span class="nav-text">内存栅栏</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Volatile%E4%BF%AE%E9%A5%B0%E7%AC%A6"><span class="nav-number">13.5.4.</span> <span class="nav-text">Volatile修饰符</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%87%8F%E5%B0%91%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE"><span class="nav-number">14.</span> <span class="nav-text">减少全局内存访问</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98%E7%9A%84%E5%B9%B6%E8%A1%8C%E5%BD%92%E7%BA%A6"><span class="nav-number">14.1.</span> <span class="nav-text">使用共享内存的并行归约</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%B1%95%E5%BC%80%E7%9A%84%E5%B9%B6%E8%A1%8C%E5%BD%92%E7%BA%A6"><span class="nav-number">14.2.</span> <span class="nav-text">使用展开的并行归约</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B5%81%E5%92%8C%E5%B9%B6%E5%8F%91"><span class="nav-number">15.</span> <span class="nav-text">流和并发</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%81%E5%92%8C%E4%BA%8B%E4%BB%B6%E6%A6%82%E8%BF%B0"><span class="nav-number">15.1.</span> <span class="nav-text">流和事件概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA%E6%B5%81"><span class="nav-number">15.2.</span> <span class="nav-text">CUDA流</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%81%E8%B0%83%E5%BA%A6"><span class="nav-number">15.3.</span> <span class="nav-text">流调度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%99%9A%E5%81%87%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="nav-number">15.3.1.</span> <span class="nav-text">虚假的依赖关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hyper-Q%E6%8A%80%E6%9C%AF"><span class="nav-number">15.3.2.</span> <span class="nav-text">Hyper-Q技术</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%81%E7%9A%84%E4%BC%98%E5%85%88%E7%BA%A7"><span class="nav-number">15.4.</span> <span class="nav-text">流的优先级</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA%E4%BA%8B%E4%BB%B6"><span class="nav-number">15.5.</span> <span class="nav-text">CUDA事件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E5%92%8C%E9%94%80%E6%AF%81"><span class="nav-number">15.5.1.</span> <span class="nav-text">创建和销毁</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%B0%E5%BD%95%E4%BA%8B%E4%BB%B6%E5%92%8C%E8%AE%A1%E7%AE%97%E8%BF%90%E8%A1%8C%E6%97%B6%E9%97%B4"><span class="nav-number">15.5.2.</span> <span class="nav-text">记录事件和计算运行时间</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%81%E5%90%8C%E6%AD%A5"><span class="nav-number">15.6.</span> <span class="nav-text">流同步</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%98%BB%E5%A1%9E%E6%B5%81%E5%92%8C%E9%9D%9E%E9%98%BB%E5%A1%9E%E6%B5%81"><span class="nav-number">15.6.1.</span> <span class="nav-text">阻塞流和非阻塞流</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%90%E5%BC%8F%E5%90%8C%E6%AD%A5"><span class="nav-number">15.6.2.</span> <span class="nav-text">隐式同步</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%98%BE%E5%BC%8F%E5%90%8C%E6%AD%A5"><span class="nav-number">15.6.3.</span> <span class="nav-text">显式同步</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%AF%E9%85%8D%E7%BD%AE%E4%BA%8B%E4%BB%B6"><span class="nav-number">15.6.4.</span> <span class="nav-text">可配置事件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%B6%E5%8F%91%E5%86%85%E6%A0%B8%E6%89%A7%E8%A1%8C"><span class="nav-number">15.7.</span> <span class="nav-text">并发内核执行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9D%9E%E7%A9%BA%E6%B5%81%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%8F%91%E5%86%85%E6%A0%B8"><span class="nav-number">15.7.1.</span> <span class="nav-text">非空流中的并发内核</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8OpenMP%E7%9A%84%E8%B0%83%E5%BA%A6%E6%93%8D%E4%BD%9C"><span class="nav-number">15.7.2.</span> <span class="nav-text">使用OpenMP的调度操作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%87%8D%E5%8F%A0%E5%86%85%E6%A0%B8%E6%89%A7%E8%A1%8C%E5%92%8C%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93"><span class="nav-number">15.8.</span> <span class="nav-text">重叠内核执行和数据传输</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E8%B0%83%E5%BA%A6%E9%87%8D%E5%8F%A0"><span class="nav-number">15.8.1.</span> <span class="nav-text">使用深度优先调度重叠</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E8%B0%83%E5%BA%A6%E9%87%8D%E5%8F%A0"><span class="nav-number">15.8.2.</span> <span class="nav-text">使用广度优先调度重叠</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E5%AE%9E%E4%BE%8B"><span class="nav-number">16.</span> <span class="nav-text">矩阵乘法实例</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Hao Yu</p>
  <div class="site-description" itemprop="description">Introduce something interesting and recode learning process, some articles are written by others, the original link has been given as much as possible, thanks to the original author</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">128</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yuhao0102" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yuhao0102" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yuhhpc0203@gmail.com" title="E-Mail → mailto:yuhhpc0203@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hao Yu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
