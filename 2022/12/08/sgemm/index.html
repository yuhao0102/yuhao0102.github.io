<!DOCTYPE html>
<html lang="zn-ch">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="在开始之前，我想提一下，这项工作的大部分都是从对cublas Kepler和Maxwell的sgemm实现的详细研究中得出的。我做了一些适度的改进，但大多数难题都由英伟达的优秀工程师和他们对硬件的专业知识解决了。本文档的目标是传播这些知识，供其他人在自己的代码中使用。我还想联系两篇关于sgemm主题的优秀论文：MAGMA原始论文（http:&#x2F;&#x2F;icl.cs.utk.edu&#x2F;projectsfile">
<meta property="og:type" content="article">
<meta property="og:title" content="SGEMM实施的完整演练">
<meta property="og:url" content="http://yoursite.com/2022/12/08/sgemm/index.html">
<meta property="og:site_name" content="Hao Yu&#39;s blog">
<meta property="og:description" content="在开始之前，我想提一下，这项工作的大部分都是从对cublas Kepler和Maxwell的sgemm实现的详细研究中得出的。我做了一些适度的改进，但大多数难题都由英伟达的优秀工程师和他们对硬件的专业知识解决了。本文档的目标是传播这些知识，供其他人在自己的代码中使用。我还想联系两篇关于sgemm主题的优秀论文：MAGMA原始论文（http:&#x2F;&#x2F;icl.cs.utk.edu&#x2F;projectsfile">
<meta property="og:locale" content="zn_CH">
<meta property="og:image" content="http://yoursite.com/img/image-20221208152310575.png">
<meta property="og:image" content="http://yoursite.com/img/image-20221208153216716.png">
<meta property="og:image" content="http://yoursite.com/img/image-20221208153237832.png">
<meta property="og:image" content="http://yoursite.com/img/image-20221208162657316.png">
<meta property="og:image" content="http://yoursite.com/img/image-20221208162949337.png">
<meta property="og:image" content="http://yoursite.com/img/image-20221208164843431.png">
<meta property="og:image" content="http://yoursite.com/img/image-20221208165042717.png">
<meta property="og:image" content="http://yoursite.com/img/image-20221208165147533.png">
<meta property="og:image" content="http://yoursite.com/img/image-20221208165203670.png">
<meta property="og:image" content="http://yoursite.com/img/image-20221208165218383.png">
<meta property="og:image" content="http://yoursite.com/img/image-20221208165238932.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910175222544.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910175237258.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910175717727.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910175947839.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910180155032.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910213658483.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910213713143.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910214848982.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910215005453.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910220109982.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910220136539.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910220741253.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910220755159.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910220841079.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910221016698.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910221600420.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910221941317.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910221952729.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910222351625.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910222431933.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910222443696.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910222453184.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910222504575.png">
<meta property="og:image" content="http://yoursite.com/img/image-20220910222717437.png">
<meta property="og:image" content="http://yoursite.com/img/v2-40ed6461ce3660156de9a0a29eb82799_b.jpg">
<meta property="og:image" content="http://yoursite.com/img/v2-c41d9b702a5f0154498996379506ae0b_b.jpg">
<meta property="og:image" content="http://yoursite.com/img/v2-e56a1cd7fb59cc20507f79deb774f383_b.jpg">
<meta property="og:image" content="http://yoursite.com/img/v2-d33f42ddbb0c3affc193011a00446b07_b.jpg">
<meta property="og:image" content="http://yoursite.com/img/v2-b186c0f6e54aaf63ee79545adf0d5f82_b.jpg">
<meta property="og:image" content="http://yoursite.com/img/v2-697f1e89317b4ee6336b7b4431e653d1_b.jpg">
<meta property="og:image" content="http://yoursite.com/img/v2-18ea022c73ac01b8903a511f031bf8a4_b.jpg">
<meta property="og:image" content="http://yoursite.com/img/v2-f28daba55db094e3aec589dd28b44d5e_b.jpg">
<meta property="og:image" content="http://yoursite.com/img/v2-767390400b8a5ec779a80f35c26a0fdb_b.jpg">
<meta property="og:image" content="http://yoursite.com/img/v2-ddacfb2cdaad74d0bbf818cb12910e34_b.jpg">
<meta property="og:image" content="http://yoursite.com/img/v2-5270e7dd93dfe37e3baf98f01e8e723d_b.jpg">
<meta property="og:image" content="http://yoursite.com/img/v2-f2712a98537424719bf96107b29eb52f_b.jpg">
<meta property="og:image" content="http://yoursite.com/img/v2-9081451191194b2134e1a0888fe14079_b.jpg">
<meta property="article:published_time" content="2022-12-08T06:55:00.000Z">
<meta property="article:modified_time" content="2023-01-19T02:14:36.000Z">
<meta property="article:author" content="Hao Yu">
<meta property="article:tag" content="积累">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/img/image-20221208152310575.png">

<link rel="canonical" href="http://yoursite.com/2022/12/08/sgemm/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zn-ch'
  };
</script>

  <title>SGEMM实施的完整演练 | Hao Yu's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hao Yu's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">The program monkey was eaten by the siege lion.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zn-ch">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/12/08/sgemm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Hao Yu">
      <meta itemprop="description" content="Introduce something interesting and recode learning process, some articles are written by others, the original link has been given as much as possible, thanks to the original author">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hao Yu's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          SGEMM实施的完整演练
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-12-08 14:55:00" itemprop="dateCreated datePublished" datetime="2022-12-08T14:55:00+08:00">2022-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-01-19 10:14:36" itemprop="dateModified" datetime="2023-01-19T10:14:36+08:00">2023-01-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>在开始之前，我想提一下，这项工作的大部分都是从对cublas Kepler和Maxwell的sgemm实现的详细研究中得出的。我做了一些适度的改进，但大多数难题都由英伟达的优秀工程师和他们对硬件的专业知识解决了。本文档的目标是传播这些知识，供其他人在自己的代码中使用。我还想联系两篇关于sgemm主题的优秀论文：MAGMA原始论文（<a href="http://icl.cs.utk.edu/projectsfiles/magma/pubs/fermi_gemm.pdf）和赖俊杰的Kepler">http://icl.cs.utk.edu/projectsfiles/magma/pubs/fermi_gemm.pdf）和赖俊杰的Kepler</a> sgemm论文（<a href="http://hal.inria.fr/docs/00/78/99/58/PDF/112_Lai.pdf）。本文档基本上是Junjie工作的扩展，但具有Maxwell架构和额外的汇编级优化。">http://hal.inria.fr/docs/00/78/99/58/PDF/112_Lai.pdf）。本文档基本上是Junjie工作的扩展，但具有Maxwell架构和额外的汇编级优化。</a></p>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>以sgemm为例，本文旨在描述如何最大化Maxwell架构及其他架构的计算能力。拥有数千个计算核心对你没有好处，除非你让它们得到数据。要做到这一点，您需要构建计算结构，以最大限度地重用通过各种内存层次结构提取的数据。在GPU上，这些是：设备内存到二级缓存，二级缓存到纹理缓存，纹理缓存到寄存器，寄存器到共享内存，共享内存到寄存器，从寄存器到指令操作数缓存（Maxwell的新功能），最后从寄存器返回到设备内存。这些数据路径中的每一条都有延迟，我们需要用指令和线程级并行性（ILP&amp;TLP）来隐藏这些延迟。此外，还可能存在bank和联合约束。所提出的sgemm代码能够克服所有这些约束，并在硬件理论错误的2%内运行。</p>
<p>本文档将介绍两种不同的布局：每个块64个线程和每个块256个线程。我将主要讨论64线程版本，因为映射更小更简单。256线程版本或多或少是相同的，只是放大了4倍。这两个版本分别针对小型或大型矩阵进行了优化。较小的64线程版本可以将矩阵拆分为4倍多的块，这在SM稀少的情况下非常有用，但代价是所需的设备内存带宽是256线程版本的两倍。在GM204硬件上，这个额外的带宽实际上超过了可用的带宽，因此只有当有更多的可用块来填充SM超过了成本时，您才想使用它（除非L2可以隐藏它）。虽然，如果您有足够的并行工作，使用流来填充SM是更好的方法。</p>
<p>在这两个版本中，我们将使用双缓冲8寄存器块来加载A和B中的每一个。双缓冲允许我们从共享内存中隐藏加载的大部分延迟：我们可以计算一个寄存器块，同时加载下一个寄存器。我们选择8个寄存器块，因为它与使用四矢量内存指令很好地对齐，并且因为我们可以将总寄存器预算保持在128以下。跨越128个寄存器的障碍将使我们的占用率从每个调度器的4个活动减少到64线程版本的3个，从256线程版本的4个减少到2个。64线程版本不太容易受到下降的影响，我实际上看到了一些矩阵大小的性能提高（减少了L2和纹理缓存稀释，每个SM的块更少），但256线程版本的工作性能稍好，每个调度程序多了1个扭曲，以覆盖延迟。我们在这两种实现中的性能都不会受到占用率下降的巨大影响，这说明了这段代码如何很好地隐藏ILP的延迟。</p>
<p>我们还将对共享内存进行双重缓冲，以便删除其中一个我们通常需要在主循环中进行的bar.syncs，而不是在存储下一批之前等待所有共享加载完成，我们只需开始写入一个新的共享区域，而其他线程可以从上一个区域读取数据。您将在下面看到，这将向主循环添加3个XOR，但这仍然比bar.syncs便宜。至于共享内存的大小，这是由每个线程块加载的内存宽度乘以主循环展开因子来定义的。我们有64个（或256个）线程，每个线程将计算8*8或64个C点。所有这些点将一起排列成正方形，因为我们从a和B均匀拉动。所以这个正方形的宽度就是总点数的平方根。对于我们的两个实现，我们计算：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">64  Threads: sqrt(64  * 8*8) = 64  units wide</span><br><span class="line">256 Threads: sqrt(256 * 8*8) = 128 units wide*</span><br></pre></td></tr></table></figure>
<p>我们的展开因子是我们一次从A和B读取、从共享存储/读取和计算的行数。它将在几个方面受到限制。我们希望能够通过尽可能多的计算工作来隐藏纹理负载的延迟。但是，我们不希望循环的大小超过指令缓存的大小。这样做会增加额外的指令获取延迟，我们需要隐藏这些延迟。在Maxwell上，我测得这个缓存为8KB。因此，这意味着我们不希望循环大小超过1024个8字节指令，其中每4个指令都是一个控制代码。所以768是有用指令的极限。此外，还有指令对齐的注意事项，因此您也希望安全地处于该值之下。简而言之，使用8的循环展开因子可以得到8 x 64=512 ffma指令加上循环所需的额外内存和整数算术指令（约40）。这使我们大大低于768。每个循环8行也与纹理内存负载的维度很好地对齐。最后，512个FFMA应该足以大部分隐藏200+时钟纹理加载延迟。</p>
<p>因此，我们现在知道了共享内存的总大小：（每个循环8行）x（块加载宽度）x（字大小）x（A的2个缓冲区）x（B的2个缓存区）。64个线程为8192字节，256个线程为16384字节。这种大小不会影响占用率，占用率由寄存器计数（我们将保持在128以下）决定。</p>
<p>下面是两种实现共享的基本内存布局。注意，我将X维度与来自A的载荷相等，并沿lda对齐，而Y维度与来自B的载荷相等并沿ldb对齐。这与x和y通常在空间上的定义方式相反，如下图所示。还要注意的是，A和C的图像被布置为转置。回想起来，我可能会把它改成B作为转置，并与A交换，但这就是我最初的计算方法。在下一节中，我将开始详细讨论64线程版本。</p>
<p><img src="/img/image-20221208152310575.png" alt="image-20221208152310575"></p>
<h1 id="64-Thread-Implementation"><a href="#64-Thread-Implementation" class="headerlink" title="64 Thread Implementation"></a>64 Thread Implementation</h1><h2 id="加载A和B，然后存储到共享"><a href="#加载A和B，然后存储到共享" class="headerlink" title="加载A和B，然后存储到共享"></a>加载A和B，然后存储到共享</h2><p>为了加载A和B矩阵，我们使用了一种在cuda c或ptx中无法有效实现的技术。我们将线程分成两半，让每一半加载一个矩阵。由于我们有64个线程，这意味着每个warp加载一个矩阵。cuda中的条件加载没有得到很好的优化，因为编译器没有努力确定加载是否在warp上均匀发生。对于纹理加载，这是必要的，因为指令每次只能处理一个纹理。因此，除了纹理加载之外，编译器还会添加一堆warp刷新以及分支和同步指令，以确保强制执行。如果Nvidia提供一种方式来提示条件或谓词是warp一致的（而不仅仅是分支，即bra.uni），那就太好了。</p>
<p>使用此技术的主要优点是，我们只需要一组跟踪寄存器来保存纹理加载索引。在主循环内部，这是一个巨大的胜利，因为它减少了我们需要的整数加法指令的一半。我们利用一切机会提高FFMA指令与非FFMA指令的比率。</p>
<p>我们还维护了4个单独的轨迹变量，以避免在每次纹理加载后使用依赖性屏障将单个轨迹变量增加ldx*2。内存指令发出时不会复制其操作数寄存器。这样做可能会节省晶体管。相反，当内存指令仍在运行时，您可以使用屏障来防止对这些寄存器的写入。在障碍处等待并不一定很糟糕，因为TLP可以启动并覆盖延迟，但减少需要覆盖的延迟总数可以帮助性能，因为这增加了有翘曲覆盖它们的机会。我们没有任何额外的循环IADDS，因为它有4个跟踪变量，只有3个额外的寄存器，这是我们可以轻松负担的。</p>
<p>所以我们将通过纹理单元加载。通过使用显式纹理加载而不是全局加载，无论是否使用非相干缓存，我们都可以获得一些好处。一是这使得代码更加简单，因为我们不需要担心加载超出范围。第二，使用相同的内核代码，我们可以加载8位或16位浮点，从而显著减少带宽和存储需求。有些应用程序不需要完全32位精度，在这种情况下这是一个巨大的胜利。</p>
<p>此外，我们将加载四元向量。这是对cublas代码的更改，在性能方面产生了最大的差异。虽然我可以理解为什么它不在立方体中使用，因为它对输入数据施加了4个字的对齐约束。立方体有一个固定的规范（这并不是说如果检测到四边对齐，它就不能选择不同的代码路径）。因此，通过使用四元向量，我们需要将lda/ldb索引向下折叠4。这有一个额外的好处，即允许我们加载索引大小为31位的矩阵，而不是常规纹理加载27位的限制。四元加载的另一个工件是我们的内存访问模式在每次提取时都会拉入并消耗全部缓存线。这意味着我们只能得到非常有限的纹理缓存使用率（1-2%），而我们的内存缓存性能将由二级缓存控制。</p>
<p>下面是一些伪代码，它只显示了主循环中的纹理加载和共享存储。你可以从地图上看到，这是非常直接的。你会注意到STS。128条指令，我们将遇到存储体冲突，但这些冲突是不可避免的，结果不会影响性能，因为批量加载和存储到向量指令中是一个双赢。此外，我甚至不确定银行冲突期间发生的指令回放是否重要，因为我认为这些指令可能会与FFMA一起发出。事实上，所有的内存操作都是在我们的主循环中发出的，根本不考虑flops计算（除非在寄存器组冲突一节中以一种微妙的方式描述）。</p>
<p>仅从这段代码和我们的主循环中时钟消耗指令的数量，我们就可以粗略估计内核所需的内存带宽上限。对于GM204，以下是数学公式：</p>
<ul>
<li><p>每个线程在每个循环中进行4个vec4 4字节的加载，或者每个循环中每个线程进行64个字节的加载。</p>
</li>
<li><p>下面我们将计算每个循环消耗大约520个时钟。</p>
</li>
<li>每个SM同时执行128个线程。</li>
<li>有16个SM的时钟频率为1.216 GHz（升压）。</li>
<li>每GB有.931 GiB：</li>
<li>64 x 128 x 16 x 1.216 x.931/520=285 GiB/秒</li>
</ul>
<p>GM204有224 GiB/sec可用。但这部分设备带宽将不需要，因为二级缓存将为其提供服务。但在设备带宽上有余量总是很好的。您的负载将不会以完全统一的方式执行，并且当它们聚集在一起时，您的净空越小，出现暂停的机会就越大。虽然只有运行接近理论吞吐量的代码才可能注意到这些暂停，但我们的代码恰好会这样做。</p>
<p>因此，您可以看到，64线程的实现对于GM204来说并不理想。然而，对于GM107来说，它是理想的，对于即将推出的具有384位内存总线的GM200来说也是如此。与256线程实现相比，这一实现使用了双倍的带宽，因此功耗更大。因此，当您有足够的数据来提供数据时，通常会首选更大的版本。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">tid = threadId.x;</span><br><span class="line">bx  = blockId.x;</span><br><span class="line">by  = blockId.y;</span><br><span class="line"></span><br><span class="line">blk = tid &gt;= 32 ? by    : bx;</span><br><span class="line">ldx = tid &gt;= 32 ? ldb/4 : lda/4;</span><br><span class="line">tex = tid &gt;= 32 ? texB  : texA;</span><br><span class="line">tid2  = (tid &gt;&gt; 4) &amp; 1;</span><br><span class="line">tid15 = tid &amp; 15;</span><br><span class="line"></span><br><span class="line">track0 = blk*64/4 + tid15 + (ldx * tid2);</span><br><span class="line">track2 = track0 + ldx*2;</span><br><span class="line">track4 = track0 + ldx*4;</span><br><span class="line">track6 = track0 + ldx*6;</span><br><span class="line"></span><br><span class="line">end = track0 + (k-8)*ldx;</span><br><span class="line"></span><br><span class="line">writeS = tid15*4*4 + tid2*64*4;</span><br><span class="line">writeS += tid &gt;= 32 ? 2048 : 0;</span><br><span class="line"></span><br><span class="line">while (track0 &lt; end)</span><br><span class="line">&#123;</span><br><span class="line">    tex.1d.v4.f32.s32 loadX0, [tex, track0];</span><br><span class="line">    tex.1d.v4.f32.s32 loadX2, [tex, track2];</span><br><span class="line">    tex.1d.v4.f32.s32 loadX4, [tex, track4];</span><br><span class="line">    tex.1d.v4.f32.s32 loadX6, [tex, track6];</span><br><span class="line"></span><br><span class="line">    st.shared.v4.f32 [writeS + 4*0*64], loadX0;</span><br><span class="line">    st.shared.v4.f32 [writeS + 4*2*64], loadX2;</span><br><span class="line">    st.shared.v4.f32 [writeS + 4*4*64], loadX4;</span><br><span class="line">    st.shared.v4.f32 [writeS + 4*6*64], loadX6;</span><br><span class="line"></span><br><span class="line">    // our loop needs one bar sync after share is loaded</span><br><span class="line">    bar.sync 0;</span><br><span class="line"></span><br><span class="line">    // Increment the track variables and swap shared buffers after the sync.</span><br><span class="line">    // We know at this point that these registers are not tied up with any in flight memory op.</span><br><span class="line">    track0 += ldx*8;</span><br><span class="line">    track2 += ldx*8;</span><br><span class="line">    track4 += ldx*8;</span><br><span class="line">    track6 += ldx*8;</span><br><span class="line">    writeS ^= 4*16*64;</span><br><span class="line"></span><br><span class="line">    // Additional loop code omitted for clarity.</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过四矢量纹理索引加载A和B：</p>
<p><img src="/img/image-20221208153216716.png" alt="image-20221208153216716"></p>
<p>使用四矢量将A和B存储到共享地址空间中：</p>
<p><img src="/img/image-20221208153237832.png" alt="image-20221208153237832"></p>
<h2 id="从共享读取"><a href="#从共享读取" class="headerlink" title="从共享读取"></a>从共享读取</h2><p>现在，共享内存已加载，我们从使用一半线程切换到处理A和B中的每一个。我们需要开始组合这些值来计算构成C点的点积（处理每一行后，我们计算块中所有C值的部分积和和）。所以每个线程都将从A的共享行和B的共享行中读取。</p>
<p>除了FFMA之外，共享负载是这个实现的真正工作。我们对它们进行双重缓冲，因此延迟最小。但我们也希望确保我们没有bank冲突，因为我们需要尽快提供这些数据。如何在没有库冲突的情况下使用四元向量从共享加载？好吧，根据文档，只要所有访问都在32个字（128字节）以内，我们就可以了。在sgemm中，这是因为我们可以安排不同的线程同时从同一共享内存位置加载，并使用共享广播机制。然而，事实证明，Maxwell的文档是不完整的，尽管warp中的所有线程都在相同的128字节内，但仍有某些模式会导致库冲突。这样做可能是为了节省芯片。所以我们只需要找到一个可行的模式。</p>
<p>在128字节内，我们可以加载8个16字节的四元组。我们将使其成为从A和B的共享内存加载的模式。我们的共享内存块是4*64=256字节宽，因此为了加载另一半，我们将展开该负载到一个相隔32个单元的额外指令中。我们不必担心bank冲突。每个矩阵的这两个四字负载形成了我们想要的8寄存器块。通过在2D中组合这两种1D模式的负载，我们可以得到下面所示的共享内存映射。该模式还表示每个线程的64个寄存器在C子矩阵中的位置（绿色方块）。</p>
<p>现在我们有了基本信息，我们需要将其分成两个warp，然后映射这些warp中的线程id。直接方法是以简单的扫描模式向下或横向加载。这导致了神秘的bank冲突。但是，如果我们使用由thread号表示的锯齿形图案，它就会起作用。我还没有对所有的负载大小和模式进行详尽的搜索，以了解哪些是有效的，哪些是无效的，但如果Nvidia为Maxwell更新他们的文档来解释这一限制，那就太好了。</p>
<p>至于找出将threadId映射到我们想要的模式（下面的readAs和readBs）所需的逻辑，我有一个简单的技术。我只是打印出每个threadId的二进制表示形式和我希望它映射到的值。当您以这种方式可视化二进制时，很容易确定需要保留、丢弃或移动哪些位以使映射工作（前提是您选择了possble映射）。</p>
<p>我还应该提到我的插图是如何被解读的。黄色方块表示线程（或TLP），绿色方块表示第一个线程的ILP。你应该能够想象得到绿色正方形的图案，并将其移动到每个黄色正方形的顶部（保持绿色与黄色的相对位置）。这应该跨越整个内存空间，这是我们共享映射的目标：a中一条线的每个点都需要与B中一条线上的每个点配对。细黑线表示线程如何被分割成warp。下面的深绿色方块是为了说明我们稍后将要进行的warp同步洗牌中的一个步骤。</p>
<p>另一个值得注意的是，cublas在这里使用了更复杂的readAs/readBs映射，这实现了相同的效果，但需要花费更多的指令。这是我的代码对cublas的一个小改进。如果您提前知道共享加载限制，那么更复杂的模式甚至是有意义的。但似乎愚蠢而直接的方法最终找到了更简单的解决方案。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">readAs = ((tid &gt;&gt; 1) &amp; 7) &lt;&lt; 4;</span><br><span class="line">readBs = (((tid &amp; 0x30) &gt;&gt; 3) | (tid &amp; 1)) &lt;&lt; 4 + 2048;</span><br><span class="line"></span><br><span class="line">while (track0 &lt; end)</span><br><span class="line">&#123;</span><br><span class="line">    // Process each of our 8 lines from shared</span><br><span class="line">    for (j = 0; j &lt; 8; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        // We fetch one line ahead while calculating the current line.</span><br><span class="line">        // Wrap the last line around to the first.</span><br><span class="line">        prefetch = (j + 1) % 8;</span><br><span class="line">        </span><br><span class="line">        // Use even/odd rows to implement our double buffer.</span><br><span class="line">        if (j &amp; 1)</span><br><span class="line">        &#123;</span><br><span class="line">            ld.shared.v4.f32 j0Ax00, [readAs + 4*(prefetch*64 + 0)];</span><br><span class="line">            ld.shared.v4.f32 j0By00, [readBs + 4*(prefetch*64 + 0)];</span><br><span class="line">            ld.shared.v4.f32 j0Ax32, [readAs + 4*(prefetch*64 + 32)];</span><br><span class="line">            ld.shared.v4.f32 j0By32, [readBs + 4*(prefetch*64 + 32)];</span><br><span class="line">        &#125;</span><br><span class="line">        else</span><br><span class="line">        &#123;</span><br><span class="line">            ld.shared.v4.f32 j1Ax00, [readAs + 4*(prefetch*64 + 0)];</span><br><span class="line">            ld.shared.v4.f32 j1By00, [readBs + 4*(prefetch*64 + 0)];</span><br><span class="line">            ld.shared.v4.f32 j1Ax32, [readAs + 4*(prefetch*64 + 32)];</span><br><span class="line">            ld.shared.v4.f32 j1By32, [readBs + 4*(prefetch*64 + 32)];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    // swap our shared memory buffers after reading out 8 lines</span><br><span class="line">    readAs ^= 4*16*64;</span><br><span class="line">    readBs ^= 4*16*64;</span><br><span class="line"></span><br><span class="line">    // Additional loop code omitted for clarity.</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>1D readAs（左侧）和readBs（顶部）在2D中组合以形成该线程块的C结果子矩阵：</p>
<p><img src="/img/image-20221208162657316.png" alt="image-20221208162657316"></p>
<h2 id="计算C：寄存器bank和重用"><a href="#计算C：寄存器bank和重用" class="headerlink" title="计算C：寄存器bank和重用"></a>计算C：寄存器bank和重用</h2><p>现在我们为线程填充了8个寄存器A和B，我们可以执行64个FFMA，这些FFMA构成了内核设计的核心工作。为了能够在全速和最低功率下计算这一点，我们需要考虑几个因素。主要是寄存器组和操作数重用。</p>
<p>Maxwell上有4个寄存器组，但与开普勒（也有4个组）不同的是，将bank分配给数字非常简单。Maxwell赋值只是寄存器数模4。在开普勒上，可以安排64条FFMA指令以消除所有存储体冲突。在麦克斯韦身上，这已经不可能了。然而，Maxwell提供了一些弥补这一点的方法，同时提供了显著减少寄存器组流量和总体芯片功耗的能力。这是操作数重用缓存。操作数重用缓存每个源操作数插槽有8个字节的数据。类似FFMA的指令有3个源操作数槽。每次发出指令时，都有一个标志可以用来指定是否要再次使用每个操作数。因此，在同一操作数槽中使用同一寄存器的下一条指令不必去寄存器组获取其值。通过此功能，您可以看到如何避免寄存器bank冲突。</p>
<p>因此，我们要采取的第一步是尽量减少操作数重用时必须隐藏的存储体冲突的数量。为此，我们需要显式选择要使用的寄存器。这是使用maxas作为汇编器的主要优点之一。ptxas在避免存储体冲突方面做得很好，但它并不完美，而且当涉及向量指令时，它做得特别糟糕（本例中的情况非常严重）。因此，我们将选择：</p>
<ul>
<li>0-63为C寄存器</li>
<li>64-71和80-87是矩阵A的双缓冲块寄存器</li>
<li>72-79和88-95是矩阵B的双缓冲块寄存器</li>
</ul>
<p>如果我们按照下面所示的8乘8矩阵排列，我们可以用每个寄存器的存储体索引为其着色。对于C寄存器，我们选择与相应的块寄存器不同的颜色。通过这种方式，您可以看到我们可以消除与C寄存器和阻塞寄存器的所有存储体冲突。这使得不可避免的16个存储体与阻塞寄存器本身发生冲突。这些以黑色显示：</p>
<p><img src="/img/image-20221208162949337.png" alt="image-20221208162949337"></p>
<p>如果没有重用缓存，这16个存储体冲突中的每一个都将导致计算中的1个时钟暂停。这将使我们的计算速度降低约20%（在520时钟循环中增加128个时钟）。但是，如果您使用—noruse标志组装sgemm代码，您将看到性能只会下降几百Gflop左右。如果你仔细阅读英伟达关于操作数收集器的专利，特别是如果你搜索涉及bank冲突的部分，这个谜团就迎刃而解了。它描述了一些缓解bank冲突的方法。很难说Maxwell是如何处理的，但这可能涉及到如何利用TLP来隐藏bank冲突延迟。因此，操作数收集器单独屏蔽存储体冲突的能力有限，但可能很快就会被淹没。通过使用持久的缓存而不仅仅是临时操作数缓冲区，硬件能够更有效地避免bank冲突暂停。它只需要汇编器使用重用标志来指导它，这样它就可以提前知道哪些寄存器值得缓存，以及在寄存器被写入时丢弃哪些寄存器。</p>
<p>优化设置重用标志的繁琐任务由maxas为您处理。留给我们的是以这样的方式对指令进行排序，以便最大限度地实现重用。最简单的排序是一个基本的双嵌套“for循环”，它将逐行遍历矩阵。这只有效地利用了重用缓存每个操作数8个字节中的4个字节，并且不会隐藏所有的存储体冲突。相反，如果您的扫描来回进行，则可以隐藏所有冲突并提高寄存器重用率（总体上为39%）。但最有效的模式是，在来回移动时，应用一个漩涡（47%的总重用率）。以下是按C寄存器号列出的FFMA指令顺序：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1, 0, 2, 3, 5, 4, 6, 7, 33, 32, 34, 35, 37, 36, 38, 39,</span><br><span class="line"></span><br><span class="line">45, 44, 46, 47, 41, 40, 42, 43, 13, 12, 14, 15, 9, 8, 10, 11,</span><br><span class="line"></span><br><span class="line">17, 16, 18, 19, 21, 20, 22, 23, 49, 48, 50, 51, 53, 52, 54, 55,</span><br><span class="line"></span><br><span class="line">61, 60, 62, 63, 57, 56, 58, 59, 29, 28, 30, 31, 25, 24, 26, 27</span><br></pre></td></tr></table></figure>
<p>您将注意到，所选漩涡尺寸在其中一个方向上的间距为2。此间距具有使C寄存器出现在交替存储体中的效果。我对这样做的原因的最佳猜测是极其微妙的。由于我们的内存指令与FFMA交错，并且这些指令没有其操作数寄存器的副本，因此它们可以在大约20个时钟周期内访问寄存器组。我们的C寄存器经常被弄脏，因此无法重复使用，所以我们总是从寄存器库中取出它们。因此，主要是这些寄存器会与我们的内存加载和存储指令发生延迟存储体冲突。可能不可能完全围绕这些银行冲突进行设计，但您可以减少它们的影响。通过在每条指令上交替使用C寄存器组，我们可以确保组冲突最多只能持续一个时钟。我运行了几个基准测试来检验这个假设，结果似乎是正确的。最后一个注意事项：使用所有四矢量加载和存储的另一个优点（除了效率更高之外）是减少了所需的内存指令数量，从而减少了延迟寄存器组冲突的机会。</p>
<p>鉴于我们知道内存操作数寄存器可能存在延迟存储体冲突，因此为这些操作数选择不同的存储体是值得尝试的。使用maxas，我们可以完全控制寄存器映射，您将在源代码中注意到，我们为track0-3、tex、readAs、readBs和writeS选择了非常特定的库。测试了这些库选择中的每一个，以最大化内核的flops性能。这是一个优化级别，我不确定cublas实现是否实现。我知道它犯的一个错误是，对于第一个FFMA，它选择了具有阻塞寄存器组冲突的C寄存器（3）。这防止了重用缓存隐藏该冲突的能力，因为之前没有将至少一个操作数加载到缓存中的指令。在GM204上，这个错误导致28 Gflops的性能损失。</p>
<p>使用FFMA的最后一个考虑是如何将它们与上述所有内存操作交织。要了解我在这里谈论的内容，请查看源代码的预处理版本。我们希望尽早使用双缓冲共享负载，以覆盖它们的延迟，因此我们将使用第一个FFMA开始双重发布它们。我们将用两条指令来分隔它们，因为内存单元似乎以一半的吞吐量最佳工作。我们将把纹理加载放在两组共享加载的中间。这样做是为了不让指令淹没内存单元。对于64线程实现，我们甚至将四个负载分成两组，并将它们放在不同的FFMA块中。我们将共享存储指令放置在尽可能低的位置，以使纹理加载有机会加载它们的操作数。我们不能将它们放在最后一个FFMA块中，因为这是我们开始为下一个循环迭代加载块寄存器的地方。</p>
<p>所有这些立场决定都经过了严格的测试，证明是最佳的。我应该注意，使用ptxas无法进行这些细粒度的排序和定位选择。事实上，ptxas倾向于优化我们的共享双缓冲加载方案。在选择寄存器组、优化操作数重用的指令排序和将内存指令精确放置在我们想要的位置之间，实现的性能可以达到理论性能的70%，而实现的性能则可以达到98%。</p>
<h2 id="warp同步无序映射"><a href="#warp同步无序映射" class="headerlink" title="warp同步无序映射"></a>warp同步无序映射</h2><p>在循环结束时，现在计算线程块的C子矩阵。所以现在是将结果存储回全局存储器的时候了。因为我们使用了来自共享的四矢量加载，所以我们的C值有点聚在一起，对于联合写入全局来说根本不是最佳的。我们可以直接将数据写出来，但我们可以做得更好。通过使用共享内存在同一warp的线程之间移动C寄存器，我们可以重新组织它以进行合并写入。您可能认为warp shuffle指令在这里最有效，但我们需要从不同的线程交换不同的寄存器，因此它不适合于此目的。</p>
<p>我们将把洗牌分成8块。上述共享内存映射上的深绿色线表示第一个块。另外7个将是C寄存器的后续垂直选择。因此，每个线程在共享内存中一次存储8个寄存器，然后立即再读取8个寄存器。但是，这些寄存器的排列方式使得我们的线程ID的重新映射可以以合并模式将数据存储到全局。因此，为了存储到共享，我们需要重新使用原始的共享内存映射，并在其中一个维度中将其从4个跨步单位折叠为一个。读取它的线程id映射将是32个值，步幅为1个单位。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tid31 = tid &amp; 31;</span><br><span class="line">tid32 = tid &amp; 32;</span><br><span class="line"></span><br><span class="line">// Remove the high bits if present from the last loop&#x27;s xor.</span><br><span class="line">// Also remove the 2048 added onto readBs.</span><br><span class="line">readAs &amp;= 0x7ff;</span><br><span class="line">readBs &amp;= 0x7ff;</span><br><span class="line"></span><br><span class="line">// Write to shared using almost the same shared mapping as before but collapse readBs down to stride one.</span><br><span class="line">writeCs = (readBs / 4) * 64 + readAs;</span><br><span class="line"></span><br><span class="line">// Read out with a mapping amenable to coalesced global writes</span><br><span class="line">readCs = ((tid32 &lt;&lt; 3) + tid31) &lt;&lt; 2;</span><br></pre></td></tr></table></figure>
<p><img src="/img/image-20221208164843431.png" alt="image-20221208164843431"></p>
<h2 id="Warp-Shuffling和联合存储到全局"><a href="#Warp-Shuffling和联合存储到全局" class="headerlink" title="Warp Shuffling和联合存储到全局"></a>Warp Shuffling和联合存储到全局</h2><p>有了上述映射，我们现在可以输出C值。注意，我们不需要bar.sync在写入共享内存之前进行同步，因为这已经在我们的最后一个循环中完成了。还要注意，由于我们不在warp之间共享数据，所以在共享内存洗牌中，我们不需要在写入和读取之间同步。只有在存储到writeC完成后，才会进行从readC的读取。注意，这里增加的共享内存延迟大部分可以用TLP隐藏，而为扭曲同步洗牌增加的净时钟只有十几个左右。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">ldc4 = ldc * 4;</span><br><span class="line"></span><br><span class="line">cx = bx*64 + tid31;</span><br><span class="line">cy = by*64 + (tid32 &gt;&gt; 1);</span><br><span class="line"></span><br><span class="line">Cy00 = (cy*ldc + cx) * 4 + C;</span><br><span class="line">Cy04 = Cy00 + ldc4 * 4;</span><br><span class="line">Cy08 = Cy00 + ldc4 * 8;</span><br><span class="line">Cy12 = Cy00 + ldc4 * 12;</span><br><span class="line"></span><br><span class="line">foreach copy vertical line of 8 registers from C into .v4.f32 cs0 and cs4</span><br><span class="line">&#123;</span><br><span class="line">    // Feed the 8 registers through the warp shuffle before storing to global</span><br><span class="line">    st.shared.v4.f32 [writeCs + 4*00], cs0;</span><br><span class="line">    st.shared.v4.f32 [writeCs + 4*32], cs4;</span><br><span class="line"></span><br><span class="line">    ld.shared.f32 cs0, [readCs + 4*(0*64 + 00)];</span><br><span class="line">    ld.shared.f32 cs1, [readCs + 4*(0*64 + 32)];</span><br><span class="line">    ld.shared.f32 cs2, [readCs + 4*(1*64 + 00)];</span><br><span class="line">    ld.shared.f32 cs3, [readCs + 4*(1*64 + 32)];</span><br><span class="line">    ld.shared.f32 cs4, [readCs + 4*(2*64 + 00)];</span><br><span class="line">    ld.shared.f32 cs5, [readCs + 4*(2*64 + 32)];</span><br><span class="line">    ld.shared.f32 cs6, [readCs + 4*(3*64 + 00)];</span><br><span class="line">    ld.shared.f32 cs7, [readCs + 4*(3*64 + 32)];</span><br><span class="line"></span><br><span class="line">    st.global.f32 [Cy00 + 4*00], cs0;</span><br><span class="line">    st.global.f32 [Cy00 + 4*32], cs1;</span><br><span class="line">    st.global.f32 [Cy04 + 4*00], cs2;</span><br><span class="line">    st.global.f32 [Cy04 + 4*32], cs3;</span><br><span class="line">    st.global.f32 [Cy08 + 4*00], cs4;</span><br><span class="line">    st.global.f32 [Cy08 + 4*32], cs5;</span><br><span class="line">    st.global.f32 [Cy12 + 4*00], cs6;</span><br><span class="line">    st.global.f32 [Cy12 + 4*32], cs7;</span><br><span class="line"></span><br><span class="line">    Cy00 += ldc4;</span><br><span class="line">    Cy04 += ldc4;</span><br><span class="line">    Cy08 += ldc4;</span><br><span class="line">    Cy12 += ldc4;</span><br><span class="line"></span><br><span class="line">    // After processing forth set shift over to the stride 32 registers</span><br><span class="line">    if (4th iteration)</span><br><span class="line">    &#123;</span><br><span class="line">        Cy00 += ldc4 * 28;</span><br><span class="line">        Cy04 += ldc4 * 28;</span><br><span class="line">        Cy08 += ldc4 * 28;</span><br><span class="line">        Cy12 += ldc4 * 28;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在下图中，蓝色方块表示如何从Cy00、Cy04、Cy08和Cy12矩阵C偏移的8个状态构造绿线。它们的垂直放置的不是步幅32的部分是到循环迭代的映射，而不是空间位置。</p>
<p><img src="/img/image-20221208165042717.png" alt="image-20221208165042717"></p>
<p>呃……所以这是一个很高的水平。代码注释中甚至包含了较低级别的细节，特别是关于如何将内存访问与计算同步的细节。注释仅在256线程版本中找到。说到这里，下面我将展示四倍多的线程如何改变映射。</p>
<h1 id="SGEMM-256-Thread-Implementation"><a href="#SGEMM-256-Thread-Implementation" class="headerlink" title="SGEMM - 256 Thread Implementation"></a>SGEMM - 256 Thread Implementation</h1><h2 id="Loading-A-and-B"><a href="#Loading-A-and-B" class="headerlink" title="Loading A and B"></a>Loading A and B</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tid = threadId.x;</span><br><span class="line">blk = tid &gt;= 128 ? blockId.y : blockId.x;</span><br><span class="line">ldx = tid &gt;= 128 ? ldb/4     : lda/4;</span><br><span class="line">tex = tid &gt;= 128 ? texB      : texA;</span><br><span class="line">tid4   = (tid &gt;&gt; 5) &amp; 3</span><br><span class="line">tid31  = tid &amp; 31</span><br><span class="line">tid96  = tid &amp; 96</span><br><span class="line">tid128 = tid &amp; 128</span><br><span class="line"></span><br><span class="line">track0 = blk*128/4 + tid31 + (ldx * tid4)</span><br><span class="line">track4 = track0 + ldx*4;</span><br><span class="line"></span><br><span class="line">end = track0 + (k-8)*ldx;</span><br></pre></td></tr></table></figure>
<h2 id="Storing-to-Shared"><a href="#Storing-to-Shared" class="headerlink" title="Storing to Shared"></a>Storing to Shared</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">writeS  = tid31*4*4 + tid4*128*4;</span><br><span class="line">writeS += tid &gt;= 128 ? 4096 : 0;</span><br></pre></td></tr></table></figure>
<p><img src="/img/image-20221208165147533.png" alt="image-20221208165147533"></p>
<h2 id="Reading-from-Shared"><a href="#Reading-from-Shared" class="headerlink" title="Reading from Shared"></a>Reading from Shared</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">readAs = ((tid128 &gt;&gt; 4) | ((tid &gt;&gt; 1) &amp; 7)) &lt;&lt; 4;</span><br><span class="line">readBs  = (((tid &amp; 0x70) &gt;&gt; 3) | (tid &amp; 1)) &lt;&lt; 4 + 4096;</span><br></pre></td></tr></table></figure>
<p><img src="/img/image-20221208165203670.png" alt="image-20221208165203670"></p>
<h2 id="Warp-Synchronous-Shuffle"><a href="#Warp-Synchronous-Shuffle" class="headerlink" title="Warp Synchronous Shuffle"></a>Warp Synchronous Shuffle</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">readAs &amp;= 0xfff;</span><br><span class="line">readBs &amp;= 0xfff;</span><br><span class="line"></span><br><span class="line">writeCs = (readBs / 4) * 128 + readAs;</span><br><span class="line"></span><br><span class="line">readCs = ((tid96 &lt;&lt; 4) | tid31 | (tid128 &gt;&gt; 2)) &lt;&lt; 2;</span><br></pre></td></tr></table></figure>
<p><img src="/img/image-20221208165218383.png" alt="image-20221208165218383"></p>
<h2 id="Storing-to-Global"><a href="#Storing-to-Global" class="headerlink" title="Storing to Global"></a>Storing to Global</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ldc4 = ldc * 4;</span><br><span class="line"></span><br><span class="line">cx = bx*128 + tid31 | (tid128 &gt;&gt; 2);</span><br><span class="line">cy = by*128 + (tid96 &gt;&gt; 1);</span><br><span class="line"></span><br><span class="line">Cy00 = (cy*ldc + cx) * 4 + C;</span><br><span class="line">Cy04 = Cy00 + ldc4*4;</span><br><span class="line">Cy08 = Cy00 + ldc4*8;</span><br><span class="line">Cy12 = Cy00 + ldc4*12;</span><br></pre></td></tr></table></figure>
<p><img src="/img/image-20221208165238932.png" alt="image-20221208165238932"></p>
<h1 id="深入浅出GPU优化系列"><a href="#深入浅出GPU优化系列" class="headerlink" title="深入浅出GPU优化系列"></a>深入浅出GPU优化系列</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>首先需要对reduce算法进行介绍。reduce算法本质上就是计算x=x0⊗x1⊗x2⊗x3……⊗xn−1⊗xn 。下面本文将详细说明如何在GPU中实现reduce算法并进行深入地优化。</p>
<h2 id="并行算法设计"><a href="#并行算法设计" class="headerlink" title="并行算法设计"></a>并行算法设计</h2><p>在GPU中，reduce采用了一种树形的计算方式。如下图所示。</p>
<p><img src="/img/image-20220910175222544.png" alt="image-20220910175222544"></p>
<p>从上至下，将数据不断地累加，直到得出最后的结果，即25。但由于GPU没有针对global数据的同步操作，只能针对block的数据进行同步。所以，一般而言将reduce分为两个阶段，其示意图如下：</p>
<p><img src="/img/image-20220910175237258.png" alt="image-20220910175237258"></p>
<p>我们仔细来看看这个事，假设给定一个长度为N的数组，需要计算该数组的所有元素之和。首先需要将数组分为m个小份。而后，在第一阶段中，开启m个block计算出m个小份的reduce值。最后，在第二阶段中，使用一个block将m个小份再次进行reduce，得到最终的结果。由于第二阶段本质上是可以调用第一个阶段的kernel，所以不做单独说明，本文只是探索<strong>第一阶段</strong>的优化技巧。</p>
<p>所以kernel接口为：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__global__ void reduce(T *input, T* output)</span><br></pre></td></tr></table></figure>
<p>其中，input代表输入的数组，即一个长度为N的数组，output代表输出数组，即第一阶段的结果，即长度为M的数组。随后要开始激动人心的coding阶段，但在CUDA编程中，我们首先需要设置三个参数:</p>
<ol>
<li><strong>BlockNum</strong>：即开启的block数量，即上面所说的M，代表需要将数组切分为几份。</li>
<li><strong>Thread_per_block</strong>:每个block中开启的线程数，一般而言，取128，256，512，1024这几个参数会比较多。</li>
<li><strong>Num_per_block</strong>:每个block需要进行reduce操作的长度。</li>
</ol>
<p>其中，<code>BlockNum* Num_per_block=N</code>。</p>
<h2 id="reduce优化"><a href="#reduce优化" class="headerlink" title="reduce优化"></a>reduce优化</h2><h3 id="reduce-baseline算法介绍"><a href="#reduce-baseline算法介绍" class="headerlink" title="reduce baseline算法介绍"></a>reduce baseline算法介绍</h3><p>Baseline算法比较简单，分为三个步骤。第一个步骤是将数据load至shared memory中，第二个步骤是在shared memory中对数据进行reduce操作，第三个步骤是将最后的结果写回global memory中。代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduce0</span><span class="params">(<span class="type">float</span> *d_in,<span class="type">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">unsigned</span> <span class="type">int</span> s=<span class="number">1</span>; s&lt;blockDim.x; s*=<span class="number">2</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid%(<span class="number">2</span>*s) == <span class="number">0</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在进行优化之前，我们需要再来好好地梳理一下这个baseline代码。优化的本质是通过软件榨干硬件资源，所以必须清楚地了解代码在硬件上的执行过程才能更好地进行优化。</p>
<p>在<strong>第一个步骤</strong>中，我们让Num_per_block与Thread_per_block一致，每个block设定为256个线程，一个block负责256个数据的reduce工作。假设需要处理32M的数据，则有128K个block。tid代表线程号，i代表在原始数组中的索引号。第tid号线程将第i号的数据从global中取出，放到shared memory的第tid元素中。比如在第0号block中，0号线程将0号元素取出，放到shared memory的第0号位置。示意图见：</p>
<p><img src="/img/image-20220910175717727.png" alt="image-20220910175717727"></p>
<p>从硬件角度来分析一下代码。为了执行代码，GPU需要分配两种资源，一个是<strong>存储资源</strong>，一个是<strong>计算资源</strong>。<strong>存储资源</strong>包括在global memory中分配的一块<code>32M× sizeof(float)</code>的空间以及在shared memory中分配的<code>256× sizeof(float)</code>的空间。需要注意的是，<strong>shared memory存在bank冲突的问题，因而需要格外小心</strong>。 <strong>计算资源</strong>其实是根据thread数量来确定的，一个block中分配256个thread线程，32个线程为一组，绑定在一个SIMD单元。所以256个线程可以简单地理解为分配了8组SIMD单元。</p>
<p>（但实际的硬件资源分配不是这样，因为一个SM的计算资源有限，不可能真的给每一个block都分配这么多的SIMD单元。）总而言之，在第一个阶段，就是tid号线程将i号数据从global memory中取出，再放进shared memory中，严谨一点的话，中间是走一遍寄存器再到shared memory中的。</p>
<p>到了<strong>第二个阶段</strong>，block中需要计算的256个元素已经全部被存储在了shared memory中，此时需要对其进行reduce操作。这个过程需要进行多轮迭代，在第一轮迭代中，如果tid%2 ==0, 则第tid号线程将shared memory中第tid号位置的值和第tid+1号的值进行相加，而后放在第tid号位置。</p>
<p>在第二轮迭代中，如果tid%4==0,则第tid号线程将shared memory中第tid号位置的值和第tid+2号的值进行相加，而后放在第tid号位置。不断迭代，则所有元素都将被累加到第0号位置。其示意图如下。其中，红色的线程代表符合if条件的线程，只有它们有任务，需要干活。</p>
<p><img src="/img/image-20220910175947839.png" alt="image-20220910175947839"></p>
<p>在<strong>第三个阶段</strong>中，block负责的256个元素之和都放置在shared memory的0号位置，此时，只需要将0号位置的元素写回即可。</p>
<h3 id="优化技巧1：解决warp-divergence"><a href="#优化技巧1：解决warp-divergence" class="headerlink" title="优化技巧1：解决warp divergence"></a>优化技巧1：解决warp divergence</h3><h4 id="现有问题"><a href="#现有问题" class="headerlink" title="现有问题"></a>现有问题</h4><p>目前reduce0存在的最大问题就是<strong>warp divergent</strong>的问题。对于一个block而言，它所有的thread都是执行同一条指令。如果存在if-else这样的分支情况的话，thread会执行所有的分支。只是不满足条件的分支，所产生的结果不会记录下来。可以在上图中看到，在每一轮迭代中都会产生两个分支，分别是红色和橙色的分支。这严重影响了代码执行的效率。</p>
<h4 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h4><p>解决的方式也比较明了，就是尽可能地让所有线程走到同一个分支里面。代码示意如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduce1</span><span class="params">(<span class="type">float</span> *d_in,<span class="type">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">unsigned</span> <span class="type">int</span> s=<span class="number">1</span>; s&lt;blockDim.x; s*=<span class="number">2</span>)&#123;</span><br><span class="line">        <span class="type">int</span> index = <span class="number">2</span>*s*tid;</span><br><span class="line">        <span class="keyword">if</span>(index &lt; blockDim.x)&#123;</span><br><span class="line">            sdata[index]+=sdata[index+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>虽然代码依旧存在着if语句，但是却与reduce0代码有所不同。我们继续假定block中存在256个thread，即拥有256/32=8个warp。当进行<strong>第1次迭代</strong>时，0-3号warp的<code>index&lt;blockDim.x</code>， 4-7号warp的<code>index&gt;=blockDim.x</code>。对于每个warp而言，都只是进入到一个分支内，所以并不会存在warp divergence的情况。</p>
<p>当进行<strong>第2次迭代</strong>时，0、1号两个warp进入计算分支。当进行<strong>第3次迭代</strong>时，只有0号warp进入计算分支。当进行<strong>第4次迭代</strong>时，只有0号warp的前16个线程进入分支。此时开始产生warp divergence。通过这种方式，我们消除了前3次迭代的warp divergence。</p>
<h3 id="优化技巧2：解决bank冲突"><a href="#优化技巧2：解决bank冲突" class="headerlink" title="优化技巧2：解决bank冲突"></a>优化技巧2：解决bank冲突</h3><h4 id="现有问题-1"><a href="#现有问题-1" class="headerlink" title="现有问题"></a>现有问题</h4><p>reduce1的最大问题是<strong>bank冲突</strong>。我们把目光聚焦在这个for循环中。并且只聚焦在<strong>0号warp</strong>。在<strong>第一次迭代</strong>中，0号线程需要去load shared memory的0号地址以及1号地址的数，然后写回到0号地址。而此时，这个warp中的16号线程，需要去load shared memory中的32号地址和33号地址。可以发现，0号地址跟32号地址产生了<strong>2路的bank冲突</strong>。</p>
<p>在<strong>第2次迭代</strong>中，0号线程需要去load shared memory中的0号地址和2号地址。这个warp中的8号线程需要load shared memory中的32号地址以及34号地址，16号线程需要load shared memory中的64号地址和68号地址，24号线程需要load shared memory中的96号地址和100号地址。</p>
<p>又因为0、32、64、96号地址对应着同一个bank，所以此时产生了<strong>4路的bank冲突</strong>。现在，可以继续算下去，8路bank冲突，16路bank冲突。由于bank冲突，所以reduce1性能受限。下图说明了在load第一个数据时所产生的bank冲突。</p>
<p><img src="/img/image-20220910180155032.png" alt="image-20220910180155032"></p>
<h4 id="解决方式-1"><a href="#解决方式-1" class="headerlink" title="解决方式"></a>解决方式</h4><p>在reduce中，解决bank冲突的方式就是把for循环逆着来。原来stride从0到256，现在stride从128到0。其伪代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduce2</span><span class="params">(<span class="type">float</span> *d_in,<span class="type">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">unsigned</span> <span class="type">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">0</span>; s&gt;&gt;=<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid &lt; s)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那为什么通过这么一个小小的改变就能消除bank冲突呢，我们继续进行分析。</p>
<p>把目光继续看到这个for循环中，并且只分析0号warp。0号线程需要load shared memory的0号元素以及128号元素。1号线程需要load shared memory中的1号元素和129号元素。这一轮迭代中，在读取第一个数时，warp中的32个线程刚好load 一行shared memory数据。再分析第2轮迭代，0号线程load 0号元素和64号元素，1号线程load 1号元素和65号元素。</p>
<p>咦，也是这样，每次load shared memory的一行。再来分析第3轮迭代，0号线程load 0号元素和32号元素，接下来不写了，总之，一个warp load shared memory的一行。没有bank冲突。到了4轮迭代，0号线程load 0号元素和16号元素。那16号线程呢，16号线程啥也不干，因为s=16，16-31号线程啥也不干，跳过去了。示意图如下：</p>
<p><img src="/img/image-20220910213658483.png" alt="image-20220910213658483"></p>
<h3 id="优化技巧3：解决idle线程"><a href="#优化技巧3：解决idle线程" class="headerlink" title="优化技巧3：解决idle线程"></a>优化技巧3：解决idle线程</h3><h4 id="现有问题-2"><a href="#现有问题-2" class="headerlink" title="现有问题"></a>现有问题</h4><p><strong>reduce2</strong>最大的问题就是线程的浪费。可以看到我们启动了256个线程，但是在第1轮迭代时只有128个线程在干活，第2轮迭代只有64个线程在干活，每次干活的线程都会减少一半。第一轮迭代示意图如下，只有前128个线程在load数据。后128个线程啥也不干，光看着。</p>
<p><img src="/img/image-20220910213713143.png" alt="image-20220910213713143"></p>
<h4 id="解决方式-2"><a href="#解决方式-2" class="headerlink" title="解决方式"></a>解决方式</h4><p>对于HPC从业者而言，我们希望变成GPU的资本家，去尽可能地压榨GPU。但是呢，在这里，每一次迭代有一半的线程不干活。而且，128-255号线程最过分，它娘的，没有任何贡献，啥也不干。想来想去，能不能让它们干点活呢。想来想去，那这样吧，让它好歹做一次加法。除了去global memory中取数外，再做一次加法。当然为了实现这个，block数就得改一改了。Block数量减少，Num_per_block增加一倍。也就是说原来一个block只需要管256个数就行，现在得管512个数了。代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduce3</span><span class="params">(<span class="type">float</span> *d_in,<span class="type">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i] + d_in[i+blockDim.x];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">unsigned</span> <span class="type">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">0</span>; s&gt;&gt;=<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid &lt; s)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过这种方式，将一些idle的线程给利用起来了。</p>
<h3 id="优化技巧4：展开最后一维减少同步"><a href="#优化技巧4：展开最后一维减少同步" class="headerlink" title="优化技巧4：展开最后一维减少同步"></a>优化技巧4：展开最后一维减少同步</h3><h4 id="现有问题-3"><a href="#现有问题-3" class="headerlink" title="现有问题"></a>现有问题</h4><p>对于reduce3来说，性能已经算是比较好了。但是依旧没有达到我们想要的效果。我们再来仔细地看看还有什么可以改进的地方。我们发现，当进行到最后几轮迭代时，此时的block中只有warp0在干活时，线程还在进行<strong>同步</strong>操作。这一条语句造成了极大的浪费。</p>
<h4 id="解决方式-3"><a href="#解决方式-3" class="headerlink" title="解决方式"></a>解决方式</h4><p>由于一个warp中的32个线程其实是在一个SIMD单元上，这32个线程每次都是执行同一条指令，这天然地保持了同步状态，因而当s=32时，即只有一个SIMD单元在工作时，完全可以将__syncthreads()这条同步代码去掉。所以我们将最后一维进行展开以减少同步。伪代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">warpReduce</span><span class="params">(<span class="keyword">volatile</span> <span class="type">float</span>* cache,<span class="type">int</span> tid)</span></span>&#123;</span><br><span class="line">    cache[tid]+=cache[tid<span class="number">+32</span>];</span><br><span class="line">    cache[tid]+=cache[tid<span class="number">+16</span>];</span><br><span class="line">    cache[tid]+=cache[tid<span class="number">+8</span>];</span><br><span class="line">    cache[tid]+=cache[tid<span class="number">+4</span>];</span><br><span class="line">    cache[tid]+=cache[tid<span class="number">+2</span>];</span><br><span class="line">    cache[tid]+=cache[tid<span class="number">+1</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduce4</span><span class="params">(<span class="type">float</span> *d_in,<span class="type">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i] + d_in[i+blockDim.x];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">unsigned</span> <span class="type">int</span> s=blockDim.x/<span class="number">2</span>; s&gt;<span class="number">32</span>; s&gt;&gt;=<span class="number">1</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid &lt; s)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid+s];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid&lt;<span class="number">32</span>)<span class="built_in">warpReduce</span>(sdata,tid);</span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以通过下面的示意图更好地了解，warp0会被绑定在一个SIMD单元上，上面有thread0-thread31。warp1会被绑在另外一个SIMD单元上，上面有thread32-thread63。由于在一个SIMD单元上，然后不管啥时候thread0和thread7肯定是同一状态，不需要同步。而thread0和thread34就不能保证同步，必须用<code>__syncthreads()</code>来保证同步操作。</p>
<h3 id="优化技巧5：完全展开减少计算"><a href="#优化技巧5：完全展开减少计算" class="headerlink" title="优化技巧5：完全展开减少计算"></a>优化技巧5：完全展开减少计算</h3><h4 id="现有问题-4"><a href="#现有问题-4" class="headerlink" title="现有问题"></a>现有问题</h4><p>其实到了这一步，reduce的效率已经足够高了。再进一步优化其实已经非常困难了。为了探索极致的性能表现，Mharris接下来给出的办法是<strong>对for循环进行完全展开</strong>。我觉得这里主要是减少for循环的开销。Mharris的实验表明这种方式有着1.41x的加速比。但是用的机器是G80，十几年前的卡。性能数据也比较老了，至于能不能真的有这么好的加速比，我们拭目以待。</p>
<h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p>我们将整个for循环进行展开，非常暴力，代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> blockSize&gt;</span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">warpReduce</span><span class="params">(<span class="keyword">volatile</span> <span class="type">float</span>* cache,<span class="type">int</span> tid)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">64</span>)cache[tid]+=cache[tid<span class="number">+32</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">32</span>)cache[tid]+=cache[tid<span class="number">+16</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">16</span>)cache[tid]+=cache[tid<span class="number">+8</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">8</span>)cache[tid]+=cache[tid<span class="number">+4</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">4</span>)cache[tid]+=cache[tid<span class="number">+2</span>];</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">2</span>)cache[tid]+=cache[tid<span class="number">+1</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> blockSize&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduce5</span><span class="params">(<span class="type">float</span> *d_in,<span class="type">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> tid=threadIdx.x;</span><br><span class="line">    sdata[tid]=d_in[i] + d_in[i+blockDim.x];</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">512</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">256</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid<span class="number">+256</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">256</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">128</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid<span class="number">+128</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">128</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">64</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid<span class="number">+64</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid&lt;<span class="number">32</span>)<span class="built_in">warpReduce</span>&lt;blockSize&gt;(sdata,tid);</span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="优化技巧6：合理设置block数量"><a href="#优化技巧6：合理设置block数量" class="headerlink" title="优化技巧6：合理设置block数量"></a>优化技巧6：合理设置block数量</h3><h4 id="现有问题-5"><a href="#现有问题-5" class="headerlink" title="现有问题"></a>现有问题</h4><p>当走到这一步的时候，能调的东西已经基本上调完了。我们再把眼光放在block和thread的设置上。之前默认了Num_per_block=Thread_per_block。也就是说，一个block开启256个线程时，这个block负责256个元素的reduce操作。那可不可以让一个block多管点数。这样的话，开启的block数量少一些。以此<strong>对block设置进行调整</strong>，获得最优block取值，这样或许能够带来一些性能收益？</p>
<h4 id="解决方式-4"><a href="#解决方式-4" class="headerlink" title="解决方式"></a>解决方式</h4><p>这样需要再思考一下block的取值。对于GPU而言，block的取值到底是多更好，还是少更好。如此对CUDA编程熟悉的同学，肯定会毫不犹豫地说：“那肯定是多更好啦。Block数量多，block可以进行快速地切换，去掩盖访存的延时。”这个问题按下不表，我们看看Mharris是怎么说的。</p>
<p>如果一个线程被分配更多的work时，可能会更好地覆盖延时。这一点比较好理解。如果线程有更多的work时，对于编译器而言，就可能有更多的机会对相关指令进行重排，从而去覆盖访存时的巨大延时。虽然这句话并没有很好地说明在某种程度上而言，block少一些会更好。但是，有一点不可否认,<strong>block需要进行合理地设置</strong>。唠唠叨叨说了很多，现在把代码贴一下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> blockSize&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduce6</span><span class="params">(<span class="type">float</span> *d_in,<span class="type">float</span> *d_out)</span></span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> sdata[THREAD_PER_BLOCK];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> tid=threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> gridSize = blockSize * <span class="number">2</span> * gridDim.x;</span><br><span class="line">    sdata[tid] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(i&lt;n)&#123;</span><br><span class="line">        sdata[tid] +=d_in[i]+d_in[i+blockSize];</span><br><span class="line">        i+=gridSize;</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// do reduction in shared mem</span></span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">512</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">256</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid<span class="number">+256</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">256</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">128</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid<span class="number">+128</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(blockSize&gt;=<span class="number">128</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(tid&lt;<span class="number">64</span>)&#123;</span><br><span class="line">            sdata[tid]+=sdata[tid<span class="number">+64</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid&lt;<span class="number">32</span>)<span class="built_in">warpReduce</span>&lt;blockSize&gt;(sdata,tid);</span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sdata[tid];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="优化技巧7：使用shuffle指令"><a href="#优化技巧7：使用shuffle指令" class="headerlink" title="优化技巧7：使用shuffle指令"></a>优化技巧7：使用shuffle指令</h3><h4 id="现有问题-6"><a href="#现有问题-6" class="headerlink" title="现有问题"></a>现有问题</h4><p>其实，对于Mharris的讲义。reduce优化就到此结束了。但是NV后来出了Shuffle指令，对于reduce优化有着非常好的效果。目前绝大多数访存类算子，像是softmax，batch_norm，reduce等，都是用Shuffle实现。所以，在这里谈一下这么把shuffle指令用在reduce优化上。</p>
<p>Shuffle指令是一组针对warp的指令。Shuffle指令最重要的特性就是<strong>warp内的寄存器可以相互访问</strong>。在没有shuffle指令的时候，各个线程在进行通信时只能通过shared memory来访问彼此的寄存器。而采用了shuffle指令之后，warp内的线程可以直接对其他线程的寄存器进行访存。通过这种方式可以减少访存的延时。除此之外，带来的最大好处就是可编程性提高了，在某些场景下，就不用shared memory了。毕竟，开发者要自己去控制 shared memory还是挺麻烦的一个事。</p>
<p>伪代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> blockSize&gt;</span><br><span class="line"><span class="function">__device__ __forceinline__ <span class="type">float</span> <span class="title">warpReduceSum</span><span class="params">(<span class="type">float</span> sum)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">32</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">16</span>);</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">16</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">8</span>);</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">8</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">4</span>);</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">4</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">if</span>(blockSize &gt;= <span class="number">2</span>)sum += __shfl_down_sync(<span class="number">0xffffffff</span>,sum,<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> blockSize&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">reduce7</span><span class="params">(<span class="type">float</span> *d_in,<span class="type">float</span> *d_out, <span class="type">unsigned</span> <span class="type">int</span> n)</span></span>&#123;</span><br><span class="line">    <span class="type">float</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//each thread loads one element from global memory to shared mem</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> i=blockIdx.x*(blockDim.x*<span class="number">2</span>)+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> tid=threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> gridSize = blockSize * <span class="number">2</span> * gridDim.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(i&lt;n)&#123;</span><br><span class="line">        sdata[tid] +=d_in[i]+d_in[i+blockSize];</span><br><span class="line">        i+=gridSize;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// shared mem for partial sums(one per warp in the block</span></span><br><span class="line">    <span class="type">static</span> __shared__ <span class="type">float</span> warpLevelSums[WARP_SIZE];</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> laneId = threadIdx.x % WARP_SIZE;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> warpId = threadIdx.x / WARP_SIZE;</span><br><span class="line"></span><br><span class="line">    sum = <span class="built_in">warpReduceSum</span>&lt;blockSize&gt;(sum);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(laneId == <span class="number">0</span>)warpLevelSums[warpId]=sum;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    sum = (threadIdx.x &lt; blockDim.x / WARP_SIZE)? warpLevelSums[laneId]:<span class="number">0</span>;</span><br><span class="line">    <span class="comment">// Final reduce using first warp</span></span><br><span class="line">    <span class="keyword">if</span>(warpId == <span class="number">0</span>)sum = <span class="built_in">warpReduceSum</span>&lt;blockSize/WARP_SIZE&gt;(sum);</span><br><span class="line">    <span class="comment">// write result for this block to global mem</span></span><br><span class="line">    <span class="keyword">if</span>(tid==<span class="number">0</span>)d_out[blockIdx.x]=sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="GEMM优化"><a href="#GEMM优化" class="headerlink" title="GEMM优化"></a>GEMM优化</h2><h3 id="前言-1"><a href="#前言-1" class="headerlink" title="前言"></a>前言</h3><p>在高性能领域，对于<strong>矩阵乘（GEMM）的优化</strong>是一个非常重要的课题。GEMM可以非常广泛地应用于航空航天、流体力学等科学计算领域，这也是之前HPC的主要应用场景。后来深度学习开展地如火如荼，由于对高算力的需要，也成为HPC的主要应用场景之一。这些年涌现了一系列的深度学习模型。模型里面最耗时的东西，包括卷积、全连接层、attention，都可以转换成GEMM操作。所以说，GEMM优化的重要性，怎么突出都不过分。</p>
<p>本篇文章主要介绍GEMM中的数据分块和如何在多级存储进行数据搬运。这也是<strong>HPC优化的核心思想，怎么样让数据放在更近的存储上来掩盖计算的延时，从而减少存储墙的影响</strong>。文章分为四个方面进行叙述，首先介绍在global memory层面如何进行分块以及数据搬运，随后介绍在shared memory层面如何进行分块以及数据搬运，而后介绍在register层面如何进行分块以及避免bank冲突，最后介绍如何进行prefetch以更好地掩盖访存时延。</p>
<h3 id="从global-memory到shared-memory"><a href="#从global-memory到shared-memory" class="headerlink" title="从global memory到shared memory"></a>从global memory到shared memory</h3><p>假设有矩阵A,B，需要计算矩阵A和B的乘，即矩阵C。A、B、C三个矩阵的维度分别为，，m∗k，k∗n，m∗n ，且三个矩阵中的数据都是单精度浮点数。对于C中每一个元素，C[i][j]，可以看作是A的一行和B的一列进行一次归约操作。采用最naive的GEMM算法，在GPU中，一共开启m∗n 个线程，每个线程需要读取矩阵A的一行与矩阵B的一列，而后将计算结果写回至矩阵C中。因而，完成计算一共需要从global memory中进行2mnk 次读操作和m*n次写操作。大量的访存操作使得GEMM效率难以提高，因而考虑global memory中进行分块，并将矩阵块放置到shared memory中。其示意图如下：</p>
<p><img src="/img/image-20220910214848982.png" alt="image-20220910214848982"></p>
<p>对global memory进行分块的GEMM算法示意图见上图右侧。首先将A、B、C三个矩阵划分为多个维度为，，bm∗bk，bk∗bn，bm∗bn 的小矩阵块。三个矩阵形成M∗K，K∗N，M∗N 的小矩阵网格。其中M=m/bm，N=n/bn，K=k/bk 。随后在GPU中开启M∗N 个block，每个block负责C中一个维度为bm∗bn 的小矩阵块的计算。计算中一共有K次迭代，每一次迭代都需要读取A中一个维度为bm∗bk 的小矩阵块和B中一个维度为bk∗bn 的小矩阵块，并将其放置在shared memory中。因而，完成C中所有元素的计算一共需要从global memory中读取M∗N∗K∗（bm∗bk+bk∗bn） ，即m∗n∗k（1/bm+1/bn） 个单精度浮点数。相比于naive的GEMM算法，访存量减少为原来的1/2∗(1/bm+1/bn) 。通过global memory中分块算法极大地减少了对global memory的访存量。并且，相比于naive算法，对global进行分块可以更充分地利用数据局部性。在naive算法中，每一个线程都需要直接从global memory中取数，其时延非常长，计算性能非常差。而进行分块后，将维度为bm∗bk，bk∗bn 的小矩阵块先存储到shared memory之中。而后计算单元进行计算时可以直接从shared memory中取数，大大减少了访存所需要的时延。</p>
<h3 id="从shared-memory到register"><a href="#从shared-memory到register" class="headerlink" title="从shared memory到register"></a>从shared memory到register</h3><p>随后，我们进一步考虑从shared memory到register的过程。在这里，只分析<strong>一个block</strong>中的计算。当进行K轮迭代中某一轮迭代时，GPU将维度为bm∗bk，bk∗bn 的小矩阵块存储到shared memory中，而后各个线程将shared memory中的数据存入register中进行计算。</p>
<p><img src="/img/image-20220910215005453.png" alt="image-20220910215005453"></p>
<p>在<strong>不对shared memory分块</strong>时，一个block中含有bm∗bn 个线程，<strong>每一个线程负责C中一个元素的计算</strong>。则一个block一共需要对shared memory进行2∗bm∗bn∗bk 次读操作。而后<strong>考虑对shared memory进行分块</strong>，对bm∗bn 的小矩阵进行再一次划分，将其划分为多个维度为rm∗rn 的子矩阵。则一个block需要负责X∗Y 个子矩阵。其中，X=bmrm ，Y=bnrn 。随后，在一个block中开启X∗Y 个线程，<strong>每个线程负责一个维度为rm∗rn 的子矩阵的计算</strong>。在计算中，一个block一共需要从shared memory读取X∗Y∗(rm+rn)∗bk ，即bm∗bn∗bk∗(1/rm+1/rn) 个单精度浮点数。相比于未分块的算法，对于shared memory中的访存量减少为原来的1/2∗(1/rm+1/rn) 。并且，由于将数据放入register中，可以直接对数据进行运算，减少了从shared memory中取数的时延。</p>
<h3 id="register分块"><a href="#register分块" class="headerlink" title="register分块"></a>register分块</h3><p>在这里，我们考虑最后一层，即register中的计算，并且只分析一个thread。在完成以上的过程后，对于一个线程而言，它现在拥有：rm 个A矩阵的寄存器值，rn 个B矩阵的寄存器值，以及rm∗rn 个C矩阵的寄存器值。通过这些寄存器的值，需要计算rm∗rn 个数。这需要rm∗rn 条FFMA指令。</p>
<p>这个时候会涉及到寄存器的bank conflict。在NV的GPU中，每个SM不仅会产生shared memroy之间的bank 冲突，也会产生寄存器之间的bank冲突。这一点对于计算密集型的算子十分重要。像shared memory一样，寄存器的Register File也会被分为几个bank，如果一条指令的的源寄存器有2个以上来自同一bank，就会产生冲突。指令会重发射，浪费一个cycle。</p>
<p>我们假设对这个thread来说，rm=4,rn=4 。并且计算C的寄存器以一种非常naive的情况分配，如下图左侧所示。则需要产生16条FFMA指令，列举如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FFMA R0, R16, R20, R0</span><br><span class="line">FFMA R1, R16, R21, R1</span><br><span class="line">……</span><br></pre></td></tr></table></figure>
<p><img src="/img/image-20220910220109982.png" alt="image-20220910220109982"></p>
<p>可以从中看出，这会产生大量的register bank冲突，所以需要对参与计算的寄存器重新进行分配和排布,如上图右侧所示。在有些地方，这种方式也可以叫做register分块。</p>
<h3 id="数据的prefetch"><a href="#数据的prefetch" class="headerlink" title="数据的prefetch"></a>数据的prefetch</h3><p>最后，我们来讲讲如何通过对数据进行prefetch来减少访存的latency。我们再来回顾GEMM的过程，并且仔细地看看这个访存的latency到底是怎么导致的。<strong>对于一个block而言</strong>，需要计算一个bm∗bn 的矩阵块，这个时候需要进行K次迭代，每次迭代都需要先将来自A和B的两个小块送到shared memory中再进行计算。而从global中访存实际上是非常慢的，所以导致了latency。虽然GPU中可以通过block的切换来掩盖这种latency，但是由于分配的shared memory比较多，活跃的block并不太多，这种延时很难被掩盖。<strong>对于一个thread</strong>，需要计算一个rm∗rn 的小矩阵，但是必须先将数据从shared memory传到寄存器上，才能开始进行计算。所以导致了每进行一次迭代，计算单元就需要停下来等待，计算单元不能被喂饱。</p>
<p>为此，需要进行数据的Prefetch来尽可能地掩盖这种latency。思想也比较简单，需要多开一个buffer，进行读写分离。示意图如下。当block进行第2轮迭代时，需要对A2和B2进行计算，在计算单元进行计算的同时，我们将A3和B3提前放置到shared memory。而后，在进行第3轮迭代时，就可以直接对shared memory中的A3和B3进行计算，而不需要等待从global memory搬运到shared memory的时间。寄存器上的Prefetch也是同理。</p>
<p><img src="/img/image-20220910220136539.png" alt="image-20220910220136539"></p>
<h2 id="GEMM算法概述"><a href="#GEMM算法概述" class="headerlink" title="GEMM算法概述"></a>GEMM算法概述</h2><p>这个章节里主要来说一下GEMM的一个计算流程，其实这一点已经在GEMM优化（一）中提及。但上一篇文章主要说得是原理，关于具体计算逻辑，还是不太直观，所以我们在这里再提一下。然后这个具体的计算逻辑分为两个阶段介绍，分别是不采用数据预取和采用数据预取，这主要是考虑到直接说数据预取，有读者可能会看得云里雾里，比较难受，所以先把不采用数据预取这个内容说明白，然后再来讲这个数据预取。</p>
<h3 id="不采用数据预取"><a href="#不采用数据预取" class="headerlink" title="不采用数据预取"></a>不采用数据预取</h3><p>首先，我们先明确一下GEMM中的具体参数。取bm=128,bn=128,bk=8,rm=8,rn=8。当这几个参数选定之后先来直观地感受一下这几个参数意义，假定给了三个矩阵，A，B，C，其维度都是2048×2048。要求解C=A×B。那么我们需要开启（2048/128）×（2048/128）=<strong>256个block</strong>，每个block里面有（128/8）×（128/8）=<strong>256个线程</strong>，每个线程需要负责计算C矩阵中8×8=64个元素的结果，每个block负责256×64=16384个元素的结果。</p>
<p>明确了上面的参数之后，我们来仔细地观察其中一个block的计算逻辑。对于这个block而言，它需要进行2048/8=256次迭代，我们先把这个迭代称为<strong>大迭代</strong>，每一次大迭代都需要把A里面128×8=1024个元素和B里面8×128=1024个元素先放到shared memory中。然后这个block中的256个线程把结果计算出来。计算完之后，再进入下一次大迭代。不断重复该过程，直至这个block负责的16384个元素的结果被求解出。大迭代示意图如下：</p>
<p><img src="/img/image-20220910220741253.png" alt="image-20220910220741253"></p>
<p>随后再具体看看每一个大迭代中，block中的线程的计算逻辑。在进行一个大迭代时，shared memory中有128×8=1024个A矩阵元素和8×128=1024个B矩阵元素。随后，每个线程需要进行8次迭代，我们把这个迭代成为<strong>小迭代</strong>。bk=8，所以有8次小迭代。每一次小迭代中，每个线程需要从shared memory中拿到A矩阵的一小列和B矩阵的一小行，即8个A的元素和8个B的元素。线程将这8+8=16个元素放置在寄存器中。每个线程需要负责8×8=64个元素的计算，一共会产生64条FFMA指令。小迭代示意图如下：</p>
<p><img src="/img/image-20220910220755159.png" alt="image-20220910220755159"></p>
<p>以上就是不采用数据预取的GEMM算法计算逻辑。总的来说，<strong>对于一个block而言，有256个大迭代，每个大迭代中又有8个小迭代</strong>。这是后续内容的基础，如果还是不太清楚的话，可以再仔细看看，把这个过程完全搞清楚后，我们再继续接下来的内容，即采用数据预取后的GEMM算法计算逻辑。</p>
<h3 id="采用数据预取"><a href="#采用数据预取" class="headerlink" title="采用数据预取"></a>采用数据预取</h3><p>采用数据预取的GEMM计算流程稍有差异。这个差异主要是体现在两个方面，第一个是开启的shared memory和寄存器数量，第二个是需要提前将一些数据放置到shared memory和寄存器中。下面来仔细说说这个流程。</p>
<p>为了实现数据预取，<strong>需要开启两倍的shared memory和寄存器</strong>。当然也可以将原来shared memory切分成两块，也就是将bm×bk和bk×bn的矩阵一分为二。以A中的小矩阵而言，变成了两个bm×bk/2。然后大迭代次数由原来的256变成了512。很多地方把这个技术叫做<strong>双缓冲</strong>，我感觉跟预取是同一个事情。无非是针对参数bk的大小换不同说法。所以在这里统一叫做数据预取。废话说得有点多。总之，我们还是开启两倍的shared memory和寄存器数据。在一个block中，原来在shared memory中需要存储的数据是bm×bk+bk×bn。现在变成了bm×bk×2+bk×bn×2。在一个thread中，为了存储A和B的数据，原来需要使用rm+rn个寄存器，现在需要使用2×(rm+rn)个寄存器。为了后续方便介绍，我们用<strong>read SM</strong>和<strong>write SM</strong>代表用来读写的两块共享内存，并用<strong>read REG</strong>和<strong>write REG</strong>来表示用来读写的两块寄存器。</p>
<p>把共享内存和寄存器的事情说明白之后，我们来看看具体的计算逻辑。在执行256次大迭代之前，我们需要提前将第0次大迭代的数据存到write SM中，并且将第0次小迭代的数据存到write REG中。在完成这一个预取过程之后，我们再来仔细地看看第0个大迭代。需要注意的是，<strong>上一轮大迭代的write SM就是这一轮迭代的read SM。上一轮小迭代的write REG就是这一轮迭代的read REG</strong>。所以在进行第0个大迭代时，上面write SM就变成了read SM。然后我们首先需要将下一轮大迭代的数据存到write SM中。由于从global memory中取数的时钟周期非常多。所以在等待数据取回的同时，对read SM中的数据进行计算。也就是我们在等待的同时，需要开启8次小迭代来进行计算。而小迭代中也存在着读写分离，在对read REG进行计算之前，需要先执行write REG的操作，通过这种方式来<strong>掩盖访存的latency</strong>。所以整体的计算逻辑如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for k in 256 big_loop:</span><br><span class="line">	prefetch next loop data to write_SM</span><br><span class="line">	// compute in read_SM</span><br><span class="line">	for iter in 8 small_loop:</span><br><span class="line">		prefecth next loop data to write_REG</span><br><span class="line">		compute in read_REG</span><br></pre></td></tr></table></figure>
<p><img src="/img/image-20220910220841079.png" alt="image-20220910220841079"></p>
<h3 id="GEMM代码解析"><a href="#GEMM代码解析" class="headerlink" title="GEMM代码解析"></a>GEMM代码解析</h3><p>在上一节中已经将GEMM算法的流程再次回顾了一遍，接下来进入到代码解析环节。这里主要是解析采用了数据预取的GEMM。由于将数据从global memroy中搬运到shared memory中还经过了寄存器，所以对prefetch过程进行了细化，这个跟前面的伪代码稍有差异。</p>
<h4 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h4><p>首先需要说明的是<strong>模板参数</strong>，这也是后续<strong>对GEMM性能进行调参的最主要参数</strong>，往往不同的参数选择对最终的GEMM性能影响极大。后面的实验会展示在不同的参数下的性能比较。前三个参数，BLOCK_SIZE_M、BLOCK_SIZE_K、BLOCK_SIZE_N分别代表上文中的<strong>bm、bk、bn</strong>。中间两个参数，THREAD_SIZE_Y、THREAD_SIZE_X代表上文中的<strong>rm、rn</strong>。最后的参数ENABLE_DOUBLE_BUFFER代表是否采用双缓冲，即是否采用数据预取，在这里，我们只讨论采用数据预取，即开启双缓冲的情况。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">template &lt;</span><br><span class="line">    const int BLOCK_SIZE_M,  // height of block of C that each  block calculate</span><br><span class="line">    const int BLOCK_SIZE_K,  // width of block of A that each  block load into shared memory</span><br><span class="line">    const int BLOCK_SIZE_N,  // width of block of C that each  block calculate</span><br><span class="line">    const int THREAD_SIZE_Y, // height of block of C that each thread calculate</span><br><span class="line">    const int THREAD_SIZE_X,  // width of block of C that each thread calculate</span><br><span class="line">    const bool ENABLE_DOUBLE_BUFFER // whether enable double buffering or not</span><br><span class="line">    &gt;</span><br></pre></td></tr></table></figure>
<p>接下来是<strong>线程类的参数</strong>。整个计算流程需要开启256个block，这256个block按照二维形态排布。而一个block中开启了256个线程，这256个线程按照二维形态进行排布。<strong>bx</strong>代表横向的block坐标，<strong>by</strong>代表竖向的block坐标。而<strong>tx</strong>代表横向的线程坐标，<strong>ty</strong>代表竖向的线程坐标。这是CUDA的基础内容，看不明白的同学可以找一些博客多理解一下，务必搞清楚。<strong>THREAD_X_PER_BLOCK</strong>代表在一个block中有多少个横向的线程，在这里等于16。<strong>THREAD_Y_PER_BLOCK</strong>代表在一个block中有多少个竖向的线程，在这里等于16。<strong>THREAD_NUM_PER_BLOCK</strong>代表在一个block中有多少个线程，在这里等于256。<strong>tid</strong>则代表当前线程在这256个线程中的id号。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// Block index</span><br><span class="line">int bx = blockIdx.x;</span><br><span class="line">int by = blockIdx.y;</span><br><span class="line"></span><br><span class="line">// Thread index</span><br><span class="line">int tx = threadIdx.x;</span><br><span class="line">int ty = threadIdx.y;</span><br><span class="line"></span><br><span class="line">// the threads number in Block of X,Y</span><br><span class="line">const int THREAD_X_PER_BLOCK = BLOCK_SIZE_N / THREAD_SIZE_X;</span><br><span class="line">const int THREAD_Y_PER_BLOCK = BLOCK_SIZE_M / THREAD_SIZE_Y;</span><br><span class="line">const int THREAD_NUM_PER_BLOCK = THREAD_X_PER_BLOCK * THREAD_Y_PER_BLOCK;</span><br><span class="line"></span><br><span class="line">// thread id in cur Block</span><br><span class="line">const int tid = ty * THREAD_X_PER_BLOCK + tx;</span><br></pre></td></tr></table></figure>
<p>随后说明开启的<strong>shared memory和register</strong>数量。<strong>As</strong>代表为了存储A矩阵中的数据所需要开启的shared memory。在一轮迭代中需要使用bm×bk的数据，为了加快后续的访存，所以需要进行一次转置。并且为了预取，开了两倍的大小，一半用来读数据，一半用来写数据。所以一共需要2×BLOCK_SIZE_K×BLOCK_SIZE_M的空间。而<strong>Bs</strong>同理，但是载入数据时并不需要转置。<strong>accum</strong>用来临时存储C的计算结果。<strong>frag_a</strong>用来加载<strong>As</strong>中的<strong>rm</strong>个数据，为了预取也开启了双倍的空间。<strong>frag_b</strong>同理。<strong>ldg_num_a</strong>稍微有点费解，需要解释一下。为了将global memory的数据块搬运到shared memory中，需要先经过寄存器。也就是说，这个数据搬运过程其实是global memory-&gt;register-&gt;shared memory。所以为了临时存储A中的数据，需要开启一定量的寄存器。在一次大迭代中，我们总共需要搬运<strong>BLOCK_SIZE_M × BLOCK_SIZE_K</strong>个float数据，然后一个block中有<strong>THREAD_NUM_PER_BLOCK</strong>个线程，采用float4进行取数，即一个线程一次取4个数。则一共需要BLOCK_SIZE_M × BLOCK_SIZE_K/(THREAD_NUM_PER_BLOCK×4)次搬运就能把所有的数搬运到寄存器上。这个搬运次数用<strong>ldg_num_a</strong>表示。为了存储BLOCK_SIZE_M <em> BLOCK_SIZE_K的数据块，每个线程需要额外开启<em>*ldg_a_reg</em></em>个寄存器进行存储。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// shared memory</span></span><br><span class="line">__shared__ <span class="type">float</span> As[<span class="number">2</span>][BLOCK_SIZE_K][BLOCK_SIZE_M];</span><br><span class="line">__shared__ <span class="type">float</span> Bs[<span class="number">2</span>][BLOCK_SIZE_K][BLOCK_SIZE_N];</span><br><span class="line"><span class="comment">// registers for C</span></span><br><span class="line"><span class="type">float</span> accum[THREAD_SIZE_Y][THREAD_SIZE_X] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="comment">// registers for A and B</span></span><br><span class="line"><span class="type">float</span> frag_a[<span class="number">2</span>][THREAD_SIZE_Y];</span><br><span class="line"><span class="type">float</span> frag_b[<span class="number">2</span>][THREAD_SIZE_X];</span><br><span class="line"><span class="comment">// registers load global memory</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> ldg_num_a = BLOCK_SIZE_M * BLOCK_SIZE_K / (THREAD_NUM_PER_BLOCK * <span class="number">4</span>);</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> ldg_num_b = BLOCK_SIZE_K * BLOCK_SIZE_N / (THREAD_NUM_PER_BLOCK * <span class="number">4</span>);</span><br><span class="line"><span class="type">float</span> ldg_a_reg[<span class="number">4</span>*ldg_num_a];</span><br><span class="line"><span class="type">float</span> ldg_b_reg[<span class="number">4</span>*ldg_num_b];</span><br></pre></td></tr></table></figure>
<p>最后需要说明的参数是在<code>global-&gt;shared memory</code>阶段用到。我们开启了256个线程，在一次大迭代中需要将128×8个元素搬运到shared memory中。我们用下面的参数说明了这个搬运的逻辑。<strong>A_TILE_THREAD_PER_ROW</strong>代表把搬运一行数据需要使用多少个线程，为了搬运A的一行，需要使用2个线程。</p>
<p><strong>A_TILE_ROW_START</strong>代表在这个维度为bm×bk的数据块中，当前线程需要搬运的数据的竖向坐标，而<strong>A_TILE_COL</strong>代表需要搬运的数据的横向坐标。对3号线程而言，由于它要搬运（1，1）号数据块中的4个元素。所以，A_TILE_ROW_START是1，A_TILE_COL是4。<strong>A_TILE_ROW_STRIDE</strong>代表在进行多次搬运时需要跨越的行。假设As是一块256×8的数据块（这个设置跟前面不一样），256个线程进行搬运，一次搬运4个数，所以要搬运两次。对于3号线程而言，分别搬运下图中的绿色数据块。</p>
<p><img src="/img/image-20220910221016698.png" alt="image-20220910221016698"></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// threads number in one row</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> A_TILE_THREAD_PER_ROW = BLOCK_SIZE_K / <span class="number">4</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> B_TILE_THREAD_PER_ROW = BLOCK_SIZE_N / <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// row number and col number that needs to be loaded by this thread</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> A_TILE_ROW_START = tid / A_TILE_THREAD_PER_ROW;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> B_TILE_ROW_START = tid / B_TILE_THREAD_PER_ROW;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> A_TILE_COL = tid % A_TILE_THREAD_PER_ROW * <span class="number">4</span>; </span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> B_TILE_COL = tid % B_TILE_THREAD_PER_ROW * <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// row stride that thread uses to load multiple rows of a tile</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> A_TILE_ROW_STRIDE = THREAD_NUM_PER_BLOCK / A_TILE_THREAD_PER_ROW;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> B_TILE_ROW_STRIDE = THREAD_NUM_PER_BLOCK / B_TILE_THREAD_PER_ROW;</span><br></pre></td></tr></table></figure>
<h4 id="大迭代前预取数据"><a href="#大迭代前预取数据" class="headerlink" title="大迭代前预取数据"></a>大迭代前预取数据</h4><p>在介绍完相关参数之后，我们来进入到具体的代码逻辑。为了代码简洁，用float4读取的过程用了两个宏，定义如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> OFFSET(row, col, ld) ((row) * (ld) + (col))</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> FETCH_FLOAT4(pointer) (reinterpret_cast<span class="string">&lt;float4*&gt;</span>(&amp;(pointer))[0])</span></span><br></pre></td></tr></table></figure>
<p>迭代前预取数据分为<strong>两个部分</strong>，<strong>第一个部分</strong>是将第一个大迭代的数据从global 预取到shared memroy中。<strong>第二个部分</strong>是将shared memory上的数据预取到寄存器中。先来看看<strong>第一个部分</strong>。这里面分别是将第一个大迭代中需要的A、B数据预取到shared memroy中。对于A矩阵而言，这个for循环代表着block中的线程需要搬运多少次才能将globa中的数据放到shared memory中。由于A需要先进行一次转置，所以先将数据先放置在寄存器中。数据按行取，然后按列存。对于B矩阵而言，数据不用转置，直接按行取，按行存。当然，这个过程中间也要经过寄存器，但是没有写出来的必要了。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// load A from global memory to shared memory</span></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="type">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_M ; i += A_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        <span class="type">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(ldg_a_reg[ldg_index]) = <span class="built_in">FETCH_FLOAT4</span>(A[<span class="built_in">OFFSET</span>(</span><br><span class="line">            BLOCK_SIZE_M * by + A_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">            A_TILE_COL, <span class="comment">// col</span></span><br><span class="line">            K )]);</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index];</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL<span class="number">+1</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index<span class="number">+1</span>];</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL<span class="number">+2</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index<span class="number">+2</span>];</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL<span class="number">+3</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index<span class="number">+3</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// load B from global memory to shared memory</span></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="type">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_K; i += B_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(Bs[<span class="number">0</span>][B_TILE_ROW_START + i][B_TILE_COL]) = <span class="built_in">FETCH_FLOAT4</span>(B[<span class="built_in">OFFSET</span>(</span><br><span class="line">                B_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">                B_TILE_COL + BLOCK_SIZE_N * bx, <span class="comment">// col</span></span><br><span class="line">                N )]);</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br></pre></td></tr></table></figure>
<p>然后就是<strong>第二个部分</strong>。将shared memory中的数据存到寄存器中。一共需要取THREAD_SIZE_Y个数，每次取4个数。这个倒没有什么好说的。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// load A from shared memory to register</span></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; thread_y += <span class="number">4</span>) &#123;</span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(frag_a[<span class="number">0</span>][thread_y]) = <span class="built_in">FETCH_FLOAT4</span>(As[<span class="number">0</span>][<span class="number">0</span>][THREAD_SIZE_Y * ty + thread_y]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// load B from shared memory to register</span></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; thread_x += <span class="number">4</span>) &#123;</span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(frag_b[<span class="number">0</span>][thread_x]) = <span class="built_in">FETCH_FLOAT4</span>(Bs[<span class="number">0</span>][<span class="number">0</span>][THREAD_SIZE_X * tx + thread_x]);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="大迭代逻辑"><a href="#大迭代逻辑" class="headerlink" title="大迭代逻辑"></a>大迭代逻辑</h4><p>在完成上一步后，我们要进入到<strong>大迭代</strong>中，按照前面的参数，我们需要<strong>进行256个大迭代</strong>。先忽略这个迭代里面的具体代码，看看这个框架，如下所示。首先要说的是<strong>write_stage_idx</strong>这个参数。之前定义了<strong>shared</strong> float As[2][BLOCK_SIZE_K][BLOCK_SIZE_M]。为了读写分离，给As开了两块空间。如果write_stage_idx=1，就对As[1]空间进行写操作，对As[0]空间进行读操作。因为我们之前将数据预取到了As[0]这个空间里，所以在第一个大迭代时，对As[0]进行读操作，对As[1]进行写操作，所以write_stage_idx=1。再来看看<strong>tile_idx</strong>这个参数，这个代表大迭代时，在A矩阵的列号。每一次大迭代要读取BLOCK_SIZE_K列，直到完成大迭代，即tile_idx=K为止。再看看循环里面的load_stage_idx，这个和write_stage_idx对应，两者保持二进制位相反即可。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> write_stage_idx = <span class="number">1</span>;</span><br><span class="line"><span class="type">int</span> tile_idx = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">do</span>&#123;</span><br><span class="line">    tile_idx += BLOCK_SIZE_K;</span><br><span class="line">    <span class="type">int</span> load_stage_idx = write_stage_idx ^ <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// compute</span></span><br><span class="line">    <span class="keyword">if</span>(tile_idx &lt; K)&#123;</span><br><span class="line">        write_stage_idx ^= <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;<span class="keyword">while</span>(tile_idx&lt; K);</span><br></pre></td></tr></table></figure>
<h4 id="大迭代详细解析"><a href="#大迭代详细解析" class="headerlink" title="大迭代详细解析"></a>大迭代详细解析</h4><p>我们在这里开始说明具体的大迭代。下面代码描述的是，如果还有下一个迭代，则将下一个迭代的数据块，搬运到寄存器上，这里面的for循环代表可能需要多次搬运。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">tile_idx += BLOCK_SIZE_K;</span><br><span class="line"><span class="comment">// load next tile from global mem</span></span><br><span class="line"><span class="keyword">if</span>(tile_idx&lt; K)&#123;</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="type">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_M ; i += A_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        <span class="type">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(ldg_a_reg[ldg_index]) = <span class="built_in">FETCH_FLOAT4</span>(A[<span class="built_in">OFFSET</span>(</span><br><span class="line">            BLOCK_SIZE_M * by + A_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">            A_TILE_COL + tile_idx, <span class="comment">// col</span></span><br><span class="line">            K )]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="type">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_K; i += B_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        <span class="type">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(ldg_b_reg[ldg_index]) = <span class="built_in">FETCH_FLOAT4</span>(B[<span class="built_in">OFFSET</span>(</span><br><span class="line">            tile_idx + B_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">            B_TILE_COL + BLOCK_SIZE_N * bx, <span class="comment">// col</span></span><br><span class="line">            N )]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>随后进入到小迭代的计算逻辑之中，<strong>load_stage_idx</strong>参数代表需要从As的哪个空间进行读数。然后是<strong>BLOCK_SIZE_K-1</strong>次小迭代。按照前面的参数配置，即需要在这里完成<strong>7次小迭代</strong>。由于在小迭代中也采用了双缓冲的方式，需要将下一轮小迭代的数据提前写入到寄存器中，这个过程需要对shared memory访存，会稍微慢点。与此同时，线程需要计算更新<strong>THREAD_SIZE_X x THREAD_SIZE_Y=8×8=64</strong>个C矩阵元素的结果。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> load_stage_idx = write_stage_idx ^ <span class="number">1</span>;</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>; j&lt;BLOCK_SIZE_K<span class="number">-1</span>; ++j)&#123;</span><br><span class="line">    <span class="comment">// load next tile from shared mem to register </span></span><br><span class="line">    <span class="comment">// load A from shared memory to register</span></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; thread_y += <span class="number">4</span>) &#123;</span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(frag_a[(j<span class="number">+1</span>)%<span class="number">2</span>][thread_y]) = <span class="built_in">FETCH_FLOAT4</span>(As[load_stage_idx][j<span class="number">+1</span>][THREAD_SIZE_Y * ty + thread_y]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// load B from shared memory to register</span></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; thread_x += <span class="number">4</span>) &#123;</span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(frag_b[(j<span class="number">+1</span>)%<span class="number">2</span>][thread_x]) = <span class="built_in">FETCH_FLOAT4</span>(Bs[load_stage_idx][j<span class="number">+1</span>][THREAD_SIZE_X * tx + thread_x]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// compute C THREAD_SIZE_X x THREAD_SIZE_Y</span></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; ++thread_y) &#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; ++thread_x) &#123;</span><br><span class="line">            accum[thread_y][thread_x] += frag_a[j%<span class="number">2</span>][thread_y] * frag_b[j%<span class="number">2</span>][thread_x];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而后需要将存储在<strong>临时寄存器</strong>的数据搬运到shared memory中。由于A矩阵需要经过一次转置，所以和B矩阵有一点不一样。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(tile_idx &lt; K)&#123;</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="type">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_M ; i += A_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        <span class="type">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">        As[write_stage_idx][A_TILE_COL][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index];</span><br><span class="line">        As[write_stage_idx][A_TILE_COL<span class="number">+1</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index<span class="number">+1</span>];</span><br><span class="line">        As[write_stage_idx][A_TILE_COL<span class="number">+2</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index<span class="number">+2</span>];</span><br><span class="line">        As[write_stage_idx][A_TILE_COL<span class="number">+3</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index<span class="number">+3</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// load B from global memory to shared memory</span></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="type">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_K; i += B_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        <span class="type">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(Bs[write_stage_idx][B_TILE_ROW_START + i][B_TILE_COL]) = <span class="built_in">FETCH_FLOAT4</span>(ldg_b_reg[ldg_index]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// use double buffer, only need one sync</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">    <span class="comment">// switch</span></span><br><span class="line">    write_stage_idx ^= <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后完成<strong>寄存器的预取</strong>，并将<strong>最后一个小迭代完成</strong>。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// load A from shared memory to register</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; thread_y += <span class="number">4</span>) &#123;</span><br><span class="line">    <span class="built_in">FETCH_FLOAT4</span>(frag_a[<span class="number">0</span>][thread_y]) = <span class="built_in">FETCH_FLOAT4</span>(As[load_stage_idx^<span class="number">1</span>][<span class="number">0</span>][THREAD_SIZE_Y * ty + thread_y]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// load B from shared memory to register</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; thread_x += <span class="number">4</span>) &#123;</span><br><span class="line">    <span class="built_in">FETCH_FLOAT4</span>(frag_b[<span class="number">0</span>][thread_x]) = <span class="built_in">FETCH_FLOAT4</span>(Bs[load_stage_idx^<span class="number">1</span>][<span class="number">0</span>][THREAD_SIZE_X * tx + thread_x]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//compute last tile mma THREAD_SIZE_X x THREAD_SIZE_Y</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; ++thread_y) &#123;</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; ++thread_x) &#123;</span><br><span class="line">        accum[thread_y][thread_x] += frag_a[<span class="number">1</span>][thread_y] * frag_b[<span class="number">1</span>][thread_x];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="计算结果写回"><a href="#计算结果写回" class="headerlink" title="计算结果写回"></a>计算结果写回</h4><p>此时，最后的计算结果已经被存储在<code>accum</code>寄存器中，需要将其写回到global memory中。这个代码比较简单，就没啥好说的了。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// store back to C</span></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; ++thread_y) &#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; thread_x+=<span class="number">4</span>) &#123;</span><br><span class="line">            <span class="built_in">FETCH_FLOAT4</span>(C[<span class="built_in">OFFSET</span>(</span><br><span class="line">                BLOCK_SIZE_M * by + ty * THREAD_SIZE_Y + thread_y,</span><br><span class="line">                BLOCK_SIZE_N * bx + tx * THREAD_SIZE_X + thread_x,</span><br><span class="line">                N)]) = <span class="built_in">FETCH_FLOAT4</span>(accum[thread_y][thread_x]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>针对GEMM性能优化，我做了一些实验，主要是想要说明这么两个问题：</p>
<ol>
<li>在<strong>不采用任何汇编的情况</strong>下，手写CUDA代码会比cublas差多少？</li>
<li>bm、bn、bk、rm、rn等相关<strong>参数对GEMM的性能表现</strong>有多大影响？</li>
</ol>
<p><strong>针对第一个问题</strong>，固定了bm、bn、bk、rm、rn的取值为64、8、64、8、8。在V100上测试了不同维度的矩阵（设置M=N=K），并且对比了cublas，其性能结果如下图。横坐标是矩阵维度，纵坐标是GFLOPS。可以在图中看出，在大维度的矩阵下，我们手写的Sgemm大概能达到平均14TFLOPS，<strong>性能表现达到cublas的 91%</strong>。V100的单精度峰值性能是15.7TFLOPS，在<strong>完全不使用汇编，并且有着较好的代码可读性的同时</strong>，我们手写的Sgemm大概能达到<strong>90%</strong>的单精度峰值效率。当然，<strong>如果不考虑代码可读性的话，这个性能可以进一步提高</strong>。在这里可以得出结论，其实也是想消除大家的一个误解。<strong>很多人觉得只有写汇编才能写出高性能的代码</strong>。其实并不是这样，<strong>性能优化中最重要的是并行算法和优化策略，单纯地将代码写成汇编并不会有多少性能提升。</strong></p>
<p><img src="/img/image-20220910221600420.png" alt="image-20220910221600420"></p>
<h3 id="从汇编代码分析程序性能"><a href="#从汇编代码分析程序性能" class="headerlink" title="从汇编代码分析程序性能"></a>从汇编代码分析程序性能</h3><p>我们为什么要去看生成的汇编代码？这主要是由于做完优化之后，我们需要有一个东西来判断机器是否能够真正地按照我们设想的模式运行。使用了float4之后，GPU是不是真的使用了向量化指令。采用循环展开之后，GPU是不是真的会进行展开？另外，CUDA C和汇编代码之间还隔着编译器。只有看最底层的汇编码，才能真正地理解我们所做的优化是在哪个地方起了作用，节省了哪个部分的耗时。</p>
<p>NV的GPU提供了ptx和sass两个层面的汇编码。Ptx本质上是一个伪汇编码，事实上机器真正能够识别的是sass码。Ptx还需要使用ptxas工具再转化成sass码才能被GPU识别。然后nv提供了cuobjdump和nvdisasm两个工具，我们可以通过这两个工具来看到最底层的汇编码。</p>
<p>NV每一代机器的指令集都有所不同。此外，NV的指令还有一个特别有意思的东西，那就是control code，后面直接用控制码表示。通过控制码将一些本来应该在硬件实现的逻辑软件化了，从而在同样大小的电路面积上塞下更大的计算单元。</p>
<p>当我们在看汇编代码的时候，我们到底看的是什么东西。这个话题可以分为两部分介绍，分别是访存密集型的kernel和计算密集型的kernel。</p>
<p>对于<strong>访存密集型的kernel</strong>，正常而言，我们需要关注的是：访问global memory的时候是不是合并访存了，访问shared memory的时候是不是有bank 冲突了。很不幸的是，在汇编代码中，这些东西其实不太能看得出来。我们主要关注的是有没有采用LDG.128的访存指令，以及计算指令的占比是不是太多，#pragma unroll是不是有效展开了。</p>
<p>对于<strong>计算密集型的kernel</strong>而言，我们重点关注计算指令的占比。这个一般跟并行策略会联系在一起。一般而言，如果并行策略不太行，那么计算指令的占比会很低，这样的话，访存所导致的latency很难被计算指令掩盖，计算效率会非常差。如果并行策略比较好，那么计算指令的占比也会非常地高。也只有当计算指令占比非常高的时候，才有可能地去逼近峰值性能。</p>
<h3 id="对于现有sgemm的代码分析及观察"><a href="#对于现有sgemm的代码分析及观察" class="headerlink" title="对于现有sgemm的代码分析及观察"></a>对于现有sgemm的代码分析及观察</h3><p>在分析之前，我们对目前已有的工作先做一个回顾。sgemm是hpc领域的经典问题，目前有大量的论文在针对不同硬件架构，不同矩阵特性进行研究。对于NV的GPU，关于sgemm最著名的工作是scott的<a href="https://link.zhihu.com/?target=https%3A//github.com/NervanaSystems/maxas/wiki/SGEMM">maxas</a>。在Maxwell架构上的部分卡上能够达到98%的浮点性能，几乎到达极限。也就是从这个工作以后，针对NV的sgemm优化工作基本上就没法做了，关于针对大矩阵的sgemm优化，也没有太多的研究价值了。当然，针对不同硬件架构的sgemm优化还是层出不出，但基本上是一些follow的工作，然后做一些小修小补。</p>
<p>我们来分析一下scott的工作。在CUDA C层面，不涉及汇编的话，优化技巧主要有3个方面：</p>
<p>技巧1，global-&gt;shared memory，采用了texture内存，将线程划分，一半线程只读A，一半线程只读B。</p>
<p>技巧2，shared memory-&gt;register，将8×8的读取变成4个4×4的读取，从而避免bank冲突。</p>
<p><img src="/img/image-20220910221941317.png" alt="image-20220910221941317"></p>
<p>技巧3，Store C矩阵的时候，为了合并访存，采用了一种非常奇怪的方式去store。</p>
<p><img src="/img/image-20220910221952729.png" alt="image-20220910221952729"></p>
<p>针对大矩阵的sgemm计算时。如果k维度足够大，global-&gt;shared memory以及store C的耗时占比会非常小，所以这两个优化技巧在大矩阵中并不能起到很大的作用。所以相对来说，<strong>技巧2会更加具有借鉴意义</strong>。</p>
<p>紧接着，我们来分析一下sgemm中最耗时的部分，也就是最内层的迭代部分。需要计算8×8×8=512次乘加运算。Scott的sgemm在maxwell产生的汇编代码如下图左，为了比较，我们将GEMM（二）中的代码sgemm_v2最后生成的<a href="https://link.zhihu.com/?target=https%3A//github.com/Liu-xiandong/How_to_optimize_in_GPU/blob/master/sgemm/asm/sgemm_pre.sm_70.cuasm">SASS码</a>放在一起用以比较。</p>
<p><img src="/img/image-20220910222351625.png" alt="image-20220910222351625"></p>
<p>可以从上面看到，512条FFMA和32条LDS指令，最核心的计算指令和访存指令都是一样的。但是GEMM（二）中用编译器产生的汇编码有更多的非计算指令存在。而且如果从上面的链接点进去的话，就会发现，FFMA指令被划到2个代码块中，相对而言，中间会多一个跳转指令。另外一个需要注意的点是scott的代码是针对Maxwell架构，所以将可以用于双发射的指令进行了单独标记。而笔者写的代码是在volta架构上编译运行的，volta架构取消了双发射。但是两个cycle发射一条FFMA指令就可以将所有的fp32 core填满。计算指令和访存指令占据不同的发射端口，计算和访存可以隔一个cycle发射。所以我的猜想是这样的，对于volta架构，t0 cycle的时候发射一条FFMA指令，t1 cycle的时候发射一条LDS指令，而后t2时刻再发射一条FFMA指令。这样的话，FFMA指令隔了2个cycle，中间还发射了一条LDS指令，但fp32的core依旧是被用满的状态。这样的话，即使没有了双发射，理论上也能将fp32 core打满。从volta架构编译出来的控制码中也可以看出一些端倪，如下，FFMA指令stall两个cycle，而LDS指令stall一个cycle。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[R---:B------:R-:W-:-:S02]         /*0cd0*/                   FFMA R115, R39.reuse, R14, R115 ;</span><br><span class="line">[----:B------:R-:W-:-:S02]         /*0ce0*/                   FFMA R114, R39, R15, R114 ;</span><br><span class="line">[----:B------:R-:W1:-:S01]         /*0cf0*/                   LDS.U.128 R36, [R40+0x2410] ;</span><br><span class="line">[R---:B------:R-:W-:-:S02]         /*0d00*/                   FFMA R113, R32.reuse, R12, R113 ;</span><br></pre></td></tr></table></figure>
<p>然后总结一下这小节的内容，从CUDA C和SASS代码的角度分析了现有sgemm实现的不足。进一步的优化工作可以从两个方面进行：1、shared memory-&gt;register，将8×8的读取变成4个4×4的读取。2、尽可能地减少非必要指令的开销，但是这个在CUDA C层面很难控制，毕竟编译器也没那么听话。</p>
<h3 id="汇编级别代码调整"><a href="#汇编级别代码调整" class="headerlink" title="汇编级别代码调整"></a>汇编级别代码调整</h3><p>好了，终于讲到了调汇编的地方。上面小节说了，优化的一个方式是尽可能地减少非必要指令的开销。但是，当我们开始调汇编的时候，还有一个更重要的事情需要做，也是在maxas、KeplerAs等一系列工作的核心，<strong>减少FFMA指令所产生的register bank冲突</strong>。这里面有两个优化技巧，一个是寄存器的重映射，另外一个是调整FFMA顺序，尽可能地在指令中使用.reuse标识以及提高双发射的效率。</p>
<h4 id="寄存器的重映射"><a href="#寄存器的重映射" class="headerlink" title="寄存器的重映射"></a>寄存器的重映射</h4><p>在这里面，由于每代架构中的硬件细节有所不同，所以register的remapping细节也有所不同。首先说一下这里面的硬件细节不同是指，不同的架构中，寄存器到bank的映射方式不同。kepler架构的映射比较奇怪，并不是很规则，如下：</p>
<p><img src="/img/image-20220910222431933.png" alt="image-20220910222431933"></p>
<p>对于Maxwell架构而言，相对来说更加简单一些，bank index即reg_index%4这么一个简单的关系。Pascal架构和Maxwell架构的寄存器bank映射关系一样。而volta架构又有一些不同，在volta之前都是4路的bank，而volta架构变成了2路的bank。</p>
<p>由于架构不一样，针对不同架构的register重映射方式也不一样。对于kepler架构，keplerAs的作者采用的映射方式如下：</p>
<p><img src="/img/image-20220910222443696.png" alt="image-20220910222443696"></p>
<p>对于Maxwell架构，Scott采用的映射方式如下：</p>
<p><img src="/img/image-20220910222453184.png" alt="image-20220910222453184"></p>
<p>上图中间那些带黑框的数字代表不可避免的寄存器冲突，scott随后又使用了指令重排来减缓寄存器的冲突。</p>
<p>而volta架构的话，Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking作者采用的方式如下，作了一个转置，然后相邻两行进行一个交换。</p>
<p><img src="/img/image-20220910222504575.png" alt="image-20220910222504575"></p>
<h4 id="指令重排"><a href="#指令重排" class="headerlink" title="指令重排"></a>指令重排</h4><p>这里的指令重排主要是针对FFMA指令的重排。作用的话，其实有两个。在maxwell架构中，scott重排主要是为了尽可能地解决对角线那些元素的寄存器bank冲突。在这里插一嘴，因为部分读者对于这个重排可能理解不是很到位。举个例子吧，要计算C矩阵中1，2，3，4，5的元素的值，正常的顺序是调用FFMA指令先算1，再算2，再算3，等等。重排的话，就是可能先算2，再算1，再算3。从指令角度的话，就是FFMA指令的排列顺序有所不同，所以叫指令重排，这个是我的个人理解。</p>
<p>重排的目的是为了更好地使用reuse标识，这个地方可以看看旷视写的<a href="https://zhuanlan.zhihu.com/p/410278370">矩阵乘终极优化指南</a>，当然，基本上也就是scott的sgemm介绍内容。读取指令的操作数的时候，有一个寄存器的reuse cache。在指令中使用这个标识就代表这个数被hold住了，下一条指令可以直接使用。这个地方，大家都是这么说的，NV也没有官方的说明，那就这么理解吧。具体示意代码如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FFMA R2, R64.reuse, R73, R2; ## R64 进入 Reuse Cache</span><br><span class="line">FFMA R3, R64.reuse, R72, R3; ## R64 从 Reuse Cache 中获取，避免与 R72 冲突</span><br></pre></td></tr></table></figure>
<p>为了更好地利用这个reuse特性，scott给了一种非常奇怪的指令排列顺序，如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> 1,  0,  2,  3,  5,  4,  6,  7, 33, 32, 34, 35, 37, 36, 38, 39, </span><br><span class="line">45, 44, 46, 47, 41, 40, 42, 43, 13, 12, 14, 15,  9,  8, 10, 11,</span><br><span class="line">17, 16, 18, 19, 21, 20, 22, 23, 49, 48, 50, 51, 53, 52, 54, 55, </span><br><span class="line">61, 60, 62, 63, 57, 56, 58, 59, 29, 28, 30, 31, 25, 24, 26, 27</span><br></pre></td></tr></table></figure>
<p>通过CUDA C说的一系列优化手段，以及寄存器的remapping和指令重排，scott的sgemm在Maxwell架构的一些卡上能够达到98%的浮点计算效率，达到了优化的天花板。</p>
<p>扯远了，再说说指令重排，keplerAs的作者张秀霞针对kepler的双发射特性对FFMA指令进行了指令重排来提高性能。这个跟scott的工作又有一些不一样的地方，大家可以对比一下。</p>
<h3 id="实验与总结"><a href="#实验与总结" class="headerlink" title="实验与总结"></a>实验与总结</h3><p>最后，我们来做一下实验。实验分成两个部分，第一个部分是CUDA C层面的再次优化，第二个部分是针对SASS代码的调优工作以及中间经历的一些波折。</p>
<h4 id="CUDA-C-调优"><a href="#CUDA-C-调优" class="headerlink" title="CUDA C 调优"></a>CUDA C 调优</h4><p>这个部分的内容主要是介绍一下怎么解决GEMM（二）所存在的shared memory bank冲突。其实scott的文章已经说了这一点，但是吧，实在是太费解了。首先，再来回顾一下这个思路。我们一个block有256个线程，8个warp，8个warp要去取shared memory中的半行元素，也就是128/2=64个元素。warp0和warp4取得是同样的16个元素。而warp里面，线程0、2、4、6、8、10、12、14是取得同样的4个元素。由于取得是同样的元素，同一个bank触发多播的机制，没有冲突。取多少元素说清楚了，就得说一下shared memory的索引了。scott给出的256线程版本索引是：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">readAs = ((tid128 &gt;&gt; <span class="number">4</span>) | ((tid &gt;&gt; <span class="number">1</span>) &amp; <span class="number">7</span>)) &lt;&lt; <span class="number">4</span>;</span><br><span class="line">readBs  = (((tid &amp; <span class="number">0x70</span>) &gt;&gt; <span class="number">3</span>) | (tid &amp; <span class="number">1</span>)) &lt;&lt; <span class="number">4</span> + <span class="number">4096</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/img/image-20220910222717437.png" alt="image-20220910222717437"></p>
<p>总之，这个索引给我整不会了。作为一个正常的人类，我实在是不太能直观地去理解这个位运算。思量许久，我决定用一种最简单粗暴的索引计算方式。我们本质上是要知道，每一个线程，对应到128个元素中的哪一个元素？这个是我们的核心问题。</p>
<p>我来说一下我的计算方法，以B矩阵对应的shared memory为例，首先，计算warp_id，也就是当前线程属于哪个warp，由tid/32即可得。随后计算lane_id，即当前线程属于这个warp上得哪个线程，由tid%32即可得。随后就是通过warp_id和lane_id来算出，对应128个元素得哪一个元素。先算(warp_id%4)×16，假设是warp2，就是上图左侧的第2个（从0算）warp。前面有2个warp，跳过了2*16=32个元素。然后再看看当前lane_id。0-15在左半边，16-31在右半边。所以lane_id/16，先看是左半边还是右半边。右半边的话，先跳过8个元素。最后再看lane_id的奇偶数，如果奇数的话，就再跳一个四个元素。代码实现如下，这个就是正常人可以看懂的方式了。对A矩阵的映射关系同理。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//load index of the tile</span></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> warp_id = tid / <span class="number">32</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> lane_id = tid % <span class="number">32</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> tile_index_b = (warp_id%<span class="number">4</span>)*<span class="number">16</span> + (lane_id/<span class="number">16</span>)*<span class="number">8</span> + (lane_id%<span class="number">2</span>)*<span class="number">4</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> tile_index_a = (warp_id/<span class="number">4</span>)*<span class="number">32</span> + ((lane_id%<span class="number">16</span>)/<span class="number">2</span>)*<span class="number">4</span>;</span><br></pre></td></tr></table></figure>
<p>然后shared memory取数的代码更改就是下面这样，以B矩阵块为例：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 改变前</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; thread_y += <span class="number">4</span>) &#123;</span><br><span class="line">     <span class="built_in">FETCH_FLOAT4</span>(frag_b[(j<span class="number">+1</span>)%<span class="number">2</span>][thread_y]) = <span class="built_in">FETCH_FLOAT4</span>(Bs[next_stage_flag][(j<span class="number">+1</span>)%BLOCK_SIZE_K][THREAD_SIZE_Y * ty + thread_y]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 改变后</span></span><br><span class="line"><span class="built_in">FETCH_FLOAT4</span>(frag_b[(j<span class="number">+1</span>)%<span class="number">2</span>][<span class="number">0</span>]) = <span class="built_in">FETCH_FLOAT4</span>(Bs[next_stage_flag][(j<span class="number">+1</span>)%BLOCK_SIZE_K][tile_index]);</span><br><span class="line"><span class="built_in">FETCH_FLOAT4</span>(frag_b[(j<span class="number">+1</span>)%<span class="number">2</span>][<span class="number">4</span>]) = <span class="built_in">FETCH_FLOAT4</span>(Bs[next_stage_flag][(j<span class="number">+1</span>)%BLOCK_SIZE_K][tile_index + <span class="number">64</span>]);</span><br></pre></td></tr></table></figure>
<p>当然，因为用来寄存C的64个元素对应的位置变化，所以最后的store C的过程也有代码变动。</p>
<p>在进行了这个修改之后，4096（M=N=K）的矩阵大概可以达到96-97%的cublas的性能。单精度峰值浮点效率达93%左右。再往下想要持平或者超越cublas的话，就只能动汇编了。</p>
<h4 id="汇编代码调优"><a href="#汇编代码调优" class="headerlink" title="汇编代码调优"></a>汇编代码调优</h4><p>在做寄存器remapping的时候，发现NVCC编译出来的代码是这个样子：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FFMA R125, R52, R44, R72 ;</span><br><span class="line">FFMA R122, R53, R44.reuse, R73 ;</span><br><span class="line">FFMA R74, R54, R44.reuse, R74 ;</span><br><span class="line">FFMA R75, R55, R44.reuse, R75 ;</span><br></pre></td></tr></table></figure>
<p>看看第一条指令，做R125=R52×R44+R72，R72的值被拿出来，然后存到了R125上。编译出来的代码有一大堆这样的指令。而我希望所有的指令都满足第3条的样子，R74=R54×R44+R74，从R74取就放回R74才最好。如果不能保证这个形式的话，就意味着，我们不能让固定的寄存器来存储矩阵C中的固定的值。这玩意做remapping的话，就不能简简单单地改寄存器号。毕竟我也不能确定不同的寄存器对应到哪个具体的值了。</p>
<p>当时想了各种方式，调整CUDA C代码来让nvcc编译出我想要的FFMA格式，但是，这个尝试并不能实现。所以接下来，有两个方式，一个是头铁，搞清楚这个100多个寄存器在512条FFMA指令中对应的物理元素，然后做remapping，这个路线中间会遇到可以预想的无数的bug和计算问题。另一个是参考Maxas，把这玩意整合到汇编器上，定义好每个寄存器的对应元素和排列顺序。然后汇编器顺带着处理，如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&lt;REGISTER_MAPPING&gt;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Temporary registers to calculate the state registers. Reuse the C output registers.</span></span><br><span class="line">    <span class="comment">// These can be dynamically allocated (~) in the available registger space to elimiate any register bank conflicts.</span></span><br><span class="line">    <span class="number">0</span><span class="number">-63</span>    ~ blk, ldx, ldx2, ldx4, k, tid1, tid4, tid7, tid31_4, xmad_t0, xmad_end, bxOrig, byOrig, loy</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Aliases for the C registers we use for initializing C (used as vectors)</span></span><br><span class="line">    <span class="number">0</span><span class="number">-63</span>    : cz&lt;<span class="number">00</span><span class="number">-63</span>&gt;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The offset we store our zero value for initializing C. Reuse a register from the second blocking registers</span></span><br><span class="line">    <span class="number">80</span>      : zOffset</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 64 C maxtrix output registers.</span></span><br><span class="line">    <span class="comment">// Use special mapping to avoid register bank conflicts between these registers and the blocking registers.</span></span><br><span class="line">     <span class="number">3</span>, <span class="number">2</span>,<span class="number">11</span>,<span class="number">10</span>,<span class="number">19</span>,<span class="number">18</span>,<span class="number">27</span>,<span class="number">26</span> : cx00y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">     <span class="number">7</span>, <span class="number">6</span>,<span class="number">15</span>,<span class="number">14</span>,<span class="number">23</span>,<span class="number">22</span>,<span class="number">31</span>,<span class="number">30</span> : cx01y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">     <span class="number">1</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">8</span>,<span class="number">17</span>,<span class="number">16</span>,<span class="number">25</span>,<span class="number">24</span> : cx02y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">     <span class="number">5</span>, <span class="number">4</span>,<span class="number">13</span>,<span class="number">12</span>,<span class="number">21</span>,<span class="number">20</span>,<span class="number">29</span>,<span class="number">28</span> : cx03y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">    <span class="number">35</span>,<span class="number">34</span>,<span class="number">43</span>,<span class="number">42</span>,<span class="number">51</span>,<span class="number">50</span>,<span class="number">59</span>,<span class="number">58</span> : cx64y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">    <span class="number">39</span>,<span class="number">38</span>,<span class="number">47</span>,<span class="number">46</span>,<span class="number">55</span>,<span class="number">54</span>,<span class="number">63</span>,<span class="number">62</span> : cx65y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">    <span class="number">33</span>,<span class="number">32</span>,<span class="number">41</span>,<span class="number">40</span>,<span class="number">49</span>,<span class="number">48</span>,<span class="number">57</span>,<span class="number">56</span> : cx66y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">    <span class="number">37</span>,<span class="number">36</span>,<span class="number">45</span>,<span class="number">44</span>,<span class="number">53</span>,<span class="number">52</span>,<span class="number">61</span>,<span class="number">60</span> : cx67y&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Double buffered register blocking used in vector loads.</span></span><br><span class="line">    <span class="comment">// Any bank conflicts that we can&#x27;t avoid in these registers we can hide with .reuse flags</span></span><br><span class="line">    <span class="number">64</span><span class="number">-79</span>   : j0Ax&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;, j0By&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line">    <span class="number">80</span><span class="number">-95</span>   : j1Ax&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;, j1By&lt;<span class="number">00</span><span class="number">-03</span>|<span class="number">64</span><span class="number">-67</span>&gt;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Registers to load A or B</span></span><br><span class="line">    <span class="number">96</span><span class="number">-103</span>  : loadX&lt;<span class="number">0</span><span class="number">-7</span>&gt;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Key global state registers for main loop and some we reuse for outputing C.</span></span><br><span class="line">    <span class="comment">// Note, tweaking the register banks of track&lt;0|4&gt;, tex, writeS, readBs, readAs impacts performance because of</span></span><br><span class="line">    <span class="comment">// delayed bank conflicts between memory operations and ffmas.</span></span><br><span class="line">    <span class="comment">// The array index bracket notation can be used to request a bank in a dynamically allocated range.</span></span><br><span class="line">    <span class="number">104</span><span class="number">-127</span> ~ track&lt;<span class="number">0</span>|<span class="number">4</span>&gt;[<span class="number">0</span>], tex[<span class="number">2</span>], readAs[<span class="number">2</span>], readBs[<span class="number">3</span>], writeS[<span class="number">3</span>], end, ldx8, tid, bx, by, tid31, tid96, tid128 <span class="comment">//, clock, smId, nSMs</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Registers to store the results back to global memory. Reuse any register not needed after the main loop.</span></span><br><span class="line">    <span class="comment">// Statically allocate cs0-7 because they&#x27;re vector registers.</span></span><br><span class="line">    <span class="number">64</span><span class="number">-71</span>   : cs&lt;<span class="number">0</span><span class="number">-7</span>&gt;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// dynamically allocated C output registers(~)</span></span><br><span class="line">    <span class="number">72</span><span class="number">-103</span>  ~ cy&lt;<span class="number">00</span>|<span class="number">04</span>|<span class="number">0</span>8|<span class="number">12</span>&gt;, Cy&lt;<span class="number">00</span>|<span class="number">04</span>|<span class="number">0</span>8|<span class="number">12</span>&gt;, ldc, ldc1, ldc4, ldc8, ldc60, writeCs, readCs, cx, ci, alpha, xmad_ci <span class="comment">//, xmad_D, D, blckDimX, gridDimX</span></span><br><span class="line"></span><br><span class="line">&lt;/REGISTER_MAPPING&gt;</span><br></pre></td></tr></table></figure>
<p>然而，我只是想简简单单写个sgemm，我并不想把我有限的周末时间全部投进去，毕竟读者也没给我钱。然后想想指令重排，通过reuse标识也能解决一部分reg的bank冲突，那就整这个吧。</p>
<p>遇到的另一个问题就是指令重排。我把里面所有存在寄存器bank冲突的指令列了出来。再来看看volta架构中的bank冲突，volta架构的寄存器有2路bank，奇数寄存器号代表bank0，偶数寄存器号代表bank1。如果FFMA指令的三个源寄存器的寄存器号都属于奇数或者偶数，那么就发生了bank冲突。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//0    FFMA R74, R36, R62.reuse, R74 ;    </span><br><span class="line">//1    FFMA R78, R34, R62.reuse, R78 ;</span><br><span class="line">//2    FFMA R16, R35, R62, R54 ;</span><br></pre></td></tr></table></figure>
<p>比如上面的代码，0号指令和1号三个源寄存器都是偶数，不考虑reuse标识的话，都有bank冲突，而2号指令就没有bank冲突。调整这3个的位置，变成：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//2    FFMA R16, R35, R62.reuse, R54 ;   </span><br><span class="line">//1    FFMA R78, R34, R62.reuse, R78 ; </span><br><span class="line">/ 0    FFMA R74, R36, R62.reuse, R74 ;</span><br></pre></td></tr></table></figure>
<p>让指令2的R62放入reuse cache中，指令1和指令0继续使用这个数，从而减少bank冲突。更改前后的代码在我的<a href="https://link.zhihu.com/?target=https%3A//github.com/Liu-xiandong/How_to_optimize_in_GPU/tree/master/sgemm/asm">github repo</a>中。但是改完之后，我发现性能提升并不是很明显，大概就是1%左右的性能提升。这可能是在sgemm_v2的基础上改的原因，当时4.1所说的shared memory bank冲突还比较明显。总之，实验大概就是这样子。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// optimize sgemm</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;assert.h&quot;</span> </span></span><br><span class="line"></span><br><span class="line"><span class="comment">// CUDA runtime</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cublas_v2.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// cal offset from row col and ld , in row-major matrix, ld is the width of the matrix</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> OFFSET(row, col, ld) ((row) * (ld) + (col))</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// transfer float4</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> FETCH_FLOAT4(pointer) (reinterpret_cast<span class="string">&lt;float4*&gt;</span>(&amp;(pointer))[0])</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> checkCudaErrors(func)				\</span></span><br><span class="line"><span class="meta">&#123;									\</span></span><br><span class="line"><span class="meta">    cudaError_t e = (func);			\</span></span><br><span class="line"><span class="meta">    <span class="keyword">if</span>(e != cudaSuccess)						                \</span></span><br><span class="line"><span class="meta">        printf (<span class="string">&quot;%s %d CUDA: %s\n&quot;</span>, __FILE__,  __LINE__, cudaGetErrorString(e));		\</span></span><br><span class="line"><span class="meta">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// K: ldA</span></span><br><span class="line"><span class="comment">// N: ldB</span></span><br><span class="line"><span class="keyword">template</span> &lt;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BLOCK_SIZE_M,  <span class="comment">// height of block of C that each thread block calculate</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BLOCK_SIZE_K,  <span class="comment">// width of block of A that each thread block load into shared memory</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BLOCK_SIZE_N,  <span class="comment">// width of block of C that each thread block calculate</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> THREAD_SIZE_Y, <span class="comment">// height of block of C that each thread calculate</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> THREAD_SIZE_X,  <span class="comment">// width of block of C that each thread calculate</span></span><br><span class="line">    <span class="type">const</span> <span class="type">bool</span> ENABLE_DOUBLE_BUFFER <span class="comment">// whether enable double buffering or not</span></span><br><span class="line">    &gt; </span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">Sgemm</span><span class="params">( </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">float</span> * __restrict__ A,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">float</span> * __restrict__ B,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">float</span> * __restrict__ C, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> M,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> N,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> <span class="type">int</span> K)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Block index</span></span><br><span class="line">    <span class="type">int</span> bx = blockIdx.x;</span><br><span class="line">    <span class="type">int</span> by = blockIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Thread index</span></span><br><span class="line">    <span class="type">int</span> tx = threadIdx.x;</span><br><span class="line">    <span class="type">int</span> ty = threadIdx.y;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// the threads number in Block of X,Y</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> THREAD_X_PER_BLOCK = BLOCK_SIZE_N / THREAD_SIZE_X;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> THREAD_Y_PER_BLOCK = BLOCK_SIZE_M / THREAD_SIZE_Y;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> THREAD_NUM_PER_BLOCK = THREAD_X_PER_BLOCK * THREAD_Y_PER_BLOCK;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// thread id in cur Block</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = ty * THREAD_X_PER_BLOCK + tx;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// shared memory</span></span><br><span class="line">    __shared__ <span class="type">float</span> As[<span class="number">2</span>][BLOCK_SIZE_K][BLOCK_SIZE_M];</span><br><span class="line">    __shared__ <span class="type">float</span> Bs[<span class="number">2</span>][BLOCK_SIZE_K][BLOCK_SIZE_N];</span><br><span class="line">    <span class="comment">// registers for C</span></span><br><span class="line">    <span class="type">float</span> accum[THREAD_SIZE_Y][THREAD_SIZE_X];</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;THREAD_SIZE_Y; i++)&#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>; j&lt;THREAD_SIZE_X; j++)&#123;</span><br><span class="line">            accum[i][j]=<span class="number">0.0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// registers for A and B</span></span><br><span class="line">    <span class="type">float</span> frag_a[<span class="number">2</span>][THREAD_SIZE_Y];</span><br><span class="line">    <span class="type">float</span> frag_b[<span class="number">2</span>][THREAD_SIZE_X];</span><br><span class="line">    <span class="comment">// registers load global memory</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> ldg_num_a = BLOCK_SIZE_M * BLOCK_SIZE_K / (THREAD_NUM_PER_BLOCK * <span class="number">4</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> ldg_num_b = BLOCK_SIZE_K * BLOCK_SIZE_N / (THREAD_NUM_PER_BLOCK * <span class="number">4</span>);</span><br><span class="line">    <span class="type">float</span> ldg_a_reg[<span class="number">4</span>*ldg_num_a];</span><br><span class="line">    <span class="type">float</span> ldg_b_reg[<span class="number">4</span>*ldg_num_b];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// threads number in one row</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> A_TILE_THREAD_PER_ROW = BLOCK_SIZE_K / <span class="number">4</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> B_TILE_THREAD_PER_ROW = BLOCK_SIZE_N / <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// row number and col number that needs to be loaded by this thread</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> A_TILE_ROW_START = tid / A_TILE_THREAD_PER_ROW;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> B_TILE_ROW_START = tid / B_TILE_THREAD_PER_ROW;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> A_TILE_COL = tid % A_TILE_THREAD_PER_ROW * <span class="number">4</span>; </span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> B_TILE_COL = tid % B_TILE_THREAD_PER_ROW * <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// row stride that thread uses to load multiple rows of a tile</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> A_TILE_ROW_STRIDE = THREAD_NUM_PER_BLOCK / A_TILE_THREAD_PER_ROW;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> B_TILE_ROW_STRIDE = THREAD_NUM_PER_BLOCK / B_TILE_THREAD_PER_ROW;</span><br><span class="line"></span><br><span class="line">    A = &amp;A[(BLOCK_SIZE_M * by)* K];</span><br><span class="line">    B = &amp;B[BLOCK_SIZE_N * bx];</span><br><span class="line"></span><br><span class="line">    <span class="comment">//load index of the tile</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> warp_id = tid / <span class="number">32</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> lane_id = tid % <span class="number">32</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> a_tile_index =  warp_id/<span class="number">2</span>*<span class="number">16</span> + lane_id/<span class="number">8</span>*<span class="number">4</span>; <span class="comment">//warp_id * 8 + (lane_id / 16)*4; // (warp_id/4)*32 + ((lane_id%16)/2)*4;</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> b_tile_index =  warp_id%<span class="number">2</span>*<span class="number">32</span> + lane_id%<span class="number">8</span>*<span class="number">4</span>; <span class="comment">//(lane_id % 16) * 4; // (warp_id%4)*16 + (lane_id/16)*8 + (lane_id%2)*4;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//transfer first tile from global mem to shared mem</span></span><br><span class="line">    <span class="comment">// load A from global memory to shared memory</span></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="type">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_M ; i += A_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        <span class="type">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(ldg_a_reg[ldg_index]) = <span class="built_in">FETCH_FLOAT4</span>(A[<span class="built_in">OFFSET</span>(</span><br><span class="line">            A_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">            A_TILE_COL, <span class="comment">// col</span></span><br><span class="line">            K )]);</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index];</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL<span class="number">+1</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index<span class="number">+1</span>];</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL<span class="number">+2</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index<span class="number">+2</span>];</span><br><span class="line">        As[<span class="number">0</span>][A_TILE_COL<span class="number">+3</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index<span class="number">+3</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// load B from global memory to shared memory</span></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> ( <span class="type">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_K; i += B_TILE_ROW_STRIDE) &#123;</span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(Bs[<span class="number">0</span>][B_TILE_ROW_START + i][B_TILE_COL]) = <span class="built_in">FETCH_FLOAT4</span>(B[<span class="built_in">OFFSET</span>(</span><br><span class="line">                B_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">                B_TILE_COL, <span class="comment">// col</span></span><br><span class="line">                N )]);</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// load A from shared memory to register</span></span><br><span class="line">    <span class="built_in">FETCH_FLOAT4</span>(frag_a[<span class="number">0</span>][<span class="number">0</span>]) = <span class="built_in">FETCH_FLOAT4</span>(As[<span class="number">0</span>][<span class="number">0</span>][a_tile_index]);</span><br><span class="line">    <span class="built_in">FETCH_FLOAT4</span>(frag_a[<span class="number">0</span>][<span class="number">4</span>]) = <span class="built_in">FETCH_FLOAT4</span>(As[<span class="number">0</span>][<span class="number">0</span>][a_tile_index + <span class="number">64</span>]);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// load B from shared memory to register</span></span><br><span class="line">    <span class="built_in">FETCH_FLOAT4</span>(frag_b[<span class="number">0</span>][<span class="number">0</span>]) = <span class="built_in">FETCH_FLOAT4</span>(Bs[<span class="number">0</span>][<span class="number">0</span>][b_tile_index]);</span><br><span class="line">    <span class="built_in">FETCH_FLOAT4</span>(frag_b[<span class="number">0</span>][<span class="number">4</span>]) = <span class="built_in">FETCH_FLOAT4</span>(Bs[<span class="number">0</span>][<span class="number">0</span>][b_tile_index + <span class="number">64</span>]);</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> write_stage_idx = <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> tile_idx = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">do</span>&#123;</span><br><span class="line">        <span class="comment">// next tile index</span></span><br><span class="line">        tile_idx += BLOCK_SIZE_K;</span><br><span class="line">        <span class="comment">// load next tile from global mem</span></span><br><span class="line">        <span class="keyword">if</span>(tile_idx&lt; K)&#123;</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> ( <span class="type">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_M ; i += A_TILE_ROW_STRIDE) &#123;</span><br><span class="line">                <span class="type">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">                <span class="built_in">FETCH_FLOAT4</span>(ldg_a_reg[ldg_index]) = <span class="built_in">FETCH_FLOAT4</span>(A[<span class="built_in">OFFSET</span>(</span><br><span class="line">                    A_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">                    A_TILE_COL + tile_idx, <span class="comment">// col</span></span><br><span class="line">                    K )]);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> ( <span class="type">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_K; i += B_TILE_ROW_STRIDE) &#123;</span><br><span class="line">                <span class="type">int</span> ldg_index = i / B_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">                <span class="built_in">FETCH_FLOAT4</span>(ldg_b_reg[ldg_index]) = <span class="built_in">FETCH_FLOAT4</span>(B[<span class="built_in">OFFSET</span>(</span><br><span class="line">                    tile_idx + B_TILE_ROW_START + i, <span class="comment">// row</span></span><br><span class="line">                    B_TILE_COL, <span class="comment">// col</span></span><br><span class="line">                    N )]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> load_stage_idx = write_stage_idx ^ <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>; j&lt;BLOCK_SIZE_K - <span class="number">1</span>; ++j)&#123;</span><br><span class="line">            <span class="comment">// load next tile from shared mem to register </span></span><br><span class="line">            <span class="comment">// load A from shared memory to register</span></span><br><span class="line">            <span class="built_in">FETCH_FLOAT4</span>(frag_a[(j<span class="number">+1</span>)%<span class="number">2</span>][<span class="number">0</span>]) = <span class="built_in">FETCH_FLOAT4</span>(As[load_stage_idx][(j<span class="number">+1</span>)][a_tile_index]);</span><br><span class="line">            <span class="built_in">FETCH_FLOAT4</span>(frag_a[(j<span class="number">+1</span>)%<span class="number">2</span>][<span class="number">4</span>]) = <span class="built_in">FETCH_FLOAT4</span>(As[load_stage_idx][(j<span class="number">+1</span>)][a_tile_index + <span class="number">64</span>]);</span><br><span class="line">            <span class="comment">// load B from shared memory to register</span></span><br><span class="line">            <span class="built_in">FETCH_FLOAT4</span>(frag_b[(j<span class="number">+1</span>)%<span class="number">2</span>][<span class="number">0</span>]) = <span class="built_in">FETCH_FLOAT4</span>(Bs[load_stage_idx][(j<span class="number">+1</span>)][b_tile_index]);</span><br><span class="line">            <span class="built_in">FETCH_FLOAT4</span>(frag_b[(j<span class="number">+1</span>)%<span class="number">2</span>][<span class="number">4</span>]) = <span class="built_in">FETCH_FLOAT4</span>(Bs[load_stage_idx][(j<span class="number">+1</span>)][b_tile_index + <span class="number">64</span>]);</span><br><span class="line">            <span class="comment">// compute C THREAD_SIZE_X x THREAD_SIZE_Y</span></span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; ++thread_y) &#123;</span><br><span class="line">                <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; ++thread_x) &#123;</span><br><span class="line">                    accum[thread_y][thread_x] += frag_a[j%<span class="number">2</span>][thread_y] * frag_b[j%<span class="number">2</span>][thread_x];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(tile_idx &lt; K)&#123;</span><br><span class="line">            <span class="comment">// load A from global memory to shared memory</span></span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> ( <span class="type">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_M ; i += A_TILE_ROW_STRIDE) &#123;</span><br><span class="line">                <span class="type">int</span> ldg_index = i / A_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">                As[write_stage_idx][A_TILE_COL][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index];</span><br><span class="line">                As[write_stage_idx][A_TILE_COL<span class="number">+1</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index<span class="number">+1</span>];</span><br><span class="line">                As[write_stage_idx][A_TILE_COL<span class="number">+2</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index<span class="number">+2</span>];</span><br><span class="line">                As[write_stage_idx][A_TILE_COL<span class="number">+3</span>][A_TILE_ROW_START + i]=ldg_a_reg[ldg_index<span class="number">+3</span>];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// load B from global memory to shared memory</span></span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> ( <span class="type">int</span> i = <span class="number">0</span> ; i &lt; BLOCK_SIZE_K; i += B_TILE_ROW_STRIDE) &#123;</span><br><span class="line">                <span class="type">int</span> ldg_index = i / B_TILE_ROW_STRIDE * <span class="number">4</span>;</span><br><span class="line">                <span class="built_in">FETCH_FLOAT4</span>(Bs[write_stage_idx][B_TILE_ROW_START + i][B_TILE_COL]) = <span class="built_in">FETCH_FLOAT4</span>(ldg_b_reg[ldg_index]);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// use double buffer, only need one sync</span></span><br><span class="line">            __syncthreads();</span><br><span class="line">            <span class="comment">// switch</span></span><br><span class="line">            write_stage_idx ^= <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// load first tile from shared mem to register of next iter</span></span><br><span class="line">        <span class="comment">// load A from shared memory to register</span></span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(frag_a[<span class="number">0</span>][<span class="number">0</span>]) = <span class="built_in">FETCH_FLOAT4</span>(As[load_stage_idx^<span class="number">1</span>][<span class="number">0</span>][a_tile_index]);</span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(frag_a[<span class="number">0</span>][<span class="number">4</span>]) = <span class="built_in">FETCH_FLOAT4</span>(As[load_stage_idx^<span class="number">1</span>][<span class="number">0</span>][a_tile_index + <span class="number">64</span>]);</span><br><span class="line">        <span class="comment">// load B from shared memory to register</span></span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(frag_b[<span class="number">0</span>][<span class="number">0</span>]) = <span class="built_in">FETCH_FLOAT4</span>(Bs[load_stage_idx^<span class="number">1</span>][<span class="number">0</span>][b_tile_index]);</span><br><span class="line">        <span class="built_in">FETCH_FLOAT4</span>(frag_b[<span class="number">0</span>][<span class="number">4</span>]) = <span class="built_in">FETCH_FLOAT4</span>(Bs[load_stage_idx^<span class="number">1</span>][<span class="number">0</span>][b_tile_index + <span class="number">64</span>]);</span><br><span class="line">        <span class="comment">// compute C THREAD_SIZE_X x THREAD_SIZE_Y</span></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> thread_y = <span class="number">0</span>; thread_y &lt; THREAD_SIZE_Y; ++thread_y) &#123;</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> thread_x = <span class="number">0</span>; thread_x &lt; THREAD_SIZE_X; ++thread_x) &#123;</span><br><span class="line">                accum[thread_y][thread_x] += frag_a[<span class="number">1</span>][thread_y] * frag_b[<span class="number">1</span>][thread_x];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;<span class="keyword">while</span>(tile_idx&lt; K);</span><br><span class="line">    </span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> c_block_row = a_tile_index;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> c_block_col = b_tile_index;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//store C00 block</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;<span class="number">4</span>; i++)&#123;</span><br><span class="line">      <span class="built_in">FETCH_FLOAT4</span>(C[<span class="built_in">OFFSET</span>(</span><br><span class="line">        BLOCK_SIZE_M * by + c_block_row + i,</span><br><span class="line">        BLOCK_SIZE_N * bx + c_block_col,</span><br><span class="line">        N)]) = <span class="built_in">FETCH_FLOAT4</span>(accum[i][<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//store C01 block</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;<span class="number">4</span>; i++)&#123;</span><br><span class="line">      <span class="built_in">FETCH_FLOAT4</span>(C[<span class="built_in">OFFSET</span>(</span><br><span class="line">        BLOCK_SIZE_M * by + c_block_row + i,</span><br><span class="line">        BLOCK_SIZE_N * bx + c_block_col + <span class="number">64</span>,</span><br><span class="line">        N)]) = <span class="built_in">FETCH_FLOAT4</span>(accum[i][<span class="number">4</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//store C10 block</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;<span class="number">4</span>; i++)&#123;</span><br><span class="line">      <span class="built_in">FETCH_FLOAT4</span>(C[<span class="built_in">OFFSET</span>(</span><br><span class="line">        BLOCK_SIZE_M * by + c_block_row + <span class="number">64</span> + i,</span><br><span class="line">        BLOCK_SIZE_N * bx + c_block_col,</span><br><span class="line">        N)]) = <span class="built_in">FETCH_FLOAT4</span>(accum[i<span class="number">+4</span>][<span class="number">0</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//store C11 block</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;<span class="number">4</span>; i++)&#123;</span><br><span class="line">      <span class="built_in">FETCH_FLOAT4</span>(C[<span class="built_in">OFFSET</span>(</span><br><span class="line">        BLOCK_SIZE_M * by + c_block_row + <span class="number">64</span> + i,</span><br><span class="line">        BLOCK_SIZE_N * bx + c_block_col + <span class="number">64</span>,</span><br><span class="line">        N)]) = <span class="built_in">FETCH_FLOAT4</span>(accum[i<span class="number">+4</span>][<span class="number">4</span>]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (argc != <span class="number">4</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;usage: ./main [M] [K] [N]\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">size_t</span> M = <span class="built_in">atoi</span>(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="type">size_t</span> K = <span class="built_in">atoi</span>(argv[<span class="number">2</span>]);</span><br><span class="line">    <span class="type">size_t</span> N = <span class="built_in">atoi</span>(argv[<span class="number">3</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">assert</span>( M%<span class="number">8</span> == <span class="number">0</span>); </span><br><span class="line">    <span class="built_in">assert</span>( N%<span class="number">8</span> == <span class="number">0</span>); </span><br><span class="line">    <span class="built_in">assert</span>( K%<span class="number">8</span> == <span class="number">0</span>); </span><br><span class="line"></span><br><span class="line">    <span class="type">size_t</span> bytes_A = <span class="built_in">sizeof</span>(<span class="type">float</span>) * M * K;</span><br><span class="line">    <span class="type">size_t</span> bytes_B = <span class="built_in">sizeof</span>(<span class="type">float</span>) * K * N;</span><br><span class="line">    <span class="type">size_t</span> bytes_C = <span class="built_in">sizeof</span>(<span class="type">float</span>) * M * N;</span><br><span class="line">    <span class="type">float</span>* h_A = (<span class="type">float</span>*)<span class="built_in">malloc</span>(bytes_A);</span><br><span class="line">    <span class="type">float</span>* h_B = (<span class="type">float</span>*)<span class="built_in">malloc</span>(bytes_B);</span><br><span class="line">    <span class="type">float</span>* h_C = (<span class="type">float</span>*)<span class="built_in">malloc</span>(bytes_C);</span><br><span class="line">    <span class="type">float</span>* h_C1 = (<span class="type">float</span>*)<span class="built_in">malloc</span>(bytes_C);</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span>* d_A;</span><br><span class="line">    <span class="type">float</span>* d_B;</span><br><span class="line">    <span class="type">float</span>* d_C;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaMalloc</span>(&amp;d_A, bytes_A));</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaMalloc</span>(&amp;d_B, bytes_B));</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaMalloc</span>(&amp;d_C, bytes_C));</span><br><span class="line">    <span class="type">double</span> msecPerMatrixMul[<span class="number">2</span>] = &#123;<span class="number">0</span>, <span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">double</span> gigaFlops[<span class="number">2</span>] = &#123;<span class="number">0</span>, <span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">double</span> flopsPerMatrixMul = <span class="number">2.0</span> * M * N * K;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// don&#x27;t edit it</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BLOCK_SIZE_M = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BLOCK_SIZE_K = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> BLOCK_SIZE_N = <span class="number">128</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> THREAD_SIZE_X = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> THREAD_SIZE_Y = <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">bool</span> ENABLE_DOUBLE_BUFFER = <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 生成A的数据</span></span><br><span class="line">    <span class="keyword">for</span>( <span class="type">int</span> i = <span class="number">0</span>; i &lt; M * K; i++ ) &#123;</span><br><span class="line">        h_A[i] = i / <span class="number">13</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 生成B的数据</span></span><br><span class="line">    <span class="keyword">for</span>( <span class="type">int</span> i = <span class="number">0</span>; i &lt; K * N; i++ ) &#123;</span><br><span class="line">        h_B[i] = i % <span class="number">13</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaMemcpy</span>( d_A, h_A, bytes_A, cudaMemcpyHostToDevice));</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaMemcpy</span>( d_B, h_B, bytes_B, cudaMemcpyHostToDevice));</span><br><span class="line">    </span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaEventCreate</span>(&amp;start));</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaEventCreate</span>(&amp;stop));</span><br><span class="line">    <span class="type">float</span> msecTotal = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> nIter = <span class="number">1000</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaMemcpy</span>( d_C, h_C, bytes_C, cudaMemcpyHostToDevice));</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaEventRecord</span>(start));</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> run = <span class="number">0</span> ; run &lt; nIter; run ++ ) &#123;</span><br><span class="line">        <span class="function">dim3 <span class="title">dimBlock</span><span class="params">(BLOCK_SIZE_N / THREAD_SIZE_X, BLOCK_SIZE_M / THREAD_SIZE_Y)</span></span>;</span><br><span class="line">        <span class="function">dim3 <span class="title">dimGrid</span><span class="params">(N / BLOCK_SIZE_N, M / BLOCK_SIZE_M)</span></span>;</span><br><span class="line">        Sgemm&lt;BLOCK_SIZE_M, BLOCK_SIZE_K, BLOCK_SIZE_N, THREAD_SIZE_Y, THREAD_SIZE_X, ENABLE_DOUBLE_BUFFER&gt; </span><br><span class="line">        &lt;&lt;&lt; dimGrid, dimBlock &gt;&gt;&gt;(d_A, d_B, d_C, M, N, K);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaEventRecord</span>(stop));</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaEventSynchronize</span>(stop));</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaEventElapsedTime</span>(&amp;msecTotal, start, stop));</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaMemcpy</span>( h_C, d_C, bytes_C, cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">    msecPerMatrixMul[<span class="number">0</span>] = msecTotal / nIter;</span><br><span class="line">    gigaFlops[<span class="number">0</span>] = (flopsPerMatrixMul * <span class="number">1.0e-9f</span>) / (msecPerMatrixMul[<span class="number">0</span>] / <span class="number">1000.0f</span>);</span><br><span class="line">    <span class="built_in">printf</span>( <span class="string">&quot;My gemm Performance= %.2f GFlop/s, Time= %.3f msec, Size= %.0f Ops,\n&quot;</span>,</span><br><span class="line">        gigaFlops[<span class="number">0</span>],</span><br><span class="line">        msecPerMatrixMul[<span class="number">0</span>],</span><br><span class="line">        flopsPerMatrixMul);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// cublas</span></span><br><span class="line">    </span><br><span class="line">    cublasHandle_t blas_handle;  </span><br><span class="line">    <span class="built_in">cublasCreate</span>(&amp;blas_handle);</span><br><span class="line">    <span class="type">float</span> alpha = <span class="number">1.0</span>;</span><br><span class="line">    <span class="type">float</span> beta = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaMemcpy</span>( d_C, h_C, bytes_C, cudaMemcpyHostToDevice));</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaEventRecord</span>(start));</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> run = <span class="number">0</span> ; run &lt; nIter; run ++ ) &#123;</span><br><span class="line">        <span class="built_in">cublasSgemm</span> (blas_handle, CUBLAS_OP_T, CUBLAS_OP_T, </span><br><span class="line">            M, N, K, &amp;alpha, </span><br><span class="line">            d_A, K, d_B, N, &amp;beta, d_C, N</span><br><span class="line">        );</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaEventRecord</span>(stop));</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaEventSynchronize</span>(stop));</span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaEventElapsedTime</span>(&amp;msecTotal, start, stop));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">checkCudaErrors</span>(<span class="built_in">cudaMemcpy</span>( h_C1, d_C, bytes_C, cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">    msecPerMatrixMul[<span class="number">1</span>] = msecTotal / nIter;</span><br><span class="line">    gigaFlops[<span class="number">1</span>] = (flopsPerMatrixMul * <span class="number">1.0e-9f</span>) / (msecPerMatrixMul[<span class="number">1</span>] / <span class="number">1000.0f</span>);</span><br><span class="line">    <span class="built_in">printf</span>( <span class="string">&quot;CuBlas Performance= %.2f GFlop/s, Time= %.3f msec, Size= %.0f Ops,\n&quot;</span>,</span><br><span class="line">        gigaFlops[<span class="number">1</span>],</span><br><span class="line">        msecPerMatrixMul[<span class="number">1</span>],</span><br><span class="line">        flopsPerMatrixMul);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cublasDestroy</span>(blas_handle); </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="type">double</span> eps = <span class="number">1.e-6</span>;  <span class="comment">// machine zero</span></span><br><span class="line">    <span class="type">bool</span> correct = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; M * N; i++) &#123;</span><br><span class="line">        <span class="type">int</span> row = i / N;</span><br><span class="line">        <span class="type">int</span> col = i % N;</span><br><span class="line">        <span class="type">double</span> abs_err = <span class="built_in">fabs</span>(h_C[i] - h_C1[col * M + row]);</span><br><span class="line">        <span class="type">double</span> dot_length = M;</span><br><span class="line">        <span class="type">double</span> abs_val = <span class="built_in">fabs</span>(h_C[i]);</span><br><span class="line">        <span class="type">double</span> rel_err = abs_err / abs_val / dot_length;</span><br><span class="line">        <span class="keyword">if</span> (rel_err &gt; eps) &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Error! Matrix[%d][%d]=%.8f, ref=%.8f error term is &gt; %E\n&quot;</span>,</span><br><span class="line">                    row, col, h_C[i], h_C1[col * M + row], eps);</span><br><span class="line">            correct = <span class="literal">false</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%s\n&quot;</span>, correct ? <span class="string">&quot;Result= PASS&quot;</span> : <span class="string">&quot;Result= FAIL&quot;</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;ratio= %f\n&quot;</span>, gigaFlops[<span class="number">0</span>] / gigaFlops[<span class="number">1</span>]);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Free Memory</span></span><br><span class="line">    <span class="built_in">cudaFree</span>(d_A);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_B);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_C);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">free</span>(h_A);</span><br><span class="line">    <span class="built_in">free</span>(h_B);</span><br><span class="line">    <span class="built_in">free</span>(h_C);</span><br><span class="line">    <span class="built_in">free</span>(h_C1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="SGEMM"><a href="#SGEMM" class="headerlink" title="SGEMM"></a>SGEMM</h1><p>在深度学习推理框架或者训练框架中，GEMM 和 Conv 是典型的<strong>计算密集型算子</strong>，例如在 Bert 和 Conformer 模型的 self-attention 模块中存在大量矩阵运算，因此深度学习框架中 GEMM 算子的底层实现好坏将会直接影响模型的推理或训练延时。</p>
<p><img src="/img/v2-40ed6461ce3660156de9a0a29eb82799_b.jpg" alt="img"></p>
<p>图1 conformer 模型中的矩阵运算</p>
<p>介绍如何进行 GEMM 优化的文章很多，即使在知乎上随手搜索 <strong>GEMM优化</strong> 词条也会有几十个条目，其中也不乏一些内容翔实、条理清楚的好文章。不过，从我个人比较主观的分析来看，大部分文章停留在方法论层面的介绍，没有落实到具体的代码实现上，<strong>理论和实践之间还是有不可跨越的鸿沟</strong>，作为一个愣头青程序员，没能看到代码总是感觉少了点意思。</p>
<p>另一方面，在 <a href="https://link.zhihu.com/?target=https%3A//github.com/flame/how-to-optimize-gemm">GitHub: How To Optimize GEMM</a> 项目中，作者通过清晰明了的代码和文档向读者介绍内存对齐、向量化、矩阵分块和数据打包等关键技术，此外，作者还给出了每一个步骤的优化点、优化效果对比和分析，实属不可多得的GEMM优化入门读物，强烈推荐！但 <a href="https://link.zhihu.com/?target=https%3A//github.com/flame/how-to-optimize-gemm">GitHub: How To Optimize GEMM</a> 作为一个入门级的项目，旨在粗粒度介绍矩阵乘算法的优化思路，并没有针对某个硬件进行针对性优化，也没有深入优化 micro kernel 的代码实现，因此该项目中的矩阵乘实现仍然存在较大的优化空间。</p>
<p>那么，能不能在介绍矩阵乘<a href="https://www.zhihu.com/search?q=优化原理&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A2849376497}">优化原理</a>的基础时搭配相应的代码实现，并且最终取得可观的性能表现呢？  </p>
<blockquote>
<p><strong>Talk is cheap. Show me the code.</strong> ― Linus Torvalds</p>
</blockquote>
<p>当然，这篇文章就是想做这个事情，<strong>本文目标</strong>有三点</p>
<ol>
<li>介绍如何在x64 CPU 上优化矩阵乘算法的思路；</li>
<li>实现一份可运行的高性能矩阵乘算法；</li>
<li>性能数据可复现；</li>
</ol>
<p><img src="/img/v2-c41d9b702a5f0154498996379506ae0b_b.jpg" alt="img"></p>
<p>图2 矩阵乘运算</p>
<p>矩阵乘运算是大学本科的基础知识，原理十分简单，此处不在赘述其数学公式和讲解。</p>
<h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><p>选取一个合适的度量指标是性能优化工作的基础，通常我们使用 GFLOPS 来衡量一个算子的性能。</p>
<h3 id="区分-FLOPS-和-FLOPs"><a href="#区分-FLOPS-和-FLOPs" class="headerlink" title="区分 FLOPS 和 FLOPs"></a>区分 FLOPS 和 FLOPs</h3><p><strong>每秒<a href="https://www.zhihu.com/search?q=浮点运算&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A2849376497}">浮点运算</a>次数</strong>(floating point operations per second, <strong>FLOPS</strong>)，即每秒所执行的浮点运算次数，是一个衡量硬件性能的指标。下表列举了常见的 FLOPS 换算指标。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>缩写</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>MFLOPS</td>
<td>每秒进行百万次   （10^6）  次浮点运算的次数</td>
</tr>
<tr>
<td>GFLOPS</td>
<td>每秒进行十亿次   （10^9）  次浮点运算的次数</td>
</tr>
<tr>
<td>TFLOPS</td>
<td>每秒进行万亿次   （10^12）次浮点运算的次数</td>
</tr>
<tr>
<td>PFLOPS</td>
<td>每秒进行千万亿次（10^15）次浮点运算的次数</td>
</tr>
<tr>
<td>EFLOPS</td>
<td>每秒进行百亿亿次（10^18）次浮点运算的次数</td>
</tr>
</tbody>
</table>
</div>
<p><strong>浮点运算量</strong>(floating point operations, <strong>FLOPs</strong>)是指浮点运算的次数，是一个衡量深度学习模型计算量的指标。</p>
<p>此外，从<strong>FLOPs</strong>延伸出另外一个指标是乘加运算量MACs。</p>
<p><strong>乘加运算量</strong>(multiplication and accumulation operations, <strong>MACs</strong>)是指乘加运算的次数，也是衡量深度模型计算量的指标。在Intel AVX指令中，扩展了对于<strong>乘加计算</strong>(fused multiply-add, <strong>FMA</strong>)指令的支持，即在支持AVX指令的CPU上，可以通过FMA计算单元使用一条指令来执行类似 A×B+CA \times B + CA \times B + C 的操作，参考 <a href="https://link.zhihu.com/?target=https%3A//www.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/compiler-reference/intrinsics/intrinsics-for-avx2/intrinsics-for-fused-multiply-add-operations/mm-fmadd-ps-mm256-fmadd-ps.html">Intel® C++ Compiler Classic Developer Guide and Reference</a> 中对于 _mm256_fmadd_ps 指令的介绍。一次乘加运算包含了两次浮点运算，一般地可以认为 MACs = 2FLOPs。</p>
<h3 id="计算-CPU-的-FLOPS"><a href="#计算-CPU-的-FLOPS" class="headerlink" title="计算 CPU 的 FLOPS"></a>计算 CPU 的 FLOPS</h3><p>从上一小节中得知，FLOPS 是一个衡量硬件性能的指标，那么我们该如何计算 CPU 的FLOPS 呢？</p>
<p><img src="/img/v2-e56a1cd7fb59cc20507f79deb774f383_b.jpg" alt="img"></p>
<p>图1 使用 lscpu 命令查看系统信息</p>
<p>上图中，红框中几条关键信息</p>
<ol>
<li>CPU(s), 逻辑核数量；</li>
<li>CPU family,  CPU系列标识，用以确定CPU属于哪一代产品。更多关于 Intel CPU Family 信息，可以参考 <a href="https://link.zhihu.com/?target=https%3A//en.wikichip.org/wiki/intel/cpuid%23CPUIDs">Intel CPUID</a>；</li>
<li>Model, 型号标识可用来确定处理器的制作技术以及属于该系列的第几代设计（或核心），型号与系列通常是相互配合使用的，用于确定计算机所安装的处理器是属于某系列处理器的哪种特定类型。</li>
<li>Model name, CPU型号名称</li>
<li>CPU MHZ: 主频</li>
</ol>
<p>下面以 <strong>“Xeon Platinum 8260Y”</strong> 细致地解释下 CPU 型号名称中隐藏的信息。</p>
<p><img src="/img/v2-d33f42ddbb0c3affc193011a00446b07_b.jpg" alt="img"></p>
<p>图2 Xeon Platinum 8260Y CPU</p>
<ul>
<li><strong>Xeon</strong> Platinum 8260Y<strong>:</strong> Intel 公司推出的<strong>至强处理器</strong>系列，具备丰富的<a href="https://www.zhihu.com/search?q=指令集&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A2849376497}">指令集</a>支持和出色的性能表现，主要针对服务器市场。除至强处理器之外，Intel 公司推出的<strong><a href="https://www.zhihu.com/search?q=酷睿处理器&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A2849376497}">酷睿处理器</a></strong>在桌面市场具备更高的知名度；</li>
<li>Xeon <strong>Platinum 8</strong>260Y<strong>:</strong> Intel 至强系列处理器分为四个级别，性能由高到低依次是铂金级Platinum（8，9）、黄金级Gold（6，7）、白银级Silver（4）和青铜级Bronze（3）；</li>
<li>Xeon Platinum 8<strong>2</strong>60Y<strong>:</strong> 处理器架构代号，1 代表Skylake ，2 代表 Cascade Lake</li>
<li>Xeon Platinum 82<strong>60Y:</strong> SKU和Extra Options信息可以参考 <a href="https://link.zhihu.com/?target=https%3A//en.wikichip.org/wiki/intel/microarchitectures/cascade_lake">Cascade Lake 架构介绍</a></li>
</ul>
<p>计算CPU FLOPS时需要两点关键信息，下面分别计算下 AVX2 和 AVX512 指令集的GFLOPS。</p>
<ol>
<li>CPU 主频</li>
<li>FMA 单元数</li>
</ol>
<p><strong>AVX2</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">单周期双精度浮点计算能力 = 2(FMA数量)* 2(乘加) ∗ 256 (YMM寄存器宽度) / 64(双精度浮点数位数) = 16</span><br><span class="line">单周期双精度浮点计算能力= 2(FMA数量)* 2(乘加) ∗ 256 (YMM寄存器宽度) / 32(双精度浮点数位数) = 32</span><br><span class="line">双精度FLOAPS = 2.5(CPU主频) * 16(单周期双精度浮点计算能力) = 40GFLOPS </span><br><span class="line">单精度FLOAPS = 2.5(CPU主频) * 32(单周期单精度浮点计算能力) = 80GFLOPS </span><br></pre></td></tr></table></figure>
<p><strong>AVX512</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">单周期双精度浮点计算能力 = 2(FMA数量)* 2(乘加) ∗ 512 (YMM寄存器宽度) / 64(双精度浮点数位数) = 32</span><br><span class="line">单周期双精度浮点计算能力= 2(FMA数量)* 2(乘加) ∗ 512 (YMM寄存器宽度) / 32(双精度浮点数位数) = 64</span><br><span class="line">双精度FLOAPS = 2.5(CPU主频) * 16(单周期双精度浮点计算能力) = 80 GFLOPS </span><br><span class="line">单精度FLOAPS = 2.5(CPU主频) * 32(单周期单精度浮点计算能力) = 160 GFLOPS </span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>指令集</th>
<th>精度</th>
<th>理论峰值算力</th>
</tr>
</thead>
<tbody>
<tr>
<td>AVX2</td>
<td>double</td>
<td>40 GFLOPS</td>
</tr>
<tr>
<td>AVX2</td>
<td>float</td>
<td>80 GFLOPS</td>
</tr>
<tr>
<td>AVX512</td>
<td>double</td>
<td>80 GFLOPS</td>
</tr>
<tr>
<td>AVX512</td>
<td>float</td>
<td>160 GFLOPS</td>
</tr>
</tbody>
</table>
</div>
<p>至此，我们已经明白了单核心CPU的理论峰值算力，下面开始进入实战环节！</p>
<h2 id="基础矩阵乘实现和优化"><a href="#基础矩阵乘实现和优化" class="headerlink" title="基础矩阵乘实现和优化"></a>基础矩阵乘实现和优化</h2><p>本节内容作为正式优化的序章，会介绍两点内容</p>
<ol>
<li>如何实现基础的 GEMM 算法并测量其性能数据；</li>
<li>如何通过一行代码达到十倍的性能提升； </li>
</ol>
<p>此处约定本文中A，B分别为左、右输入矩阵，C为输出矩阵，并且三者的形状信息如下</p>
<p>A:M×KA: M \times K A: M \times K  的输入矩阵</p>
<p>B:K×NB: K \times NB: K \times N 的输入矩阵</p>
<p>C:M×NC: M \times NC: M \times N 的输出矩阵</p>
<h3 id="基础-GEMM-实现和度量"><a href="#基础-GEMM-实现和度量" class="headerlink" title="基础 GEMM 实现和度量"></a><strong>基础 GEMM 实现和度量</strong></h3><p>下面的代码应该都不陌生，矩阵乘算法是编程初学者经典的练习题之一。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">void naive_row_major_sgemm(const float* A, const float* B, float* C, const int M,</span><br><span class="line">               const int N, const int K) &#123;</span><br><span class="line">  for (int m = 0; m &lt; M; ++m) &#123;</span><br><span class="line">    for (int n = 0; n &lt; N; ++n) &#123;</span><br><span class="line">      for (int k = 0; k &lt; K; ++k) &#123;</span><br><span class="line">        C[m * N + n] += A[m * K + k] * B[k * N + n]; </span><br><span class="line">      &#125;   </span><br><span class="line">    &#125;   </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从矩阵乘的原理可知，矩阵乘算法的浮点运算量为 2×M×N×K2 \times M \times N \times K2 \times M \times N \times K，所以</p>
<p>GEMM:GFLOPs=2×M×N×Klatency×10−9GEMM : GFLOPs = \frac{2 \times M \times N \times K}{latency} \times 10^{-9} GEMM : GFLOPs = \frac{2 \times M \times N \times K}{latency} \times 10^{-9}  </p>
<p>下面实现一个朴素的GFLOPs 计算函数，相应的代码均会在 GitHub 仓库中提供。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">void Benchmark(const std::vector&lt;int64_t&gt;&amp; dims,</span><br><span class="line">               std::function&lt;void(void)&gt; func) &#123;</span><br><span class="line">  const int warmup_times = 10;</span><br><span class="line">  const int infer_times = 20;</span><br><span class="line"></span><br><span class="line">  // warmup</span><br><span class="line">  for (int i = 0; i &lt; warmup_times; ++i) func();</span><br><span class="line">  </span><br><span class="line">  // run </span><br><span class="line">  auto dtime = dclock();</span><br><span class="line">  for (int i = 0; i &lt; infer_times; ++i) func();</span><br><span class="line">  </span><br><span class="line">  // latency</span><br><span class="line">  dtime = dclock() - dtime;</span><br><span class="line"></span><br><span class="line">  // compute GLOPs</span><br><span class="line">  auto flops = 2.0f * product(dims) * 1.0e-09;</span><br><span class="line">  flops = flops * infer_times / dtime;</span><br><span class="line"></span><br><span class="line">  // print</span><br><span class="line">  std::cout &lt;&lt; std::setw(20) &lt;&lt; &quot; GFLOPs: &quot; &lt;&lt; flops &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>实测，naive_row_major_sgemm 的性能数据如下</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Shape(M, N, K)</th>
<th>GFLOPs</th>
</tr>
</thead>
<tbody>
<tr>
<td>(64, 64, 64)</td>
<td>1.97</td>
</tr>
<tr>
<td>(128, 128, 128)</td>
<td>1.65</td>
</tr>
<tr>
<td>(256, 256, 256)</td>
<td>1.44</td>
</tr>
<tr>
<td>(512, 512, 512)</td>
<td>0.95</td>
</tr>
<tr>
<td>(1024, 1024, 1024)</td>
<td>0.62</td>
</tr>
</tbody>
</table>
</div>
<p>从<a href="https://www.zhihu.com/search?q=测试数据&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A2849376497}">测试数据</a>来看，随着矩阵尺寸的增大，GFLOPs 在不断下降。从上文的分析中可知，单核CPU的理论峰值算力是<strong>80 GFLOPS</strong>，naive_row_major_sgemm 和理论峰值算力之间的差距非常大，完全没有发挥出CPU的算力。</p>
<p>naive_row_major_sgemm 性能极差的核心原因是<strong>在计算时发生了大量的cache miss</strong>。</p>
<p><img src="/img/v2-b186c0f6e54aaf63ee79545adf0d5f82_b.jpg" alt="img"></p>
<p>图3 基础 GEMM 实现示例 </p>
<h3 id="一行代码优化十倍性能"><a href="#一行代码优化十倍性能" class="headerlink" title="一行代码优化十倍性能"></a><strong>一行<a href="https://www.zhihu.com/search?q=代码优化&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A2849376497}">代码优化</a>十倍性能</strong></h3><p>在分析清楚 naive_row_major_sgemm 性能极差的主要原因后，我们通过<strong>循环重排</strong>来优化访存。注意，naive_row_major_sgemm 和 optimize_row_major_sgemm 虽然只有一行代码的差距，但是性能却相差近十倍！</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">void optimize_row_major_sgemm(const float* A, const float* B, float* C, const int M,</span><br><span class="line">               const int N, const int K) &#123;</span><br><span class="line">  for (int m = 0; m &lt; M; ++m) &#123;</span><br><span class="line">    for (int k = 0; k &lt; K; ++k) &#123;</span><br><span class="line">      for (int n = 0; n &lt; N; ++n) &#123;</span><br><span class="line">        C[m * N + n] += A[m * K + k] * B[k * N + n]; </span><br><span class="line">      &#125;   </span><br><span class="line">    &#125;   </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>Shape(M, N, K)</th>
<th>naive GFLOPs</th>
<th>optimize GFLOps</th>
</tr>
</thead>
<tbody>
<tr>
<td>(64, 64, 64)</td>
<td>1.97</td>
<td>11.20</td>
</tr>
<tr>
<td>(128, 128, 128)</td>
<td>1.65</td>
<td>11.84</td>
</tr>
<tr>
<td>(256, 256, 256)</td>
<td>1.44</td>
<td>12.04</td>
</tr>
<tr>
<td>(512, 512, 512)</td>
<td>0.95</td>
<td>11.43</td>
</tr>
<tr>
<td>(1024, 1024, 1024)</td>
<td>0.62</td>
<td>10.79</td>
</tr>
</tbody>
</table>
</div>
<p>根据上表中的数据，可以直接体会到性能优化的魔力。一行代码，十倍加速。</p>
<p><img src="/img/v2-697f1e89317b4ee6336b7b4431e653d1_b.jpg" alt="img"></p>
<p>图4 优化访存后的 GEMM 实现示例</p>
<h2 id="BLAS-接口简介"><a href="#BLAS-接口简介" class="headerlink" title="BLAS 接口简介"></a>BLAS 接口简介</h2><p>截止到目前为止，已经具有 <strong>naive_row_major_sgemm</strong> 和 <strong>optimize_row_major_sgemm</strong> 两份实现，虽然<strong>optimize_row_major_sgemm</strong> 在性能上有一定的优化，但距离真正的<a href="https://www.zhihu.com/search?q=高性能计算&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A2849376497}">高性能计算</a>库的要求还相差甚远。</p>
<p>即使抛开性能问题不谈，目前 optimize_row_major_sgemm 也很难视为一个合格的<a href="https://www.zhihu.com/search?q=库函数&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A2849376497}">库函数</a>，因为该函数在接口定义上太过随意，别人很难直接复用。众所周知，矩阵乘优化已经是非常成熟的课题了，其中自然衍生了许多标准，以方便不同开发者或者研究人员之间工作的交流和复用，其中最基础的便是 BLAS接口规范。</p>
<p><strong>BLAS（basic linear algebra subroutine）</strong>是一系列基本<strong>线性代数运算</strong>函数<strong><a href="https://link.zhihu.com/?target=https%3A//wuli.wiki/online/BLAS.html%23note1">1</a></strong>的<strong>接口（interface）</strong>标准。 这里的线性代数运算是指例如矢量的<a href="https://www.zhihu.com/search?q=线性组合&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A2849376497}">线性组合</a>，矩阵乘以矢量，矩阵乘以矩阵等。接口在这里指的是诸如哪个函数名实现什么功能，有几个输入和输出变量，分别是什么。</p>
<p>注意 BLAS 是一个接口的标准而不是某种具体<strong>实现（implementation）</strong>。简单来说，就是不同的作者可以各自写出不同版本的 BLAS 库，实现同样的接口和功能，但每个函数内部的算法可以不同。 这些不同导致了不同版本的 BLAS 在不同机器上运行的速度也不同。</p>
<p>C:=alpha×A×B+beta×CC := alpha \times A \times B + beta \times CC := alpha \times A \times B + beta \times C </p>
<ul>
<li><strong>A,</strong> 形状为(M, K)的列主序矩阵</li>
<li><strong>B,</strong> 形状为(M, K)的列主序矩阵</li>
<li><strong>C,</strong> 形状为(M, K)的列主序矩阵  </li>
</ul>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">void sgemm(char transa, char transb, int M, int N, int K, float alpha, </span><br><span class="line">            const float* A, int lda, const float* B, int ldb, float beta,  float* C, int ldc); </span><br></pre></td></tr></table></figure>
<ul>
<li><strong>transa,</strong> 设置矩阵A是否转置的标识位，’N’ 表示不转置, ‘T’ 表示转置；</li>
<li><strong>transb,</strong> 设置矩阵A是否转置的标识位，’N’ 表示不转置, ‘T’ 表示转置；</li>
<li><strong>M,</strong> M 维度的值；</li>
<li><strong>N,</strong> N 维度的值；</li>
<li><strong>K,</strong> K 维度的值；</li>
<li><strong>alpha,</strong> 系数；</li>
<li><strong>A,</strong> A 矩阵指针；</li>
<li><strong>lda,</strong> A矩阵 leading dimension的值；</li>
<li><strong>B,</strong> B 矩阵指针； </li>
<li><strong>ldb,</strong> B矩阵 leading dimension的值；</li>
<li><strong>beta,</strong> 系数；</li>
<li><strong>C,</strong> 结果矩阵C矩阵指针；</li>
<li><strong>ldc,</strong> C矩阵 leading dimension的值；</li>
</ul>
<p>注： <strong>leading dimension</strong>，对于一个 MxN 的行<a href="https://www.zhihu.com/search?q=优先矩阵&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A2849376497}">优先矩阵</a>，leading dimension 为 N；对于一个 MxN 的列优先矩阵，leading dimension 为 M。</p>
<p>介绍完 BLAS 接口之后，我们以 BLAS 接口的格式编写一份 <strong>列优先的矩阵乘实现</strong> 作为后续优化工作的比较基准。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">void naive_col_major_sgemm(</span><br><span class="line">        char transa,</span><br><span class="line">        char transb,</span><br><span class="line">        int M, int N, int K, </span><br><span class="line">        const float alpha,</span><br><span class="line">        const float * src_a, int lda,</span><br><span class="line">        const float * src_b, int ldb, </span><br><span class="line">        const float beta,</span><br><span class="line">        float * dst, int ldc)</span><br><span class="line">&#123;</span><br><span class="line">    int a_stride_m = transa == &#x27;n&#x27; ? 1 : lda;</span><br><span class="line">    int a_stride_k = transa == &#x27;n&#x27; ? lda : 1;</span><br><span class="line">    int b_stride_k = transb == &#x27;n&#x27; ? 1 : ldb;</span><br><span class="line">    int b_stride_n = transb == &#x27;n&#x27; ? ldb : 1;</span><br><span class="line"></span><br><span class="line">    for(int m=0;m&lt;M;m++) &#123;</span><br><span class="line">        for(int n=0;n&lt;N;n++) &#123;</span><br><span class="line">            float acc = 0.f;</span><br><span class="line">            const float * a_ptr = src_a + m * a_stride_m;</span><br><span class="line">            const float * b_ptr = src_b + n * b_stride_n;</span><br><span class="line"></span><br><span class="line">            for(int k=0;k&lt;K;k++) &#123;</span><br><span class="line">                acc += a_ptr[0] * b_ptr[0];</span><br><span class="line">                a_ptr += a_stride_k;</span><br><span class="line">                b_ptr += b_stride_k;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            dst[m + n * ldc ] = alpha * acc + beta * dst[m + n * ldc];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="深度优化矩阵乘实现"><a href="#深度优化矩阵乘实现" class="headerlink" title="深度优化矩阵乘实现"></a>深度优化矩阵乘实现</h2><p>从本节起，开始演示如何优化矩阵乘算法，以达到 80% 以上的硬件性能利用率。</p>
<p>一般而言，矩阵乘优化有以下技巧，在GEMM、GEMV的实现中都可以去套用。</p>
<ol>
<li>循环重排；</li>
<li>数据分块；</li>
<li>数组打包；</li>
<li>向量指令集；</li>
<li>寄存器优化；</li>
<li>多线程；</li>
</ol>
<p>在<strong>基础函数乘实现和优化</strong>一节中得知，矩阵乘实现性能差的原因在与数据 cache miss 率很高，因此我们进行的一个优化就是数据打包。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">void avx2_col_major_sgemm(char transa, char transb,int M, int N, int K, float alpha, float* A, int lda,</span><br><span class="line">                          float* B, int ldb, float beta, float* C, int ldc) &#123;</span><br><span class="line">  if (alpha == 0) return;</span><br><span class="line"></span><br><span class="line">  float beta_div_alpha = beta / alpha;</span><br><span class="line"></span><br><span class="line">  constexpr int Mr = 64;</span><br><span class="line">  constexpr int Kr = 256;</span><br><span class="line"></span><br><span class="line">  constexpr int mr = 16;</span><br><span class="line">  constexpr int nr = 6;</span><br><span class="line"></span><br><span class="line">  // Cache a is 64 x 256</span><br><span class="line">  float* pack_a = (float*)_mm_malloc(Mr * Kr * sizeof(float), 32);</span><br><span class="line">  // Cache b is 256 x N</span><br><span class="line">  float* pack_b = (float*)_mm_malloc(Kr * DivUp(N, nr) * sizeof(float), 32);</span><br><span class="line"></span><br><span class="line">  float* tmp_pack_a = pack_a;</span><br><span class="line">  float* tmp_pack_b = pack_b;</span><br><span class="line"></span><br><span class="line">  for (int k = 0; k &lt; K; k += Kr) &#123;</span><br><span class="line">    float cur_beta = 1.0 / alpha;</span><br><span class="line">    if (k == 0) cur_beta = beta_div_alpha;</span><br><span class="line"></span><br><span class="line">    int cur_k = std::min(K - k, Kr);</span><br><span class="line"></span><br><span class="line">    // jump to k-th row of matrix B</span><br><span class="line">    pack_no_trans(B + k, ldb, tmp_pack_b, Kr, cur_k, N);</span><br><span class="line"></span><br><span class="line">    for (int i = 0; i &lt; M; i += Mr) &#123;</span><br><span class="line">      int cur_m = std::min(M - i, Mr);</span><br><span class="line"></span><br><span class="line">      pack_trans(A + i + k * lda, lda, tmp_pack_a, Kr, cur_k, cur_m);</span><br><span class="line"></span><br><span class="line">      for (int j = 0; j &lt; N;) &#123;</span><br><span class="line">        int cur_n = std::min(int(N - j), nr);</span><br><span class="line">        float* cur_c = C + i + j * ldc;</span><br><span class="line"></span><br><span class="line">        float* packed_cur_b = tmp_pack_b + DivDown(j, nr) * Kr + j % nr;</span><br><span class="line"></span><br><span class="line">        sgemm_block_n(cur_m, cur_n, cur_k, alpha, tmp_pack_a, lda, packed_cur_b,</span><br><span class="line">                      ldb, cur_beta, cur_c, ldc);</span><br><span class="line">        j += cur_n;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  _mm_free(pack_a);</span><br><span class="line">  _mm_free(pack_b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在后文的讲解中，为方便起见，统一设置 <strong>M = N = K = 512</strong> 为例，来演示矩阵乘优化。</p>
<h3 id="数据打包"><a href="#数据打包" class="headerlink" title="数据打包"></a>数据打包</h3><p>从系统信息上看，L1 数据缓存和<a href="https://www.zhihu.com/search?q=指令缓存&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A2849376497}">指令缓存</a>均为 32 K，32K 的 L1d cache 可以容纳 32 * 1024 / 4 = 8192 个<a href="https://www.zhihu.com/search?q=单精度浮点数&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A2849376497}">单精度浮点数</a>。因此，当 M, N, K 足够大的时候，L1d cache 无法持有三个矩阵所有的数据，便会发生cache miss，这也解释了上文中为什么矩阵越大、性能越差。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">L1d cache:           32K</span><br><span class="line">L1i cache:           32K</span><br><span class="line">L2 cache:            4096K</span><br><span class="line">L3 cache:            36608K</span><br></pre></td></tr></table></figure>
<p>在 <strong>avx2_col_major_sgemm</strong> 的实现代码中，为矩阵A 开辟了 64 x 256 x 4 bytes / 1024 = 64 K 的存储区域，为矩阵B 开辟了  256 x <em>Divp(N=512,6 ) = 256 x 516 x 4 bytes / 1024 = 516 K</em> 的存储区域，目的是防止矩阵A和矩阵B过大，以至于在L2 cache 中发生cache miss 的情况，所以一次只在L2中加载矩阵A和矩阵B的子矩阵，保证不会发生cache miss。 </p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">constexpr int Mr = 64;</span><br><span class="line">constexpr int Kr = 256;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// Cache a is 64 x 256</span><br><span class="line">float* pack_a = (float*)_mm_malloc(Mr * Kr * sizeof(float), 32);</span><br><span class="line">// Cache b is 256 x N</span><br><span class="line">float* pack_b = (float*)_mm_malloc(Kr * DivUp(N, nr) * sizeof(float), 32);</span><br></pre></td></tr></table></figure>
<p>矩阵乘实现从计算方法上来区分，可以分为 Inner Product 和 Outer Product 两种计算方法，解释如下</p>
<ol>
<li>Inner Product: 按行切分A矩阵，按列切分B矩阵，使用A矩阵的一个按行切分的子块同B矩阵按列切分的子块做<a href="https://www.zhihu.com/search?q=矩阵乘法&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A2849376497}">矩阵乘法</a>，即求得结果矩阵C矩阵的一个子矩阵。依次循环，求得最终结果。</li>
</ol>
<p><img src="/img/v2-18ea022c73ac01b8903a511f031bf8a4_b.jpg" alt="img"></p>
<p>图5 矩阵分块运算(inner product)</p>
<p>\2. Outer Product: 按列切分A矩阵，按行切分B矩阵，使用A矩阵的一个按列切分的子块同B矩阵按行切分的子块做矩阵乘法，求得一个形状同C矩阵相同的中间结果矩阵。依次循环，对所有的中间结果矩阵求和，可得最终结果。下图中，将A、B矩阵切分为4个子矩阵，然后进行4次矩阵乘，再对 C1、C2、C3 和 C4 进行求和，可以算出最终结果。</p>
<p><img src="/img/v2-f28daba55db094e3aec589dd28b44d5e_b.jpg" alt="img"></p>
<p>图6 矩阵分块运算示例(outer product)</p>
<p>在 <strong>avx2_col_major_sgemm</strong> 的实现代码中，按照如下方式对矩阵A（512 x 512）、B（512 x 512）进行切分计算，具体步骤如下：</p>
<ol>
<li>将矩阵A（512 x 512）切分为 2 x 8 = 16 个形状为 64 x 256 的子矩阵；</li>
<li>将矩阵B（512 x 512）切分为 2 个形状为 256 x 512 的子矩阵；</li>
<li>对矩阵B的1号子矩阵进行数据打包， 然后对矩阵A的1号子矩阵进行数据打包，对TMPA1（64x256）和 TMPB1（256 x 512）进行一次矩阵乘运算，求得图中的 c11 (64 x 512) ; 在对矩阵A的2号子矩阵进行数据打包，求得c12;依次循环，直到求得 c18; </li>
<li>对矩阵B的2号子矩阵进行数据打包， 然后对矩阵A的9号子矩阵进行数据打包，对TMPA1（64x256）和 TMPB1（256 x 512）进行一次矩阵乘运算，求得图中的 c21 (64 x 512) ; 在对矩阵A的10号子矩阵进行数据打包，求得c22;依次循环，直到求得 c28; </li>
<li>对中间结果矩阵进行求和，可得最终的结果矩阵 C。</li>
</ol>
<p><img src="/img/v2-767390400b8a5ec779a80f35c26a0fdb_b.jpg" alt="img"></p>
<p>图7 M=N=K=512 矩阵的切分计算示例 </p>
<p>通过上面的介绍，相信读者已经对如何进行矩阵分块有了清晰的认识，其<a href="https://www.zhihu.com/search?q=实矩阵&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={&quot;sourceType&quot;%3A&quot;answer&quot;%2C&quot;sourceId&quot;%3A2849376497}">实矩阵</a>分块的思想很简单，就是将原始输入矩阵切分为小矩阵，使得L2 cache可以容纳计算所需的小矩阵。</p>
<p>现在已经粗粒度的讲解了如何对矩阵A和矩阵B进行分块计算，那么矩阵A的子矩阵（64 x 256）和 矩阵B的子矩阵（256 x 512）是如何计算的呢？</p>
<ol>
<li>在数据打包时，将子矩阵 a（64 x 256）按行进行切分，分为 4 个形状为 16 x 256 的小矩阵；</li>
<li>在数据打包时，将子矩阵 b（256 x 512）按列进行切分，分为 86 个形状为 256 x 6 的小矩阵；当子矩阵 b 的列数不是 6 的整数倍时，需在数据打包时，进行 padding。</li>
<li>使用子矩阵 a 的1号子矩阵（16 x 256）依次和子矩阵 b 的86个子矩阵进行矩阵乘计算，计算结果为 （16 x 256）X  （256 x 6）= （16 x 6）；最终可得（16 x 6）x 86 个子矩阵；</li>
<li>依此遍历子矩阵 a 的1、2、3、4号子矩阵进行步骤3中的运算；</li>
</ol>
<p><img src="/img/v2-ddacfb2cdaad74d0bbf818cb12910e34_b.jpg" alt="img"></p>
<p>图8 左矩阵(64x256)和右矩阵(256x512)的计算</p>
<p>上文的描述中，详细介绍如何对矩阵A的子矩阵（64 x 256）和矩阵B的子矩阵（256 x 512）进行计算，后面会结合代码对如何使用SIMD指令进行数据打包的细节演示。</p>
<p><strong>矩阵A的数据打包</strong></p>
<p><img src="/img/v2-5270e7dd93dfe37e3baf98f01e8e723d_b.jpg" alt="img"></p>
<p>图9 矩阵A的数据打包</p>
<p>代码实现，暂时没进行深入讲解，比较好理解。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line">//  pack block_size on leading dimension, t denotes transpose.</span><br><span class="line">//  eg. input:   A MxN matrix in row major, so the storage-format is (M, N)</span><br><span class="line">//      output:  B MxN matrix in col major(N-packed), so the storage-format is</span><br><span class="line">//                    (divUp(N, 16), M, 16)</span><br><span class="line">void pack_trans(float* a, int lda, float* b, int ldb, int m, int n) &#123;</span><br><span class="line">  constexpr int block_size = 16;</span><br><span class="line">  int i = 0;</span><br><span class="line"></span><br><span class="line">  for (; i + 64 &lt;= n; i += 64) &#123;</span><br><span class="line">    float* cur_a = a + i;</span><br><span class="line">    float* cur_b = b + i * ldb;</span><br><span class="line">    pack_trans_4x16(cur_a, lda, cur_b, ldb, m, block_size);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void pack_trans_4x16(float* a, const int lda, float* b, const int ldb, int m, int n) &#123;</span><br><span class="line">  const int m4 = m / 4;</span><br><span class="line">  const int m1 = m % 1;</span><br><span class="line">  const int block_size = 64;</span><br><span class="line">  const int ldbx16 = ldb * 16;  //(256 * 64)</span><br><span class="line"></span><br><span class="line">  float* tmpa = a;</span><br><span class="line">  // 保存指针 A 的4列元素</span><br><span class="line">  float* ar0 = tmpa + 0 * lda;</span><br><span class="line">  float* ar1 = tmpa + 1 * lda;</span><br><span class="line">  float* ar2 = tmpa + 2 * lda;</span><br><span class="line">  float* ar3 = tmpa + 3 * lda;</span><br><span class="line"></span><br><span class="line">  float* tmpb = b;</span><br><span class="line">  float* br0 = tmpb + 0 * ldbx16;</span><br><span class="line">  float* br1 = tmpb + 1 * ldbx16;</span><br><span class="line">  float* br2 = tmpb + 2 * ldbx16;</span><br><span class="line">  float* br3 = tmpb + 3 * ldbx16;</span><br><span class="line"></span><br><span class="line">  // 循环 256 / 4 = 64 次，每次 pack 4 x 16 = 64 个数据</span><br><span class="line">  for (int i = 0; i &lt; m4; ++i) &#123;</span><br><span class="line">    &#123;</span><br><span class="line">      __m256 v00 = _mm256_loadu_ps(ar0);</span><br><span class="line">      __m256 v01 = _mm256_loadu_ps(ar0 + 8);</span><br><span class="line">      __m256 v10 = _mm256_loadu_ps(ar1);</span><br><span class="line">      __m256 v11 = _mm256_loadu_ps(ar1 + 8);</span><br><span class="line">      __m256 v20 = _mm256_loadu_ps(ar2);</span><br><span class="line">      __m256 v21 = _mm256_loadu_ps(ar2 + 8);</span><br><span class="line">      __m256 v30 = _mm256_loadu_ps(ar3);</span><br><span class="line">      __m256 v31 = _mm256_loadu_ps(ar3 + 8);</span><br><span class="line"></span><br><span class="line">      _mm256_storeu_ps(br0 + 0, v00);</span><br><span class="line">      _mm256_storeu_ps(br0 + 8, v01);</span><br><span class="line">      _mm256_storeu_ps(br0 + 16, v10);</span><br><span class="line">      _mm256_storeu_ps(br0 + 24, v11);</span><br><span class="line">      _mm256_storeu_ps(br0 + 32, v20);</span><br><span class="line">      _mm256_storeu_ps(br0 + 40, v21);</span><br><span class="line">      _mm256_storeu_ps(br0 + 48, v30);</span><br><span class="line">      _mm256_storeu_ps(br0 + 56, v31);</span><br><span class="line">    &#125;</span><br><span class="line">    &#123;</span><br><span class="line">      __m256 v00 = _mm256_loadu_ps(ar0 + 16);</span><br><span class="line">      __m256 v01 = _mm256_loadu_ps(ar0 + 24);</span><br><span class="line">      __m256 v10 = _mm256_loadu_ps(ar1 + 16);</span><br><span class="line">      __m256 v11 = _mm256_loadu_ps(ar1 + 24);</span><br><span class="line">      __m256 v20 = _mm256_loadu_ps(ar2 + 16);</span><br><span class="line">      __m256 v21 = _mm256_loadu_ps(ar2 + 24);</span><br><span class="line">      __m256 v30 = _mm256_loadu_ps(ar3 + 16);</span><br><span class="line">      __m256 v31 = _mm256_loadu_ps(ar3 + 24);</span><br><span class="line"></span><br><span class="line">      _mm256_storeu_ps(br1 + 0, v00);</span><br><span class="line">      _mm256_storeu_ps(br1 + 8, v01);</span><br><span class="line">      _mm256_storeu_ps(br1 + 16, v10);</span><br><span class="line">      _mm256_storeu_ps(br1 + 24, v11);</span><br><span class="line">      _mm256_storeu_ps(br1 + 32, v20);</span><br><span class="line">      _mm256_storeu_ps(br1 + 40, v21);</span><br><span class="line">      _mm256_storeu_ps(br1 + 48, v30);</span><br><span class="line">      _mm256_storeu_ps(br1 + 56, v31);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#123;</span><br><span class="line">      __m256 v00 = _mm256_loadu_ps(ar0 + 32);</span><br><span class="line">      __m256 v01 = _mm256_loadu_ps(ar0 + 40);</span><br><span class="line">      __m256 v10 = _mm256_loadu_ps(ar1 + 32);</span><br><span class="line">      __m256 v11 = _mm256_loadu_ps(ar1 + 40);</span><br><span class="line">      __m256 v20 = _mm256_loadu_ps(ar2 + 32);</span><br><span class="line">      __m256 v21 = _mm256_loadu_ps(ar2 + 40);</span><br><span class="line">      __m256 v30 = _mm256_loadu_ps(ar3 + 32);</span><br><span class="line">      __m256 v31 = _mm256_loadu_ps(ar3 + 40);</span><br><span class="line"></span><br><span class="line">      _mm256_storeu_ps(br2 + 0, v00);</span><br><span class="line">      _mm256_storeu_ps(br2 + 8, v01);</span><br><span class="line">      _mm256_storeu_ps(br2 + 16, v10);</span><br><span class="line">      _mm256_storeu_ps(br2 + 24, v11);</span><br><span class="line">      _mm256_storeu_ps(br2 + 32, v20);</span><br><span class="line">      _mm256_storeu_ps(br2 + 40, v21);</span><br><span class="line">      _mm256_storeu_ps(br2 + 48, v30);</span><br><span class="line">      _mm256_storeu_ps(br2 + 56, v31);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#123;</span><br><span class="line">      __m256 v00 = _mm256_loadu_ps(ar0 + 48);</span><br><span class="line">      __m256 v01 = _mm256_loadu_ps(ar0 + 56);</span><br><span class="line">      __m256 v10 = _mm256_loadu_ps(ar1 + 48);</span><br><span class="line">      __m256 v11 = _mm256_loadu_ps(ar1 + 56);</span><br><span class="line">      __m256 v20 = _mm256_loadu_ps(ar2 + 48);</span><br><span class="line">      __m256 v21 = _mm256_loadu_ps(ar2 + 56);</span><br><span class="line">      __m256 v30 = _mm256_loadu_ps(ar3 + 48);</span><br><span class="line">      __m256 v31 = _mm256_loadu_ps(ar3 + 56);</span><br><span class="line"></span><br><span class="line">      _mm256_storeu_ps(br3 + 0, v00);</span><br><span class="line">      _mm256_storeu_ps(br3 + 8, v01);</span><br><span class="line">      _mm256_storeu_ps(br3 + 16, v10);</span><br><span class="line">      _mm256_storeu_ps(br3 + 24, v11);</span><br><span class="line">      _mm256_storeu_ps(br3 + 32, v20);</span><br><span class="line">      _mm256_storeu_ps(br3 + 40, v21);</span><br><span class="line">      _mm256_storeu_ps(br3 + 48, v30);</span><br><span class="line">      _mm256_storeu_ps(br3 + 56, v31);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ar0 += 4 * lda;</span><br><span class="line">    ar1 += 4 * lda;</span><br><span class="line">    ar2 += 4 * lda;</span><br><span class="line">    ar3 += 4 * lda;</span><br><span class="line"></span><br><span class="line">    br0 += block_size;</span><br><span class="line">    br1 += block_size;</span><br><span class="line">    br2 += block_size;</span><br><span class="line">    br3 += block_size;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>矩阵B的数据打包</strong></p>
<p><img src="/img/v2-f2712a98537424719bf96107b29eb52f_b.jpg" alt="img"></p>
<p>图10 矩阵B的数据打包</p>
<p>代码实现，暂时没进行深入讲解，比较好理解。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">id pack_no_trans_n6(float* a, const int lda, float* b, const int ldb,</span><br><span class="line">                      const int m, const int n) &#123;</span><br><span class="line">  const int m8 = m / 8;</span><br><span class="line">  const int m1 = m % 8;</span><br><span class="line">  const int block_size = n;</span><br><span class="line"></span><br><span class="line">  float* tmpa = a;</span><br><span class="line">  float* tmpb = b;</span><br><span class="line">  float* a0 = tmpa + 0 * lda;</span><br><span class="line">  float* a1 = tmpa + 1 * lda;</span><br><span class="line">  float* a2 = tmpa + 2 * lda;</span><br><span class="line">  float* a3 = tmpa + 3 * lda;</span><br><span class="line">  float* a4 = tmpa + 4 * lda;</span><br><span class="line">  float* a5 = tmpa + 5 * lda;</span><br><span class="line"></span><br><span class="line">  for (int i = 0; i &lt; m8; i++) &#123;</span><br><span class="line">    __m256 v0 = _mm256_loadu_ps(a0);</span><br><span class="line">    __m256 v1 = _mm256_loadu_ps(a1);</span><br><span class="line">    __m256 v2 = _mm256_loadu_ps(a2);</span><br><span class="line">    __m256 v3 = _mm256_loadu_ps(a3);</span><br><span class="line">    __m256 v4 = _mm256_loadu_ps(a4);</span><br><span class="line">    __m256 v5 = _mm256_loadu_ps(a5);</span><br><span class="line"></span><br><span class="line">    __m256 unpack0 = _mm256_unpacklo_ps(v0, v1);</span><br><span class="line">    __m256 unpack1 = _mm256_unpackhi_ps(v0, v1);</span><br><span class="line">    __m256 unpack2 = _mm256_unpacklo_ps(v2, v3);</span><br><span class="line">    __m256 unpack3 = _mm256_unpackhi_ps(v2, v3);</span><br><span class="line">    __m256 unpack4 = _mm256_unpacklo_ps(v4, v5);</span><br><span class="line">    __m256 unpack5 = _mm256_unpackhi_ps(v4, v5);</span><br><span class="line"></span><br><span class="line">    __m256 shf0 = _mm256_shuffle_ps(unpack0, unpack2, 0x44);</span><br><span class="line">    __m256 shf1 = _mm256_shuffle_ps(unpack4, unpack0, 0xe4);</span><br><span class="line">    __m256 shf2 = _mm256_shuffle_ps(unpack2, unpack4, 0xee);</span><br><span class="line">    __m256 shf3 = _mm256_shuffle_ps(unpack5, unpack1, 0xe4);</span><br><span class="line">    __m256 shf4 = _mm256_shuffle_ps(unpack3, unpack5, 0xee);</span><br><span class="line">    __m256 shf5 = _mm256_shuffle_ps(unpack1, unpack3, 0x44);</span><br><span class="line"></span><br><span class="line">    __m128 low_shf1 = _mm256_castps256_ps128(shf1);</span><br><span class="line">    __m256 res0 = _mm256_insertf128_ps(shf0, low_shf1, 0x1);</span><br><span class="line">    __m256 res1 = _mm256_permute2f128_ps(shf0, shf1, 0x31);</span><br><span class="line"></span><br><span class="line">    __m128 low_shf5 = _mm256_castps256_ps128(shf5);</span><br><span class="line">    __m256 res2 = _mm256_insertf128_ps(shf2, low_shf5, 0x1);</span><br><span class="line">    __m256 res3 = _mm256_permute2f128_ps(shf2, shf5, 0x31);</span><br><span class="line"></span><br><span class="line">    __m128 low_shf4 = _mm256_castps256_ps128(shf4);</span><br><span class="line">    __m256 res4 = _mm256_insertf128_ps(shf3, low_shf4, 0x1);</span><br><span class="line">    __m256 res5 = _mm256_permute2f128_ps(shf3, shf4, 0x31);</span><br><span class="line"></span><br><span class="line">    constexpr int vsize_in_bytes = 8;</span><br><span class="line">    _mm256_storeu_ps(tmpb + 0 * vsize_in_bytes, res0);</span><br><span class="line">    _mm256_storeu_ps(tmpb + 1 * vsize_in_bytes, res2);</span><br><span class="line">    _mm256_storeu_ps(tmpb + 2 * vsize_in_bytes, res4);</span><br><span class="line">    _mm256_storeu_ps(tmpb + 3 * vsize_in_bytes, res1);</span><br><span class="line">    _mm256_storeu_ps(tmpb + 4 * vsize_in_bytes, res3);</span><br><span class="line">    _mm256_storeu_ps(tmpb + 5 * vsize_in_bytes, res5);</span><br><span class="line"></span><br><span class="line">    tmpb += 6 * vsize_in_bytes;</span><br><span class="line"></span><br><span class="line">    // jump to another 8 float point values</span><br><span class="line">    a0 += vsize_in_bytes;</span><br><span class="line">    a1 += vsize_in_bytes;</span><br><span class="line">    a2 += vsize_in_bytes;</span><br><span class="line">    a3 += vsize_in_bytes;</span><br><span class="line">    a4 += vsize_in_bytes;</span><br><span class="line">    a5 += vsize_in_bytes;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="寄存器优化-Micro-Kernel"><a href="#寄存器优化-Micro-Kernel" class="headerlink" title="寄存器优化(Micro Kernel)"></a>寄存器优化(Micro Kernel)</h3><p>在数据打包的讲解中，有以下描述</p>
<blockquote>
<p>使用子矩阵 a 的1号子矩阵（16 x 256）依次和子矩阵 b 的86个子矩阵进行矩阵乘计算，计算结果为 （16 x 256）X  （256 x 6）= （16 x 6）；最终可得（16 x 6）x 86 个子矩阵；  </p>
</blockquote>
<p>在 <strong>avx2_col_major_sgemm</strong> 的实现中，使用 A(16, 8) * B(8, 6) = C(16, 6) 的Micro Kernel，其计算思路如下，图片和下面的描述均来自一篇很好的文章 <a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/LF_AI/article/details/125308378">《OneDNN GEMM(AVX FP32)算法浅析》</a>。</p>
<p><img src="/img/v2-9081451191194b2134e1a0888fe14079_b.jpg" alt="img"></p>
<p>图11 micro kernel 寄存器优化</p>
<p>Micro Kernel 的计算步骤如下描述</p>
<ol>
<li>在 micro kernel 中，首先使用12个YMM寄存器用以保存结果矩阵 C（shape 为 16x6）；</li>
<li>通过_mm256_loadu_ps指令将A矩阵的第一列移动到两个YMM寄存器中（这里假设为YMM0以及YMM1）；</li>
<li>对于B矩阵第一行的第一个元素，使用_mm256_broadcast_ss指令进行广播并存储到一个YMM寄存器内（这里假设为YMM2），然后使用fma指令_mm256_fmadd_ps将YMM0和YMM1内的元素与YMM2内元素对应相乘，并将结果累加到C矩阵的两个YMM寄存器内，这里假设为YMM4以及YMM5；</li>
<li>沿着B矩阵第一行进行循环，重复步骤2，B矩阵广播当前行内其它数据时重复使用YMM2寄存器，并将计算结果依次累加到YMM6~YMM15寄存器内；</li>
<li>A矩阵前进一列，B矩阵前进一行，并重复步骤1~3，最终完成整个C(16, 6)矩阵的计算。</li>
</ol>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line">void col_major_micro_kernel_m16n6(const int K, const float alpha,</span><br><span class="line">                                  const float* src_a, const int lda,</span><br><span class="line">                                  const float* src_b, int ldb, const float beta,</span><br><span class="line">                                  float* dst_c, int ldc) &#123;</span><br><span class="line">  constexpr int m_block_size = 16;</span><br><span class="line">  constexpr int n_block_size = 6;</span><br><span class="line"></span><br><span class="line">  // Load result matrix c (shape 16x6) into 12 x __m256 vector values</span><br><span class="line">  __m256 c00 = _mm256_loadu_ps(dst_c + 0 * ldc);</span><br><span class="line">  __m256 c01 = _mm256_loadu_ps(dst_c + 0 * ldc + 8);</span><br><span class="line"></span><br><span class="line">  __m256 c10 = _mm256_loadu_ps(dst_c + 1 * ldc);</span><br><span class="line">  __m256 c11 = _mm256_loadu_ps(dst_c + 1 * ldc + 8);</span><br><span class="line"></span><br><span class="line">  __m256 c20 = _mm256_loadu_ps(dst_c + 2 * ldc);</span><br><span class="line">  __m256 c21 = _mm256_loadu_ps(dst_c + 2 * ldc + 8);</span><br><span class="line"></span><br><span class="line">  __m256 c30 = _mm256_loadu_ps(dst_c + 3 * ldc);</span><br><span class="line">  __m256 c31 = _mm256_loadu_ps(dst_c + 3 * ldc + 8);</span><br><span class="line"></span><br><span class="line">  __m256 c40 = _mm256_loadu_ps(dst_c + 4 * ldc);</span><br><span class="line">  __m256 c41 = _mm256_loadu_ps(dst_c + 4 * ldc + 8);</span><br><span class="line"></span><br><span class="line">  __m256 c50 = _mm256_loadu_ps(dst_c + 5 * ldc);</span><br><span class="line">  __m256 c51 = _mm256_loadu_ps(dst_c + 5 * ldc + 8);</span><br><span class="line"></span><br><span class="line">  // c = c * beta</span><br><span class="line">  __m256 vbeta = _mm256_set1_ps(beta);</span><br><span class="line"></span><br><span class="line">  c00 = _mm256_mul_ps(c00, vbeta);</span><br><span class="line">  c01 = _mm256_mul_ps(c01, vbeta);</span><br><span class="line"></span><br><span class="line">  c10 = _mm256_mul_ps(c10, vbeta);</span><br><span class="line">  c11 = _mm256_mul_ps(c11, vbeta);</span><br><span class="line"></span><br><span class="line">  c20 = _mm256_mul_ps(c20, vbeta);</span><br><span class="line">  c21 = _mm256_mul_ps(c21, vbeta);</span><br><span class="line"></span><br><span class="line">  c30 = _mm256_mul_ps(c30, vbeta);</span><br><span class="line">  c31 = _mm256_mul_ps(c31, vbeta);</span><br><span class="line"></span><br><span class="line">  c40 = _mm256_mul_ps(c40, vbeta);</span><br><span class="line">  c41 = _mm256_mul_ps(c41, vbeta);</span><br><span class="line"></span><br><span class="line">  c50 = _mm256_mul_ps(c50, vbeta);</span><br><span class="line">  c51 = _mm256_mul_ps(c51, vbeta);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  for (int k = 0; k &lt; K; ++k) &#123;</span><br><span class="line">    __m256 a0 = _mm256_loadu_ps(src_a);</span><br><span class="line">    __m256 a1 = _mm256_loadu_ps(src_a + 8);</span><br><span class="line"></span><br><span class="line">    __m256 vb = _mm256_broadcast_ss(src_b);</span><br><span class="line">    c00 = _mm256_fmadd_ps(a0, vb, c00);</span><br><span class="line">    c01 = _mm256_fmadd_ps(a1, vb, c01);</span><br><span class="line"></span><br><span class="line">    vb = _mm256_broadcast_ss(src_b + 1);</span><br><span class="line">    c10 = _mm256_fmadd_ps(a0, vb, c10);</span><br><span class="line">    c11 = _mm256_fmadd_ps(a1, vb, c11);</span><br><span class="line"></span><br><span class="line">    vb = _mm256_broadcast_ss(src_b + 2);</span><br><span class="line">    c20 = _mm256_fmadd_ps(a0, vb, c20);</span><br><span class="line">    c21 = _mm256_fmadd_ps(a1, vb, c21);</span><br><span class="line"></span><br><span class="line">    vb = _mm256_broadcast_ss(src_b + 3);</span><br><span class="line">    c30 = _mm256_fmadd_ps(a0, vb, c30);</span><br><span class="line">    c31 = _mm256_fmadd_ps(a1, vb, c31);</span><br><span class="line"></span><br><span class="line">    vb = _mm256_broadcast_ss(src_b + 4);</span><br><span class="line">    c40 = _mm256_fmadd_ps(a0, vb, c40);</span><br><span class="line">    c41 = _mm256_fmadd_ps(a1, vb, c41);</span><br><span class="line"></span><br><span class="line">    vb = _mm256_broadcast_ss(src_b + 5);</span><br><span class="line">    c50 = _mm256_fmadd_ps(a0, vb, c50);</span><br><span class="line">    c51 = _mm256_fmadd_ps(a1, vb, c51);</span><br><span class="line"></span><br><span class="line">    src_a += m_block_size;</span><br><span class="line">    src_b += n_block_size;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  __m256 valpha = _mm256_set1_ps(alpha);</span><br><span class="line">  c00 = _mm256_mul_ps(c00, valpha);</span><br><span class="line">  c01 = _mm256_mul_ps(c01, valpha);</span><br><span class="line"></span><br><span class="line">  c10 = _mm256_mul_ps(c10, valpha);</span><br><span class="line">  c11 = _mm256_mul_ps(c11, valpha);</span><br><span class="line"></span><br><span class="line">  c20 = _mm256_mul_ps(c20, valpha);</span><br><span class="line">  c21 = _mm256_mul_ps(c21, valpha);</span><br><span class="line"></span><br><span class="line">  c30 = _mm256_mul_ps(c30, valpha);</span><br><span class="line">  c31 = _mm256_mul_ps(c31, valpha);</span><br><span class="line"></span><br><span class="line">  c40 = _mm256_mul_ps(c40, valpha);</span><br><span class="line">  c41 = _mm256_mul_ps(c41, valpha);</span><br><span class="line"></span><br><span class="line">  c50 = _mm256_mul_ps(c50, valpha);</span><br><span class="line">  c51 = _mm256_mul_ps(c51, valpha);</span><br><span class="line"></span><br><span class="line">  _mm256_storeu_ps(dst_c + 0 * ldc, c00);</span><br><span class="line">  _mm256_storeu_ps(dst_c + 0 * ldc + 8, c01);</span><br><span class="line"></span><br><span class="line">  _mm256_storeu_ps(dst_c + 1 * ldc, c10);</span><br><span class="line">  _mm256_storeu_ps(dst_c + 1 * ldc + 8, c11);</span><br><span class="line"></span><br><span class="line">  _mm256_storeu_ps(dst_c + 2 * ldc, c20);</span><br><span class="line">  _mm256_storeu_ps(dst_c + 2 * ldc + 8, c21);</span><br><span class="line"></span><br><span class="line">  _mm256_storeu_ps(dst_c + 3 * ldc, c30);</span><br><span class="line">  _mm256_storeu_ps(dst_c + 3 * ldc + 8, c31);</span><br><span class="line"></span><br><span class="line">  _mm256_storeu_ps(dst_c + 4 * ldc, c40);</span><br><span class="line">  _mm256_storeu_ps(dst_c + 4 * ldc + 8, c41);</span><br><span class="line"></span><br><span class="line">  _mm256_storeu_ps(dst_c + 5 * ldc, c50);</span><br><span class="line">  _mm256_storeu_ps(dst_c + 5 * ldc + 8, c51);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="性能数据"><a href="#性能数据" class="headerlink" title="性能数据"></a>性能数据</h3><p>在经历过漫长的讲解之后，那么 avx2_col_major_sgemm 的性能究竟如何呢？且看下表中的数据，表中使用了两个比较基准作为参照，分别是</p>
<ol>
<li><strong>Naive</strong>, 最基础的矩阵乘算法实现，代码文中已经提供；</li>
<li><strong>oneDNN sgemm</strong>，oneDNN是英特尔公司大名鼎鼎的多平台支持、高性能计算库，其前身是 mkldnn。oneDNN 在各类硬件上都进行了深度优化，特别是在Intel CPU 上，其性能数据非常具备参考价值。</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>Shape(M, N, K)</th>
<th>Naive GFLOPs</th>
<th>oneDNN sgemm GFLOPs</th>
<th>avx2_col_major_sgemm</th>
</tr>
</thead>
<tbody>
<tr>
<td>(64, 64, 64)</td>
<td>1.96</td>
<td>32.97</td>
<td>35.42</td>
</tr>
<tr>
<td>(128, 128, 128)</td>
<td>1.65</td>
<td>62.69</td>
<td>40.36</td>
</tr>
<tr>
<td>(256, 256, 256)</td>
<td>1.44</td>
<td>73.19</td>
<td>65.84</td>
</tr>
<tr>
<td>(512, 512, 512)</td>
<td>0.95</td>
<td>70.06</td>
<td>67.65</td>
</tr>
<tr>
<td>(1024, 1024, 1024)</td>
<td>0.61</td>
<td>79.73</td>
<td>69.12</td>
</tr>
</tbody>
</table>
</div>
<p>从数据上来看，avx2_col_major_sgemm 相较于 Naive 实现已经具备了质的飞跃，并且在多数shapes下可以取得接近 oneDNN 的性能，不过在 (128, 128, 128) 下和oneDNN 存在比较大的性能差异，这也说明 avx2_col_major_sgemm 仍然存在一定的优化空间。这很令人兴奋，不是么？</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%A7%AF%E7%B4%AF/" rel="tag"># 积累</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/10/03/cuda%E6%95%99%E7%A8%8B%E5%8A%A0%E4%BB%A3%E7%A0%81/" rel="prev" title="CUDA教程">
      <i class="fa fa-chevron-left"></i> CUDA教程
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/12/22/CUDA%E5%AE%98%E6%96%B9%E6%89%8B%E5%86%8C/" rel="next" title="CUDA 编程手册">
      CUDA 编程手册 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Overview"><span class="nav-number">1.</span> <span class="nav-text">Overview</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#64-Thread-Implementation"><span class="nav-number">2.</span> <span class="nav-text">64 Thread Implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BDA%E5%92%8CB%EF%BC%8C%E7%84%B6%E5%90%8E%E5%AD%98%E5%82%A8%E5%88%B0%E5%85%B1%E4%BA%AB"><span class="nav-number">2.1.</span> <span class="nav-text">加载A和B，然后存储到共享</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%8E%E5%85%B1%E4%BA%AB%E8%AF%BB%E5%8F%96"><span class="nav-number">2.2.</span> <span class="nav-text">从共享读取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97C%EF%BC%9A%E5%AF%84%E5%AD%98%E5%99%A8bank%E5%92%8C%E9%87%8D%E7%94%A8"><span class="nav-number">2.3.</span> <span class="nav-text">计算C：寄存器bank和重用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#warp%E5%90%8C%E6%AD%A5%E6%97%A0%E5%BA%8F%E6%98%A0%E5%B0%84"><span class="nav-number">2.4.</span> <span class="nav-text">warp同步无序映射</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Warp-Shuffling%E5%92%8C%E8%81%94%E5%90%88%E5%AD%98%E5%82%A8%E5%88%B0%E5%85%A8%E5%B1%80"><span class="nav-number">2.5.</span> <span class="nav-text">Warp Shuffling和联合存储到全局</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SGEMM-256-Thread-Implementation"><span class="nav-number">3.</span> <span class="nav-text">SGEMM - 256 Thread Implementation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Loading-A-and-B"><span class="nav-number">3.1.</span> <span class="nav-text">Loading A and B</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Storing-to-Shared"><span class="nav-number">3.2.</span> <span class="nav-text">Storing to Shared</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reading-from-Shared"><span class="nav-number">3.3.</span> <span class="nav-text">Reading from Shared</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Warp-Synchronous-Shuffle"><span class="nav-number">3.4.</span> <span class="nav-text">Warp Synchronous Shuffle</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Storing-to-Global"><span class="nav-number">3.5.</span> <span class="nav-text">Storing to Global</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BAGPU%E4%BC%98%E5%8C%96%E7%B3%BB%E5%88%97"><span class="nav-number">4.</span> <span class="nav-text">深入浅出GPU优化系列</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">4.1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1"><span class="nav-number">4.2.</span> <span class="nav-text">并行算法设计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reduce%E4%BC%98%E5%8C%96"><span class="nav-number">4.3.</span> <span class="nav-text">reduce优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#reduce-baseline%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D"><span class="nav-number">4.3.1.</span> <span class="nav-text">reduce baseline算法介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A71%EF%BC%9A%E8%A7%A3%E5%86%B3warp-divergence"><span class="nav-number">4.3.2.</span> <span class="nav-text">优化技巧1：解决warp divergence</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%B0%E6%9C%89%E9%97%AE%E9%A2%98"><span class="nav-number">4.3.2.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E5%BC%8F"><span class="nav-number">4.3.2.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A72%EF%BC%9A%E8%A7%A3%E5%86%B3bank%E5%86%B2%E7%AA%81"><span class="nav-number">4.3.3.</span> <span class="nav-text">优化技巧2：解决bank冲突</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%B0%E6%9C%89%E9%97%AE%E9%A2%98-1"><span class="nav-number">4.3.3.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E5%BC%8F-1"><span class="nav-number">4.3.3.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A73%EF%BC%9A%E8%A7%A3%E5%86%B3idle%E7%BA%BF%E7%A8%8B"><span class="nav-number">4.3.4.</span> <span class="nav-text">优化技巧3：解决idle线程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%B0%E6%9C%89%E9%97%AE%E9%A2%98-2"><span class="nav-number">4.3.4.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E5%BC%8F-2"><span class="nav-number">4.3.4.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A74%EF%BC%9A%E5%B1%95%E5%BC%80%E6%9C%80%E5%90%8E%E4%B8%80%E7%BB%B4%E5%87%8F%E5%B0%91%E5%90%8C%E6%AD%A5"><span class="nav-number">4.3.5.</span> <span class="nav-text">优化技巧4：展开最后一维减少同步</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%B0%E6%9C%89%E9%97%AE%E9%A2%98-3"><span class="nav-number">4.3.5.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E5%BC%8F-3"><span class="nav-number">4.3.5.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A75%EF%BC%9A%E5%AE%8C%E5%85%A8%E5%B1%95%E5%BC%80%E5%87%8F%E5%B0%91%E8%AE%A1%E7%AE%97"><span class="nav-number">4.3.6.</span> <span class="nav-text">优化技巧5：完全展开减少计算</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%B0%E6%9C%89%E9%97%AE%E9%A2%98-4"><span class="nav-number">4.3.6.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="nav-number">4.3.6.2.</span> <span class="nav-text">解决方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A76%EF%BC%9A%E5%90%88%E7%90%86%E8%AE%BE%E7%BD%AEblock%E6%95%B0%E9%87%8F"><span class="nav-number">4.3.7.</span> <span class="nav-text">优化技巧6：合理设置block数量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%B0%E6%9C%89%E9%97%AE%E9%A2%98-5"><span class="nav-number">4.3.7.1.</span> <span class="nav-text">现有问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E5%BC%8F-4"><span class="nav-number">4.3.7.2.</span> <span class="nav-text">解决方式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A77%EF%BC%9A%E4%BD%BF%E7%94%A8shuffle%E6%8C%87%E4%BB%A4"><span class="nav-number">4.3.8.</span> <span class="nav-text">优化技巧7：使用shuffle指令</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%B0%E6%9C%89%E9%97%AE%E9%A2%98-6"><span class="nav-number">4.3.8.1.</span> <span class="nav-text">现有问题</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GEMM%E4%BC%98%E5%8C%96"><span class="nav-number">4.4.</span> <span class="nav-text">GEMM优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E8%A8%80-1"><span class="nav-number">4.4.1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8Eglobal-memory%E5%88%B0shared-memory"><span class="nav-number">4.4.2.</span> <span class="nav-text">从global memory到shared memory</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8Eshared-memory%E5%88%B0register"><span class="nav-number">4.4.3.</span> <span class="nav-text">从shared memory到register</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#register%E5%88%86%E5%9D%97"><span class="nav-number">4.4.4.</span> <span class="nav-text">register分块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%9A%84prefetch"><span class="nav-number">4.4.5.</span> <span class="nav-text">数据的prefetch</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GEMM%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0"><span class="nav-number">4.5.</span> <span class="nav-text">GEMM算法概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E9%87%87%E7%94%A8%E6%95%B0%E6%8D%AE%E9%A2%84%E5%8F%96"><span class="nav-number">4.5.1.</span> <span class="nav-text">不采用数据预取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%87%E7%94%A8%E6%95%B0%E6%8D%AE%E9%A2%84%E5%8F%96"><span class="nav-number">4.5.2.</span> <span class="nav-text">采用数据预取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GEMM%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="nav-number">4.5.3.</span> <span class="nav-text">GEMM代码解析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="nav-number">4.5.3.1.</span> <span class="nav-text">参数说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%A7%E8%BF%AD%E4%BB%A3%E5%89%8D%E9%A2%84%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="nav-number">4.5.3.2.</span> <span class="nav-text">大迭代前预取数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%A7%E8%BF%AD%E4%BB%A3%E9%80%BB%E8%BE%91"><span class="nav-number">4.5.3.3.</span> <span class="nav-text">大迭代逻辑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%A7%E8%BF%AD%E4%BB%A3%E8%AF%A6%E7%BB%86%E8%A7%A3%E6%9E%90"><span class="nav-number">4.5.3.4.</span> <span class="nav-text">大迭代详细解析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C%E5%86%99%E5%9B%9E"><span class="nav-number">4.5.3.5.</span> <span class="nav-text">计算结果写回</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">4.5.4.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8E%E6%B1%87%E7%BC%96%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD"><span class="nav-number">4.5.5.</span> <span class="nav-text">从汇编代码分析程序性能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E4%BA%8E%E7%8E%B0%E6%9C%89sgemm%E7%9A%84%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90%E5%8F%8A%E8%A7%82%E5%AF%9F"><span class="nav-number">4.5.6.</span> <span class="nav-text">对于现有sgemm的代码分析及观察</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%87%E7%BC%96%E7%BA%A7%E5%88%AB%E4%BB%A3%E7%A0%81%E8%B0%83%E6%95%B4"><span class="nav-number">4.5.7.</span> <span class="nav-text">汇编级别代码调整</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AF%84%E5%AD%98%E5%99%A8%E7%9A%84%E9%87%8D%E6%98%A0%E5%B0%84"><span class="nav-number">4.5.7.1.</span> <span class="nav-text">寄存器的重映射</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E4%BB%A4%E9%87%8D%E6%8E%92"><span class="nav-number">4.5.7.2.</span> <span class="nav-text">指令重排</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E4%B8%8E%E6%80%BB%E7%BB%93"><span class="nav-number">4.5.8.</span> <span class="nav-text">实验与总结</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CUDA-C-%E8%B0%83%E4%BC%98"><span class="nav-number">4.5.8.1.</span> <span class="nav-text">CUDA C 调优</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B1%87%E7%BC%96%E4%BB%A3%E7%A0%81%E8%B0%83%E4%BC%98"><span class="nav-number">4.5.8.2.</span> <span class="nav-text">汇编代码调优</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SGEMM"><span class="nav-number">5.</span> <span class="nav-text">SGEMM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="nav-number">5.1.</span> <span class="nav-text">基础知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8C%BA%E5%88%86-FLOPS-%E5%92%8C-FLOPs"><span class="nav-number">5.1.1.</span> <span class="nav-text">区分 FLOPS 和 FLOPs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97-CPU-%E7%9A%84-FLOPS"><span class="nav-number">5.1.2.</span> <span class="nav-text">计算 CPU 的 FLOPS</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%9F%A9%E9%98%B5%E4%B9%98%E5%AE%9E%E7%8E%B0%E5%92%8C%E4%BC%98%E5%8C%96"><span class="nav-number">5.2.</span> <span class="nav-text">基础矩阵乘实现和优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80-GEMM-%E5%AE%9E%E7%8E%B0%E5%92%8C%E5%BA%A6%E9%87%8F"><span class="nav-number">5.2.1.</span> <span class="nav-text">基础 GEMM 实现和度量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E8%A1%8C%E4%BB%A3%E7%A0%81%E4%BC%98%E5%8C%96%E5%8D%81%E5%80%8D%E6%80%A7%E8%83%BD"><span class="nav-number">5.2.2.</span> <span class="nav-text">一行代码优化十倍性能</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BLAS-%E6%8E%A5%E5%8F%A3%E7%AE%80%E4%BB%8B"><span class="nav-number">5.3.</span> <span class="nav-text">BLAS 接口简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%8C%96%E7%9F%A9%E9%98%B5%E4%B9%98%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.4.</span> <span class="nav-text">深度优化矩阵乘实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%89%93%E5%8C%85"><span class="nav-number">5.4.1.</span> <span class="nav-text">数据打包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%84%E5%AD%98%E5%99%A8%E4%BC%98%E5%8C%96-Micro-Kernel"><span class="nav-number">5.4.2.</span> <span class="nav-text">寄存器优化(Micro Kernel)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E6%95%B0%E6%8D%AE"><span class="nav-number">5.4.3.</span> <span class="nav-text">性能数据</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Hao Yu</p>
  <div class="site-description" itemprop="description">Introduce something interesting and recode learning process, some articles are written by others, the original link has been given as much as possible, thanks to the original author</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">128</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hao Yu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
