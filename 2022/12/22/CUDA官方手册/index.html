<!DOCTYPE html>
<html lang="zn-ch">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="From Ken He 1.CUDA简介1.1 我们为什么要使用GPUGPU（Graphics Processing Unit）在相同的价格和功率范围内，比CPU提供更高的指令吞吐量和内存带宽。许多应用程序利用这些更高的能力，在GPU上比在CPU上运行得更快(参见GPU应用程序)。其他计算设备，如FPGA，也非常节能，但提供的编程灵活性要比GPU少得多。 GPU和CPU在功能上的差异是因为它们的设">
<meta property="og:type" content="article">
<meta property="og:title" content="CUDA 编程手册">
<meta property="og:url" content="http://yoursite.com/2022/12/22/CUDA%E5%AE%98%E6%96%B9%E6%89%8B%E5%86%8C/index.html">
<meta property="og:site_name" content="Hao Yu&#39;s blog">
<meta property="og:description" content="From Ken He 1.CUDA简介1.1 我们为什么要使用GPUGPU（Graphics Processing Unit）在相同的价格和功率范围内，比CPU提供更高的指令吞吐量和内存带宽。许多应用程序利用这些更高的能力，在GPU上比在CPU上运行得更快(参见GPU应用程序)。其他计算设备，如FPGA，也非常节能，但提供的编程灵活性要比GPU少得多。 GPU和CPU在功能上的差异是因为它们的设">
<meta property="og:locale" content="zn_CH">
<meta property="og:image" content="http://yoursite.com/img/gpu-devotes-more-transistors-to-data-processing.png">
<meta property="og:image" content="http://yoursite.com/img/gpu-computing-applications.png">
<meta property="og:image" content="http://yoursite.com/img/automatic-scalability.png">
<meta property="og:image" content="http://yoursite.com/img/grid-of-thread-blocks.png">
<meta property="og:image" content="http://yoursite.com/img/automatic-scalability.png">
<meta property="og:image" content="http://yoursite.com/img/memory-hierarchy.png">
<meta property="og:image" content="http://yoursite.com/img/heterogeneous-programming.png">
<meta property="og:image" content="http://yoursite.com/img/matrix-multiplication-without-shared-memory.png">
<meta property="og:image" content="http://yoursite.com/img/matrix-multiplication-with-shared-memory.png">
<meta property="og:image" content="http://yoursite.com/img/child-graph.png">
<meta property="og:image" content="http://yoursite.com/img/create-a-graph.png">
<meta property="og:image" content="http://yoursite.com/img/compatibility-of-cuda-versions.png">
<meta property="og:image" content="http://yoursite.com/img/number_of_warps.png">
<meta property="og:image" content="http://yoursite.com/img/Throughput.png">
<meta property="og:image" content="http://yoursite.com/img/ID.png">
<meta property="og:image" content="http://yoursite.com/img/shfl.png">
<meta property="og:image" content="http://yoursite.com/img/shfl_up.png">
<meta property="og:image" content="http://yoursite.com/img/shfl_down.png">
<meta property="og:image" content="http://yoursite.com/img/shufl_xor.png">
<meta property="og:image" content="http://yoursite.com/img/parent-child-launch-nesting.png">
<meta property="og:image" content="http://yoursite.com/img/kernel-nodes.png">
<meta property="og:image" content="http://yoursite.com/img/new-alloc-node.png">
<meta property="og:image" content="http://yoursite.com/img/adding-new-alloc-nodes.png">
<meta property="og:image" content="http://yoursite.com/img/sequentially-launched-graphs.png">
<meta property="og:image" content="http://yoursite.com/img/nearest-point-sampling-of-1-d-texture-of-4-texels.png">
<meta property="og:image" content="http://yoursite.com/img/linear-filtering-of-1-d-texture-of-4-texels.png">
<meta property="og:image" content="http://yoursite.com/img/1-d-table-lookup-using-linear-filtering.png">
<meta property="og:image" content="http://yoursite.com/img/examples-of-global-memory-accesses.png">
<meta property="og:image" content="http://yoursite.com/img/examples-of-strided-shared-memory-accesses.png">
<meta property="og:image" content="http://yoursite.com/img/examples-of-irregular-shared-memory-accesses.png">
<meta property="og:image" content="http://yoursite.com/img/examples-of-strided-shared-memory-accesses.png">
<meta property="og:image" content="http://yoursite.com/img/examples-of-irregular-shared-memory-accesses.png">
<meta property="og:image" content="http://yoursite.com/img/library-context-management.png">
<meta property="article:published_time" content="2022-12-22T10:01:00.000Z">
<meta property="article:modified_time" content="2022-12-22T14:31:39.000Z">
<meta property="article:author" content="Hao Yu">
<meta property="article:tag" content="C++">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/img/gpu-devotes-more-transistors-to-data-processing.png">

<link rel="canonical" href="http://yoursite.com/2022/12/22/CUDA%E5%AE%98%E6%96%B9%E6%89%8B%E5%86%8C/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zn-ch'
  };
</script>

  <title>CUDA 编程手册 | Hao Yu's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hao Yu's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">The program monkey was eaten by the siege lion.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/resume.pdf" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">11</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">0</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">128</span></a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/yuhao0102" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zn-ch">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2022/12/22/CUDA%E5%AE%98%E6%96%B9%E6%89%8B%E5%86%8C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Hao Yu">
      <meta itemprop="description" content="Introduce something interesting and recode learning process, some articles are written by others, the original link has been given as much as possible, thanks to the original author">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hao Yu's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CUDA 编程手册
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-12-22 18:01:00 / Modified: 22:31:39" itemprop="dateCreated datePublished" datetime="2022-12-22T18:01:00+08:00">2022-12-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>From Ken He</p>
<h1 id="1-CUDA简介"><a href="#1-CUDA简介" class="headerlink" title="1.CUDA简介"></a>1.CUDA简介</h1><h2 id="1-1-我们为什么要使用GPU"><a href="#1-1-我们为什么要使用GPU" class="headerlink" title="1.1 我们为什么要使用GPU"></a>1.1 我们为什么要使用GPU</h2><p>GPU（Graphics Processing Unit）在相同的价格和功率范围内，比CPU提供更高的指令吞吐量和内存带宽。许多应用程序利用这些更高的能力，在GPU上比在CPU上运行得更快(参见<a href="https://www.nvidia.com/object/gpu-applications.html">GPU应用程序</a>)。其他计算设备，如FPGA，也非常节能，但提供的编程灵活性要比GPU少得多。</p>
<p>GPU和CPU在功能上的差异是因为它们的设计目标不同。虽然 CPU 旨在以尽可能快的速度执行一系列称为线程的操作，并且可以并行执行数十个这样的线程。但GPU却能并行执行成千上万个(摊销较慢的单线程性能以实现更大的吞吐量)。</p>
<p>GPU 专门用于高度并行计算，因此设计时更多的晶体管用于数据处理，而不是数据缓存和流量控制。</p>
<p>下图显示了 CPU 与 GPU 的芯片资源分布示例。</p>
<p><img src="/img/gpu-devotes-more-transistors-to-data-processing.png" alt="The GPU Devotes More Transistors to Data Processing"></p>
<p>将更多晶体管用于数据处理，例如浮点计算，有利于高度并行计算。GPU可以通过计算隐藏内存访问延迟，而不是依靠大数据缓存和复杂的流控制来避免长时间的内存访问延迟，这两者在晶体管方面都是昂贵的。</p>
<h2 id="1-2-CUDA®：通用并行计算平台和编程模型"><a href="#1-2-CUDA®：通用并行计算平台和编程模型" class="headerlink" title="1.2 CUDA®：通用并行计算平台和编程模型"></a>1.2 CUDA®：通用并行计算平台和编程模型</h2><p>2006 年 11 月，NVIDIA® 推出了 CUDA®，这是一种通用并行计算平台和编程模型，它利用 NVIDIA GPU 中的并行计算引擎以比 CPU 更有效的方式解决许多复杂的计算问题。</p>
<p>CUDA 附带一个软件环境，允许开发人员使用 C++ 作为高级编程语言。 如下图所示，支持其他语言、应用程序编程接口或基于指令的方法，例如 FORTRAN、DirectCompute、OpenACC。</p>
<p><img src="/img/gpu-computing-applications.png" alt="gpu-computing-applications.png"></p>
<h2 id="1-3-可扩展的编程模型"><a href="#1-3-可扩展的编程模型" class="headerlink" title="1.3 可扩展的编程模型"></a>1.3 可扩展的编程模型</h2><p>多核 CPU 和众核 GPU 的出现意味着主流处理器芯片现在是并行系统。挑战在于开发能够透明地扩展可并行的应用软件，来利用不断增加的处理器内核数量。就像 3D 图形应用程序透明地将其并行性扩展到具有广泛不同内核数量的多核 GPU 一样。</p>
<p>CUDA 并行编程模型旨在克服这一挑战，同时为熟悉 C 等标准编程语言的程序员保持较低的学习曲线。</p>
<p>其核心是三个关键抽象——线程组的层次结构、共享内存和屏障同步——它们只是作为最小的语言扩展集向程序员公开。</p>
<p>这些抽象提供了细粒度的数据并行和线程并行，嵌套在粗粒度的数据并行和任务并行中。它们指导程序员将问题划分为可以由线程块并行独立解决的粗略子问题，并将每个子问题划分为可以由块内所有线程并行协作解决的更精细的部分。</p>
<p>这种分解通过允许线程在解决每个子问题时进行协作来保留语言表达能力，同时实现自动可扩展性。实际上，每个线程块都可以在 GPU 内的任何可用multiprocessor上以乱序、并发或顺序调度，以便编译的 CUDA 程序可以在任意数量的多处理器上执行，如下图所示，并且只有运行时系统需要知道物理multiprocessor个数。 </p>
<p>这种可扩展的编程模型允许 GPU 架构通过简单地扩展multiprocessor和内存分区的数量来跨越广泛的市场范围：高性能发烧友 GeForce GPU ，专业的 Quadro 和 Tesla 计算产品 (有关所有支持 CUDA 的 GPU 的列表，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-enabled-gpus">支持 CUDA 的 GPU</a>）。</p>
<p><img src="/img/automatic-scalability.png" alt="automatic-scalability.png"></p>
<p>注意：GPU 是围绕一系列流式多处理器 (SM: Streaming Multiprocessors) 构建的（有关详细信息，请参<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation">阅硬件实现</a>）。 多线程程序被划分为彼此独立执行的线程块，因此具有更多multiprocessor的 GPU 将比具有更少多处理器的 GPU 在更短的时间内完成程序执行。</p>
<h1 id="2-编程模型"><a href="#2-编程模型" class="headerlink" title="2.编程模型"></a>2.编程模型</h1><p>本章通过概述CUDA编程模型是如何在c++中公开的，来介绍CUDA的主要概念。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-interface">编程接口</a>中给出了对 CUDA C++ 的广泛描述。</p>
<p>本章和下一章中使用的向量加法示例的完整代码可以在 vectorAdd <a href="https://docs.nvidia.com/cuda/cuda-samples/index.html#vector-addition">CUDA示例</a>中找到。</p>
<h2 id="2-1-内核"><a href="#2-1-内核" class="headerlink" title="2.1 内核"></a>2.1 内核</h2><p>CUDA C++ 通过允许程序员定义称为kernel的 C++ 函数来扩展 C++，当调用内核时，由 N 个不同的 CUDA 线程并行执行 N 次，而不是像常规 C++ 函数那样只执行一次。</p>
<p>使用 <code>__global__</code> 声明说明符定义内核，并使用新的 <code>&lt;&lt;&lt;...&gt;&gt;&gt;</code> 执行配置语法指定内核调用的 CUDA 线程数（请参阅 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#c-language-extensions">C++ 语言扩展</a>）。 每个执行内核的线程都有一个唯一的线程 ID，可以通过内置变量在内核中访问。</p>
<p>作为说明，以下示例代码使用内置变量 <code>threadIdx</code> 将两个大小为 N 的向量 A 和 B 相加，并将结果存储到向量 C 中：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Kernel definition</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">VecAdd</span><span class="params">(<span class="type">float</span>* A, <span class="type">float</span>* B, <span class="type">float</span>* C)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = threadIdx.x;</span><br><span class="line">    C[i] = A[i] + B[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// Kernel invocation with N threads</span></span><br><span class="line">    VecAdd&lt;&lt;&lt;<span class="number">1</span>, N&gt;&gt;&gt;(A, B, C);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里，执行 VecAdd() 的 N 个线程中的每一个线程都会执行一个加法。</p>
<h2 id="2-2-线程层次"><a href="#2-2-线程层次" class="headerlink" title="2.2 线程层次"></a>2.2 线程层次</h2><p>为方便起见，threadIdx 是一个 3 分量向量，因此可以使用一维、二维或三维的线程索引来识别线程，形成一个一维、二维或三维的线程块，称为block。 这提供了一种跨域的元素（例如向量、矩阵或体积）调用计算的方法。</p>
<p>线程的索引和它的线程 ID 以一种直接的方式相互关联：对于一维块，它们是相同的； 对于大小为(Dx, Dy)的二维块，索引为(x, y)的线程的线程ID为(x + y*Dx)； 对于大小为 (Dx, Dy, Dz) 的三维块，索引为 (x, y, z) 的线程的线程 ID 为 (x + y*Dx + z*Dx*Dy)。</p>
<p>例如，下面的代码将两个大小为NxN的矩阵A和B相加，并将结果存储到矩阵C中:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Kernel definition</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatAdd</span><span class="params">(<span class="type">float</span> A[N][N], <span class="type">float</span> B[N][N],</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">float</span> C[N][N])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = threadIdx.x;</span><br><span class="line">    <span class="type">int</span> j = threadIdx.y;</span><br><span class="line">    C[i][j] = A[i][j] + B[i][j];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// Kernel invocation with one block of N * N * 1 threads</span></span><br><span class="line">    <span class="type">int</span> numBlocks = <span class="number">1</span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">threadsPerBlock</span><span class="params">(N, N)</span></span>;</span><br><span class="line">    MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个块的线程数量是有限制的，因为一个块的所有线程都应该驻留在同一个处理器核心上，并且必须共享该核心有限的内存资源。在当前的gpu上，一个线程块可能包含多达1024个线程。</p>
<p>但是，一个内核可以由多个形状相同的线程块执行，因此线程总数等于每个块的线程数乘以块数。</p>
<p>块被组织成一维、二维或三维的线程块网格(<code>grid</code>)，如下图所示。网格中的线程块数量通常由正在处理的数据的大小决定，通常超过系统中的处理器数量。</p>
<p><img src="/img/grid-of-thread-blocks.png" alt="grid-of-thread-blocks.png"></p>
<figure class="highlight plaintext"><figcaption><span>```语法中指定的每个块的线程数和每个网格的块数可以是 ```int``` 或 `dim3` 类型。如上例所示，可以指定二维块或网格。</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">网格中的每个块都可以由一个一维、二维或三维的惟一索引标识，该索引可以通过内置的`blockIdx`变量在内核中访问。线程块的维度可以通过内置的`blockDim`变量在内核中访问。</span><br><span class="line"></span><br><span class="line">扩展前面的`MatAdd()`示例来处理多个块，代码如下所示。</span><br><span class="line"></span><br><span class="line">```C++</span><br><span class="line">// Kernel definition</span><br><span class="line">__global__ void MatAdd(float A[N][N], float B[N][N],</span><br><span class="line">float C[N][N])</span><br><span class="line">&#123;</span><br><span class="line">    int i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    int j = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    if (i &lt; N &amp;&amp; j &lt; N)</span><br><span class="line">        C[i][j] = A[i][j] + B[i][j];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">    // Kernel invocation</span><br><span class="line">    dim3 threadsPerBlock(16, 16);</span><br><span class="line">    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);</span><br><span class="line">    MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>线程块大小为16x16(256个线程)，尽管在本例中是任意更改的，但这是一种常见的选择。网格是用足够的块创建的，这样每个矩阵元素就有一个线程来处理。为简单起见，本例假设每个维度中每个网格的线程数可以被该维度中每个块的线程数整除，尽管事实并非如此。</p>
<p>程块需要独立执行：必须可以以任何顺序执行它们，并行或串行。 这种独立性要求允许跨任意数量的内核以任意顺序调度线程块，如下图所示，使程序员能够编写随内核数量扩展的代码。</p>
<p><img src="/img/automatic-scalability.png" alt="automatic-scalability.png"></p>
<p>块内的线程可以通过一些共享内存共享数据并通过同步它们的执行来协调内存访问来进行协作。 更准确地说，可以通过调用 <code>__syncthreads()</code> 内部函数来指定内核中的同步点； <code>__syncthreads()</code> 充当屏障，块中的所有线程必须等待，然后才能继续。 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory">Shared Memory</a> 给出了一个使用共享内存的例子。 除了<code>__syncthreads()</code> 之外，<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cooperative-groups">Cooperative Groups API</a> 还提供了一组丰富的线程同步示例。</p>
<p>为了高效协作，共享内存是每个处理器内核附近的低延迟内存（很像 L1 缓存），并且 <code>__syncthreads()</code> 是轻量级的。</p>
<h2 id="2-3-存储单元层次"><a href="#2-3-存储单元层次" class="headerlink" title="2.3 存储单元层次"></a>2.3 存储单元层次</h2><p>CUDA 线程可以在执行期间从多个内存空间访问数据，如下图所示。每个线程都有私有的本地内存。 每个线程块都具有对该块的所有线程可见的共享内存，并且具有与该块相同的生命周期。 所有线程都可以访问相同的全局内存。</p>
<p><img src="/img/memory-hierarchy.png" alt="memory-hierarchy.png"></p>
<p>还有两个额外的只读内存空间可供所有线程访问：常量和纹理内存空间。 全局、常量和纹理内存空间针对不同的内存使用进行了优化（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">设备内存访问</a>）。 纹理内存还为某些特定数据格式提供不同的寻址模式以及数据过滤（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-and-surface-memory">纹理和表面内存</a>）。</p>
<p>全局、常量和纹理内存空间在同一应用程序的内核启动中是持久的。</p>
<h2 id="2-4-异构编程"><a href="#2-4-异构编程" class="headerlink" title="2.4 异构编程"></a>2.4 异构编程</h2><p>如下图所示，CUDA 编程模型假定 CUDA 线程在物理独立的设备上执行，该设备作为运行 C++ 程序的主机的协处理器运行。例如，当内核在 GPU 上执行而 C++ 程序的其余部分在 CPU 上执行时，就是这种情况。</p>
<p><img src="/img/heterogeneous-programming.png" alt="heterogeneous-programming.png"></p>
<p>CUDA 编程模型还假设主机(<code>host</code>)和设备(<code>device</code>)都在 DRAM 中维护自己独立的内存空间，分别称为主机内存和设备内存。因此，程序通过调用 CUDA 运行时（在<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-interface">编程接口</a>中描述）来管理内核可见的全局、常量和纹理内存空间。这包括设备内存分配和释放以及主机和设备内存之间的数据传输。</p>
<p>统一内存提供托管内存来桥接主机和设备内存空间。托管内存可从系统中的所有 CPU 和 GPU 访问，作为具有公共地址空间的单个连贯内存映像。此功能可实现设备内存的超额订阅，并且无需在主机和设备上显式镜像数据，从而大大简化了移植应用程序的任务。有关统一内存的介绍，请参阅统一<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-unified-memory-programming-hd">内存编程</a>。</p>
<p>注:串行代码在主机(<code>host</code>)上执行，并行代码在设备(<code>device</code>)上执行。</p>
<h2 id="2-5-异步SIMT编程模型"><a href="#2-5-异步SIMT编程模型" class="headerlink" title="2.5 异步SIMT编程模型"></a>2.5 异步SIMT编程模型</h2><p>在 CUDA 编程模型中，线程是进行计算或内存操作的最低抽象级别。 从基于 NVIDIA Ampere GPU 架构的设备开始，CUDA 编程模型通过异步编程模型为内存操作提供加速。 异步编程模型定义了与 CUDA 线程相关的异步操作的行为。</p>
<p>异步编程模型为 CUDA 线程之间的同步定义了<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#aw-barrier">异步屏障</a>的行为。 该模型还解释并定义了如何使用 cuda::memcpy_async 在 GPU计算时从全局内存中异步移动数据。</p>
<h3 id="2-5-1-异步操作"><a href="#2-5-1-异步操作" class="headerlink" title="2.5.1 异步操作"></a>2.5.1 异步操作</h3><p>异步操作定义为由CUDA线程发起的操作，并且与其他线程一样异步执行。在结构良好的程序中，一个或多个CUDA线程与异步操作同步。发起异步操作的CUDA线程不需要在同步线程中.</p>
<p>这样的异步线程（as-if 线程）总是与发起异步操作的 CUDA 线程相关联。异步操作使用同步对象来同步操作的完成。这样的同步对象可以由用户显式管理（例如，<code>cuda::memcpy_async</code>）或在库中隐式管理（例如，<code>cooperative_groups::memcpy_async</code>）。</p>
<p>同步对象可以是 <code>cuda::barrier</code> 或 <code>cuda::pipeline</code>。这些对象在<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#aw-barrier">Asynchronous Barrier</a> 和 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memcpy_async_pipeline">Asynchronous Data Copies using cuda::pipeline</a>.中进行了详细说明。这些同步对象可以在不同的线程范围内使用。作用域定义了一组线程，这些线程可以使用同步对象与异步操作进行同步。下表定义了CUDA c++中可用的线程作用域，以及可以与每个线程同步的线程。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Thread Scope</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>cuda::thread_scope::thread_scope_thread</td>
<td>Only the CUDA thread which initiated asynchronous operations synchronizes.</td>
</tr>
<tr>
<td>cuda::thread_scope::thread_scope_block</td>
<td>All or any CUDA threads within the same thread block as the initiating thread synchronizes.</td>
</tr>
<tr>
<td>cuda::thread_scope::thread_scope_device</td>
<td>All or any CUDA threads in the same GPU device as the initiating thread synchronizes.</td>
</tr>
<tr>
<td>cuda::thread_scope::thread_scope_system</td>
<td>All or any CUDA or CPU threads in the same system as the initiating thread synchronizes.</td>
</tr>
</tbody>
</table>
</div>
<p>这些线程作用域是在CUDA<a href="https://nvidia.github.io/libcudacxx/extended_api/thread_scopes.html">标准c++库</a>中作为标准c++的扩展实现的。</p>
<h2 id="2-6-Compute-Capability"><a href="#2-6-Compute-Capability" class="headerlink" title="2.6 Compute Capability"></a>2.6 Compute Capability</h2><p>设备的<code>Compute Capability</code>由版本号表示，有时也称其“SM版本”。该版本号标识GPU硬件支持的特性，并由应用程序在运行时使用，以确定当前GPU上可用的硬件特性和指令。</p>
<p><code>Compute Capability</code>包括一个主要版本号X和一个次要版本号Y，用X.Y表示</p>
<p>主版本号相同的设备具有相同的核心架构。设备的主要修订号是8，为<code>NVIDIA Ampere GPU</code>的体系结构的基础上,7基于<code>Volta</code>设备架构,6设备基于<code>Pascal</code>架构,5设备基于<code>Maxwell</code>架构,3基于<code>Kepler</code>架构的设备,2设备基于<code>Fermi</code>架构,1是基于<code>Tesla</code>架构的设备。</p>
<p>次要修订号对应于对核心架构的增量改进，可能包括新特性。</p>
<p><code>Turing</code>是计算能力7.5的设备架构，是基于Volta架构的增量更新。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-enabled-gpus">CUDA-Enabled GPUs</a> 列出了所有支持 CUDA 的设备及其计算能力。<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities">Compute Capabilities</a>给出了每个计算能力的技术规格。</p>
<p>注意:特定GPU的计算能力版本不应与CUDA版本(如CUDA 7.5、CUDA 8、CUDA 9)混淆，CUDA版本指的是CUDA软件平台的版本。CUDA平台被应用开发人员用来创建运行在许多代GPU架构上的应用程序，包括未来尚未发明的GPU架构。尽管CUDA平台的新版本通常会通过支持新的GPU架构的计算能力版本来增加对该架构的本地支持，但CUDA平台的新版本通常也会包含软件功能。</p>
<p>从CUDA 7.0和CUDA 9.0开始，不再支持<code>Tesla</code>和<code>Fermi</code>架构。</p>
<h1 id="第三章编程接口"><a href="#第三章编程接口" class="headerlink" title="第三章编程接口"></a>第三章编程接口</h1><p>CUDA C++ 为熟悉 C++ 编程语言的用户提供了一种简单的途径，可以轻松编写由设备执行的程序。</p>
<p>它由c++语言的最小扩展集和运行时库组成。</p>
<p>编程模型中引入了核心语言扩展。它们允许程序员将内核定义为 C++ 函数，并在每次调用函数时使用一些新语法来指定网格和块的维度。所有扩展的完整描述可以在 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#c-language-extensions">C++ 语言扩展</a>中找到。任何包含这些扩展名的源文件都必须使用 nvcc 进行编译，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compilation-with-nvcc">使用NVCC编译</a>中所述。</p>
<p>运行时在 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-c-runtime">CUDA Runtime</a> 中引入。它提供了在主机上执行的 C 和 C++ 函数，用于分配和释放设备内存、在主机内存和设备内存之间传输数据、管理具有多个设备的系统等。运行时的完整描述可以在 CUDA 参考手册中找到。</p>
<p>运行时构建在较低级别的 C API（即 CUDA 驱动程序 API）之上，应用程序也可以访问该 API。驱动程序 API 通过公开诸如 CUDA 上下文（类似于设备的主机进程）和 CUDA 模块（类似于设备的动态加载库）等较低级别的概念来提供额外的控制级别。大多数应用程序不使用驱动程序 API，因为它们不需要这种额外的控制级别，并且在使用运行时时，上下文和模块管理是隐式的，从而产生更简洁的代码。由于运行时可与驱动程序 API 互操作，因此大多数需要驱动程序 API 功能的应用程序可以默认使用运行时 API，并且仅在需要时使用驱动程序 API。 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#driver-api">Driver API</a> 中介绍了驱动API并在参考手册中进行了全面描述。</p>
<h2 id="3-1利用NVCC编译"><a href="#3-1利用NVCC编译" class="headerlink" title="3.1利用NVCC编译"></a>3.1利用NVCC编译</h2><p>内核可以使用称为 <code>PTX</code> 的 CUDA 指令集架构来编写，<code>PTX</code> 参考手册中对此进行了描述。 然而，使用高级编程语言（如 C++）通常更有效。 在这两种情况下，内核都必须通过 <code>nvcc</code> 编译成二进制代码才能在设备上执行。</p>
<p><code>nvcc</code> 是一种编译器驱动程序，可简化编译 <code>C++</code> 或 <code>PTX</code> 代码：它提供简单且熟悉的命令行选项，并通过调用实现不同编译阶段的工具集合来执行它们。 本节概述了 <code>nvcc</code> 工作流程和命令选项。 完整的描述可以在 <code>nvcc</code> 用户手册中找到。</p>
<h3 id="3-1-1编译流程"><a href="#3-1-1编译流程" class="headerlink" title="3.1.1编译流程"></a>3.1.1编译流程</h3><h4 id="3-1-1-1-离线编译"><a href="#3-1-1-1-离线编译" class="headerlink" title="3.1.1.1 离线编译"></a>3.1.1.1 离线编译</h4><p>使用 nvcc 编译的源文件可以包含主机代码（即在<code>host</code>上执行的代码）和设备代码（即在<code>device</code>上执行的代码。 nvcc 的基本工作流程包括将设备代码与主机代码分离，然后： </p>
<ul>
<li>将设备代码编译成汇编形式（<code>PTX</code> 代码）或二进制形式（<code>cubin</code> 对象）</li>
<li>并通过CUDA运行时函数的调用来替换 &lt;&lt;&lt;…&gt;&gt;&gt; 语法对主机代码进行修改，以从 <code>PTX</code> 代码或 <code>cubin</code> 对象加载和启动每个编译的内核。</li>
</ul>
<p>修改后的主机代码要么作为 C++ 代码输出，然后使用另一个工具编译，要么直接作为目标代码输出，方法是让 nvcc 在最后编译阶段调用主机编译器。</p>
<p>然后应用程序可以：</p>
<ul>
<li>链接到已编译的主机代码（这是最常见的情况），</li>
<li>或者忽略修改后的主机代码（如果有）并使用 CUDA 驱动程序 API（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#driver-api">驱动程序 API</a>）来加载和执行 <code>PTX</code> 代码或 <code>cubin</code> 对象。</li>
</ul>
<h4 id="3-1-1-2-即时编译"><a href="#3-1-1-2-即时编译" class="headerlink" title="3.1.1.2 即时编译"></a>3.1.1.2 即时编译</h4><p>应用程序在运行时加载的任何 <code>PTX</code> 代码都由设备驱动程序进一步编译为二进制代码。这称为即时编译。即时编译增加了应用程序加载时间，但允许应用程序受益于每个新设备驱动程序带来的任何新编译器改进。它也是应用程序能够运行在编译时不存在的设备上的唯一方式，如应用<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#application-compatibility">程序兼容性</a>中所述。</p>
<p>当设备驱动程序为某些应用程序实时编译一些 <code>PTX</code> 代码时，它会自动缓存生成二进制代码的副本，以避免在应用程序的后续调用中重复编译。缓存（称为计算缓存）在设备驱动程序升级时自动失效，因此应用程序可以从设备驱动程序中内置的新即时编译器的改进中受益。</p>
<p>环境变量可用于控制即时编译，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars"> CUDA 环境变量</a>中所述</p>
<p>作为使用 <code>nvcc</code> 编译 CUDA C++ 设备代码的替代方法，<code>NVRTC</code> 可用于在运行时将 CUDA C++ 设备代码编译为 PTX。 <code>NVRTC</code> 是 CUDA C++ 的运行时编译库；更多信息可以在 <code>NVRTC</code> 用户指南中找到。</p>
<h3 id="3-1-2-Binary-兼容性"><a href="#3-1-2-Binary-兼容性" class="headerlink" title="3.1.2 Binary 兼容性"></a>3.1.2 Binary 兼容性</h3><p>二进制代码是特定于体系结构的。 使用指定目标体系结构的编译器选项 <code>-code</code> 生成 <code>cubin</code> 对象：例如，使用 <code>-code=sm_35</code> 编译会为<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability">计算能力</a>为 3.5 的设备生成二进制代码。 从一个次要修订版到下一个修订版都保证了二进制兼容性，但不能保证从一个次要修订版到前一个修订版或跨主要修订版。 换句话说，为计算能力 X.y 生成的 cubin 对象只会在计算能力 X.z 且 z≥y 的设备上执行。</p>
<p>注意：仅桌面支持二进制兼容性。 Tegra 不支持它。 此外，不支持桌面和 Tegra 之间的二进制兼容性。</p>
<h3 id="3-1-3-PTX-兼容性"><a href="#3-1-3-PTX-兼容性" class="headerlink" title="3.1.3 PTX 兼容性"></a>3.1.3 PTX 兼容性</h3><p>某些 PTX 指令仅在具有更高计算能力的设备上受支持。 例如，<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions">Warp Shuffle Functions</a> 仅在计算能力 3.0 及以上的设备上支持。 -arch 编译器选项指定将 C++ 编译为 PTX 代码时假定的计算能力。 因此，例如，包含 <code>warp shuffle</code> 的代码必须使用 -arch=compute_30（或更高版本）进行编译。</p>
<p>为某些特定计算能力生成的 PTX 代码始终可以编译为具有更大或相等计算能力的二进制代码。 请注意，从早期 PTX 版本编译的二进制文件可能无法使用某些硬件功能。 例如，从为计算能力 6.0 (Pascal) 生成的 PTX 编译的计算能力 7.0 (Volta) 的二进制目标设备将不会使用 Tensor Core 指令，因为这些指令在 Pascal 上不可用。 因此，最终二进制文件的性能可能会比使用最新版本的 PTX 生成的二进制文件更差。</p>
<h3 id="3-1-4-应用程序兼容性"><a href="#3-1-4-应用程序兼容性" class="headerlink" title="3.1.4 应用程序兼容性"></a>3.1.4 应用程序兼容性</h3><p>要在具有特定计算能力的设备上执行代码，应用程序必须加载与此计算能力兼容的二进制或 PTX 代码，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#binary-compatibility">二进制兼容性</a>和 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#ptx-compatibility">PTX 兼容性</a>中所述。 特别是，为了能够在具有更高计算能力的未来架构上执行代码（尚无法生成二进制代码），应用程序必须加载将为这些设备实时编译的 PTX 代码（参见<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#just-in-time-compilation">即时编译</a>）。</p>
<p>哪些 <code>PTX</code> 和二进制代码嵌入到 CUDA C++ 应用程序中由 <code>-arch</code> 和 <code>-code</code> 编译器选项或 <code>-gencode</code> 编译器选项控制，详见 nvcc 用户手册。 例如:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nvcc x.cu</span><br><span class="line">        -gencode arch=compute_50,code=sm_50</span><br><span class="line">        -gencode arch=compute_60,code=sm_60</span><br><span class="line">        -gencode arch=compute_70,code=\<span class="string">&quot;compute_70,sm_70\&quot;</span></span><br></pre></td></tr></table></figure>
<p>嵌入与计算能力 5.0 和 6.0（第一和第二<code>-gencode</code> 选项）兼容的二进制代码以及与计算能力 7.0（第三<code>-gencode</code> 选项）兼容的 PTX 和二进制代码。</p>
<p>生成主机代码以在运行时自动选择最合适的代码来加载和执行，在上面的示例中，这些代码将是：</p>
<ul>
<li>具有计算能力 5.0 和 5.2 的设备的 5.0 二进制代码，</li>
<li>具有计算能力 6.0 和 6.1 的设备的 6.0 二进制代码，</li>
<li>具有计算能力 7.0 和 7.5 的设备的 7.0 二进制代码，</li>
<li>PTX 代码在运行时编译为具有计算能力 8.0 和 8.6 的设备的二进制代码。</li>
</ul>
<p>例如，<code>x.cu</code> 可以有一个优化代码的方法，使用 warp shuffle 操作，这些操作仅在计算能力 3.0 及更高版本的设备中受支持。 <code>__CUDA_ARCH__</code> 宏可用于根据计算能力区分各种代码方案。 它仅为设备代码定义。 例如，当使用 <code>-arch=compute_35</code> 编译时，<code>__CUDA_ARCH__</code> 等于 350。</p>
<p>使用驱动 API 的应用程序必须编译代码以分离文件并在运行时显式加载和执行最合适的文件。</p>
<p>Volta 架构引入了独立线程调度，它改变了在 GPU 上调度线程的方式。 对于依赖于以前架构中 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture">SIMT 调度</a>的特定行为的代码，独立线程调度可能会改变参与线程的集合，从而导致不正确的结果。 为了在实现<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#independent-thread-scheduling-7-x">独立线程调度</a>中详述的纠正措施的同时帮助迁移，Volta 开发人员可以使用编译器选项组合 -arch=compute_60 -code=sm_70 选择加入 Pascal 的线程调度。</p>
<p>nvcc 用户手册列出了 <code>-arch、-code</code> 和 <code>-gencode</code> 编译器选项的各种简写。 例如，<code>-arch=sm_70</code> 是 <code>-arch=compute_70 -code=compute_70,sm_70</code> 的简写（与 <code>-gencode arch=compute_70,code=\&quot;compute_70,sm_70\&quot;</code> 相同）。</p>
<h3 id="3-1-5-C-兼容性"><a href="#3-1-5-C-兼容性" class="headerlink" title="3.1.5 C++兼容性"></a>3.1.5 C++兼容性</h3><p>编译器前端根据 C++ 语法规则处理 CUDA 源文件。 主机代码支持完整的 C++。 但是，设备代码仅完全支持 C++ 的一个子集，如 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#c-cplusplus-language-support">C++ 语言支持</a>中所述。</p>
<h3 id="3-1-6-64位支持"><a href="#3-1-6-64位支持" class="headerlink" title="3.1.6 64位支持"></a>3.1.6 64位支持</h3><p>64 位版本的 <code>nvcc</code> 以 64 位模式编译设备代码（即指针是 64 位的）。 以 64 位模式编译的设备代码仅支持以 64 位模式编译的主机代码。</p>
<p>同样，32 位版本的 <code>nvcc</code> 以 32 位模式编译设备代码，而以 32 位模式编译的设备代码仅支持以 32 位模式编译的主机代码。</p>
<p>32 位版本的 <code>nvcc</code> 也可以使用 -m64 编译器选项以 64 位模式编译设备代码。</p>
<p>64 位版本的 <code>nvcc</code> 也可以使用 -m32 编译器选项以 32 位模式编译设备代码。</p>
<h2 id="3-2-CUDA运行时"><a href="#3-2-CUDA运行时" class="headerlink" title="3.2 CUDA运行时"></a>3.2 CUDA运行时</h2><p>运行时在 <code>cudart</code> 库中实现，该库链接到应用程序，可以通过 <code>cudart.lib</code> 或 <code>libcudart.a</code> 静态链接，也可以通过 <code>cudart.dll</code> 或 <code>libcudart.so</code> 动态链接。 需要 <code>cudart.dll</code> 或 <code>cudart.so</code> 进行动态链接的应用程序通常将它们作为应用程序安装包的一部分。 只有在链接到同一 CUDA 运行时实例的组件之间传递 CUDA 运行时符号的地址才是安全的。</p>
<p>它的所有入口都以 <code>cuda</code> 为前缀。</p>
<p>如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#heterogeneous-programming">异构编程</a>中所述，CUDA 编程模型假设系统由主机和设备组成，每个设备都有自己独立的内存。 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory">设备内存</a>概述了用于管理设备内存的运行时函数。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory">共享内存</a>说明了使用线程层次结构中引入的共享内存来最大化性能。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#page-locked-host-memory">Page-Locked Host Memory</a> 引入了 page-locked 主机内存，它需要将内核执行与主机设备内存之间的数据传输重叠。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution">异步并发</a>执行描述了用于在系统的各个级别启用异步并发执行的概念和 API。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#multi-device-system">多设备系统</a>展示了编程模型如何扩展到具有多个设备连接到同一主机的系统。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#error-checking">错误检查</a>描述了如何正确检查运行时生成的错误。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#call-stack">调用堆栈</a>提到了用于管理 CUDA C++ 调用堆栈的运行时函数。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-and-surface-memory">Texture and Surface Memory</a> 呈现了纹理和表面内存空间，它们提供了另一种访问设备内存的方式；它们还公开了 GPU 纹理硬件的一个子集。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#graphics-interoperability">图形互操作性</a>介绍了运行时提供的各种功能，用于与两个主要图形 API（OpenGL 和 Direct3D）进行互操作。</p>
<h4 id="3-2-1-初始化"><a href="#3-2-1-初始化" class="headerlink" title="3.2.1 初始化"></a>3.2.1 初始化</h4><p>运行时没有显式的初始化函数；它在第一次调用运行时函数时进行初始化（更具体地说，除了参考手册的错误处理和版本管理部分中的函数之外的任何函数）。在计时运行时函数调用以及将第一次调用的错误代码解释到运行时时，需要牢记这一点。</p>
<p>运行时为系统中的每个设备创建一个 CUDA 上下文（有关 CUDA 上下文的更多详细信息，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#context">上下文</a>）。此<code>context</code>是此设备的主要上下文，并在需要此设备上的活动上下文的第一个运行时函数中初始化。它在应用程序的所有主机线程之间共享。作为此上下文创建的一部分，设备代码会在必要时进行即时编译（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#just-in-time-compilation">即时编译</a>）并加载到设备内存中。这一切都是透明地发生的。如果需要，例如对于驱动程序 API 互操作性，可以从驱动程序 API 访问设备的主要上下文，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#interoperability-between-runtime-and-driver-apis">运行时和驱动程序 API 之间的互操作性</a>中所述。</p>
<p>当主机线程调用 cudaDeviceReset() 时，这会破坏主机线程当前操作的设备的主要上下文（即设备选择中定义的当前设备）。 任何将此设备作为当前设备的主机线程进行的下一个运行时函数调用将为该设备创建一个新的主上下文。</p>
<p>注意：CUDA接口使用全局状态，在主机程序初始化时初始化，在主机程序终止时销毁。 CUDA 运行时和驱动程序无法检测此状态是否无效，因此在程序启动或 main 后终止期间使用任何这些接口（隐式或显式）将导致未定义的行为。</p>
<h3 id="3-2-2-设备存储"><a href="#3-2-2-设备存储" class="headerlink" title="3.2.2 设备存储"></a>3.2.2 设备存储</h3><p>如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#heterogeneous-programming">异构编程</a>中所述，CUDA 编程模型假设系统由主机和设备组成，每个设备都有自己独立的内存。 内核在设备内存之外运行，因此运行时提供了分配、解除分配和复制设备内存以及在主机内存和设备内存之间传输数据的功能。</p>
<p>设备内存可以分配为线性内存或 <code>CUDA 数组</code>。</p>
<p>CUDA 数组是针对纹理获取优化的不透明内存布局。 它们在<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-and-surface-memory">纹理和表面内存</a>中有所描述。</p>
<p>线性内存分配在一个统一的地址空间中，这意味着单独分配的实体可以通过指针相互引用，例如在二叉树或链表中。 地址空间的大小取决于主机系统 (CPU) 和所用 GPU 的计算能力：</p>
<p>Table 1. Linear Memory Address Space</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>x86_64 (AMD64)</th>
<th>POWER (ppc64le)</th>
<th>ARM64</th>
</tr>
</thead>
<tbody>
<tr>
<td>up to compute capability 5.3 (Maxwell)</td>
<td>40bit</td>
<td>40bit</td>
<td>40bit</td>
</tr>
<tr>
<td>compute capability 6.0 (Pascal) or newer</td>
<td>up to 47bit</td>
<td>up to 49bit</td>
<td>up to 48bit</td>
</tr>
</tbody>
</table>
</div>
<p>注意：在计算能力为 5.3 (Maxwell) 及更早版本的设备上，CUDA 驱动程序会创建一个未提交的 40 位虚拟地址预留，以确保内存分配（指针）在支持的范围内。 此预留显示为预留虚拟内存，但在程序实际分配内存之前不会占用任何物理内存。</p>
<p>线性内存通常使用 <code>cudaMalloc()</code> 分配并使用 <code>cudaFree()</code> 释放，主机内存和设备内存之间的数据传输通常使用 <code>cudaMemcpy()</code> 完成。 在Kernels的向量加法代码示例中，需要将向量从主机内存复制到设备内存：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Device code</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">VecAdd</span><span class="params">(<span class="type">float</span>* A, <span class="type">float</span>* B, <span class="type">float</span>* C, <span class="type">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (i &lt; N)</span><br><span class="line">        C[i] = A[i] + B[i];</span><br><span class="line">&#125;</span><br><span class="line">            </span><br><span class="line"><span class="comment">// Host code</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> N = ...;</span><br><span class="line">    <span class="type">size_t</span> size = N * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate input vectors h_A and h_B in host memory</span></span><br><span class="line">    <span class="type">float</span>* h_A = (<span class="type">float</span>*)<span class="built_in">malloc</span>(size);</span><br><span class="line">    <span class="type">float</span>* h_B = (<span class="type">float</span>*)<span class="built_in">malloc</span>(size);</span><br><span class="line">    <span class="type">float</span>* h_C = (<span class="type">float</span>*)<span class="built_in">malloc</span>(size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialize input vectors</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate vectors in device memory</span></span><br><span class="line">    <span class="type">float</span>* d_A;</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_A, size);</span><br><span class="line">    <span class="type">float</span>* d_B;</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_B, size);</span><br><span class="line">    <span class="type">float</span>* d_C;</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_C, size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Copy vectors from host memory to device memory</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_A, h_A, size, cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_B, h_B, size, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Invoke kernel</span></span><br><span class="line">    <span class="type">int</span> threadsPerBlock = <span class="number">256</span>;</span><br><span class="line">    <span class="type">int</span> blocksPerGrid =</span><br><span class="line">            (N + threadsPerBlock - <span class="number">1</span>) / threadsPerBlock;</span><br><span class="line">    VecAdd&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(d_A, d_B, d_C, N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Copy result from device memory to host memory</span></span><br><span class="line">    <span class="comment">// h_C contains the result in host memory</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(h_C, d_C, size, cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Free device memory</span></span><br><span class="line">    <span class="built_in">cudaFree</span>(d_A);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_B);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_C);</span><br><span class="line">            </span><br><span class="line">    <span class="comment">// Free host memory</span></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>线性内存也可以通过 <code>cudaMallocPitch()</code> 和 <code>cudaMalloc3D()</code>分配。 建议将这些函数用于 2D 或 3D 数组的分配，因为它确保分配被适当地填充以满足<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">设备内存访问</a>中描述的对齐要求，从而确保在访问行地址或在 2D 数组和其他区域设备内存之间执行复制时获得最佳性能（使用 cudaMemcpy2D() 和 cudaMemcpy3D() 函数）。 返回的间距（或步幅）必须用于访问数组元素。 以下代码示例分配一个<code>width x height</code>的2D浮点数组，并显示如何在设备代码中循环遍历数组元素： </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Host code</span></span><br><span class="line"><span class="type">int</span> width = <span class="number">64</span>, height = <span class="number">64</span>;</span><br><span class="line"><span class="type">float</span>* devPtr;</span><br><span class="line"><span class="type">size_t</span> pitch;</span><br><span class="line"><span class="built_in">cudaMallocPitch</span>(&amp;devPtr, &amp;pitch,</span><br><span class="line">                width * <span class="built_in">sizeof</span>(<span class="type">float</span>), height);</span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">100</span>, <span class="number">512</span>&gt;&gt;&gt;(devPtr, pitch, width, height);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Device code</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MyKernel</span><span class="params">(<span class="type">float</span>* devPtr,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="type">size_t</span> pitch, <span class="type">int</span> width, <span class="type">int</span> height)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> r = <span class="number">0</span>; r &lt; height; ++r) &#123;</span><br><span class="line">        <span class="type">float</span>* row = (<span class="type">float</span>*)((<span class="type">char</span>*)devPtr + r * pitch);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; width; ++c) &#123;</span><br><span class="line">            <span class="type">float</span> element = row[c];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以下代码示例分配了一个<code>width x height x depth</code> 的3D浮点数组，并展示了如何在设备代码中循环遍历数组元素：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Host code</span></span><br><span class="line"><span class="type">int</span> width = <span class="number">64</span>, height = <span class="number">64</span>, depth = <span class="number">64</span>;</span><br><span class="line">cudaExtent extent = <span class="built_in">make_cudaExtent</span>(width * <span class="built_in">sizeof</span>(<span class="type">float</span>),</span><br><span class="line">                                    height, depth);</span><br><span class="line">cudaPitchedPtr devPitchedPtr;</span><br><span class="line"><span class="built_in">cudaMalloc3D</span>(&amp;devPitchedPtr, extent);</span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">100</span>, <span class="number">512</span>&gt;&gt;&gt;(devPitchedPtr, width, height, depth);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Device code</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MyKernel</span><span class="params">(cudaPitchedPtr devPitchedPtr,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="type">int</span> width, <span class="type">int</span> height, <span class="type">int</span> depth)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">char</span>* devPtr = devPitchedPtr.ptr;</span><br><span class="line">    <span class="type">size_t</span> pitch = devPitchedPtr.pitch;</span><br><span class="line">    <span class="type">size_t</span> slicePitch = pitch * height;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> z = <span class="number">0</span>; z &lt; depth; ++z) &#123;</span><br><span class="line">        <span class="type">char</span>* slice = devPtr + z * slicePitch;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> y = <span class="number">0</span>; y &lt; height; ++y) &#123;</span><br><span class="line">            <span class="type">float</span>* row = (<span class="type">float</span>*)(slice + y * pitch);</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> x = <span class="number">0</span>; x &lt; width; ++x) &#123;</span><br><span class="line">                <span class="type">float</span> element = row[x];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意：为避免分配过多内存从而影响系统范围的性能，请根据问题大小向用户请求分配参数。 如果分配失败，您可以回退到其他较慢的内存类型（cudaMallocHost()、cudaHostRegister() 等），或者返回一个错误，告诉用户需要多少内存被拒绝。 如果您的应用程序由于某种原因无法请求分配参数，我们建议对支持它的平台使用 cudaMallocManaged()。</p>
<p>参考手册列出了用于在使用 <code>cudaMalloc()</code> 分配的线性内存、使用 <code>cudaMallocPitch()</code> 或 <code>cudaMalloc3D()</code>分配的线性内存、CUDA 数组以及为在全局或常量内存空间中声明的变量分配的内存之间复制内存的所有各种函数。</p>
<p>以下代码示例说明了通过运行时 API 访问全局变量的各种方法：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">__constant__ <span class="type">float</span> constData[<span class="number">256</span>];</span><br><span class="line"><span class="type">float</span> data[<span class="number">256</span>];</span><br><span class="line"><span class="built_in">cudaMemcpyToSymbol</span>(constData, data, <span class="built_in">sizeof</span>(data));</span><br><span class="line"><span class="built_in">cudaMemcpyFromSymbol</span>(data, constData, <span class="built_in">sizeof</span>(data));</span><br><span class="line"></span><br><span class="line">__device__ <span class="type">float</span> devData;</span><br><span class="line"><span class="type">float</span> value = <span class="number">3.14f</span>;</span><br><span class="line"><span class="built_in">cudaMemcpyToSymbol</span>(devData, &amp;value, <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line"></span><br><span class="line">__device__ <span class="type">float</span>* devPointer;</span><br><span class="line"><span class="type">float</span>* ptr;</span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;ptr, <span class="number">256</span> * <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line"><span class="built_in">cudaMemcpyToSymbol</span>(devPointer, &amp;ptr, <span class="built_in">sizeof</span>(ptr));</span><br></pre></td></tr></table></figure>
<p><code>cudaGetSymbolAddress()</code> 用于检索指向为全局内存空间中声明的变量分配的内存的地址。 分配内存的大小是通过 <code>cudaGetSymbolSize()</code> 获得的。 </p>
<h3 id="3-2-3-L2级设备内存管理"><a href="#3-2-3-L2级设备内存管理" class="headerlink" title="3.2.3 L2级设备内存管理"></a>3.2.3 L2级设备内存管理</h3><p>当一个 CUDA 内核重复访问全局内存中的一个数据区域时，这种数据访问可以被认为是持久化的。 另一方面，如果数据只被访问一次，那么这种数据访问可以被认为是流式的。</p>
<p>从 CUDA 11.0 开始，计算能力 8.0 及以上的设备能够影响 L2 缓存中数据的持久性，从而可能提供对全局内存的更高带宽和更低延迟的访问。</p>
<h4 id="3-2-3-1-为持久访问预留L2缓存"><a href="#3-2-3-1-为持久访问预留L2缓存" class="headerlink" title="3.2.3.1 为持久访问预留L2缓存"></a>3.2.3.1 为持久访问预留L2缓存</h4><p>可以留出一部分 L2 缓存用于持久化对全局内存的数据访问。 持久访问优先使用 L2 缓存的这个预留部分，而对全局内存的正常访问或流式访问只能在持久访问未使用 L2 的这一部分使用。</p>
<p>可以在以下限制内调整用于持久访问的 L2 缓存预留大小：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaGetDeviceProperties</span>(&amp;prop, device_id);                </span><br><span class="line"><span class="type">size_t</span> size = <span class="built_in">min</span>(<span class="built_in">int</span>(prop.l2CacheSize * <span class="number">0.75</span>), prop.persistingL2CacheMaxSize);</span><br><span class="line"><span class="built_in">cudaDeviceSetLimit</span>(cudaLimitPersistingL2CacheSize, size); <span class="comment">/* set-aside 3/4 of L2 cache for persisting accesses or the max allowed*/</span> </span><br></pre></td></tr></table></figure>
<p>在多实例 GPU (MIG) 模式下配置 GPU 时，L2 缓存预留功能被禁用。</p>
<p>使用多进程服务 (MPS) 时，<code>cudaDeviceSetLimit</code> 无法更改 L2 缓存预留大小。 相反，只能在 MPS 服务器启动时通过环境变量 <code>CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT</code> 指定预留大小。</p>
<h4 id="3-2-3-2-L2持久化访问策略"><a href="#3-2-3-2-L2持久化访问策略" class="headerlink" title="3.2.3.2 L2持久化访问策略"></a>3.2.3.2 L2持久化访问策略</h4><p>访问策略窗口指定全局内存的连续区域和L2缓存中的持久性属性，用于该区域内的访问。</p>
<p>下面的代码示例显示了如何使用 CUDA 流设置L2持久访问窗口。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cudaStreamAttrValue stream_attribute;                                         <span class="comment">// Stream level attributes data structure</span></span><br><span class="line">stream_attribute.accessPolicyWindow.base_ptr  = <span class="built_in">reinterpret_cast</span>&lt;<span class="type">void</span>*&gt;(ptr); <span class="comment">// Global Memory data pointer</span></span><br><span class="line">stream_attribute.accessPolicyWindow.num_bytes = num_bytes;                    <span class="comment">// Number of bytes for persistence access.</span></span><br><span class="line">                                                                              <span class="comment">// (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize)</span></span><br><span class="line">stream_attribute.accessPolicyWindow.hitRatio  = <span class="number">0.6</span>;                          <span class="comment">// Hint for cache hit ratio</span></span><br><span class="line">stream_attribute.accessPolicyWindow.hitProp   = cudaAccessPropertyPersisting; <span class="comment">// Type of access property on cache hit</span></span><br><span class="line">stream_attribute.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;  <span class="comment">// Type of access property on cache miss.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//Set the attributes to a CUDA stream of type cudaStream_t</span></span><br><span class="line"><span class="built_in">cudaStreamSetAttribute</span>(stream, cudaStreamAttributeAccessPolicyWindow, &amp;stream_attribute); </span><br></pre></td></tr></table></figure>
<p>当内核随后在 CUDA 流中执行时，全局内存范围 [ptr..ptr+num_bytes) 内的内存访问比对其他全局内存位置的访问更有可能保留在 L2 缓存中。</p>
<p>也可以为 CUDA Graph Kernel Node节点设置 L2 持久性，如下例所示：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cudaKernelNodeAttrValue node_attribute;                                     <span class="comment">// Kernel level attributes data structure</span></span><br><span class="line">node_attribute.accessPolicyWindow.base_ptr  = <span class="built_in">reinterpret_cast</span>&lt;<span class="type">void</span>*&gt;(ptr); <span class="comment">// Global Memory data pointer</span></span><br><span class="line">node_attribute.accessPolicyWindow.num_bytes = num_bytes;                    <span class="comment">// Number of bytes for persistence access.</span></span><br><span class="line">                                                                            <span class="comment">// (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize)</span></span><br><span class="line">node_attribute.accessPolicyWindow.hitRatio  = <span class="number">0.6</span>;                          <span class="comment">// Hint for cache hit ratio</span></span><br><span class="line">node_attribute.accessPolicyWindow.hitProp   = cudaAccessPropertyPersisting; <span class="comment">// Type of access property on cache hit</span></span><br><span class="line">node_attribute.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;  <span class="comment">// Type of access property on cache miss.</span></span><br><span class="line">                                    </span><br><span class="line"><span class="comment">//Set the attributes to a CUDA Graph Kernel node of type cudaGraphNode_t</span></span><br><span class="line"><span class="built_in">cudaGraphKernelNodeSetAttribute</span>(node, cudaKernelNodeAttributeAccessPolicyWindow, &amp;node_attribute); </span><br></pre></td></tr></table></figure>
<p><code>hitRatio</code> 参数可用于指定接收 <code>hitProp</code> 属性的访问比例。 在上面的两个示例中，全局内存区域 [ptr..ptr+num_bytes) 中 60% 的内存访问具有持久属性，40% 的内存访问具有流属性。 哪些特定的内存访问被归类为持久（<code>hitProp</code>）是随机的，概率大约为 <code>hitRatio</code>； 概率分布取决于硬件架构和内存范围。</p>
<p>例如，如果 L2 预留缓存大小为 16KB，而 accessPolicyWindow 中的 num_bytes 为 32KB：</p>
<ul>
<li><code>hitRatio</code> 为 0.5 时，硬件将随机选择 32KB 窗口中的 16KB 指定为持久化并缓存在预留的 L2 缓存区域中。</li>
<li><code>hitRatio</code> 为 1.0 时，硬件将尝试在预留的 L2 缓存区域中缓存整个 32KB 窗口。 由于预留区域小于窗口，缓存行将被逐出以将 32KB 数据中最近使用的 16KB 保留在 L2 缓存的预留部分中。</li>
</ul>
<p>因此，<code>hitRatio</code> 可用于避免缓存的破坏，并总体减少移入和移出 L2 高速缓存的数据量。</p>
<p>低于 1.0 的 <code>hitRatio</code> 值可用于手动控制来自并发 CUDA 流的不同 <code>accessPolicyWindows</code> 可以缓存在 L2 中的数据量。 例如，让 L2 预留缓存大小为 16KB； 两个不同 CUDA 流中的两个并发内核，每个都有一个 16KB 的 <code>accessPolicyWindow</code>，并且两者的 <code>hitRatio</code> 值都为 1.0，在竞争共享 L2 资源时，可能会驱逐彼此的缓存。 但是，如果两个 <code>accessPolicyWindows</code> 的 <code>hitRatio</code> 值都为 0.5，则它们将不太可能逐出自己或彼此的持久缓存。 </p>
<h4 id="3-2-3-3-L2访问属性"><a href="#3-2-3-3-L2访问属性" class="headerlink" title="3.2.3.3 L2访问属性"></a>3.2.3.3 L2访问属性</h4><p>为不同的全局内存数据访问定义了三种类型的访问属性：</p>
<ol>
<li><code>cudaAccessPropertyStreaming</code>：使用流属性发生的内存访问不太可能在 L2 缓存中持续存在，因为这些访问优先被驱逐。</li>
<li><code>cudaAccessPropertyPersisting</code>：使用持久属性发生的内存访问更有可能保留在 L2 缓存中，因为这些访问优先保留在 L2 缓存的预留部分中。</li>
<li><code>cudaAccessPropertyNormal</code>：此访问属性强制将先前应用的持久访问属性重置为正常状态。来自先前 CUDA 内核的具有持久性属性的内存访问可能会在其预期用途之后很长时间保留在 L2 缓存中。这种使用后的持久性减少了不使用持久性属性的后续内核可用的 L2 缓存量。使用 <code>cudaAccessPropertyNormal</code> 属性重置访问属性窗口会删除先前访问的持久（优先保留）状态，就像先前访问没有访问属性一样。</li>
</ol>
<h4 id="3-2-3-4-L2持久性示例"><a href="#3-2-3-4-L2持久性示例" class="headerlink" title="3.2.3.4 L2持久性示例"></a>3.2.3.4 L2持久性示例</h4><p>以下示例显示如何为持久访问预留 L2 缓存，通过 CUDA Stream 在 CUDA 内核中使用预留的 L2 缓存，然后重置 L2 缓存。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">cudaStream_t stream;</span><br><span class="line"><span class="built_in">cudaStreamCreate</span>(&amp;stream);                                                                  <span class="comment">// Create CUDA stream</span></span><br><span class="line"></span><br><span class="line">cudaDeviceProp prop;                                                                        <span class="comment">// CUDA device properties variable</span></span><br><span class="line"><span class="built_in">cudaGetDeviceProperties</span>( &amp;prop, device_id);                                                 <span class="comment">// Query GPU properties</span></span><br><span class="line"><span class="type">size_t</span> size = <span class="built_in">min</span>( <span class="built_in">int</span>(prop.l2CacheSize * <span class="number">0.75</span>) , prop.persistingL2CacheMaxSize );</span><br><span class="line"><span class="built_in">cudaDeviceSetLimit</span>( cudaLimitPersistingL2CacheSize, size);                                  <span class="comment">// set-aside 3/4 of L2 cache for persisting accesses or the max allowed</span></span><br><span class="line"></span><br><span class="line"><span class="type">size_t</span> window_size = <span class="built_in">min</span>(prop.accessPolicyMaxWindowSize, num_bytes);                        <span class="comment">// Select minimum of user defined num_bytes and max window size.</span></span><br><span class="line"></span><br><span class="line">cudaStreamAttrValue stream_attribute;                                                       <span class="comment">// Stream level attributes data structure</span></span><br><span class="line">stream_attribute.accessPolicyWindow.base_ptr  = <span class="built_in">reinterpret_cast</span>&lt;<span class="type">void</span>*&gt;(data1);               <span class="comment">// Global Memory data pointer</span></span><br><span class="line">stream_attribute.accessPolicyWindow.num_bytes = window_size;                                <span class="comment">// Number of bytes for persistence access</span></span><br><span class="line">stream_attribute.accessPolicyWindow.hitRatio  = <span class="number">0.6</span>;                                        <span class="comment">// Hint for cache hit ratio</span></span><br><span class="line">stream_attribute.accessPolicyWindow.hitProp   = cudaAccessPropertyPersisting;               <span class="comment">// Persistence Property</span></span><br><span class="line">stream_attribute.accessPolicyWindow.missProp  = cudaAccessPropertyStreaming;                <span class="comment">// Type of access property on cache miss</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaStreamSetAttribute</span>(stream, cudaStreamAttributeAccessPolicyWindow, &amp;stream_attribute);   <span class="comment">// Set the attributes to a CUDA Stream</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    cuda_kernelA&lt;&lt;&lt;grid_size,block_size,<span class="number">0</span>,stream&gt;&gt;&gt;(data1);                                 <span class="comment">// This data1 is used by a kernel multiple times</span></span><br><span class="line">&#125;                                                                                           <span class="comment">// [data1 + num_bytes) benefits from L2 persistence</span></span><br><span class="line">cuda_kernelB&lt;&lt;&lt;grid_size,block_size,<span class="number">0</span>,stream&gt;&gt;&gt;(data1);                                     <span class="comment">// A different kernel in the same stream can also benefit</span></span><br><span class="line">                                                                                            <span class="comment">// from the persistence of data1</span></span><br><span class="line"></span><br><span class="line">stream_attribute.accessPolicyWindow.num_bytes = <span class="number">0</span>;                                          <span class="comment">// Setting the window size to 0 disable it</span></span><br><span class="line"><span class="built_in">cudaStreamSetAttribute</span>(stream, cudaStreamAttributeAccessPolicyWindow, &amp;stream_attribute);   <span class="comment">// Overwrite the access policy attribute to a CUDA Stream</span></span><br><span class="line"><span class="built_in">cudaCtxResetPersistingL2Cache</span>();                                                            <span class="comment">// Remove any persistent lines in L2 </span></span><br><span class="line"></span><br><span class="line">cuda_kernelC&lt;&lt;&lt;grid_size,block_size,<span class="number">0</span>,stream&gt;&gt;&gt;(data2);                                     <span class="comment">// data2 can now benefit from full L2 in normal mode</span></span><br></pre></td></tr></table></figure>
<h4 id="3-2-3-5-将L2-Access重置为Normal"><a href="#3-2-3-5-将L2-Access重置为Normal" class="headerlink" title="3.2.3.5 将L2 Access重置为Normal"></a>3.2.3.5 将L2 Access重置为Normal</h4><p>来自之前CUDA内核的L2缓存在被使用后可能会长期保存在L2中。因此，L2缓存重设为正常状态对于流或正常内存访问很重要，以便以正常优先级使用L2缓存。有三种方法可以将持久访问重置为正常状态。</p>
<ol>
<li>使用访问属性<code>cudaAccessPropertyNormal</code>重置之前的持久化内存区域。</li>
<li>通过调用<code>cudaCtxResetPersistingL2Cache()</code>将所有持久L2缓存线重置为正常。</li>
<li>最终，未触及的空间会自动重置为正常。对自动复位的依赖性很强</li>
</ol>
<h4 id="3-2-3-6-管理L2预留缓存的利用率"><a href="#3-2-3-6-管理L2预留缓存的利用率" class="headerlink" title="3.2.3.6 管理L2预留缓存的利用率"></a>3.2.3.6 管理L2预留缓存的利用率</h4><p>在不同 CUDA 流中同时执行的多个 CUDA 内核可能具有分配给它们的流的不同访问策略窗口。 但是，L2 预留缓存部分在所有这些并发 CUDA 内核之间共享。 因此，这个预留缓存部分的净利用率是所有并发内核单独使用的总和。 将内存访问指定为持久访问的好处会随着持久访问的数量超过预留的 L2 缓存容量而减少。</p>
<p>要管理预留 L2 缓存部分的利用率，应用程序必须考虑以下事项：</p>
<ul>
<li>L2 预留缓存的大小。</li>
<li>可以同时执行的 CUDA 内核。</li>
<li>可以同时执行的所有 CUDA 内核的访问策略窗口。</li>
<li>何时以及如何需要 L2 重置以允许正常或流式访问以同等优先级利用先前预留的 L2 缓存。</li>
</ul>
<h4 id="3-2-3-7-查询L2缓存属性"><a href="#3-2-3-7-查询L2缓存属性" class="headerlink" title="3.2.3.7 查询L2缓存属性"></a>3.2.3.7 查询L2缓存属性</h4><p>与 L2 缓存相关的属性是 <code>cudaDeviceProp</code> 结构的一部分，可以使用 CUDA 运行时 API <code>cudaGetDeviceProperties</code> 进行查询</p>
<p>CUDA 设备属性包括：</p>
<ul>
<li><code>l2CacheSize</code>：GPU 上可用的二级缓存数量。</li>
<li><code>persistingL2CacheMaxSize</code>：可以为持久内存访问留出的 L2 缓存的最大数量。</li>
<li><code>accessPolicyMaxWindowSize</code>：访问策略窗口的最大尺寸。</li>
</ul>
<h4 id="3-2-3-8-控制L2缓存预留大小用于持久内存访问"><a href="#3-2-3-8-控制L2缓存预留大小用于持久内存访问" class="headerlink" title="3.2.3.8 控制L2缓存预留大小用于持久内存访问"></a>3.2.3.8 控制L2缓存预留大小用于持久内存访问</h4><p>使用 CUDA 运行时 API <code>cudaDeviceGetLimit</code> 查询用于持久内存访问的 L2 预留缓存大小，并使用 CUDA 运行时 API <code>cudaDeviceSetLimit</code> 作为 <code>cudaLimit</code> 进行设置。 设置此限制的最大值是 <code>cudaDeviceProp::persistingL2CacheMaxSize</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> <span class="title class_">cudaLimit</span> &#123;</span><br><span class="line">    <span class="comment">/* other fields not shown */</span></span><br><span class="line">    cudaLimitPersistingL2CacheSize</span><br><span class="line">&#125;; </span><br></pre></td></tr></table></figure>
<h3 id="3-2-4共享内存"><a href="#3-2-4共享内存" class="headerlink" title="3.2.4共享内存"></a>3.2.4共享内存</h3><p>如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#variable-memory-space-specifiers">可变内存空间说明</a>中所述，共享内存是使用 <code>__shared__</code> 内存空间说明符分配的。</p>
<p>正如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy">线程层次结构</a>中提到的和<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory">共享内存</a>中详述的那样，共享内存预计比全局内存快得多。 它可以用作暂存器内存（或软件管理的缓存），以最大限度地减少来自 CUDA 块的全局内存访问，如下面的矩阵乘法示例所示。</p>
<p><img src="/img/matrix-multiplication-without-shared-memory.png" alt="matrix-multiplication-without-shared-memory.png"></p>
<p>以下代码示例是不利用共享内存的矩阵乘法的简单实现。 每个线程读取 A 的一行和 B 的一列，并计算 C 的相应元素，如图所示。因此，从全局内存中读取 A 为 B.width 次，而 B 为读取 A.height 次。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Matrices are stored in row-major order:</span></span><br><span class="line"><span class="comment">// M(row, col) = *(M.elements + row * M.width + col)</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> &#123;</span><br><span class="line">    <span class="type">int</span> width;</span><br><span class="line">    <span class="type">int</span> height;</span><br><span class="line">    <span class="type">float</span>* elements;</span><br><span class="line">&#125; Matrix;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Thread block size</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> BLOCK_SIZE 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Forward declaration of the matrix multiplication kernel</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatMulKernel</span><span class="params">(<span class="type">const</span> Matrix, <span class="type">const</span> Matrix, Matrix)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Matrix multiplication - Host code</span></span><br><span class="line"><span class="comment">// Matrix dimensions are assumed to be multiples of BLOCK_SIZE</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">MatMul</span><span class="params">(<span class="type">const</span> Matrix A, <span class="type">const</span> Matrix B, Matrix C)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Load A and B to device memory</span></span><br><span class="line">    Matrix d_A;</span><br><span class="line">    d_A.width = A.width; d_A.height = A.height;</span><br><span class="line">    <span class="type">size_t</span> size = A.width * A.height * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_A.elements, size);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_A.elements, A.elements, size,</span><br><span class="line">               cudaMemcpyHostToDevice);</span><br><span class="line">    Matrix d_B;</span><br><span class="line">    d_B.width = B.width; d_B.height = B.height;</span><br><span class="line">    size = B.width * B.height * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_B.elements, size);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_B.elements, B.elements, size,</span><br><span class="line">               cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate C in device memory</span></span><br><span class="line">    Matrix d_C;</span><br><span class="line">    d_C.width = C.width; d_C.height = C.height;</span><br><span class="line">    size = C.width * C.height * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_C.elements, size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Invoke kernel</span></span><br><span class="line">    <span class="function">dim3 <span class="title">dimBlock</span><span class="params">(BLOCK_SIZE, BLOCK_SIZE)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">dimGrid</span><span class="params">(B.width / dimBlock.x, A.height / dimBlock.y)</span></span>;</span><br><span class="line">    MatMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_A, d_B, d_C);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Read C from device memory</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(C.elements, d_C.elements, size,</span><br><span class="line">               cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Free device memory</span></span><br><span class="line">    <span class="built_in">cudaFree</span>(d_A.elements);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_B.elements);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_C.elements);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Matrix multiplication kernel called by MatMul()</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatMulKernel</span><span class="params">(Matrix A, Matrix B, Matrix C)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Each thread computes one element of C</span></span><br><span class="line">    <span class="comment">// by accumulating results into Cvalue</span></span><br><span class="line">    <span class="type">float</span> Cvalue = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> row = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    <span class="type">int</span> col = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> e = <span class="number">0</span>; e &lt; A.width; ++e)</span><br><span class="line">        Cvalue += A.elements[row * A.width + e]</span><br><span class="line">                * B.elements[e * B.width + col];</span><br><span class="line">    C.elements[row * C.width + col] = Cvalue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以下代码示例是利用共享内存的矩阵乘法实现。在这个实现中，每个线程块负责计算C的一个方形子矩阵Csub，块内的每个线程负责计算Csub的一个元素。如图所示，Csub 等于两个矩形矩阵的乘积：维度 A 的子矩阵 (A.width, block_size) 与 Csub 具有相同的行索引，以及维度 B 的子矩阵(block_size, A.width ) 具有与 Csub 相同的列索引。为了适应设备的资源，这两个矩形矩阵根据需要被分成多个尺寸为 block_size 的方阵，并且 Csub 被计算为这些方阵的乘积之和。这些乘积中的每一个都是通过首先将两个对应的方阵从全局内存加载到共享内存中的，一个线程加载每个矩阵的一个元素，然后让每个线程计算乘积的一个元素。每个线程将这些乘积中的每一个的结果累积到一个寄存器中，并在完成后将结果写入全局内存。</p>
<p><img src="/img/matrix-multiplication-with-shared-memory.png" alt="matrix-multiplication-with-shared-memory.png"></p>
<p>通过以这种方式将计算分块，我们利用了快速共享内存并节省了大量的全局内存带宽，因为 A 只从全局内存中读取 (B.width / block_size) 次，而 B 被读取 (A.height / block_size) 次.</p>
<p>前面代码示例中的 Matrix 类型增加了一个 stride 字段，因此子矩阵可以用相同的类型有效地表示。 <code>__device__</code> 函数用于获取和设置元素并从矩阵构建任何子矩阵。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Matrices are stored in row-major order:</span></span><br><span class="line"><span class="comment">// M(row, col) = *(M.elements + row * M.stride + col)</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> &#123;</span><br><span class="line">    <span class="type">int</span> width;</span><br><span class="line">    <span class="type">int</span> height;</span><br><span class="line">    <span class="type">int</span> stride; </span><br><span class="line">    <span class="type">float</span>* elements;</span><br><span class="line">&#125; Matrix;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get a matrix element</span></span><br><span class="line"><span class="function">__device__ <span class="type">float</span> <span class="title">GetElement</span><span class="params">(<span class="type">const</span> Matrix A, <span class="type">int</span> row, <span class="type">int</span> col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> A.elements[row * A.stride + col];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Set a matrix element</span></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">SetElement</span><span class="params">(Matrix A, <span class="type">int</span> row, <span class="type">int</span> col,</span></span></span><br><span class="line"><span class="params"><span class="function">                           <span class="type">float</span> value)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    A.elements[row * A.stride + col] = value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is</span></span><br><span class="line"><span class="comment">// located col sub-matrices to the right and row sub-matrices down</span></span><br><span class="line"><span class="comment">// from the upper-left corner of A</span></span><br><span class="line"> <span class="function">__device__ Matrix <span class="title">GetSubMatrix</span><span class="params">(Matrix A, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Matrix Asub;</span><br><span class="line">    Asub.width    = BLOCK_SIZE;</span><br><span class="line">    Asub.height   = BLOCK_SIZE;</span><br><span class="line">    Asub.stride   = A.stride;</span><br><span class="line">    Asub.elements = &amp;A.elements[A.stride * BLOCK_SIZE * row</span><br><span class="line">                                         + BLOCK_SIZE * col];</span><br><span class="line">    <span class="keyword">return</span> Asub;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Thread block size</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> BLOCK_SIZE 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Forward declaration of the matrix multiplication kernel</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatMulKernel</span><span class="params">(<span class="type">const</span> Matrix, <span class="type">const</span> Matrix, Matrix)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Matrix multiplication - Host code</span></span><br><span class="line"><span class="comment">// Matrix dimensions are assumed to be multiples of BLOCK_SIZE</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">MatMul</span><span class="params">(<span class="type">const</span> Matrix A, <span class="type">const</span> Matrix B, Matrix C)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Load A and B to device memory</span></span><br><span class="line">    Matrix d_A;</span><br><span class="line">    d_A.width = d_A.stride = A.width; d_A.height = A.height;</span><br><span class="line">    <span class="type">size_t</span> size = A.width * A.height * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_A.elements, size);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_A.elements, A.elements, size,</span><br><span class="line">               cudaMemcpyHostToDevice);</span><br><span class="line">    Matrix d_B;</span><br><span class="line">    d_B.width = d_B.stride = B.width; d_B.height = B.height;</span><br><span class="line">    size = B.width * B.height * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_B.elements, size);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(d_B.elements, B.elements, size,</span><br><span class="line">    cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate C in device memory</span></span><br><span class="line">    Matrix d_C;</span><br><span class="line">    d_C.width = d_C.stride = C.width; d_C.height = C.height;</span><br><span class="line">    size = C.width * C.height * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;d_C.elements, size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Invoke kernel</span></span><br><span class="line">    <span class="function">dim3 <span class="title">dimBlock</span><span class="params">(BLOCK_SIZE, BLOCK_SIZE)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">dimGrid</span><span class="params">(B.width / dimBlock.x, A.height / dimBlock.y)</span></span>;</span><br><span class="line">    MatMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_A, d_B, d_C);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Read C from device memory</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(C.elements, d_C.elements, size,</span><br><span class="line">               cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Free device memory</span></span><br><span class="line">    <span class="built_in">cudaFree</span>(d_A.elements);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_B.elements);</span><br><span class="line">    <span class="built_in">cudaFree</span>(d_C.elements);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Matrix multiplication kernel called by MatMul()</span></span><br><span class="line"> <span class="function">__global__ <span class="type">void</span> <span class="title">MatMulKernel</span><span class="params">(Matrix A, Matrix B, Matrix C)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Block row and column</span></span><br><span class="line">    <span class="type">int</span> blockRow = blockIdx.y;</span><br><span class="line">    <span class="type">int</span> blockCol = blockIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Each thread block computes one sub-matrix Csub of C</span></span><br><span class="line">    Matrix Csub = <span class="built_in">GetSubMatrix</span>(C, blockRow, blockCol);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Each thread computes one element of Csub</span></span><br><span class="line">    <span class="comment">// by accumulating results into Cvalue</span></span><br><span class="line">    <span class="type">float</span> Cvalue = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Thread row and column within Csub</span></span><br><span class="line">    <span class="type">int</span> row = threadIdx.y;</span><br><span class="line">    <span class="type">int</span> col = threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Loop over all the sub-matrices of A and B that are</span></span><br><span class="line">    <span class="comment">// required to compute Csub</span></span><br><span class="line">    <span class="comment">// Multiply each pair of sub-matrices together</span></span><br><span class="line">    <span class="comment">// and accumulate the results</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> m = <span class="number">0</span>; m &lt; (A.width / BLOCK_SIZE); ++m) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Get sub-matrix Asub of A</span></span><br><span class="line">        Matrix Asub = <span class="built_in">GetSubMatrix</span>(A, blockRow, m);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Get sub-matrix Bsub of B</span></span><br><span class="line">        Matrix Bsub = <span class="built_in">GetSubMatrix</span>(B, m, blockCol);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Shared memory used to store Asub and Bsub respectively</span></span><br><span class="line">        __shared__ <span class="type">float</span> As[BLOCK_SIZE][BLOCK_SIZE];</span><br><span class="line">        __shared__ <span class="type">float</span> Bs[BLOCK_SIZE][BLOCK_SIZE];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Load Asub and Bsub from device memory to shared memory</span></span><br><span class="line">        <span class="comment">// Each thread loads one element of each sub-matrix</span></span><br><span class="line">        As[row][col] = <span class="built_in">GetElement</span>(Asub, row, col);</span><br><span class="line">        Bs[row][col] = <span class="built_in">GetElement</span>(Bsub, row, col);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Synchronize to make sure the sub-matrices are loaded</span></span><br><span class="line">        <span class="comment">// before starting the computation</span></span><br><span class="line">        __syncthreads();</span><br><span class="line">        <span class="comment">// Multiply Asub and Bsub together</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> e = <span class="number">0</span>; e &lt; BLOCK_SIZE; ++e)</span><br><span class="line">            Cvalue += As[row][e] * Bs[e][col];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Synchronize to make sure that the preceding</span></span><br><span class="line">        <span class="comment">// computation is done before loading two new</span></span><br><span class="line">        <span class="comment">// sub-matrices of A and B in the next iteration</span></span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Write Csub to device memory</span></span><br><span class="line">    <span class="comment">// Each thread writes one element</span></span><br><span class="line">    <span class="built_in">SetElement</span>(Csub, row, col, Cvalue);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-5-Page-Locked主机内存"><a href="#3-2-5-Page-Locked主机内存" class="headerlink" title="3.2.5 Page-Locked主机内存"></a>3.2.5 Page-Locked主机内存</h3><p>运行时提供的函数允许使用锁页（也称为固定）主机内存（与 malloc() 分配的常规可分页主机内存相反）：</p>
<ul>
<li><code>cudaHostAlloc()</code> 和 <code>cudaFreeHost()</code> 分配和释放锁页主机内存；</li>
<li><code>cudaHostRegister()</code> 将 <code>malloc()</code> 分配的内存范围变为锁页内存（有关限制，请参阅参考手册）。</li>
</ul>
<p>使用页面锁定的主机内存有几个好处：</p>
<ul>
<li>锁页主机内存和设备内存之间的复制可以与<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution">异步并发执行</a>中提到的某些设备的内核执行同时执行。</li>
<li>在某些设备上，锁页主机内存可以映射到设备的地址空间，从而无需将其复制到设备内存或从设备内存复制，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#mapped-memory">映射内存</a>中所述。</li>
<li>在具有前端总线的系统上，如果主机内存被分配为页锁定，则主机内存和设备内存之间的带宽更高，如果另外分配为合并访存，则它甚至更高，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#write-combining-memory">合并写入内存</a>中所述。</li>
</ul>
<p>然而，锁页主机内存是一种稀缺资源，因此锁页内存中的分配将在可分页内存中分配之前很久就开始失败。 此外，通过减少操作系统可用于分页的物理内存量，消耗过多的页面锁定内存会降低整体系统性能。</p>
<p>注意：页面锁定的主机内存不会缓存在非 I/O 一致的 Tegra 设备上。 此外，非 I/O 一致的 Tegra 设备不支持 cudaHostRegister()。</p>
<p>简单的零拷贝 CUDA 示例附带关于页面锁定内存 API 的详细文档。</p>
<h4 id="3-2-5-1-Portable-Memory"><a href="#3-2-5-1-Portable-Memory" class="headerlink" title="3.2.5.1 Portable Memory"></a>3.2.5.1 Portable Memory</h4><p>一块锁页内存可以与系统中的任何设备一起使用（有关多设备系统的更多详细信息，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#multi-device-system">多设备系统</a>），但默认情况下，使用上述锁页内存的好处只是与分配块时当前的设备一起可用（并且所有设备共享相同的统一地址空间，如果有，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#unified-virtual-address-space">统一虚拟地址空间</a>中所述）。块需要通过将标志<code>cudaHostAllocPortable</code>传递给<code>cudaHostAlloc()</code>来分配，或者通过将标志<code>cudaHostRegisterPortable</code>传递给<code>cudaHostRegister()</code>来锁定页面。</p>
<h4 id="3-2-5-2-写合并内存"><a href="#3-2-5-2-写合并内存" class="headerlink" title="3.2.5.2 写合并内存"></a>3.2.5.2 写合并内存</h4><p>默认情况下，锁页主机内存被分配为可缓存的。它可以选择分配为写组合，而不是通过将标志 <code>cudaHostAllocWriteCombined</code> 传递给 <code>cudaHostAlloc()</code>。 写入组合内存释放了主机的 L1 和 L2 缓存资源，为应用程序的其余部分提供更多缓存。 此外，在通过 PCI Express 总线的传输过程中，写入组合内存不会被窥探，这可以将传输性能提高多达 40%。</p>
<p>从主机读取写组合内存非常慢，因此写组合内存通常应用于仅主机写入的内存。</p>
<p>应避免在 WC 内存上使用 CPU 原子指令，因为并非所有 CPU 实现都能保证该功能。</p>
<h4 id="3-2-5-3-Mapped-Memory"><a href="#3-2-5-3-Mapped-Memory" class="headerlink" title="3.2.5.3 Mapped Memory"></a>3.2.5.3 Mapped Memory</h4><p>通过将标志 <code>cudaHostAllocMapped</code> 传递给 <code>cudaHostAlloc()</code> 或通过将标志 <code>cudaHostRegisterMapped</code> 传递给 <code>cudaHostRegister()</code>，也可以将锁页主机内存块映射到设备的地址空间。因此，这样的块通常有两个地址：一个在主机内存中，由 <code>cudaHostAlloc()</code> 或 <code>malloc()</code> 返回，另一个在设备内存中，可以使用 <code>cudaHostGetDevicePointer()</code> 检索，然后用于从内核中访问该块。唯一的例外是使用 <code>cudaHostAlloc()</code> 分配的指针，以及<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#unified-virtual-address-space">统一虚拟地址空间</a>中提到的主机和设备使用统一地址空间。</p>
<p>直接从内核中访问主机内存不会提供与设备内存相同的带宽，但确实有一些优势：</p>
<ul>
<li>无需在设备内存中分配一个块，并在该块和主机内存中的块之间复制数据；数据传输是根据内核的需要隐式执行的；</li>
<li>无需使用流（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#concurrent-data-transfers">并发数据传输</a>）将数据传输与内核执行重叠；内核发起的数据传输自动与内核执行重叠。</li>
</ul>
<p>然而，由于映射的锁页内存在主机和设备之间共享，因此应用程序必须使用流或事件同步内存访问（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution">异步并发执行</a>）以避免任何潜在的 read-after-write、write-after-read 或 write-after-write危险。</p>
<p>为了能够检索到任何映射的锁页内存的设备指针，必须在执行任何其他 CUDA 调用之前通过使用 <code>cudaDeviceMapHost</code> 标志调用 <code>cudaSetDeviceFlags()</code> 来启用页面锁定内存映射。否则， <code>cudaHostGetDevicePointer()</code> 将返回错误。</p>
<p>如果设备不支持映射的锁页主机内存，<code>cudaHostGetDevicePointer()</code> 也会返回错误。应用程序可以通过检查 <code>canMapHostMemory</code> 设备属性（请参阅[设备枚举](<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-enumeration）来查询此功能，对于支持映射锁页主机内存的设备，该属性等于">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-enumeration）来查询此功能，对于支持映射锁页主机内存的设备，该属性等于</a> 1。</p>
<p>请注意，从主机或其他设备的角度来看，在映射的锁页内存上运行的原子函数（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions">原子函数</a>）不是原子的。</p>
<p>另请注意，CUDA 运行时要求从主机和其他设备的角度来看，从设备启动到主机内存的 1 字节、2 字节、4 字节和 8 字节自然对齐的加载和存储保留为单一访问设备。在某些平台上，内存的原子操作可能会被硬件分解为单独的加载和存储操作。这些组件加载和存储操作对保留自然对齐的访问具有相同的要求。例如，CUDA 运行时不支持 PCI Express 总线拓扑，其中 PCI Express 桥将 8 字节自然对齐的写入拆分为设备和主机之间的两个 4 字节写入。 </p>
<h3 id="3-2-6-异步并发执行"><a href="#3-2-6-异步并发执行" class="headerlink" title="3.2.6 异步并发执行"></a>3.2.6 异步并发执行</h3><p>CUDA 将以下操作公开为可以彼此同时操作的独立任务：</p>
<ul>
<li>在主机上计算；</li>
<li>设备上的计算；</li>
<li>从主机到设备的内存传输；</li>
<li>从设备到主机的内存传输；</li>
<li>在给定设备的内存中进行内存传输；</li>
<li>设备之间的内存传输。</li>
</ul>
<p>这些操作之间实现的并发级别将取决于设备的功能和计算能力，如下所述。</p>
<h4 id="3-2-6-1-主机和设备之间的并发执行"><a href="#3-2-6-1-主机和设备之间的并发执行" class="headerlink" title="3.2.6.1 主机和设备之间的并发执行"></a>3.2.6.1 主机和设备之间的并发执行</h4><p>在设备完成请求的任务之前，异步库函数将控制权返回给宿主线程，从而促进了主机的并发执行。使用异步调用，许多设备操作可以在适当的设备资源可用时排队，由CUDA驱动程序执行。这减轻了主机线程管理设备的大部分责任，让它自由地执行其他任务。以下设备操作对主机是异步的:</p>
<ul>
<li>内核启动;</li>
<li>内存复制在单个设备的内存中;</li>
<li>从主机到设备内存拷贝的内存块大小不超过64kb的;</li>
<li>由带有Async后缀的函数执行的内存拷贝;</li>
<li>内存设置函数调用。<br>  程序员可以通过将<code>CUDA_LAUNCH_BLOCKING</code>环境变量设置为1来全局禁用系统上运行的所有CUDA应用程序的内核启动的异步性。此特性仅用于调试目的，不应用作使生产软件可靠运行的一种方法。</li>
</ul>
<p>如果通过分析器（Nsight、Visual Profiler）收集硬件计数器，则内核启动是同步的，除非启用了并发内核分析。如果异步内存复制涉及非页面锁定的主机内存，它们也将是同步的。</p>
<h4 id="3-2-6-2-并行执行内核"><a href="#3-2-6-2-并行执行内核" class="headerlink" title="3.2.6.2 并行执行内核"></a>3.2.6.2 并行执行内核</h4><p>某些计算能力 2.x 及更高版本的设备可以同时执行多个内核。 应用程序可以通过检查 <code>concurrentKernels</code> 设备属性（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-enumeration">设备枚举</a>）来查询此功能，对于支持它的设备，该属性等于 1。</p>
<p>设备可以同时执行的内核启动的最大数量取决于其计算能力，并在<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications__technical-specifications-per-compute-capability">表15</a> 中列出。</p>
<p>来自一个 CUDA 上下文的内核不能与来自另一个 CUDA 上下文的内核同时执行。</p>
<p>使用许多纹理或大量本地内存的内核不太可能与其他内核同时执</p>
<h4 id="3-2-6-3-数据传输和内核执行的重叠"><a href="#3-2-6-3-数据传输和内核执行的重叠" class="headerlink" title="3.2.6.3 数据传输和内核执行的重叠"></a>3.2.6.3 数据传输和内核执行的重叠</h4><p>一些设备可以在内核执行的同时执行与 GPU 之间的异步内存复制。 应用程序可以通过检查 <code>asyncEngineCount</code> 设备属性（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-enumeration">设备枚举</a>）来查询此功能，对于支持它的设备，该属性大于零。 如果复制中涉及主机内存，则它必须是页锁定的。</p>
<p>还可以与内核执行（在支持 <code>concurrentKernels</code> 设备属性的设备上）或与设备之间的拷贝（对于支持 <code>asyncEngineCount</code> 属性的设备）同时执行设备内复制。 使用标准内存复制功能启动设备内复制，目标地址和源地址位于同一设备上。</p>
<h4 id="3-2-6-4-并行数据传输"><a href="#3-2-6-4-并行数据传输" class="headerlink" title="3.2.6.4 并行数据传输"></a>3.2.6.4 并行数据传输</h4><p>某些计算能力为 2.x 及更高版本的设备可以重叠设备之间的数据拷贝。 应用程序可以通过检查 <code>asyncEngineCount</code> 设备属性（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-enumeration">设备枚举</a>）来查询此功能，对于支持它的设备，该属性等于 2。 为了重叠，传输中涉及的任何主机内存都必须是页面锁定的。</p>
<h4 id="3-2-6-5-流"><a href="#3-2-6-5-流" class="headerlink" title="3.2.6.5 流"></a>3.2.6.5 流</h4><p>应用程序通过流管理上述并发操作。 流是按顺序执行的命令序列（可能由不同的主机线程发出）。 另一方面，不同的流可能会彼此乱序或同时执行它们的命令； 不能保证此行为，因此不应依赖其正确性（例如，内核间通信未定义）。 当满足命令的所有依赖项时，可以执行在流上发出的命令。 依赖关系可以是先前在同一流上启动的命令或来自其他流的依赖关系。 同步调用的成功完成保证了所有启动的命令都完成了。</p>
<h5 id="3-2-6-5-1-创建与销毁"><a href="#3-2-6-5-1-创建与销毁" class="headerlink" title="3.2.6.5.1 创建与销毁"></a>3.2.6.5.1 创建与销毁</h5><p>流是通过创建一个流对象并将其指定为一系列内核启动和主机 &lt;-&gt; 设备内存拷贝的流参数来定义的。 以下代码示例创建两个流并在锁页内存中分配一个浮点数组 <code>hostPtr</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cudaStream_t stream[<span class="number">2</span>];</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)</span><br><span class="line">    <span class="built_in">cudaStreamCreate</span>(&amp;stream[i]);</span><br><span class="line"><span class="type">float</span>* hostPtr;</span><br><span class="line"><span class="built_in">cudaMallocHost</span>(&amp;hostPtr, <span class="number">2</span> * size);</span><br></pre></td></tr></table></figure>
<p>这些流中的每一个都由以下代码示例定义为从主机到设备的一次内存复制、一次内核启动和从设备到主机的一次内存复制的序列：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i) &#123;</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(inputDevPtr + i * size, hostPtr + i * size,</span><br><span class="line">                    size, cudaMemcpyHostToDevice, stream[i]);</span><br><span class="line">    MyKernel &lt;&lt;&lt;<span class="number">100</span>, <span class="number">512</span>, <span class="number">0</span>, stream[i]&gt;&gt;&gt;</span><br><span class="line">          (outputDevPtr + i * size, inputDevPtr + i * size, size);</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(hostPtr + i * size, outputDevPtr + i * size,</span><br><span class="line">                    size, cudaMemcpyDeviceToHost, stream[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个流将其输入数组 <code>hostPtr</code> 的部分复制到设备内存中的数组 <code>inputDevPtr</code>，通过调用 <code>MyKernel()</code> 处理设备上的 <code>inputDevPtr</code>，并将结果 <code>outputDevPtr</code> 复制回 <code>hostPtr</code> 的同一部分。 重叠行为描述了此示例中的流如何根据设备的功能重叠。 请注意，<code>hostPtr</code> 必须指向锁页主机内存才能发生重叠。</p>
<p>通过调用 <code>cudaStreamDestroy()</code>释放流:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)</span><br><span class="line">    <span class="built_in">cudaStreamDestroy</span>(stream[i]);</span><br></pre></td></tr></table></figure>
<p>如果调用 <code>cudaStreamDestroy()</code> 时设备仍在流中工作，则该函数将立即返回，并且一旦设备完成流中的所有工作，与流关联的资源将自动释放。</p>
<h5 id="3-2-6-5-2-默认流"><a href="#3-2-6-5-2-默认流" class="headerlink" title="3.2.6.5.2 默认流"></a>3.2.6.5.2 默认流</h5><p>未指定任何流参数或等效地将流参数设置为零的内核启动和主机 &lt;-&gt; 设备内存拷贝将发布到默认流。因此它们按顺序执行。</p>
<p>对于使用 <code>--default-stream per-thread</code> 编译标志编译的代码（或在包含 CUDA 头文件（cuda.h 和 cuda_runtime.h）之前定义 <code>CUDA_API_PER_THREAD_DEFAULT_STREAM</code> 宏），默认流是常规流，并且每个主机线程有自己的默认流。</p>
<p>注意：当代码由 nvcc 编译时，<code>#define CUDA_API_PER_THREAD_DEFAULT_STREAM 1</code> 不能用于启用此行为，因为 nvcc 在翻译单元的顶部隐式包含 cuda_runtime.h。在这种情况下，需要使用 <code>--default-stream</code> 每个线程编译标志，或者需要使用 <code>-DCUDA_API_PER_THREAD_DEFAULT_STREAM=1</code> 编译器标志定义 <code>CUDA_API_PER_THREAD_DEFAULT_STREAM</code> 宏。</p>
<p>对于使用 <code>--default-stream legacy</code> 编译标志编译的代码，默认流是称为 NULL 流的特殊流，每个设备都有一个用于所有主机线程的 NULL 流。 NULL 流很特殊，因为它会导致隐式同步，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#implicit-synchronization">隐式同步</a>中所述。</p>
<p>对于在没有指定 <code>--default-stream</code> 编译标志的情况下编译的代码， <code>--default-stream legacy</code> 被假定为默认值。</p>
<h5 id="3-2-6-5-3-显式同步"><a href="#3-2-6-5-3-显式同步" class="headerlink" title="3.2.6.5.3 显式同步"></a>3.2.6.5.3 显式同步</h5><p>有多种方法可以显式地同步流。</p>
<p><code>cudaDeviceSynchronize()</code> 一直等待，直到所有主机线程的所有流中的所有先前命令都完成。</p>
<p><code>cudaStreamSynchronize()</code> 将流作为参数并等待，直到给定流中的所有先前命令都已完成。 它可用于将主机与特定流同步，允许其他流继续在设备上执行。</p>
<p><code>cudaStreamWaitEvent()</code> 将流和事件作为参数（有关事件的描述，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#events">事件</a>），并在调用 <code>cudaStreamWaitEvent()</code> 后使添加到给定流的所有命令延迟执行，直到给定事件完成。</p>
<p><code>cudaStreamQuery()</code> 为应用程序提供了一种方法来了解流中所有前面的命令是否已完成。</p>
<h4 id="3-2-6-5-4-隐式同步"><a href="#3-2-6-5-4-隐式同步" class="headerlink" title="3.2.6.5.4 隐式同步"></a>3.2.6.5.4 隐式同步</h4><p>如果主机线程在它们之间发出以下任一操作，则来自不同流的两个命令不能同时运行：</p>
<ul>
<li>页面锁定的主机内存分配，</li>
<li>设备内存分配，</li>
<li>设备内存设置，</li>
<li>两个地址之间的内存拷贝到同一设备内存，</li>
<li>对 NULL 流的任何 CUDA 命令，</li>
<li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-3-0">计算能力 3.x </a>和<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-3-0">计算能力 7.x</a> 中描述的 L1/共享内存配置之间的切换。</li>
</ul>
<p>对于支持并发内核执行且计算能力为 3.0 或更低的设备，任何需要依赖项检查以查看流内核启动是否完成的操作：</p>
<ul>
<li>仅当从 CUDA 上下文中的任何流启动的所有先前内核的所有线程块都已开始执行时，才能开始执行；</li>
<li>阻止所有以后从 CUDA 上下文中的任何流启动内核，直到检查内核启动完成。</li>
</ul>
<p>需要依赖检查的操作包括与正在检查的启动相同的流中的任何其他命令以及对该流的任何 <code>cudaStreamQuery()</code> 调用。 因此，应用程序应遵循以下准则来提高并发内核执行的潜力：</p>
<ul>
<li>所有独立操作都应该在依赖操作之前发出，</li>
<li>任何类型的同步都应该尽可能地延迟。</li>
</ul>
<h5 id="3-2-6-5-5-重叠行为"><a href="#3-2-6-5-5-重叠行为" class="headerlink" title="3.2.6.5.5 重叠行为"></a>3.2.6.5.5 重叠行为</h5><p>两个流之间的执行重叠量取决于向每个流发出命令的顺序以及设备是否支持数据传输和内核执行的重叠（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#overlap-of-data-transfer-and-kernel-execution">数据传输和内核执行的重叠</a>）、并发内核执行（ 请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#concurrent-kernel-execution">并发内核执行</a>）和并发数据传输（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#concurrent-data-transfers">并发数据传输</a>）。</p>
<p>例如,在设备不支持并行数据传输,这两个流的代码示例创建和销毁不重叠,因为由stream[1]发起的内存复制会在stream[0]发起的内存复制之后执行。如果代码以以下方式重写(并且假设设备支持数据传输和内核执行的重叠)</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(inputDevPtr + i * size, hostPtr + i * size,</span><br><span class="line">                    size, cudaMemcpyHostToDevice, stream[i]);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)</span><br><span class="line">    MyKernel&lt;&lt;&lt;<span class="number">100</span>, <span class="number">512</span>, <span class="number">0</span>, stream[i]&gt;&gt;&gt;</span><br><span class="line">          (outputDevPtr + i * size, inputDevPtr + i * size, size);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i)</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(hostPtr + i * size, outputDevPtr + i * size,</span><br><span class="line">                    size, cudaMemcpyDeviceToHost, stream[i]);</span><br></pre></td></tr></table></figure>
<p>那么在stream[1]上从主机到设备的内存复制 与stream[0]上内核启动重叠。</p>
<p>在支持并发数据传输的设备上，<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#creation-and-destruction-streams">Creation 和 Destruction</a> 的代码示例的两个流确实重叠：在stream[1]上从主机到设备的内存复制 与在stream[0]上从设备到主机的内存复制甚至在stream[0]上内核启动（假设设备支持数据传输和内核执行的重叠）。但是，对于计算能力为 3.0 或更低的设备，内核执行不可能重叠，因为在stream[0]上从设备到主机的内存复制之后，第二次在stream[1]上内核启动，因此它被阻塞，直到根据隐式同步，在stream[0]上第一个内核启动已完成。如果代码如上重写，内核执行重叠（假设设备支持并发内核执行），因为在stream[0]上从设备到主机的内存复制之前，第二次在stream[1]上内核启动被。但是，在这种情况下，根据<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#implicit-synchronization">隐式同步</a>，在stream[0]上从设备到主机的内存复制仅与在stream[1]上内核启动的最后一个线程块重叠，这只能代表总数的一小部分内核的执行时间。 </p>
<h5 id="3-2-6-5-6-Host函数-回调"><a href="#3-2-6-5-6-Host函数-回调" class="headerlink" title="3.2.6.5.6 Host函数(回调)"></a>3.2.6.5.6 Host函数(回调)</h5><p>运行时提供了一种通过 <code>cudaLaunchHostFunc()</code> 在任何点将 CPU 函数调用插入到流中的方法。 在回调之前向流发出的所有命令都完成后，在主机上执行提供的函数。</p>
<p>以下代码示例在向每个流发出主机到设备内存副本、内核启动和设备到主机内存副本后，将主机函数 MyCallback 添加到两个流中的每一个。 每个设备到主机的内存复制完成后，该函数将在主机上开始执行。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> CUDART_CB <span class="title">MyCallback</span><span class="params">(cudaStream_t stream, cudaError_t status, <span class="type">void</span> *data)</span></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Inside callback %d\n&quot;</span>, (<span class="type">size_t</span>)data);</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i) &#123;</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(devPtrIn[i], hostPtr[i], size, cudaMemcpyHostToDevice, stream[i]);</span><br><span class="line">    MyKernel&lt;&lt;&lt;<span class="number">100</span>, <span class="number">512</span>, <span class="number">0</span>, stream[i]&gt;&gt;&gt;(devPtrOut[i], devPtrIn[i], size);</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(hostPtr[i], devPtrOut[i], size, cudaMemcpyDeviceToHost, stream[i]);</span><br><span class="line">    <span class="built_in">cudaLaunchHostFunc</span>(stream[i], MyCallback, (<span class="type">void</span>*)i);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在主机函数之后在流中发出的命令不会在函数完成之前开始执行。</p>
<p>在流中的主机函数不得进行 CUDA API 调用（直接或间接），因为如果它进行这样的调用导致死锁，它可能最终会等待自身。</p>
<h5 id="3-2-6-5-7-流优先级"><a href="#3-2-6-5-7-流优先级" class="headerlink" title="3.2.6.5.7 流优先级"></a>3.2.6.5.7 流优先级</h5><p>可以在创建时使用 <code>cudaStreamCreateWithPriority()</code>指定流的相对优先级。 可以使用 <code>cudaDeviceGetStreamPriorityRange()</code> 函数获得允许的优先级范围，按 [最高优先级，最低优先级] 排序。 在运行时，高优先级流中的待处理工作优先于低优先级流中的待处理工作。</p>
<p>以下代码示例获取当前设备允许的优先级范围，并创建具有最高和最低可用优先级的流。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// get the range of stream priorities for this device</span></span><br><span class="line"><span class="type">int</span> priority_high, priority_low;</span><br><span class="line"><span class="built_in">cudaDeviceGetStreamPriorityRange</span>(&amp;priority_low, &amp;priority_high);</span><br><span class="line"><span class="comment">// create streams with highest and lowest available priorities</span></span><br><span class="line">cudaStream_t st_high, st_low;</span><br><span class="line"><span class="built_in">cudaStreamCreateWithPriority</span>(&amp;st_high, cudaStreamNonBlocking, priority_high);</span><br><span class="line"><span class="built_in">cudaStreamCreateWithPriority</span>(&amp;st_low, cudaStreamNonBlocking, priority_low);</span><br></pre></td></tr></table></figure>
<h4 id="3-2-6-6-CUDA图"><a href="#3-2-6-6-CUDA图" class="headerlink" title="3.2.6.6 CUDA图"></a>3.2.6.6 CUDA图</h4><p>CUDA Graphs 为 CUDA 中的工作提交提供了一种新模型。图是一系列操作，例如内核启动，由依赖关系连接，独立于其执行定义。这允许一个图被定义一次，然后重复启动。将图的定义与其执行分开可以实现许多优化：首先，与流相比，CPU 启动成本降低，因为大部分设置都是提前完成的；其次，将整个工作流程呈现给 CUDA 可以实现优化，这可能无法通过流的分段工作提交机制实现。</p>
<p>要查看图形可能的优化，请考虑流中发生的情况：当您将内核放入流中时，主机驱动程序会执行一系列操作，以准备在 GPU 上执行内核。这些设置和启动内核所必需的操作是必须为发布的每个内核支付的间接成本。对于执行时间较短的 GPU 内核，这种开销成本可能是整个端到端执行时间的很大一部分。</p>
<p>使用图的工作提交分为三个不同的阶段：定义、实例化和执行。</p>
<ul>
<li>在定义阶段，程序创建图中操作的描述以及它们之间的依赖关系。</li>
<li>实例化获取图模板的快照，对其进行验证，并执行大部分工作的设置和初始化，目的是最大限度地减少启动时需要完成的工作。 生成的实例称为可执行图。</li>
<li>可执行图可以启动到流中，类似于任何其他 CUDA 工作。 它可以在不重复实例化的情况下启动任意次数。</li>
</ul>
<h5 id="3-2-6-6-1图架构"><a href="#3-2-6-6-1图架构" class="headerlink" title="3.2.6.6.1图架构"></a>3.2.6.6.1图架构</h5><p>一个操作在图中形成一个节点。 操作之间的依赖关系是边。 这些依赖关系限制了操作的执行顺序。</p>
<p>一个操作可以在它所依赖的节点完成后随时调度。 调度由 CUDA 系统决定。</p>
<h5 id="3-2-6-6-1-1-节点类型"><a href="#3-2-6-6-1-1-节点类型" class="headerlink" title="3.2.6.6.1.1 节点类型"></a>3.2.6.6.1.1 节点类型</h5><p>图节点可以是以下之一：</p>
<ul>
<li>核函数</li>
<li>CPU函数调用</li>
<li>内存拷贝</li>
<li>内存设置</li>
<li>空节点</li>
<li>等待事件</li>
<li>记录事件</li>
<li>发出外部信号量的信号</li>
<li>等待外部信号量</li>
<li>子图：执行单独的嵌套图。 请参下图。</li>
</ul>
<p><img src="/img/child-graph.png" alt="child-graph.png"></p>
<h5 id="3-2-6-6-2利用API创建图"><a href="#3-2-6-6-2利用API创建图" class="headerlink" title="3.2.6.6.2利用API创建图"></a>3.2.6.6.2利用API创建图</h5><p>可以通过两种机制创建图：显式 API 和流捕获。 以下是创建和执行下图的示例。<br><img src="/img/create-a-graph.png" alt="create-a-graph.png"></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create the graph - it starts out empty</span></span><br><span class="line"><span class="built_in">cudaGraphCreate</span>(&amp;graph, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// For the purpose of this example, we&#x27;ll create</span></span><br><span class="line"><span class="comment">// the nodes separately from the dependencies to</span></span><br><span class="line"><span class="comment">// demonstrate that it can be done in two stages.</span></span><br><span class="line"><span class="comment">// Note that dependencies can also be specified </span></span><br><span class="line"><span class="comment">// at node creation. </span></span><br><span class="line"><span class="built_in">cudaGraphAddKernelNode</span>(&amp;a, graph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;nodeParams);</span><br><span class="line"><span class="built_in">cudaGraphAddKernelNode</span>(&amp;b, graph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;nodeParams);</span><br><span class="line"><span class="built_in">cudaGraphAddKernelNode</span>(&amp;c, graph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;nodeParams);</span><br><span class="line"><span class="built_in">cudaGraphAddKernelNode</span>(&amp;d, graph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;nodeParams);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Now set up dependencies on each node</span></span><br><span class="line"><span class="built_in">cudaGraphAddDependencies</span>(graph, &amp;a, &amp;b, <span class="number">1</span>);     <span class="comment">// A-&gt;B</span></span><br><span class="line"><span class="built_in">cudaGraphAddDependencies</span>(graph, &amp;a, &amp;c, <span class="number">1</span>);     <span class="comment">// A-&gt;C</span></span><br><span class="line"><span class="built_in">cudaGraphAddDependencies</span>(graph, &amp;b, &amp;d, <span class="number">1</span>);     <span class="comment">// B-&gt;D</span></span><br><span class="line"><span class="built_in">cudaGraphAddDependencies</span>(graph, &amp;c, &amp;d, <span class="number">1</span>);     <span class="comment">// C-&gt;D</span></span><br></pre></td></tr></table></figure>
<h5 id="3-2-6-6-3-使用流捕获创建图"><a href="#3-2-6-6-3-使用流捕获创建图" class="headerlink" title="3.2.6.6.3 使用流捕获创建图"></a>3.2.6.6.3 使用流捕获创建图</h5><p>流捕获提供了一种从现有的基于流的 API 创建图的机制。 将工作启动到流中的一段代码，包括现有代码，可以等同于用与 <code>cudaStreamBeginCapture()</code> 和 <code>cudaStreamEndCapture()</code> 的调用。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cudaGraph_t graph;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaStreamBeginCapture</span>(stream);</span><br><span class="line"></span><br><span class="line">kernel_A&lt;&lt;&lt; ..., stream &gt;&gt;&gt;(...);</span><br><span class="line">kernel_B&lt;&lt;&lt; ..., stream &gt;&gt;&gt;(...);</span><br><span class="line"><span class="built_in">libraryCall</span>(stream);</span><br><span class="line">kernel_C&lt;&lt;&lt; ..., stream &gt;&gt;&gt;(...);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaStreamEndCapture</span>(stream, &amp;graph);</span><br></pre></td></tr></table></figure>
<p>对 <code>cudaStreamBeginCapture()</code> 的调用将流置于捕获模式。 捕获流时，启动到流中的工作不会排队执行。 相反，它被附加到正在逐步构建的内部图中。 然后通过调用 <code>cudaStreamEndCapture()</code> 返回此图，这也结束了流的捕获模式。 由流捕获主动构建的图称为捕获图(<code>capture graph</code>)。</p>
<p>流捕获可用于除 <code>cudaStreamLegacy</code>（“NULL 流”）之外的任何 CUDA 流。 请注意，它可以在 <code>cudaStreamPerThread</code> 上使用。 如果程序正在使用legacy stream，则可以将stream 0 重新定义为不更改功能的每线程流。 请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#default-stream">默认流</a>。</p>
<p>可以使用 <code>cudaStreamIsCapturing()</code> 查询是否正在捕获流。</p>
<h5 id="3-2-6-6-3-1-跨流依赖性和事件"><a href="#3-2-6-6-3-1-跨流依赖性和事件" class="headerlink" title="3.2.6.6.3.1 跨流依赖性和事件"></a>3.2.6.6.3.1 跨流依赖性和事件</h5><p>流捕获可以处理用 <code>cudaEventRecord()</code> 和 <code>cudaStreamWaitEvent()</code> 表示的跨流依赖关系，前提是正在等待的事件被记录到同一个捕获图中。</p>
<p>当事件记录在处于捕获模式的流中时，它会导致捕获事件。捕获的事件表示捕获图中的一组节点。</p>
<p>当流等待捕获的事件时，如果尚未将流置于捕获模式，则它会将流置于捕获模式，并且流中的下一个项目将对捕获事件中的节点具有额外的依赖关系。然后将两个流捕获到同一个捕获图。</p>
<p>当流捕获中存在跨流依赖时，仍然必须在调用 <code>cudaStreamBeginCapture()</code> 的同一流中调用 <code>cudaStreamEndCapture()</code>；这是原始流。由于基于事件的依赖关系，被捕获到同一捕获图的任何其他流也必须连接回原始流。如下所示。在 <code>cudaStreamEndCapture()</code> 时，捕获到同一捕获图的所有流都将退出捕获模式。未能重新加入原始流将导致整个捕获操作失败。 </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// stream1 is the origin stream</span></span><br><span class="line"><span class="built_in">cudaStreamBeginCapture</span>(stream1);</span><br><span class="line"></span><br><span class="line">kernel_A&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Fork into stream2</span></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(event1, stream1);</span><br><span class="line"><span class="built_in">cudaStreamWaitEvent</span>(stream2, event1);</span><br><span class="line"></span><br><span class="line">kernel_B&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(...);</span><br><span class="line">kernel_C&lt;&lt;&lt; ..., stream2 &gt;&gt;&gt;(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Join stream2 back to origin stream (stream1)</span></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(event2, stream2);</span><br><span class="line"><span class="built_in">cudaStreamWaitEvent</span>(stream1, event2);</span><br><span class="line"></span><br><span class="line">kernel_D&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// End capture in the origin stream</span></span><br><span class="line"><span class="built_in">cudaStreamEndCapture</span>(stream1, &amp;graph);</span><br><span class="line"></span><br><span class="line"><span class="comment">// stream1 and stream2 no longer in capture mode  </span></span><br></pre></td></tr></table></figure>
<p>上述代码返回的图如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#creating-a-graph-using-api__fig-creating-using-graph-apis">图 10</a> 所示。</p>
<p>注意：当流退出捕获模式时，流中的下一个未捕获项（如果有）仍将依赖于最近的先前未捕获项，尽管已删除中间项。</p>
<h5 id="3-2-6-6-3-2-禁止和未处理的操作"><a href="#3-2-6-6-3-2-禁止和未处理的操作" class="headerlink" title="3.2.6.6.3.2 禁止和未处理的操作"></a>3.2.6.6.3.2 禁止和未处理的操作</h5><p>同步或查询正在捕获的流或捕获的事件的执行状态是无效的，因为它们不代表计划执行的项目。当任何关联流处于捕获模式时，查询包含活动流捕获的更广泛句柄（例如设备或上下文句柄）的执行状态或同步也是无效的。</p>
<p>当捕获同一上下文中的任何流时，并且它不是使用 <code>cudaStreamNonBlocking</code> 创建的，任何使用旧流的尝试都是无效的。这是因为legacy stream句柄始终包含这些其他流；legacy stream将创建对正在捕获的流的依赖，并且查询它或同步它会查询或同步正在捕获的流。</p>
<p>因此在这种情况下调用同步 API 也是无效的。同步 API，例如 cudaMemcpy()，将工作legacy stream并在返回之前对其进行同步。</p>
<p>注意：作为一般规则，当依赖关系将捕获的内容与未捕获的内容联系起来并排队执行时，CUDA 更喜欢返回错误而不是忽略依赖关系。将流放入或退出捕获模式时会出现异常；这切断了在模式转换之前和之后添加到流中的项目之间的依赖关系。</p>
<p>通过等待来自正在捕获并且与与事件不同的捕获图相关联的流中的捕获事件来合并两个单独的捕获图是无效的。等待正在捕获的流中的未捕获事件是无效的。</p>
<p>图中当前不支持将异步操作排入流的少量 API，如果使用正在捕获的流调用，则会返回错误，例如 <code>cudaStreamAttachMemAsync()</code>。</p>
<h5 id="3-2-6-6-3-3失效"><a href="#3-2-6-6-3-3失效" class="headerlink" title="3.2.6.6.3.3失效"></a>3.2.6.6.3.3失效</h5><p>在流捕获期间尝试无效操作时，任何关联的捕获图都将失效。 当捕获图无效时，进一步使用正在捕获的任何流或与该图关联的捕获事件将无效并将返回错误，直到使用 <code>cudaStreamEndCapture()</code>结束流捕获。 此调用将使关联的流脱离捕获模式，但也会返回错误值和 NULL 图。</p>
<h5 id="3-2-6-6-4-更新实例化图"><a href="#3-2-6-6-4-更新实例化图" class="headerlink" title="3.2.6.6.4 更新实例化图"></a>3.2.6.6.4 更新实例化图</h5><p>使用图的工作提交分为三个不同的阶段：定义、实例化和执行。在工作流不改变的情况下，定义和实例化的开销可以分摊到许多执行中，并且图提供了明显优于流的优势。</p>
<p>图是工作流的快照，包括内核、参数和依赖项，以便尽可能快速有效地重放它。在工作流发生变化的情况下，图会过时，必须进行修改。对图结构（例如拓扑或节点类型）的重大更改将需要重新实例化源图，因为必须重新应用各种与拓扑相关的优化技术。</p>
<p>重复实例化的成本会降低图执行带来的整体性能优势，但通常只有节点参数（例如内核参数和 cudaMemcpy 地址）发生变化，而图拓扑保持不变。对于这种情况，CUDA 提供了一种称为“图形更新”的轻量级机制，它允许就地修改某些节点参数，而无需重建整个图形。这比重新实例化要有效得多。</p>
<p>更新将在下次启动图时生效，因此它们不会影响以前的图启动，即使它们在更新时正在运行。一个图可能会被重复更新和重新启动，因此多个更新/启动可以在一个流上排队。</p>
<p>CUDA 提供了两种更新实例化图的机制，全图更新和单个节点更新。整个图更新允许用户提供一个拓扑相同的 <code>cudaGraph_t</code> 对象，其节点包含更新的参数。单个节点更新允许用户显式更新单个节点的参数。当大量节点被更新时，或者当调用者不知道图拓扑时（即，图是由库调用的流捕获产生的），使用更新的 <code>cudaGraph_t</code> 会更方便。当更改的数量很少并且用户拥有需要更新的节点的句柄时，首选使用单个节点更新。单个节点更新跳过未更改节点的拓扑检查和比较，因此在许多情况下它可以更有效。以下部分更详细地解释了每种方法。 </p>
<h5 id="3-2-6-6-4-1-图更新限制"><a href="#3-2-6-6-4-1-图更新限制" class="headerlink" title="3.2.6.6.4.1 图更新限制"></a>3.2.6.6.4.1 图更新限制</h5><p>内核节点：</p>
<ul>
<li>函数的所属上下文不能改变。</li>
<li>其功能最初未使用 CUDA 动态并行性的节点无法更新为使用 CUDA 动态并行性的功能。</li>
</ul>
<p>cudaMemset 和 cudaMemcpy 节点：</p>
<ul>
<li>操作数分配/映射到的 CUDA 设备不能更改。</li>
<li>源/目标内存必须从与原始源/目标内存相同的上下文中分配。</li>
<li>只能更改一维 cudaMemset/cudaMemcpy 节点。</li>
<li><p>额外的 memcpy 节点限制：</p>
</li>
<li><p>不支持更改源或目标内存类型（即 cudaPitchedPtr、cudaArray_t 等）或传输类型（即 cudaMemcpyKind）。</p>
</li>
</ul>
<p>外部信号量等待节点和记录节点：</p>
<ul>
<li>不支持更改信号量的数量。</li>
<li>对主机节点、事件记录节点或事件等待节点的更新没有限制。</li>
</ul>
<h5 id="3-2-6-6-4-2全图更新"><a href="#3-2-6-6-4-2全图更新" class="headerlink" title="3.2.6.6.4.2全图更新"></a>3.2.6.6.4.2全图更新</h5><p><code>cudaGraphExecUpdate()</code> 允许使用相同拓扑图（“更新”图）中的参数更新实例化图（“原始图”）。 更新图的拓扑必须与用于实例化 <code>cudaGraphExec_t</code> 的原始图相同。 此外，将节点添加到原始图或从中删除的顺序必须与将节点添加到更新图（或从中删除）的顺序相匹配。 因此，在使用流捕获时，必须以相同的顺序捕获节点，而在使用显式图形节点创建 API 时，必须以相同的顺序添加或删除所有节点。</p>
<p>以下示例显示了如何使用 API 更新实例化图：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">cudaGraphExec_t graphExec = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    cudaGraph_t graph;</span><br><span class="line">    cudaGraphExecUpdateResult updateResult;</span><br><span class="line">    cudaGraphNode_t errorNode;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// In this example we use stream capture to create the graph.</span></span><br><span class="line">    <span class="comment">// You can also use the Graph API to produce a graph.</span></span><br><span class="line">    <span class="built_in">cudaStreamBeginCapture</span>(stream, cudaStreamCaptureModeGlobal);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Call a user-defined, stream based workload, for example</span></span><br><span class="line">    <span class="built_in">do_cuda_work</span>(stream);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaStreamEndCapture</span>(stream, &amp;graph);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If we&#x27;ve already instantiated the graph, try to update it directly</span></span><br><span class="line">    <span class="comment">// and avoid the instantiation overhead</span></span><br><span class="line">    <span class="keyword">if</span> (graphExec != <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="comment">// If the graph fails to update, errorNode will be set to the</span></span><br><span class="line">        <span class="comment">// node causing the failure and updateResult will be set to a</span></span><br><span class="line">        <span class="comment">// reason code.</span></span><br><span class="line">        <span class="built_in">cudaGraphExecUpdate</span>(graphExec, graph, &amp;errorNode, &amp;updateResult);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Instantiate during the first iteration or whenever the update</span></span><br><span class="line">    <span class="comment">// fails for any reason</span></span><br><span class="line">    <span class="keyword">if</span> (graphExec == <span class="literal">NULL</span> || updateResult != cudaGraphExecUpdateSuccess) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// If a previous update failed, destroy the cudaGraphExec_t</span></span><br><span class="line">        <span class="comment">// before re-instantiating it</span></span><br><span class="line">        <span class="keyword">if</span> (graphExec != <span class="literal">NULL</span>) &#123;</span><br><span class="line">            <span class="built_in">cudaGraphExecDestroy</span>(graphExec);</span><br><span class="line">        &#125;   </span><br><span class="line">        <span class="comment">// Instantiate graphExec from graph. The error node and</span></span><br><span class="line">        <span class="comment">// error message parameters are unused here.</span></span><br><span class="line">        <span class="built_in">cudaGraphInstantiate</span>(&amp;graphExec, graph, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">0</span>);</span><br><span class="line">    &#125;   </span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaGraphDestroy</span>(graph);</span><br><span class="line">    <span class="built_in">cudaGraphLaunch</span>(graphExec, stream);</span><br><span class="line">    <span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>典型的工作流程是使用流捕获或图 API 创建初始 <code>cudaGraph_t</code>。 然后 <code>cudaGraph_t</code> 被实例化并正常启动。 初始启动后，使用与初始图相同的方法创建新的 <code>cudaGraph_t</code>，并调用 <code>cudaGraphExecUpdate()</code>。 如果图更新成功，由上面示例中的 <code>updateResult</code> 参数指示，则启动更新的 <code>cudaGraphExec_t</code>。 如果由于任何原因更新失败，则调用 <code>cudaGraphExecDestroy()</code> 和 <code>cudaGraphInstantiate()</code> 来销毁原始的 <code>cudaGraphExec_t</code> 并实例化一个新的。</p>
<p>也可以直接更新 <code>cudaGraph_t</code> 节点（即，使用 <code>cudaGraphKernelNodeSetParams()</code>）并随后更新 <code>cudaGraphExec_t</code>，但是使用下一节中介绍的显式节点更新 API 会更有效。</p>
<p>有关使用情况和当前限制的更多信息，请参阅 <a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__GRAPH.html#group__CUDART__GRAPH">Graph API</a>。</p>
<h5 id="3-2-6-6-4-3-单个节点更新"><a href="#3-2-6-6-4-3-单个节点更新" class="headerlink" title="3.2.6.6.4.3 单个节点更新"></a>3.2.6.6.4.3 单个节点更新</h5><p>实例化的图节点参数可以直接更新。 这消除了实例化的开销以及创建新 <code>cudaGraph_t</code> 的开销。 如果需要更新的节点数相对于图中的总节点数较小，则最好单独更新节点。 以下方法可用于更新 <code>cudaGraphExec_t</code> 节点：</p>
<ul>
<li><code>cudaGraphExecKernelNodeSetParams()</code></li>
<li><code>cudaGraphExecMemcpyNodeSetParams()</code></li>
<li><code>cudaGraphExecMemsetNodeSetParams()</code></li>
<li><code>cudaGraphExecHostNodeSetParams()</code></li>
<li><code>cudaGraphExecChildGraphNodeSetParams()</code></li>
<li><code>cudaGraphExecEventRecordNodeSetEvent()</code></li>
<li><code>cudaGraphExecEventWaitNodeSetEvent()</code></li>
<li><code>cudaGraphExecExternalSemaphoresSignalNodeSetParams()</code></li>
<li><code>cudaGraphExecExternalSemaphoresWaitNodeSetParams()</code></li>
</ul>
<p>有关使用情况和当前限制的更多信息，请参阅 <a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__GRAPH.html#group__CUDART__GRAPH">Graph API</a>。</p>
<h5 id="3-2-6-6-5-使用图API"><a href="#3-2-6-6-5-使用图API" class="headerlink" title="3.2.6.6.5 使用图API"></a>3.2.6.6.5 使用图API</h5><p><code>cudaGraph_t</code> 对象不是线程安全的。 用户有责任确保多个线程不会同时访问同一个 <code>cudaGraph_t</code>。</p>
<p><code>cudaGraphExec_t</code> 不能与自身同时运行。 <code>cudaGraphExec_t</code> 的启动将在之前启动相同的可执行图之后进行。</p>
<p>图形执行在流中完成，以便与其他异步工作进行排序。 但是，流仅用于排序； 它不限制图的内部并行性，也不影响图节点的执行位置。</p>
<p>请参阅<a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__GRAPH.html#group__CUDART__GRAPH">图API</a>。</p>
<h4 id="3-2-6-7-事件"><a href="#3-2-6-7-事件" class="headerlink" title="3.2.6.7 事件"></a>3.2.6.7 事件</h4><p>运行时还提供了一种密切监视设备进度以及执行准确计时的方法，方法是让应用程序异步记录程序中任何点的事件，并查询这些事件何时完成。 当事件之前的所有任务（或给定流中的所有命令）都已完成时，事件已完成。 空流中的事件在所有流中的所有先前任务和命令都完成后完成。</p>
<h5 id="3-2-6-7-1-创建和销毁"><a href="#3-2-6-7-1-创建和销毁" class="headerlink" title="3.2.6.7.1 创建和销毁"></a>3.2.6.7.1 创建和销毁</h5><p>以下代码示例创建两个事件：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t start, stop;</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br></pre></td></tr></table></figure>
<p>它们以这种方式被销毁：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaEventDestroy</span>(start);</span><br><span class="line"><span class="built_in">cudaEventDestroy</span>(stop);</span><br></pre></td></tr></table></figure>
<h5 id="3-2-6-7-2-计算时间"><a href="#3-2-6-7-2-计算时间" class="headerlink" title="3.2.6.7.2 计算时间"></a>3.2.6.7.2 计算时间</h5><p>可以用以下方式来计时:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i) &#123;</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(inputDev + i * size, inputHost + i * size,</span><br><span class="line">                    size, cudaMemcpyHostToDevice, stream[i]);</span><br><span class="line">    MyKernel&lt;&lt;&lt;<span class="number">100</span>, <span class="number">512</span>, <span class="number">0</span>, stream[i]&gt;&gt;&gt;</span><br><span class="line">               (outputDev + i * size, inputDev + i * size, size);</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(outputHost + i * size, outputDev + i * size,</span><br><span class="line">                    size, cudaMemcpyDeviceToHost, stream[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line"><span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line"><span class="type">float</span> elapsedTime;</span><br><span class="line"><span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br></pre></td></tr></table></figure>
<h4 id="3-2-6-8同步调用"><a href="#3-2-6-8同步调用" class="headerlink" title="3.2.6.8同步调用"></a>3.2.6.8同步调用</h4><p>调用同步函数时，在设备完成请求的任务之前，控制不会返回给主机线程。 在主机线程执行任何其他 CUDA 调用之前，可以通过调用带有一些特定标志的 <code>cudaSetDeviceFlags()</code> 来指定主机线程是否会产生、阻塞或自旋（有关详细信息，请参阅参考手册）。</p>
<h3 id="3-2-7-多设备系统"><a href="#3-2-7-多设备系统" class="headerlink" title="3.2.7 多设备系统"></a>3.2.7 多设备系统</h3><h4 id="3-2-7-1设备枚举"><a href="#3-2-7-1设备枚举" class="headerlink" title="3.2.7.1设备枚举"></a>3.2.7.1设备枚举</h4><p>一个主机系统可以有多个设备。 以下代码示例显示了如何枚举这些设备、查询它们的属性并确定启用 CUDA 的设备的数量。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> deviceCount;</span><br><span class="line"><span class="built_in">cudaGetDeviceCount</span>(&amp;deviceCount);</span><br><span class="line"><span class="type">int</span> device;</span><br><span class="line"><span class="keyword">for</span> (device = <span class="number">0</span>; device &lt; deviceCount; ++device) &#123;</span><br><span class="line">    cudaDeviceProp deviceProp;</span><br><span class="line">    <span class="built_in">cudaGetDeviceProperties</span>(&amp;deviceProp, device);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Device %d has compute capability %d.%d.\n&quot;</span>,</span><br><span class="line">           device, deviceProp.major, deviceProp.minor);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-2-7-2-设备选择"><a href="#3-2-7-2-设备选择" class="headerlink" title="3.2.7.2 设备选择"></a>3.2.7.2 设备选择</h4><p>主机线程可以通过调用 <code>cudaSetDevice()</code>随时设置它所操作的设备。 设备内存分配和内核启动在当前设置的设备上进行； 流和事件是与当前设置的设备相关联的。 如果未调用 <code>cudaSetDevice()</code>，则当前设备为设备0。</p>
<p>以下代码示例说明了设置当前设备如何影响内存分配和内核执行。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">size_t</span> size = <span class="number">1024</span> * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">0</span>);            <span class="comment">// Set device 0 as current</span></span><br><span class="line"><span class="type">float</span>* p0;</span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;p0, size);       <span class="comment">// Allocate memory on device 0</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p0); <span class="comment">// Launch kernel on device 0</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">1</span>);            <span class="comment">// Set device 1 as current</span></span><br><span class="line"><span class="type">float</span>* p1;</span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;p1, size);       <span class="comment">// Allocate memory on device 1</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p1); <span class="comment">// Launch kernel on device 1</span></span><br></pre></td></tr></table></figure>
<h4 id="3-2-7-3-流和事件行为"><a href="#3-2-7-3-流和事件行为" class="headerlink" title="3.2.7.3 流和事件行为"></a>3.2.7.3 流和事件行为</h4><p>如果在与当前设备无关的流上启动内核将失败，如以下代码示例所示。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">0</span>);               <span class="comment">// Set device 0 as current</span></span><br><span class="line">cudaStream_t s0;</span><br><span class="line"><span class="built_in">cudaStreamCreate</span>(&amp;s0);          <span class="comment">// Create stream s0 on device 0</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">100</span>, <span class="number">64</span>, <span class="number">0</span>, s0&gt;&gt;&gt;(); <span class="comment">// Launch kernel on device 0 in s0</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">1</span>);               <span class="comment">// Set device 1 as current</span></span><br><span class="line">cudaStream_t s1;</span><br><span class="line"><span class="built_in">cudaStreamCreate</span>(&amp;s1);          <span class="comment">// Create stream s1 on device 1</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">100</span>, <span class="number">64</span>, <span class="number">0</span>, s1&gt;&gt;&gt;(); <span class="comment">// Launch kernel on device 1 in s1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// This kernel launch will fail:</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">100</span>, <span class="number">64</span>, <span class="number">0</span>, s0&gt;&gt;&gt;(); <span class="comment">// Launch kernel on device 1 in s0</span></span><br></pre></td></tr></table></figure>
<p>即使将内存复制运行在与当前设备无关的流，它也会成功。</p>
<p>如果输入事件和输入流关联到不同的设备，<code>cudaEventRecord()</code> 将失败。</p>
<p>如果两个输入事件关联到不同的设备， <code>cudaEventElapsedTime()</code> 将失败。</p>
<p>即使输入事件关联到与当前设备不同的设备，<code>cudaEventSynchronize()</code> 和 <code>cudaEventQuery()</code> 也会成功。</p>
<p>即使输入流和输入事件关联到不同的设备，<code>cudaStreamWaitEvent()</code> 也会成功。 因此，<code>cudaStreamWaitEvent()</code>可用于使多个设备相互同步。</p>
<p>每个设备都有自己的默认流（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#default-stream">默认流</a>），因此向设备的默认流发出的命令可能会乱序执行或与向任何其他设备的默认流发出的命令同时执行。</p>
<h4 id="3-2-7-4-Peer-to-Peer的内存访问"><a href="#3-2-7-4-Peer-to-Peer的内存访问" class="headerlink" title="3.2.7.4 Peer-to-Peer的内存访问"></a>3.2.7.4 Peer-to-Peer的内存访问</h4><p>根据系统属性，特别是 PCIe 或 NVLINK 拓扑结构，设备能够相互寻址对方的内存（即，在一个设备上执行的内核可以取消引用指向另一设备内存的指针）。 如果 <code>cudaDeviceCanAccessPeer()</code> 为这两个设备返回 true，则在两个设备之间支持这种对等内存访问功能。</p>
<p>对等内存访问仅在 64 位应用程序中受支持，并且必须通过调用 <code>cudaDeviceEnablePeerAccess()</code> 在两个设备之间启用，如以下代码示例所示。 在未启用 <code>NVSwitch</code> 的系统上，每个设备最多可支持系统范围内的八个对等连接。</p>
<p>两个设备使用统一的地址空间（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#unified-virtual-address-space">统一虚拟地址空间</a>），因此可以使用相同的指针来寻址两个设备的内存，如下面的代码示例所示。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">0</span>);                   <span class="comment">// Set device 0 as current</span></span><br><span class="line"><span class="type">float</span>* p0;</span><br><span class="line"><span class="type">size_t</span> size = <span class="number">1024</span> * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;p0, size);              <span class="comment">// Allocate memory on device 0</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p0);        <span class="comment">// Launch kernel on device 0</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">1</span>);                   <span class="comment">// Set device 1 as current</span></span><br><span class="line"><span class="built_in">cudaDeviceEnablePeerAccess</span>(<span class="number">0</span>, <span class="number">0</span>);   <span class="comment">// Enable peer-to-peer access</span></span><br><span class="line">                                    <span class="comment">// with device 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Launch kernel on device 1</span></span><br><span class="line"><span class="comment">// This kernel launch can access memory on device 0 at address p0</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p0);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="3-2-7-4-1-Linux上的IOMMU"><a href="#3-2-7-4-1-Linux上的IOMMU" class="headerlink" title="3.2.7.4.1 Linux上的IOMMU"></a>3.2.7.4.1 Linux上的IOMMU</h5><p>仅在 Linux 上，CUDA 和显示驱动程序不支持启用 IOMMU 的裸机 PCIe 对等内存复制。 但是，CUDA 和显示驱动程序确实支持通过 VM 传递的 IOMMU。 因此，Linux 上的用户在本机裸机系统上运行时，应禁用 IOMMU。 如启用 IOMMU，将 VFIO 驱动程序用作虚拟机的 PCIe 通道。</p>
<p>在 Windows 上，上述限制不存在。</p>
<p>另请参阅<a href="https://download.nvidia.com/XFree86/Linux-x86_64/396.51/README/dma_issues.html">在 64 位平台上分配 DMA 缓冲区</a>。</p>
<h4 id="3-2-7-5-Peer-to-Peer内存拷贝"><a href="#3-2-7-5-Peer-to-Peer内存拷贝" class="headerlink" title="3.2.7.5 Peer-to-Peer内存拷贝"></a>3.2.7.5 Peer-to-Peer内存拷贝</h4><p>可以在两个不同设备的内存之间执行内存复制。</p>
<p>当两个设备使用统一地址空间时（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#unified-virtual-address-space">统一虚拟地址空间</a>），这是使用设备内存中提到的常规内存复制功能完成的。</p>
<p>否则，这将使用 <code>cudaMemcpyPeer()</code>、<code>cudaMemcpyPeerAsync()</code>、cudaMemcpy3DPeer() 或 <code>cudaMemcpy3DPeerAsync()</code> 完成，如以下代码示例所示。 </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">0</span>);                   <span class="comment">// Set device 0 as current</span></span><br><span class="line"><span class="type">float</span>* p0;</span><br><span class="line"><span class="type">size_t</span> size = <span class="number">1024</span> * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;p0, size);              <span class="comment">// Allocate memory on device 0</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">1</span>);                   <span class="comment">// Set device 1 as current</span></span><br><span class="line"><span class="type">float</span>* p1;</span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;p1, size);              <span class="comment">// Allocate memory on device 1</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">0</span>);                   <span class="comment">// Set device 0 as current</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p0);        <span class="comment">// Launch kernel on device 0</span></span><br><span class="line"><span class="built_in">cudaSetDevice</span>(<span class="number">1</span>);                   <span class="comment">// Set device 1 as current</span></span><br><span class="line"><span class="built_in">cudaMemcpyPeer</span>(p1, <span class="number">1</span>, p0, <span class="number">0</span>, size); <span class="comment">// Copy p0 to p1</span></span><br><span class="line">MyKernel&lt;&lt;&lt;<span class="number">1000</span>, <span class="number">128</span>&gt;&gt;&gt;(p1);   </span><br></pre></td></tr></table></figure>
<p>两个不同设备的内存之间的拷贝（在隐式 NULL 流中）：</p>
<ul>
<li>直到之前向任一设备发出的所有命令都完成后才会启动，并且</li>
<li>在复制到任一设备之后发出的任何命令（请参<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution">阅异步并发执行</a>）可以开始之前运行完成。</li>
</ul>
<p>与流的正常行为一致，两个设备的内存之间的异步拷贝可能与另一个流中的拷贝或内核重叠。</p>
<p>请注意，如果通过 <code>cudaDeviceEnablePeerAccess()</code> 在两个设备之间启用Peer-to-Peer访问，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#peer-to-peer-memory-access">Peer-to-Peer内存访问</a>中所述，这两个设备之间的Peer-to-Peer内存复制不再需要通过主机, 因此速度更快。</p>
<h3 id="统一虚拟地址空间"><a href="#统一虚拟地址空间" class="headerlink" title="统一虚拟地址空间"></a>统一虚拟地址空间</h3><p>当应用程序作为 64 位进程运行时，单个地址空间用于主机和计算能力 2.0 及更高版本的所有设备。通过 CUDA API 调用进行的所有主机内存分配以及受支持设备上的所有设备内存分配都在此虚拟地址范围内。作为结果：</p>
<ul>
<li>通过 CUDA 分配的主机或使用统一地址空间的任何设备上的任何内存的位置都可以使用 <code>cudaPointerGetAttributes()</code> 从指针的值中确定。</li>
<li>当复制到或从任何使用统一地址空间的设备的内存中复制时，可以将 <code>cudaMemcpy*()</code> 的 <code>cudaMemcpyKind</code> 参数设置为 <code>cudaMemcpyDefault</code> 以根据指针确定位置。只要当前设备使用统一寻址，这也适用于未通过 CUDA 分配的主机指针。</li>
<li>通过 <code>cudaHostAlloc()</code> 进行的分配可以在使用统一地址空间的所有设备之间自动移植（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#portable-memory">可移植内存</a>），并且 cudaHostAlloc() 返回的指针可以直接在这些设备上运行的内核中使用（即，没有需要通过 <code>cudaHostGetDevicePointer()</code> 获取设备指针，如映射内存中所述。</li>
</ul>
<p>应用程序可以通过检查 <code>UnifiedAddressing</code> 设备属性（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-enumeration">设备枚举</a>）是否等于 1 来查询统一地址空间是否用于特定设备。 </p>
<h3 id="3-2-9-进程间通信"><a href="#3-2-9-进程间通信" class="headerlink" title="3.2.9 进程间通信"></a>3.2.9 进程间通信</h3><p>由主机线程创建的任何设备内存指针或事件句柄都可以被同一进程中的任何其他线程直接引用。然而，它在这个进程之外是无效的，因此不能被属于不同进程的线程直接引用。</p>
<p>要跨进程共享设备内存指针和事件，应用程序必须使用进程间通信 API，参考手册中有详细描述。 IPC API 仅支持 Linux 上的 64 位进程以及计算能力 2.0 及更高版本的设备。请注意，cudaMallocManaged 分配不支持 IPC API。</p>
<p>使用此 API，应用程序可以使用 <code>cudaIpcGetMemHandle()</code> 获取给定设备内存指针的 IPC 句柄，使用标准 IPC 机制（例如，进程间共享内存或文件）将其传递给另一个进程，并使用 <code>cudaIpcOpenMemHandle()</code> 检索设备来自 IPC 句柄的指针，该指针是其他进程中的有效指针。可以使用类似的入口点共享事件句柄。</p>
<p>请注意，出于性能原因，由 <code>cudaMalloc()</code> 进行的分配可能会从更大的内存块中进行子分配。在这种情况下，CUDA IPC API 将共享整个底层内存块，这可能导致其他子分配被共享，这可能导致进程之间的信息泄露。为了防止这种行为，建议仅共享具有 2MiB 对齐大小的分配。</p>
<p>使用 IPC API 的一个示例是单个主进程生成一批输入数据，使数据可用于多个辅助进程，而无需重新生成或复制。</p>
<p>使用 CUDA IPC 相互通信的应用程序应使用相同的 CUDA 驱动程序和运行时进行编译、链接和运行。</p>
<p>注意：自 CUDA 11.5 起，L4T 和具有计算能力 7.x 及更高版本的嵌入式 Linux Tegra 设备仅支持事件共享 IPC API。 Tegra 平台仍然不支持内存共享 IPC API。</p>
<h3 id="3-2-10-错误检查"><a href="#3-2-10-错误检查" class="headerlink" title="3.2.10 错误检查"></a>3.2.10 错误检查</h3><p>所有运行时函数都返回错误代码，但对于异步函数（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution">异步并发执行</a>），此错误代码不可能报告任何可能发生在设备上的异步错误，因为函数在设备完成任务之前返回；错误代码仅报告执行任务之前主机上发生的错误，通常与参数验证有关；如果发生异步错误，会被后续一些不相关的运行时函数调用报告。</p>
<p>因此，在某些异步函数调用之后检查异步错误的唯一方法是在调用之后通过调用 <code>cudaDeviceSynchronize()</code>（或使用<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution">异步并发执行</a>中描述的任何其他同步机制）并检查 <code>cudaDeviceSynchronize()</code>。</p>
<p>运行时为每个初始化为<code>cudaSuccess</code> 的主机线程维护一个错误变量，并在每次发生错误时被错误代码覆盖（无论是参数验证错误还是异步错误）。 <code>cudaPeekAtLastError()</code> 返回此变量。 <code>cudaGetLastError()</code> 返回此变量并将其重置为 <code>cudaSuccess</code>。</p>
<p>内核启动不返回任何错误代码，因此必须在内核启动后立即调用 <code>cudaPeekAtLastError()</code> 或 <code>cudaGetLastError()</code> 以检索任何启动前错误。为了确保 <code>cudaPeekAtLastError()</code> 或 <code>cudaGetLastError()</code> 返回的任何错误不是源自内核启动之前的调用，必须确保在内核启动之前将运行时错误变量设置为 <code>cudaSuccess</code>，例如，通过调用<code>cudaGetLastError()</code> 在内核启动之前。内核启动是异步的，因此要检查异步错误，应用程序必须在内核启动和调用 <code>cudaPeekAtLastError()</code> 或 <code>cudaGetLastError()</code> 之间进行同步。</p>
<p>请注意，<code>cudaStreamQuery()</code> 和 <code>cudaEventQuery()</code> 可能返回的 <code>cudaErrorNotReady</code> 不被视为错误，因此 <code>cudaPeekAtLastError()</code> 或 <code>cudaGetLastError()</code> 不会报告。</p>
<h3 id="3-2-11-调用栈"><a href="#3-2-11-调用栈" class="headerlink" title="3.2.11 调用栈"></a>3.2.11 调用栈</h3><p>在计算能力 2.x 及更高版本的设备上，调用堆栈的大小可以使用 <code>cudaDeviceGetLimit()</code> 查询并使用 <code>cudaDeviceSetLimit()</code> 设置。</p>
<p>当调用堆栈溢出时，如果应用程序通过 CUDA 调试器（cuda-gdb、Nsight）运行，内核调用将失败并出现堆栈溢出错误，否则会出现未指定的启动错误。</p>
<h3 id="3-2-12-纹理内存和表面内存-surface-memory"><a href="#3-2-12-纹理内存和表面内存-surface-memory" class="headerlink" title="3.2.12 纹理内存和表面内存(surface memory)"></a>3.2.12 纹理内存和表面内存(surface memory)</h3><p>CUDA 支持 GPU 用于图形访问纹理和表面内存的纹理硬件子集。 如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">设备内存访问</a>中所述，从纹理或表面内存而不是全局内存读取数据可以带来多项性能优势。</p>
<p>有两种不同的 API 可以访问纹理和表面内存：</p>
<ul>
<li>所有设备都支持的纹理引用 API，</li>
<li>仅在计算能力 3.x 及更高版本的设备上支持的纹理对象 API。<br>  纹理引用 API 具有纹理对象 API 没有的限制。 它们在 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-reference-api">[[DEPRECATED]] 纹理引用 API </a>中被提及。</li>
</ul>
<h4 id="3-2-12-1纹理内存"><a href="#3-2-12-1纹理内存" class="headerlink" title="3.2.12.1纹理内存"></a>3.2.12.1纹理内存</h4><p>使用纹理函数中描述的设备函数从内核读取纹理内存。 调用这些函数之一读取纹理的过程称为纹理提取。 每个纹理提取指定一个参数，称为纹理对象 API 的纹理对象或纹理引用 API 的纹理引用。</p>
<p>纹理对象或纹理引用指定：</p>
<ul>
<li><p>纹理，即提取的纹理内存。 纹理对象在运行时创建，并在创建纹理对象时指定纹理，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-object-api">纹理对象 API </a>中所述。 纹理引用是在编译时创建的，纹理是在运行时通过 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-reference-api">[[DEPRECATED]] Texture Reference API </a>中描述的运行时函数将纹理引用绑定到纹理来指定的； 几个不同的纹理引用可能绑定到相同的纹理或内存中重叠的纹理。 纹理可以是线性内存的任何区域或 CUDA 数组（在 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-arrays">CUDA 数组</a>中描述）。</p>
</li>
<li><p>它的维数指定纹理是使用一个纹理坐标的一维数组、使用两个纹理坐标的二维数组还是使用三个纹理坐标的三维数组。数组的元素称为<code>texels</code>，是纹理元素的缩写。纹理的宽度、高度和深度是指数组在每个维度上的大小。<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications__technical-specifications-per-compute-capability">表 15</a> 列出了取决于设备计算能力的最大纹理宽度、高度和深度。</p>
</li>
<li><p><code>texels</code>的类型，仅限于基本整数和单精度浮点类型以及从基本向量类型派生的内置向量类型中定义的任何 1、2 和 4 分量向量类型整数和单精度浮点类型。</p>
</li>
<li><p>读取模式，等同于 <code>cudaReadModeNormalizedFloat</code> 或 <code>cudaReadModeElementType</code>。如果是 <code>cudaReadModeNormalizedFloat</code> 并且 texel 的类型是 16 位或 8 位整数类型，则纹理获取返回的值实际上是作为浮点类型返回的，并且整数类型的全范围映射到 [0.0 , 1.0] 表示无符号整数类型，[-1.0, 1.0] 表示有符号整数类型；例如，值为 0xff 的无符号 8 位纹理元素读取为 1。如果是 <code>cudaReadModeElementType</code>，则不执行转换。</p>
</li>
<li><p>纹理坐标是否标准化。默认情况下，使用 [0, N-1] 范围内的浮点坐标（通过 Texture Functions 的函数）引用纹理，其中 N 是与坐标对应的维度中纹理的大小。例如，大小为 64x32 的纹理将分别使用 x 和 y 维度的 [0, 63] 和 [0, 31] 范围内的坐标进行引用。标准化纹理坐标导致坐标被指定在[0.0,1.0-1/N]范围内，而不是[0,N-1]，所以相同的64x32纹理将在x和y维度的[0,1 -1/N]范围内被标准化坐标定位。如果纹理坐标独立于纹理大小，则归一化纹理坐标自然适合某些应用程序的要求。</p>
</li>
<li><p>寻址方式。使用超出范围的坐标调用 B.8 节的设备函数是有效的。寻址模式定义了在这种情况下会发生什么。默认寻址模式是将坐标限制在有效范围内：[0, N) 用于非归一化坐标，[0.0, 1.0) 用于归一化坐标。如果指定了边框模式，则纹理坐标超出范围的纹理提取将返回零。对于归一化坐标，还可以使用环绕模式和镜像模式。使用环绕模式时，每个坐标 x 都转换为 frac(x)=x - floor(x)，其中 floor(x) 是不大于 x 的最大整数。使用镜像模式时，如果 floor(x) 为偶数，则每个坐标 x 转换为 frac(x)，如果 floor(x) 为奇数，则转换为 1-frac(x)。寻址模式被指定为一个大小为 3 的数组，其第一个、第二个和第三个元素分别指定第一个、第二个和第三个纹理坐标的寻址模式；寻址模式为<code>cudaAddressModeBorder</code>、<code>cudaAddressModeClamp</code>、<code>cudaAddressModeWrap</code>和<code>cudaAddressModeMirror</code>； <code>cudaAddressModeWrap</code> 和 <code>cudaAddressModeMirror</code> 仅支持标准化纹理坐标</p>
</li>
<li><p>过滤模式指定如何根据输入纹理坐标计算获取纹理时返回的值。线性纹理过滤只能对配置为返回浮点数据的纹理进行。它在相邻纹素之间执行低精度插值。启用后，将读取纹理提取位置周围的<code>texels</code>，并根据纹理坐标落在<code>texels</code>之间的位置对纹理提取的返回值进行插值。对一维纹理进行简单线性插值，对二维纹理进行双线性插值，对三维纹理进行三线性插值。 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-fetching">Texture Fetching</a> 提供了有关纹理获取的更多细节。过滤模式等于 <code>cudaFilterModePoint</code> 或 <code>cudaFilterModeLinear</code>。如果是<code>cudaFilterModePoint</code>，则返回值是纹理坐标最接近输入纹理坐标的<code>texel</code>。如果是<code>cudaFilterModeLinear</code>，则返回值是纹理坐标最接近的两个（一维纹理）、四个（二维纹理）或八个（三维纹理）<code>texel</code>的线性插值输入纹理坐标。 <code>cudaFilterModeLinear</code> 仅对浮点类型的返回值有效。</p>
</li>
</ul>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-object-api">纹理对象 API </a>。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-reference-api">[[DEPRECATED]] Texture Reference API</a></p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#sixteen-bit-floating-point-textures">16位浮点纹理</a>解释了如何处理16位浮点纹理。</p>
<p>纹理也可以分层，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#layered-textures">分层纹理</a>中所述。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cubemap-textures">立方体贴图纹理</a>和<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cubemap-layered-textures">立方体贴图分层纹理</a>描述了一种特殊类型的纹理，立方体贴图纹理。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-gather">Texture Gather </a>描述了一种特殊的纹理获取，纹理收集。</p>
<h5 id="3-2-12-1-1-纹理对象API"><a href="#3-2-12-1-1-纹理对象API" class="headerlink" title="3.2.12.1.1 纹理对象API"></a>3.2.12.1.1 纹理对象API</h5><p>使用 <code>cudaCreateTextureObject()</code> 从指定纹理的 <code>struct cudaResourceDesc</code> 类型的资源描述和定义如下的纹理描述创建纹理对象：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">cudaTextureDesc</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">cudaTextureAddressMode</span> addressMode[<span class="number">3</span>];</span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">cudaTextureFilterMode</span>  filterMode;</span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">cudaTextureReadMode</span>    readMode;</span><br><span class="line">    <span class="type">int</span>                         sRGB;</span><br><span class="line">    <span class="type">int</span>                         normalizedCoords;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span>                maxAnisotropy;</span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">cudaTextureFilterMode</span>  mipmapFilterMode;</span><br><span class="line">    <span class="type">float</span>                       mipmapLevelBias;</span><br><span class="line">    <span class="type">float</span>                       minMipmapLevelClamp;</span><br><span class="line">    <span class="type">float</span>                       maxMipmapLevelClamp;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>addressMode</code> 指定寻址模式；</li>
<li><code>filterMode</code> 指定过滤模式；</li>
<li><code>readMode</code> 指定读取模式；</li>
<li><code>normalizedCoords</code> 指定纹理坐标是否被归一化；</li>
<li><code>sRGB、maxAnisotropy、mipmapFilterMode、mipmapLevelBias、minMipmapLevelClamp</code> 和 <code>maxMipmapLevelClamp</code> 请参阅的参考手册。</li>
</ul>
<p>以下代码示例将一些简单的转换内核应用于纹理。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Simple transformation kernel</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">transformKernel</span><span class="params">(<span class="type">float</span>* output,</span></span></span><br><span class="line"><span class="params"><span class="function">                                cudaTextureObject_t texObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">int</span> width, <span class="type">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">float</span> theta)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Calculate normalized texture coordinates</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> u = x / (<span class="type">float</span>)width;</span><br><span class="line">    <span class="type">float</span> v = y / (<span class="type">float</span>)height;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Transform coordinates</span></span><br><span class="line">    u -= <span class="number">0.5f</span>;</span><br><span class="line">    v -= <span class="number">0.5f</span>;</span><br><span class="line">    <span class="type">float</span> tu = u * <span class="built_in">cosf</span>(theta) - v * <span class="built_in">sinf</span>(theta) + <span class="number">0.5f</span>;</span><br><span class="line">    <span class="type">float</span> tv = v * <span class="built_in">cosf</span>(theta) + u * <span class="built_in">sinf</span>(theta) + <span class="number">0.5f</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Read from texture and write to global memory</span></span><br><span class="line">    output[y * width + x] = <span class="built_in">tex2D</span>&lt;<span class="type">float</span>&gt;(texObj, tu, tv);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Host code</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> height = <span class="number">1024</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> width = <span class="number">1024</span>;</span><br><span class="line">    <span class="type">float</span> angle = <span class="number">0.5</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate and set some host data</span></span><br><span class="line">    <span class="type">float</span> *h_data = (<span class="type">float</span> *)std::<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(<span class="type">float</span>) * width * height);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; height * width; ++i)</span><br><span class="line">        h_data[i] = i;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate CUDA array in device memory</span></span><br><span class="line">    cudaChannelFormatDesc channelDesc =</span><br><span class="line">        <span class="built_in">cudaCreateChannelDesc</span>(<span class="number">32</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, cudaChannelFormatKindFloat);</span><br><span class="line">    cudaArray_t cuArray;</span><br><span class="line">    <span class="built_in">cudaMallocArray</span>(&amp;cuArray, &amp;channelDesc, width, height);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set pitch of the source (the width in memory in bytes of the 2D array pointed</span></span><br><span class="line">    <span class="comment">// to by src, including padding), we dont have any padding</span></span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> spitch = width * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="comment">// Copy data located at address h_data in host memory to device memory</span></span><br><span class="line">    <span class="built_in">cudaMemcpy2DToArray</span>(cuArray, <span class="number">0</span>, <span class="number">0</span>, h_data, spitch, width * <span class="built_in">sizeof</span>(<span class="type">float</span>),</span><br><span class="line">                        height, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Specify texture</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">cudaResourceDesc</span> resDesc;</span><br><span class="line">    <span class="built_in">memset</span>(&amp;resDesc, <span class="number">0</span>, <span class="built_in">sizeof</span>(resDesc));</span><br><span class="line">    resDesc.resType = cudaResourceTypeArray;</span><br><span class="line">    resDesc.res.array.array = cuArray;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Specify texture object parameters</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">cudaTextureDesc</span> texDesc;</span><br><span class="line">    <span class="built_in">memset</span>(&amp;texDesc, <span class="number">0</span>, <span class="built_in">sizeof</span>(texDesc));</span><br><span class="line">    texDesc.addressMode[<span class="number">0</span>] = cudaAddressModeWrap;</span><br><span class="line">    texDesc.addressMode[<span class="number">1</span>] = cudaAddressModeWrap;</span><br><span class="line">    texDesc.filterMode = cudaFilterModeLinear;</span><br><span class="line">    texDesc.readMode = cudaReadModeElementType;</span><br><span class="line">    texDesc.normalizedCoords = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create texture object</span></span><br><span class="line">    cudaTextureObject_t texObj = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaCreateTextureObject</span>(&amp;texObj, &amp;resDesc, &amp;texDesc, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate result of transformation in device memory</span></span><br><span class="line">    <span class="type">float</span> *output;</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;output, width * height * <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Invoke kernel</span></span><br><span class="line">    <span class="function">dim3 <span class="title">threadsperBlock</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">numBlocks</span><span class="params">((width + threadsperBlock.x - <span class="number">1</span>) / threadsperBlock.x,</span></span></span><br><span class="line"><span class="params"><span class="function">                    (height + threadsperBlock.y - <span class="number">1</span>) / threadsperBlock.y)</span></span>;</span><br><span class="line">    transformKernel&lt;&lt;&lt;numBlocks, threadsperBlock&gt;&gt;&gt;(output, texObj, width, height,</span><br><span class="line">                                                    angle);</span><br><span class="line">    <span class="comment">// Copy data from device back to host</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(h_data, output, width * height * <span class="built_in">sizeof</span>(<span class="type">float</span>),</span><br><span class="line">                cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Destroy texture object</span></span><br><span class="line">    <span class="built_in">cudaDestroyTextureObject</span>(texObj);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Free device memory</span></span><br><span class="line">    <span class="built_in">cudaFreeArray</span>(cuArray);</span><br><span class="line">    <span class="built_in">cudaFree</span>(output);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Free host memory</span></span><br><span class="line">    <span class="built_in">free</span>(h_data);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="3-2-12-1-2-已弃用-纹理引用-API"><a href="#3-2-12-1-2-已弃用-纹理引用-API" class="headerlink" title="3.2.12.1.2 [[已弃用]] 纹理引用 API"></a>3.2.12.1.2 [[已弃用]] 纹理引用 API</h5><p>纹理参考 API 已弃用。</p>
<p>纹理引用的某些属性是不可变的，必须在编译时知道； 它们是在声明纹理引用时指定的。 纹理引用在文件范围内声明为纹理类型的变量：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">texture&lt;DataType, Type, ReadMode&gt; texRef;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>DataType</code> 指定纹素的类型；</li>
<li><code>Type</code> 指定纹理参考的类型，等于 <code>cudaTextureType1D</code>、<code>cudaTextureType2D</code> 或 <code>cudaTextureType3D</code>，分别用于一维、二维或三维纹理，或 <code>cudaTextureType1DLayered</code> 或 <code>cudaTextureType2DLayered</code> 用于一维或二维 分别分层纹理； <code>Type</code> 是一个可选参数，默认为 <code>cudaTextureType1D</code>；</li>
<li><code>ReadMode</code> 指定读取模式； 它是一个可选参数，默认为 <code>cudaReadModeElementType</code>。</li>
</ul>
<p>纹理引用只能声明为静态全局变量，不能作为参数传递给函数。</p>
<p>纹理引用的其他属性是可变的，并且可以在运行时通过主机运行时进行更改。 如参考手册中所述，运行时 API 具有低级 C 样式接口和高级 C++ 样式接口。 纹理类型在高级 API 中定义为公开派生自低级 API 中定义的 <code>textureReference</code>类型的结构，如下所示：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">textureReference</span> &#123;</span><br><span class="line">    <span class="type">int</span>                          normalized;</span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">cudaTextureFilterMode</span>   filterMode;</span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">cudaTextureAddressMode</span>  addressMode[<span class="number">3</span>];</span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">cudaChannelFormatDesc</span> channelDesc;</span><br><span class="line">    <span class="type">int</span>                          sRGB;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span>                 maxAnisotropy;</span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">cudaTextureFilterMode</span>   mipmapFilterMode;</span><br><span class="line">    <span class="type">float</span>                        mipmapLevelBias;</span><br><span class="line">    <span class="type">float</span>                        minMipmapLevelClamp;</span><br><span class="line">    <span class="type">float</span>                        maxMipmapLevelClamp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>normalized</code> 指定纹理坐标是否被归一化；</li>
<li><code>filterMode</code> 指定过滤模式；</li>
<li><code>addressMode</code> 指定寻址模式；</li>
<li><code>channelDesc</code> 描述了<code>texel</code>的格式； 它必须匹配纹理引用声明的 <code>DataType</code> 参数； <code>channelDesc</code> 属于以下类型：</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">cudaChannelFormatDesc</span> &#123;</span><br><span class="line">  <span class="type">int</span> x, y, z, w;</span><br><span class="line">  <span class="keyword">enum</span> <span class="title class_">cudaChannelFormatKind</span> f;</span><br><span class="line">&#125;;</span><br><span class="line">其中 x、y、z 和 w 等于返回值的每个分量的位数，f 为：</span><br><span class="line"></span><br><span class="line">*cudaChannelFormatKindSigned 如果这些组件是有符号整数类型，</span><br><span class="line">*cudaChannelFormatKindUnsigned 如果它们是无符号整数类型，</span><br><span class="line">*cudaChannelFormatKindFloat 如果它们是浮点类型。</span><br></pre></td></tr></table></figure>
<ul>
<li><code>sRGB、maxAnisotropy、mipmapFilterMode、mipmapLevelBias、minMipmapLevelClamp 和 maxMipmapLevelClamp</code> 请参阅参考手册</li>
</ul>
<p><code>normalized</code>、<code>addressMode</code> 和 <code>filterMode</code> 可以直接在主机代码中修改。</p>
<p>在纹理内存中读取之前内核可以使用纹理引用，纹理引用必须绑定到纹理，使用 <code>cudaBindTexture()</code> 或 <code>cudaBindTexture2D()</code> 用于线性内存，或 <code>cudaBindTextureToArray()</code> 用于 CUDA 数组。 <code>cudaUnbindTexture()</code> 用于取消绑定纹理引用。 一旦纹理引用被解除绑定，它可以安全地重新绑定到另一个数组，即使使用之前绑定的纹理的内核还没有完成。 建议使用 <code>cudaMallocPitch()</code> 在线性内存中分配二维纹理，并使用 <code>cudaMallocPitch()</code> 返回的间距作为 <code>cudaBindTexture2D()</code> 的输入参数。</p>
<p>以下代码示例将 2D 纹理引用绑定到 devPtr 指向的线性内存：</p>
<ul>
<li>使用低层次API:</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">texture&lt;<span class="type">float</span>, cudaTextureType2D,</span><br><span class="line">        cudaReadModeElementType&gt; texRef;</span><br><span class="line">textureReference* texRefPtr;</span><br><span class="line"><span class="built_in">cudaGetTextureReference</span>(&amp;texRefPtr, &amp;texRef);</span><br><span class="line">cudaChannelFormatDesc channelDesc =</span><br><span class="line">                             <span class="built_in">cudaCreateChannelDesc</span>&lt;<span class="type">float</span>&gt;();</span><br><span class="line"><span class="type">size_t</span> offset;</span><br><span class="line"><span class="built_in">cudaBindTexture2D</span>(&amp;offset, texRefPtr, devPtr, &amp;channelDesc,</span><br><span class="line">                  width, height, pitch);</span><br></pre></td></tr></table></figure>
<ul>
<li>使用高层次API:</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">texture&lt;<span class="type">float</span>, cudaTextureType2D,</span><br><span class="line">        cudaReadModeElementType&gt; texRef;</span><br><span class="line">cudaChannelFormatDesc channelDesc =</span><br><span class="line">                             <span class="built_in">cudaCreateChannelDesc</span>&lt;<span class="type">float</span>&gt;();</span><br><span class="line"><span class="type">size_t</span> offset;</span><br><span class="line"><span class="built_in">cudaBindTexture2D</span>(&amp;offset, texRef, devPtr, channelDesc,</span><br><span class="line">                  width, height, pitch);</span><br></pre></td></tr></table></figure>
<p>以下代码示例将 2D 纹理引用绑定到 CUDA 数组 <code>cuArray</code>：</p>
<ul>
<li>使用低层次API:</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">texture&lt;<span class="type">float</span>, cudaTextureType2D,</span><br><span class="line">        cudaReadModeElementType&gt; texRef;</span><br><span class="line">textureReference* texRefPtr;</span><br><span class="line"><span class="built_in">cudaGetTextureReference</span>(&amp;texRefPtr, &amp;texRef);</span><br><span class="line">cudaChannelFormatDesc channelDesc;</span><br><span class="line"><span class="built_in">cudaGetChannelDesc</span>(&amp;channelDesc, cuArray);</span><br><span class="line"><span class="built_in">cudaBindTextureToArray</span>(texRef, cuArray, &amp;channelDesc);</span><br></pre></td></tr></table></figure>
<ul>
<li>使用高层次API:</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">texture&lt;<span class="type">float</span>, cudaTextureType2D,</span><br><span class="line">        cudaReadModeElementType&gt; texRef;</span><br><span class="line"><span class="built_in">cudaBindTextureToArray</span>(texRef, cuArray);</span><br></pre></td></tr></table></figure>
<p>将纹理绑定到纹理引用时指定的格式必须与声明纹理引用时指定的参数匹配； 否则，纹理提取的结果是未定义的。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications__technical-specifications-per-compute-capability">如表 15</a> 中指定的，可以绑定到内核的纹理数量是有限的。</p>
<p>以下代码示例将一些简单的转换内核应用于纹理。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 2D float texture</span></span><br><span class="line">texture&lt;<span class="type">float</span>, cudaTextureType2D, cudaReadModeElementType&gt; texRef;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Simple transformation kernel</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">transformKernel</span><span class="params">(<span class="type">float</span>* output,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">int</span> width, <span class="type">int</span> height,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">float</span> theta)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Calculate normalized texture coordinates</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> u = x / (<span class="type">float</span>)width;</span><br><span class="line">    <span class="type">float</span> v = y / (<span class="type">float</span>)height;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Transform coordinates</span></span><br><span class="line">    u -= <span class="number">0.5f</span>;</span><br><span class="line">    v -= <span class="number">0.5f</span>;</span><br><span class="line">    <span class="type">float</span> tu = u * <span class="built_in">cosf</span>(theta) - v * <span class="built_in">sinf</span>(theta) + <span class="number">0.5f</span>;</span><br><span class="line">    <span class="type">float</span> tv = v * <span class="built_in">cosf</span>(theta) + u * <span class="built_in">sinf</span>(theta) + <span class="number">0.5f</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Read from texture and write to global memory</span></span><br><span class="line">    output[y * width + x] = <span class="built_in">tex2D</span>(texRef, tu, tv);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Host code</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Allocate CUDA array in device memory</span></span><br><span class="line">    cudaChannelFormatDesc channelDesc =</span><br><span class="line">               <span class="built_in">cudaCreateChannelDesc</span>(<span class="number">32</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,</span><br><span class="line">                                     cudaChannelFormatKindFloat);</span><br><span class="line">    cudaArray* cuArray;</span><br><span class="line">    <span class="built_in">cudaMallocArray</span>(&amp;cuArray, &amp;channelDesc, width, height);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Copy to device memory some data located at address h_data</span></span><br><span class="line">    <span class="comment">// in host memory </span></span><br><span class="line">    <span class="built_in">cudaMemcpyToArray</span>(cuArray, <span class="number">0</span>, <span class="number">0</span>, h_data, size,</span><br><span class="line">                      cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set texture reference parameters</span></span><br><span class="line">    texRef.addressMode[<span class="number">0</span>] = cudaAddressModeWrap;</span><br><span class="line">    texRef.addressMode[<span class="number">1</span>] = cudaAddressModeWrap;</span><br><span class="line">    texRef.filterMode     = cudaFilterModeLinear;</span><br><span class="line">    texRef.normalized     = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Bind the array to the texture reference</span></span><br><span class="line">    <span class="built_in">cudaBindTextureToArray</span>(texRef, cuArray, channelDesc);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate result of transformation in device memory</span></span><br><span class="line">    <span class="type">float</span>* output;</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;output, width * height * <span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Invoke kernel</span></span><br><span class="line">    <span class="function">dim3 <span class="title">dimBlock</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">dimGrid</span><span class="params">((width  + dimBlock.x - <span class="number">1</span>) / dimBlock.x,</span></span></span><br><span class="line"><span class="params"><span class="function">                 (height + dimBlock.y - <span class="number">1</span>) / dimBlock.y)</span></span>;</span><br><span class="line">    transformKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(output, width, height,</span><br><span class="line">                                           angle);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Free device memory</span></span><br><span class="line">    <span class="built_in">cudaFreeArray</span>(cuArray);</span><br><span class="line">    <span class="built_in">cudaFree</span>(output);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="3-2-12-1-3-16位浮点类型纹理"><a href="#3-2-12-1-3-16位浮点类型纹理" class="headerlink" title="3.2.12.1.3 16位浮点类型纹理"></a>3.2.12.1.3 16位浮点类型纹理</h5><p>CUDA 数组支持的 16 位浮点或 <em>half</em> 格式与 IEEE 754-2008 binary2 格式相同。</p>
<p>CUDA C++ 不支持匹配的数据类型，但提供了通过 <code>unsigned short</code> 类型与 32 位浮点格式相互转换的内在函数：<code>__float2half_rn(float)</code> 和 <code>__half2float(unsigned short)</code>。 这些功能仅在设备代码中受支持。 例如，主机代码的等效函数可以在 OpenEXR 库中找到。</p>
<p>在执行任何过滤之前，在纹理提取期间，16 位浮点组件被提升为 32 位浮点。</p>
<p>可以通过调用 <code>cudaCreateChannelDescHalf*()</code> 函数来创建 16 位浮点格式的通道描述。</p>
<h5 id="3-2-12-1-4-分层纹理"><a href="#3-2-12-1-4-分层纹理" class="headerlink" title="3.2.12.1.4 分层纹理"></a>3.2.12.1.4 分层纹理</h5><p>一维或二维分层纹理（在 Direct3D 中也称为纹理数组，在 OpenGL 中也称为数组纹理）是由一系列层组成的纹理，这些层都是具有相同维度、大小和数据类型的常规纹理.</p>
<p>使用整数索引和浮点纹理坐标来寻址一维分层纹理；索引表示序列中的层，坐标表示该层中的<code>texel</code>。使用整数索引和两个浮点纹理坐标来寻址二维分层纹理；索引表示序列中的层，坐标表示该层中的<code>texel</code> 。</p>
<p>分层纹理只能是一个 CUDA 数组，方法是使用 <code>cudaArrayLayered</code> 标志调用的<code>cudaMalloc3DArray()</code>（一维分层纹理的高度为零）。</p>
<p>使用 <code>tex1DLayered()、tex1DLayered()、tex2DLayered() 和 tex2DLayered()</code> 中描述的设备函数获取分层纹理。纹理过滤（请参阅纹理提取）仅在层内完成，而不是跨层。</p>
<p>分层纹理仅在计算能力 2.0 及更高版本的设备上受支持。</p>
<h5 id="3-2-12-1-5-立方体纹理-Cubemap-Textures"><a href="#3-2-12-1-5-立方体纹理-Cubemap-Textures" class="headerlink" title="3.2.12.1.5 立方体纹理(Cubemap Textures)"></a>3.2.12.1.5 立方体纹理(Cubemap Textures)</h5><p>Cubemap Textures是一种特殊类型的二维分层纹理，它有六层代表立方体的面：</p>
<ul>
<li>层的宽度等于它的高度。</li>
<li>立方体贴图使用三个纹理坐标 x、y 和 z 进行寻址，这些坐标被解释为从立方体中心发出并指向立方体的一个面和对应于该面的层内的texel的方向矢量。 更具体地说，面部是由具有最大量级 m 的坐标选择的，相应的层使用坐标 <code>(s/m+1)/2</code> 和 <code>(t/m+1)/2</code> 来寻址，其中 s 和 t 在表中定义 .</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>face</th>
<th>m</th>
<th>s</th>
<th>t</th>
</tr>
</thead>
<tbody>
<tr>
<td>\</td>
<td>x\</td>
<td>&gt; \</td>
<td>y\</td>
<td>and \</td>
<td>x\</td>
<td>&gt; \</td>
<td>z\</td>
<td></td>
<td>x &gt; 0</td>
<td>0</td>
<td>x</td>
<td>-z</td>
<td>-y</td>
</tr>
<tr>
<td>\</td>
<td>x\</td>
<td>&gt; \</td>
<td>y\</td>
<td>and \</td>
<td>x\</td>
<td>&gt; \</td>
<td>z\</td>
<td></td>
<td>x &lt; 0</td>
<td>1</td>
<td>-x</td>
<td>z</td>
<td>-y</td>
</tr>
<tr>
<td>\</td>
<td>y\</td>
<td>&gt; \</td>
<td>x\</td>
<td>and \</td>
<td>y\</td>
<td>&gt; \</td>
<td>z\</td>
<td></td>
<td>y &gt; 0</td>
<td>2</td>
<td>y</td>
<td>x</td>
<td>z</td>
</tr>
<tr>
<td>\</td>
<td>y\</td>
<td>&gt; \</td>
<td>x\</td>
<td>and \</td>
<td>y\</td>
<td>&gt; \</td>
<td>z\</td>
<td></td>
<td>y &lt; 0</td>
<td>3</td>
<td>-y</td>
<td>x</td>
<td>-z</td>
</tr>
<tr>
<td>\</td>
<td>z\</td>
<td>&gt; \</td>
<td>x\</td>
<td>and \</td>
<td>z\</td>
<td>&gt; \</td>
<td>y\</td>
<td></td>
<td>z &gt; 0</td>
<td>4</td>
<td>z</td>
<td>x</td>
<td>-y</td>
</tr>
<tr>
<td>\</td>
<td>z\</td>
<td>&gt; \</td>
<td>x\</td>
<td>and \</td>
<td>z\</td>
<td>&gt; \</td>
<td>y\</td>
<td></td>
<td>z &lt; 0</td>
<td>5</td>
<td>-z</td>
<td>-x</td>
<td>-y</td>
</tr>
</tbody>
</table>
</div>
<p>通过使用 <code>cudaArrayCubemap</code> 标志调用 <code>cudaMalloc3DArray()</code>，立方体贴图纹理只能是 CUDA 数组。</p>
<p>立方体贴图纹理是使用 <code>texCubemap()</code>和 <code>texCubemap()</code> 中描述的设备函数获取的。</p>
<p>Cubemap 纹理仅在计算能力 2.0 及更高版本的设备上受支持。 </p>
<h5 id="3-2-12-1-6-分层的立方体纹理内存-Cubemap-Layered-Textures"><a href="#3-2-12-1-6-分层的立方体纹理内存-Cubemap-Layered-Textures" class="headerlink" title="3.2.12.1.6 分层的立方体纹理内存(Cubemap Layered Textures)"></a>3.2.12.1.6 分层的立方体纹理内存(Cubemap Layered Textures)</h5><p>立方体贴图分层纹理是一种分层纹理，其层是相同维度的立方体贴图。</p>
<p>使用整数索引和三个浮点纹理坐标来处理立方体贴图分层纹理； 索引表示序列中的立方体贴图，坐标表示该立方体贴图中的纹理元素。</p>
<p>通过使用 <code>cudaArrayLayered</code> 和 <code>cudaArrayCubemap</code> 标志调用的 <code>cudaMalloc3DArray()</code>，立方体贴图分层纹理只能是 CUDA 数组。</p>
<p>立方体贴图分层纹理是使用 <code>texCubemapLayered()</code> 和 <code>texCubemapLayered()</code> 中描述的设备函数获取的。 纹理过滤（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-fetching">纹理提取</a>）仅在层内完成，而不是跨层。</p>
<p>Cubemap 分层纹理仅在计算能力 2.0 及更高版本的设备上受支持。</p>
<h5 id="3-2-12-1-7-纹理收集-Texture-Gather"><a href="#3-2-12-1-7-纹理收集-Texture-Gather" class="headerlink" title="3.2.12.1.7 纹理收集(Texture Gather)"></a>3.2.12.1.7 纹理收集(Texture Gather)</h5><p>纹理聚集是一种特殊的纹理提取，仅适用于二维纹理。它由 <code>tex2Dgather()</code> 函数执行，该函数具有与 <code>tex2D()</code> 相同的参数，外加一个等于 0、1、2 或 3 的附加 comp 参数（参见 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#tex2dgather">tex2Dgather()</a> 和 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#tex2dgather-object">tex2Dgather()</a>）。它返回四个 32 位数字，对应于在常规纹理提取期间用于双线性过滤的四个texel中每一个的分量 comp 的值。例如，如果这些纹理像素的值是 (253, 20, 31, 255), (250, 25, 29, 254), (249, 16, 37, 253), (251, 22, 30, 250)，并且comp 为 2，<code>tex2Dgather()</code> 返回 (31, 29, 37, 30)。</p>
<p>请注意，纹理坐标仅使用 8 位小数精度计算。因此，对于 <code>tex2D()</code> 将使用 1.0 作为其权重之一（α 或 β，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#linear-filtering">线性过滤</a>）的情况，<code>tex2Dgather()</code> 可能会返回意外结果。例如，x 纹理坐标为 2.49805：xB=x-0.5=1.99805，但是 xB 的小数部分以 8 位定点格式存储。由于 0.99805 比 255.f/256.f 更接近 256.f/256.f，因此 xB 的值为 2。因此，在这种情况下，<code>tex2Dgather()</code> 将返回 x 中的索引 2 和 3，而不是索引1 和 2。</p>
<p>纹理收集仅支持使用 <code>cudaArrayTextureGather</code> 标志创建的 CUDA 数组，其宽度和高度小于<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications__technical-specifications-per-compute-capability">表 15</a> 中为纹理收集指定的最大值，该最大值小于常规纹理提取。</p>
<p>纹理收集仅在计算能力 2.0 及更高版本的设备上受支持。</p>
<h4 id="3-2-12-2-表面内存-Surface-Memory"><a href="#3-2-12-2-表面内存-Surface-Memory" class="headerlink" title="3.2.12.2 表面内存(Surface Memory)"></a>3.2.12.2 表面内存(Surface Memory)</h4><p>对于计算能力 2.0 及更高版本的设备，可以使用 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#surface-functions">Surface Functions</a> 中描述的函数通过表面对象或表面引用来读取和写入使用 <code>cudaArraySurfaceLoadStore</code> 标志创建的 CUDA 数组（在 Cubemap Surfaces 中描述）。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications__technical-specifications-per-compute-capability">表 15</a> 列出了最大表面宽度、高度和深度，具体取决于设备的计算能力。</p>
<h5 id="3-2-12-2-1-表面内存对象API"><a href="#3-2-12-2-1-表面内存对象API" class="headerlink" title="3.2.12.2.1 表面内存对象API"></a>3.2.12.2.1 表面内存对象API</h5><p>使用 <code>cudaCreateSurfaceObject()</code> 从 <code>struct cudaResourceDesc</code> 类型的资源描述中创建表面内存对象。</p>
<p>以下代码示例将一些简单的转换内核应用于纹理。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Simple copy kernel</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">copyKernel</span><span class="params">(cudaSurfaceObject_t inputSurfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                           cudaSurfaceObject_t outputSurfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                           <span class="type">int</span> width, <span class="type">int</span> height)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Calculate surface coordinates</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    <span class="keyword">if</span> (x &lt; width &amp;&amp; y &lt; height) &#123;</span><br><span class="line">        uchar4 data;</span><br><span class="line">        <span class="comment">// Read from input surface</span></span><br><span class="line">        <span class="built_in">surf2Dread</span>(&amp;data,  inputSurfObj, x * <span class="number">4</span>, y);</span><br><span class="line">        <span class="comment">// Write to output surface</span></span><br><span class="line">        <span class="built_in">surf2Dwrite</span>(data, outputSurfObj, x * <span class="number">4</span>, y);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Host code</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> height = <span class="number">1024</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> width = <span class="number">1024</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate and set some host data</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> *h_data =</span><br><span class="line">        (<span class="type">unsigned</span> <span class="type">char</span> *)std::<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(<span class="type">unsigned</span> <span class="type">char</span>) * width * height * <span class="number">4</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; height * width * <span class="number">4</span>; ++i)</span><br><span class="line">        h_data[i] = i;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate CUDA arrays in device memory</span></span><br><span class="line">    cudaChannelFormatDesc channelDesc =</span><br><span class="line">        <span class="built_in">cudaCreateChannelDesc</span>(<span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>, cudaChannelFormatKindUnsigned);</span><br><span class="line">    cudaArray_t cuInputArray;</span><br><span class="line">    <span class="built_in">cudaMallocArray</span>(&amp;cuInputArray, &amp;channelDesc, width, height,</span><br><span class="line">                    cudaArraySurfaceLoadStore);</span><br><span class="line">    cudaArray_t cuOutputArray;</span><br><span class="line">    <span class="built_in">cudaMallocArray</span>(&amp;cuOutputArray, &amp;channelDesc, width, height,</span><br><span class="line">                    cudaArraySurfaceLoadStore);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set pitch of the source (the width in memory in bytes of the 2D array</span></span><br><span class="line">    <span class="comment">// pointed to by src, including padding), we dont have any padding</span></span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> spitch = <span class="number">4</span> * width * <span class="built_in">sizeof</span>(<span class="type">unsigned</span> <span class="type">char</span>);</span><br><span class="line">    <span class="comment">// Copy data located at address h_data in host memory to device memory</span></span><br><span class="line">    <span class="built_in">cudaMemcpy2DToArray</span>(cuInputArray, <span class="number">0</span>, <span class="number">0</span>, h_data, spitch,</span><br><span class="line">                        <span class="number">4</span> * width * <span class="built_in">sizeof</span>(<span class="type">unsigned</span> <span class="type">char</span>), height,</span><br><span class="line">                        cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Specify surface</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">cudaResourceDesc</span> resDesc;</span><br><span class="line">    <span class="built_in">memset</span>(&amp;resDesc, <span class="number">0</span>, <span class="built_in">sizeof</span>(resDesc));</span><br><span class="line">    resDesc.resType = cudaResourceTypeArray;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create the surface objects</span></span><br><span class="line">    resDesc.res.array.array = cuInputArray;</span><br><span class="line">    cudaSurfaceObject_t inputSurfObj = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaCreateSurfaceObject</span>(&amp;inputSurfObj, &amp;resDesc);</span><br><span class="line">    resDesc.res.array.array = cuOutputArray;</span><br><span class="line">    cudaSurfaceObject_t outputSurfObj = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaCreateSurfaceObject</span>(&amp;outputSurfObj, &amp;resDesc);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Invoke kernel</span></span><br><span class="line">    <span class="function">dim3 <span class="title">threadsperBlock</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">numBlocks</span><span class="params">((width + threadsperBlock.x - <span class="number">1</span>) / threadsperBlock.x,</span></span></span><br><span class="line"><span class="params"><span class="function">                    (height + threadsperBlock.y - <span class="number">1</span>) / threadsperBlock.y)</span></span>;</span><br><span class="line">    copyKernel&lt;&lt;&lt;numBlocks, threadsperBlock&gt;&gt;&gt;(inputSurfObj, outputSurfObj, width,</span><br><span class="line">                                                height);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Copy data from device back to host</span></span><br><span class="line">    <span class="built_in">cudaMemcpy2DFromArray</span>(h_data, spitch, cuOutputArray, <span class="number">0</span>, <span class="number">0</span>,</span><br><span class="line">                            <span class="number">4</span> * width * <span class="built_in">sizeof</span>(<span class="type">unsigned</span> <span class="type">char</span>), height,</span><br><span class="line">                            cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Destroy surface objects</span></span><br><span class="line">    <span class="built_in">cudaDestroySurfaceObject</span>(inputSurfObj);</span><br><span class="line">    <span class="built_in">cudaDestroySurfaceObject</span>(outputSurfObj);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Free device memory</span></span><br><span class="line">    <span class="built_in">cudaFreeArray</span>(cuInputArray);</span><br><span class="line">    <span class="built_in">cudaFreeArray</span>(cuOutputArray);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Free host memory</span></span><br><span class="line">    <span class="built_in">free</span>(h_data);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="3-2-12-2-3-立方体表面内存"><a href="#3-2-12-2-3-立方体表面内存" class="headerlink" title="3.2.12.2.3 立方体表面内存"></a>3.2.12.2.3 立方体表面内存</h5><p>使用 <code>surfCubemapread()</code> 和 <code>surfCubemapwrite()</code>（<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#surfcubemapread">surfCubemapread</a> 和 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#surfcubemapwrite">surfCubemapwrite</a>）作为二维分层表面来访问立方体贴图表面内存，即，使用表示面的整数索引和寻址对应于该面的层内的纹素的两个浮点纹理坐标 . 面的顺序如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cubemap-textures__cubemap-fetch">表 2</a>所示。</p>
<h5 id="3-2-12-2-4-立方体分层表面内存"><a href="#3-2-12-2-4-立方体分层表面内存" class="headerlink" title="3.2.12.2.4 立方体分层表面内存"></a>3.2.12.2.4 立方体分层表面内存</h5><p>使用 <code>surfCubemapLayeredread()</code> 和 <code>surfCubemapLayeredwrite()</code>（<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#surfcubemaplayeredread">surfCubemapLayeredread()</a> 和 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#surfcubemaplayeredwrite">surfCubemapLayeredwrite()</a>）作为二维分层表面来访问立方体贴图分层表面，即，使用表示立方体贴图之一的面和两个浮点纹理的整数索引 坐标寻址对应于该面的层内的纹理元素。 面的顺序如表 2 所示，因此例如 index ((2 * 6) + 3) 会访问第三个立方体贴图的第四个面。</p>
<h4 id="3-2-12-3-CUDA-Array"><a href="#3-2-12-3-CUDA-Array" class="headerlink" title="3.2.12.3 CUDA Array"></a>3.2.12.3 CUDA Array</h4><p>CUDA Array是针对纹理获取优化的不透明内存布局。 它们是一维、二维或三维，由元素组成，每个元素有 1、2 或 4 个分量，可以是有符号或无符号 8 位、16 位或 32 位整数、16 位浮点数、 或 32 位浮点数。 CUDA Array只能由内核通过纹理内存中描述的纹理获取或表面内存中描述的表面读取和写入来访问。</p>
<h4 id="3-2-12-4-读写一致性"><a href="#3-2-12-4-读写一致性" class="headerlink" title="3.2.12.4 读写一致性"></a>3.2.12.4 读写一致性</h4><p>纹理和表面内存被缓存（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">设备内存访问</a>），并且在同一个内核调用中，缓存在全局内存写入和表面内存写入方面并不保持一致，因此任何纹理获取或表面内存读取到一个地址 ,在同一个内核调用中通过全局写入或表面写入写入会返回未定义的数据。 换句话说，线程可以安全地读取某个纹理或表面内存位置，前提是该内存位置已被先前的内核调用或内存拷贝更新，但如果它先前已由同一个线程或来自同一线程的另一个线程更新，则不能内核调用。</p>
<h3 id="3-2-13图形一致性"><a href="#3-2-13图形一致性" class="headerlink" title="3.2.13图形一致性"></a>3.2.13图形一致性</h3><p>来自 OpenGL 和 Direct3D 的一些资源可能会映射到 CUDA 的地址空间中，以使 CUDA 能够读取 OpenGL 或 Direct3D 写入的数据，或者使 CUDA 能够写入数据以供 OpenGL 或 Direct3D 使用。</p>
<p>资源必须先注册到 CUDA，然后才能使用 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#opengl-interoperability">OpenGL 互操作</a>和 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#direct3d-interoperability">Direct3D 互操作</a>中提到的函数进行映射。这些函数返回一个指向 <code>struct cudaGraphicsResource</code> 类型的 CUDA 图形资源的指针。注册资源可能会产生高开销，因此通常每个资源只调用一次。使用 <code>cudaGraphicsUnregisterResource()</code> 取消注册 CUDA 图形资源。每个打算使用该资源的 CUDA 上下文都需要单独注册它。</p>
<p>将资源注册到 CUDA 后，可以根据需要使用 <code>cudaGraphicsMapResources()</code> 和 <code>cudaGraphicsUnmapResources()</code>多次映射和取消映射。可以调用 <code>cudaGraphicsResourceSetMapFlags()</code> 来指定 CUDA 驱动程序可以用来优化资源管理的使用提示（只写、只读）。</p>
<p>内核可以使用 <code>cudaGraphicsResourceGetMappedPointer()</code> 返回的设备内存地址来读取或写入映射的资源，对于缓冲区，使用 <code>cudaGraphicsSubResourceGetMappedArray()</code> 的 CUDA 数组。</p>
<p>在映射时通过 OpenGL、Direct3D 或其他 CUDA 上下文访问资源会产生未定义的结果。 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#opengl-interoperability">OpenGL 互操作</a>和 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#direct3d-interoperability">Direct3D 互操作</a>为每个图形 API 和一些代码示例提供了细节。 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#sli-interoperability">SLI 互操作</a>给出了系统何时处于 SLI 模式的细节。</p>
<h4 id="3-2-13-1-OpenGL-一致性"><a href="#3-2-13-1-OpenGL-一致性" class="headerlink" title="3.2.13.1. OpenGL 一致性"></a>3.2.13.1. OpenGL 一致性</h4><p>可以映射到 CUDA 地址空间的 OpenGL 资源是 OpenGL 缓冲区、纹理和渲染缓冲区对象。</p>
<p>使用 <code>cudaGraphicsGLRegisterBuffer()</code> 注册缓冲区对象。在 CUDA 中，它显示为设备指针，因此可以由内核或通过 <code>cudaMemcpy()</code> 调用读取和写入。</p>
<p>使用 <code>cudaGraphicsGLRegisterImage()</code> 注册纹理或渲染缓冲区对象。在 CUDA 中，它显示为 CUDA 数组。内核可以通过将数组绑定到纹理或表面引用来读取数组。如果资源已使用 <code>cudaGraphicsRegisterFlagsSurfaceLoadStore</code> 标志注册，他们还可以通过表面写入函数对其进行写入。该数组也可以通过 <code>cudaMemcpy2D()</code> 调用来读取和写入。 <code>cudaGraphicsGLRegisterImage()</code> 支持具有 1、2 或 4 个分量和内部浮点类型（例如，<code>GL_RGBA_FLOAT32</code>）、标准化整数（例如，<code>GL_RGBA8、GL_INTENSITY16</code>）和非标准化整数（例如，<code>GL_RGBA8UI</code>）的所有纹理格式（请注意，由于非标准化整数格式需要 OpenGL 3.0，它们只能由着色器编写，而不是固定函数管道）。</p>
<p>正在共享资源的 OpenGL 上下文对于进行任何 OpenGL 互操作性 API 调用的主机线程来说必须是最新的。</p>
<h4 id="请注意：当-OpenGL-纹理设置为无绑定时（例如，通过使用-glGetTextureHandle-glGetImageHandle-API-请求图像或纹理句柄），它不能在-CUDA-中注册。应用程序需要在请求图像或纹理句柄之前注册纹理以进行互操作。"><a href="#请注意：当-OpenGL-纹理设置为无绑定时（例如，通过使用-glGetTextureHandle-glGetImageHandle-API-请求图像或纹理句柄），它不能在-CUDA-中注册。应用程序需要在请求图像或纹理句柄之前注册纹理以进行互操作。" class="headerlink" title="请注意：当 OpenGL 纹理设置为无绑定时（例如，通过使用 glGetTextureHandle*/glGetImageHandle* API 请求图像或纹理句柄），它不能在 CUDA 中注册。应用程序需要在请求图像或纹理句柄之前注册纹理以进行互操作。"></a>请注意：当 OpenGL 纹理设置为无绑定时（例如，通过使用 <code>glGetTextureHandle*/glGetImageHandle*</code> API 请求图像或纹理句柄），它不能在 CUDA 中注册。应用程序需要在请求图像或纹理句柄之前注册纹理以进行互操作。</h4><p>以下代码示例使用内核动态修改存储在顶点缓冲区对象中的 2D width x height 网格：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">GLuint positionsVBO;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">cudaGraphicsResource</span>* positionsVBO_CUDA;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Initialize OpenGL and GLUT for device 0</span></span><br><span class="line">    <span class="comment">// and make the OpenGL context current</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="built_in">glutDisplayFunc</span>(display);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Explicitly set device 0</span></span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create buffer object and register it with CUDA</span></span><br><span class="line">    <span class="built_in">glGenBuffers</span>(<span class="number">1</span>, &amp;positionsVBO);</span><br><span class="line">    <span class="built_in">glBindBuffer</span>(GL_ARRAY_BUFFER, positionsVBO);</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> size = width * height * <span class="number">4</span> * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">glBufferData</span>(GL_ARRAY_BUFFER, size, <span class="number">0</span>, GL_DYNAMIC_DRAW);</span><br><span class="line">    <span class="built_in">glBindBuffer</span>(GL_ARRAY_BUFFER, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaGraphicsGLRegisterBuffer</span>(&amp;positionsVBO_CUDA,</span><br><span class="line">                                 positionsVBO,</span><br><span class="line">                                 cudaGraphicsMapFlagsWriteDiscard);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Launch rendering loop</span></span><br><span class="line">    <span class="built_in">glutMainLoop</span>();</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">display</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Map buffer object for writing from CUDA</span></span><br><span class="line">    float4* positions;</span><br><span class="line">    <span class="built_in">cudaGraphicsMapResources</span>(<span class="number">1</span>, &amp;positionsVBO_CUDA, <span class="number">0</span>);</span><br><span class="line">    <span class="type">size_t</span> num_bytes; </span><br><span class="line">    <span class="built_in">cudaGraphicsResourceGetMappedPointer</span>((<span class="type">void</span>**)&amp;positions,</span><br><span class="line">                                         &amp;num_bytes,  </span><br><span class="line">                                         positionsVBO_CUDA));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Execute kernel</span></span><br><span class="line">    <span class="function">dim3 <span class="title">dimBlock</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">dimGrid</span><span class="params">(width / dimBlock.x, height / dimBlock.y, <span class="number">1</span>)</span></span>;</span><br><span class="line">    createVertices&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(positions, time,</span><br><span class="line">                                          width, height);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Unmap buffer object</span></span><br><span class="line">    <span class="built_in">cudaGraphicsUnmapResources</span>(<span class="number">1</span>, &amp;positionsVBO_CUDA, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Render from buffer object</span></span><br><span class="line">    <span class="built_in">glClear</span>(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);</span><br><span class="line">    <span class="built_in">glBindBuffer</span>(GL_ARRAY_BUFFER, positionsVBO);</span><br><span class="line">    <span class="built_in">glVertexPointer</span>(<span class="number">4</span>, GL_FLOAT, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">glEnableClientState</span>(GL_VERTEX_ARRAY);</span><br><span class="line">    <span class="built_in">glDrawArrays</span>(GL_POINTS, <span class="number">0</span>, width * height);</span><br><span class="line">    <span class="built_in">glDisableClientState</span>(GL_VERTEX_ARRAY);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Swap buffers</span></span><br><span class="line">    <span class="built_in">glutSwapBuffers</span>();</span><br><span class="line">    <span class="built_in">glutPostRedisplay</span>();</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">deleteVBO</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">cudaGraphicsUnregisterResource</span>(positionsVBO_CUDA);</span><br><span class="line">    <span class="built_in">glDeleteBuffers</span>(<span class="number">1</span>, &amp;positionsVBO);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">createVertices</span><span class="params">(float4* positions, <span class="type">float</span> time,</span></span></span><br><span class="line"><span class="params"><span class="function">                               <span class="type">unsigned</span> <span class="type">int</span> width, <span class="type">unsigned</span> <span class="type">int</span> height)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Calculate uv coordinates</span></span><br><span class="line">    <span class="type">float</span> u = x / (<span class="type">float</span>)width;</span><br><span class="line">    <span class="type">float</span> v = y / (<span class="type">float</span>)height;</span><br><span class="line">    u = u * <span class="number">2.0f</span> - <span class="number">1.0f</span>;</span><br><span class="line">    v = v * <span class="number">2.0f</span> - <span class="number">1.0f</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// calculate simple sine wave pattern</span></span><br><span class="line">    <span class="type">float</span> freq = <span class="number">4.0f</span>;</span><br><span class="line">    <span class="type">float</span> w = <span class="built_in">sinf</span>(u * freq + time)</span><br><span class="line">            * <span class="built_in">cosf</span>(v * freq + time) * <span class="number">0.5f</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Write positions</span></span><br><span class="line">    positions[y * width + x] = <span class="built_in">make_float4</span>(u, w, v, <span class="number">1.0f</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 Windows 和 Quadro GPU 上，<code>cudaWGLGetDevice()</code> 可用于检索与 <code>wglEnumGpusNV()</code> 返回的句柄关联的 CUDA 设备。 Quadro GPU 在多 GPU 配置中提供比 GeForce 和 Tesla GPU 更高性能的 OpenGL 互操作性，其中 OpenGL 渲染在 Quadro GPU 上执行，CUDA 计算在系统中的其他 GPU 上执行。</p>
<h4 id="3-2-13-2-Direct3D-一致性"><a href="#3-2-13-2-Direct3D-一致性" class="headerlink" title="3.2.13.2. Direct3D 一致性"></a>3.2.13.2. Direct3D 一致性</h4><p>Direct3D 9Ex、Direct3D 10 和 Direct3D 11 支持 Direct3D 互操作性。</p>
<p>CUDA 上下文只能与满足以下条件的 Direct3D 设备互操作： Direct3D 9Ex 设备必须使用设置为 <code>D3DDEVTYPE_HAL</code> 的 <code>DeviceType</code> 和使用 <code>D3DCREATE_HARDWARE_VERTEXPROCESSING</code> 标志的 <code>BehaviorFlags</code> 创建； Direct3D 10 和 Direct3D 11 设备必须在 <code>DriverType</code> 设置为 <code>D3D_DRIVER_TYPE_HARDWARE</code> 的情况下创建。</p>
<p>可以映射到 CUDA 地址空间的 Direct3D 资源是 Direct3D 缓冲区、纹理和表面。 这些资源使用 <code>cudaGraphicsD3D9RegisterResource()</code>、<code>cudaGraphicsD3D10RegisterResource()</code> 和 <code>cudaGraphicsD3D11RegisterResource()</code> 注册。</p>
<p>以下代码示例使用内核动态修改存储在顶点缓冲区对象中的 2D width x height网格。</p>
<p>Direct3D 9 Version:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">IDirect3D9* D3D;</span><br><span class="line">IDirect3DDevice9* device;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">CUSTOMVERTEX</span> &#123;</span><br><span class="line">    FLOAT x, y, z;</span><br><span class="line">    DWORD color;</span><br><span class="line">&#125;;</span><br><span class="line">IDirect3DVertexBuffer9* positionsVB;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">cudaGraphicsResource</span>* positionsVB_CUDA;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> dev;</span><br><span class="line">    <span class="comment">// Initialize Direct3D</span></span><br><span class="line">    D3D = <span class="built_in">Direct3DCreate9Ex</span>(D3D_SDK_VERSION);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Get a CUDA-enabled adapter</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> adapter = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (; adapter &lt; g_pD3D-&gt;<span class="built_in">GetAdapterCount</span>(); adapter++) &#123;</span><br><span class="line">        D3DADAPTER_IDENTIFIER9 adapterId;</span><br><span class="line">        g_pD3D-&gt;<span class="built_in">GetAdapterIdentifier</span>(adapter, <span class="number">0</span>, &amp;adapterId);</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">cudaD3D9GetDevice</span>(&amp;dev, adapterId.DeviceName)</span><br><span class="line">            == cudaSuccess)</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">     <span class="comment">// Create device</span></span><br><span class="line">    ...</span><br><span class="line">    D3D-&gt;<span class="built_in">CreateDeviceEx</span>(adapter, D3DDEVTYPE_HAL, hWnd,</span><br><span class="line">                        D3DCREATE_HARDWARE_VERTEXPROCESSING,</span><br><span class="line">                        &amp;params, <span class="literal">NULL</span>, &amp;device);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Use the same device</span></span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create vertex buffer and register it with CUDA</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> size = width * height * <span class="built_in">sizeof</span>(CUSTOMVERTEX);</span><br><span class="line">    device-&gt;<span class="built_in">CreateVertexBuffer</span>(size, <span class="number">0</span>, D3DFVF_CUSTOMVERTEX,</span><br><span class="line">                               D3DPOOL_DEFAULT, &amp;positionsVB, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaGraphicsD3D9RegisterResource</span>(&amp;positionsVB_CUDA,</span><br><span class="line">                                     positionsVB,</span><br><span class="line">                                     cudaGraphicsRegisterFlagsNone);</span><br><span class="line">    <span class="built_in">cudaGraphicsResourceSetMapFlags</span>(positionsVB_CUDA,</span><br><span class="line">                                    cudaGraphicsMapFlagsWriteDiscard);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Launch rendering loop</span></span><br><span class="line">    <span class="keyword">while</span> (...) &#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="built_in">Render</span>();</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Render</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Map vertex buffer for writing from CUDA</span></span><br><span class="line">    float4* positions;</span><br><span class="line">    <span class="built_in">cudaGraphicsMapResources</span>(<span class="number">1</span>, &amp;positionsVB_CUDA, <span class="number">0</span>);</span><br><span class="line">    <span class="type">size_t</span> num_bytes; </span><br><span class="line">    <span class="built_in">cudaGraphicsResourceGetMappedPointer</span>((<span class="type">void</span>**)&amp;positions,</span><br><span class="line">                                         &amp;num_bytes,  </span><br><span class="line">                                         positionsVB_CUDA));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Execute kernel</span></span><br><span class="line">    <span class="function">dim3 <span class="title">dimBlock</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">dimGrid</span><span class="params">(width / dimBlock.x, height / dimBlock.y, <span class="number">1</span>)</span></span>;</span><br><span class="line">    createVertices&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(positions, time,</span><br><span class="line">                                          width, height);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Unmap vertex buffer</span></span><br><span class="line">    <span class="built_in">cudaGraphicsUnmapResources</span>(<span class="number">1</span>, &amp;positionsVB_CUDA, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Draw and present</span></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">releaseVB</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">cudaGraphicsUnregisterResource</span>(positionsVB_CUDA);</span><br><span class="line">    positionsVB-&gt;<span class="built_in">Release</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">createVertices</span><span class="params">(float4* positions, <span class="type">float</span> time,</span></span></span><br><span class="line"><span class="params"><span class="function">                               <span class="type">unsigned</span> <span class="type">int</span> width, <span class="type">unsigned</span> <span class="type">int</span> height)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Calculate uv coordinates</span></span><br><span class="line">    <span class="type">float</span> u = x / (<span class="type">float</span>)width;</span><br><span class="line">    <span class="type">float</span> v = y / (<span class="type">float</span>)height;</span><br><span class="line">    u = u * <span class="number">2.0f</span> - <span class="number">1.0f</span>;</span><br><span class="line">    v = v * <span class="number">2.0f</span> - <span class="number">1.0f</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Calculate simple sine wave pattern</span></span><br><span class="line">    <span class="type">float</span> freq = <span class="number">4.0f</span>;</span><br><span class="line">    <span class="type">float</span> w = <span class="built_in">sinf</span>(u * freq + time)</span><br><span class="line">            * <span class="built_in">cosf</span>(v * freq + time) * <span class="number">0.5f</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Write positions</span></span><br><span class="line">    positions[y * width + x] =</span><br><span class="line">                <span class="built_in">make_float4</span>(u, w, v, __int_as_float(<span class="number">0xff00ff00</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Direct3D 10 Version</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line">ID3D10Device* device;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">CUSTOMVERTEX</span> &#123;</span><br><span class="line">    FLOAT x, y, z;</span><br><span class="line">    DWORD color;</span><br><span class="line">&#125;;</span><br><span class="line">ID3D10Buffer* positionsVB;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">cudaGraphicsResource</span>* positionsVB_CUDA;</span><br><span class="line">            </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> dev;</span><br><span class="line">    <span class="comment">// Get a CUDA-enabled adapter</span></span><br><span class="line">    IDXGIFactory* factory;</span><br><span class="line">    <span class="built_in">CreateDXGIFactory</span>(__uuidof(IDXGIFactory), (<span class="type">void</span>**)&amp;factory);</span><br><span class="line">    IDXGIAdapter* adapter = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; !adapter; ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">FAILED</span>(factory-&gt;<span class="built_in">EnumAdapters</span>(i, &amp;adapter))</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">cudaD3D10GetDevice</span>(&amp;dev, adapter) == cudaSuccess)</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        adapter-&gt;<span class="built_in">Release</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    factory-&gt;<span class="built_in">Release</span>();</span><br><span class="line">            </span><br><span class="line">    <span class="comment">// Create swap chain and device</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="built_in">D3D10CreateDeviceAndSwapChain</span>(adapter, </span><br><span class="line">                                  D3D10_DRIVER_TYPE_HARDWARE, <span class="number">0</span>, </span><br><span class="line">                                  D3D10_CREATE_DEVICE_DEBUG,</span><br><span class="line">                                  D3D10_SDK_VERSION, </span><br><span class="line">                                  &amp;swapChainDesc, &amp;swapChain,</span><br><span class="line">                                  &amp;device);</span><br><span class="line">    adapter-&gt;<span class="built_in">Release</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Use the same device</span></span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create vertex buffer and register it with CUDA</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> size = width * height * <span class="built_in">sizeof</span>(CUSTOMVERTEX);</span><br><span class="line">    D3D10_BUFFER_DESC bufferDesc;</span><br><span class="line">    bufferDesc.Usage          = D3D10_USAGE_DEFAULT;</span><br><span class="line">    bufferDesc.ByteWidth      = size;</span><br><span class="line">    bufferDesc.BindFlags      = D3D10_BIND_VERTEX_BUFFER;</span><br><span class="line">    bufferDesc.CPUAccessFlags = <span class="number">0</span>;</span><br><span class="line">    bufferDesc.MiscFlags      = <span class="number">0</span>;</span><br><span class="line">    device-&gt;<span class="built_in">CreateBuffer</span>(&amp;bufferDesc, <span class="number">0</span>, &amp;positionsVB);</span><br><span class="line">    <span class="built_in">cudaGraphicsD3D10RegisterResource</span>(&amp;positionsVB_CUDA,</span><br><span class="line">                                      positionsVB,</span><br><span class="line">                                      cudaGraphicsRegisterFlagsNone);</span><br><span class="line">                                      <span class="built_in">cudaGraphicsResourceSetMapFlags</span>(positionsVB_CUDA,</span><br><span class="line">                                      cudaGraphicsMapFlagsWriteDiscard);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Launch rendering loop</span></span><br><span class="line">    <span class="keyword">while</span> (...) &#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="built_in">Render</span>();</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="built_in">Render</span>()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Map vertex buffer for writing from CUDA</span></span><br><span class="line">    float4* positions;</span><br><span class="line">    <span class="built_in">cudaGraphicsMapResources</span>(<span class="number">1</span>, &amp;positionsVB_CUDA, <span class="number">0</span>);</span><br><span class="line">    <span class="type">size_t</span> num_bytes; </span><br><span class="line">    <span class="built_in">cudaGraphicsResourceGetMappedPointer</span>((<span class="type">void</span>**)&amp;positions,</span><br><span class="line">                                         &amp;num_bytes,  </span><br><span class="line">                                         positionsVB_CUDA));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Execute kernel</span></span><br><span class="line">    <span class="function">dim3 <span class="title">dimBlock</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">dimGrid</span><span class="params">(width / dimBlock.x, height / dimBlock.y, <span class="number">1</span>)</span></span>;</span><br><span class="line">    createVertices&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(positions, time,</span><br><span class="line">                                          width, height);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Unmap vertex buffer</span></span><br><span class="line">    <span class="built_in">cudaGraphicsUnmapResources</span>(<span class="number">1</span>, &amp;positionsVB_CUDA, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Draw and present</span></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">releaseVB</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">cudaGraphicsUnregisterResource</span>(positionsVB_CUDA);</span><br><span class="line">    positionsVB-&gt;<span class="built_in">Release</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">createVertices</span><span class="params">(float4* positions, <span class="type">float</span> time,</span></span></span><br><span class="line"><span class="params"><span class="function">                               <span class="type">unsigned</span> <span class="type">int</span> width, <span class="type">unsigned</span> <span class="type">int</span> height)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Calculate uv coordinates</span></span><br><span class="line">    <span class="type">float</span> u = x / (<span class="type">float</span>)width;</span><br><span class="line">    <span class="type">float</span> v = y / (<span class="type">float</span>)height;</span><br><span class="line">    u = u * <span class="number">2.0f</span> - <span class="number">1.0f</span>;</span><br><span class="line">    v = v * <span class="number">2.0f</span> - <span class="number">1.0f</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Calculate simple sine wave pattern</span></span><br><span class="line">    <span class="type">float</span> freq = <span class="number">4.0f</span>;</span><br><span class="line">    <span class="type">float</span> w = <span class="built_in">sinf</span>(u * freq + time)</span><br><span class="line">            * <span class="built_in">cosf</span>(v * freq + time) * <span class="number">0.5f</span>;</span><br><span class="line">            </span><br><span class="line">    <span class="comment">// Write positions</span></span><br><span class="line">    positions[y * width + x] =</span><br><span class="line">                <span class="built_in">make_float4</span>(u, w, v, __int_as_float(<span class="number">0xff00ff00</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Direct3D 11 Version</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line">ID3D11Device* device;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">CUSTOMVERTEX</span> &#123;</span><br><span class="line">    FLOAT x, y, z;</span><br><span class="line">    DWORD color;</span><br><span class="line">&#125;;</span><br><span class="line">ID3D11Buffer* positionsVB;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">cudaGraphicsResource</span>* positionsVB_CUDA;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> dev;</span><br><span class="line">    <span class="comment">// Get a CUDA-enabled adapter</span></span><br><span class="line">    IDXGIFactory* factory;</span><br><span class="line">    <span class="built_in">CreateDXGIFactory</span>(__uuidof(IDXGIFactory), (<span class="type">void</span>**)&amp;factory);</span><br><span class="line">    IDXGIAdapter* adapter = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; !adapter; ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">FAILED</span>(factory-&gt;<span class="built_in">EnumAdapters</span>(i, &amp;adapter))</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">cudaD3D11GetDevice</span>(&amp;dev, adapter) == cudaSuccess)</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        adapter-&gt;<span class="built_in">Release</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    factory-&gt;<span class="built_in">Release</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create swap chain and device</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="built_in">sFnPtr_D3D11CreateDeviceAndSwapChain</span>(adapter, </span><br><span class="line">                                         D3D11_DRIVER_TYPE_HARDWARE,</span><br><span class="line">                                         <span class="number">0</span>, </span><br><span class="line">                                         D3D11_CREATE_DEVICE_DEBUG,</span><br><span class="line">                                         featureLevels, <span class="number">3</span>,</span><br><span class="line">                                         D3D11_SDK_VERSION, </span><br><span class="line">                                         &amp;swapChainDesc, &amp;swapChain,</span><br><span class="line">                                         &amp;device,</span><br><span class="line">                                         &amp;featureLevel,</span><br><span class="line">                                         &amp;deviceContext);</span><br><span class="line">    adapter-&gt;<span class="built_in">Release</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Use the same device</span></span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create vertex buffer and register it with CUDA</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> size = width * height * <span class="built_in">sizeof</span>(CUSTOMVERTEX);</span><br><span class="line">    D3D11_BUFFER_DESC bufferDesc;</span><br><span class="line">    bufferDesc.Usage          = D3D11_USAGE_DEFAULT;</span><br><span class="line">    bufferDesc.ByteWidth      = size;</span><br><span class="line">    bufferDesc.BindFlags      = D3D11_BIND_VERTEX_BUFFER;</span><br><span class="line">    bufferDesc.CPUAccessFlags = <span class="number">0</span>;</span><br><span class="line">    bufferDesc.MiscFlags      = <span class="number">0</span>;</span><br><span class="line">    device-&gt;<span class="built_in">CreateBuffer</span>(&amp;bufferDesc, <span class="number">0</span>, &amp;positionsVB);</span><br><span class="line">    <span class="built_in">cudaGraphicsD3D11RegisterResource</span>(&amp;positionsVB_CUDA,</span><br><span class="line">                                      positionsVB,</span><br><span class="line">                                      cudaGraphicsRegisterFlagsNone);</span><br><span class="line">    <span class="built_in">cudaGraphicsResourceSetMapFlags</span>(positionsVB_CUDA,</span><br><span class="line">                                    cudaGraphicsMapFlagsWriteDiscard);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Launch rendering loop</span></span><br><span class="line">    <span class="keyword">while</span> (...) &#123;</span><br><span class="line">        ...</span><br><span class="line">        <span class="built_in">Render</span>();</span><br><span class="line">        ...</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="built_in">Render</span>()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Map vertex buffer for writing from CUDA</span></span><br><span class="line">    float4* positions;</span><br><span class="line">    <span class="built_in">cudaGraphicsMapResources</span>(<span class="number">1</span>, &amp;positionsVB_CUDA, <span class="number">0</span>);</span><br><span class="line">    <span class="type">size_t</span> num_bytes; </span><br><span class="line">    <span class="built_in">cudaGraphicsResourceGetMappedPointer</span>((<span class="type">void</span>**)&amp;positions,</span><br><span class="line">                                         &amp;num_bytes,  </span><br><span class="line">                                         positionsVB_CUDA));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Execute kernel</span></span><br><span class="line">    <span class="function">dim3 <span class="title">dimBlock</span><span class="params">(<span class="number">16</span>, <span class="number">16</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">dimGrid</span><span class="params">(width / dimBlock.x, height / dimBlock.y, <span class="number">1</span>)</span></span>;</span><br><span class="line">    createVertices&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(positions, time,</span><br><span class="line">                                          width, height);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Unmap vertex buffer</span></span><br><span class="line">    <span class="built_in">cudaGraphicsUnmapResources</span>(<span class="number">1</span>, &amp;positionsVB_CUDA, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Draw and present</span></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">releaseVB</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">cudaGraphicsUnregisterResource</span>(positionsVB_CUDA);</span><br><span class="line">    positionsVB-&gt;<span class="built_in">Release</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">__global__ <span class="type">void</span> <span class="title">createVertices</span><span class="params">(float4* positions, <span class="type">float</span> time,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">unsigned</span> <span class="type">int</span> width, <span class="type">unsigned</span> <span class="type">int</span> height)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Calculate uv coordinates</span></span><br><span class="line">    <span class="type">float</span> u = x / (<span class="type">float</span>)width;</span><br><span class="line">    <span class="type">float</span> v = y / (<span class="type">float</span>)height;</span><br><span class="line">    u = u * <span class="number">2.0f</span> - <span class="number">1.0f</span>;</span><br><span class="line">    v = v * <span class="number">2.0f</span> - <span class="number">1.0f</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Calculate simple sine wave pattern</span></span><br><span class="line">    <span class="type">float</span> freq = <span class="number">4.0f</span>;</span><br><span class="line">    <span class="type">float</span> w = <span class="built_in">sinf</span>(u * freq + time)</span><br><span class="line">            * <span class="built_in">cosf</span>(v * freq + time) * <span class="number">0.5f</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Write positions</span></span><br><span class="line">    positions[y * width + x] =</span><br><span class="line">                <span class="built_in">make_float4</span>(u, w, v, __int_as_float(<span class="number">0xff00ff00</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-13-3-SLI一致性"><a href="#3-2-13-3-SLI一致性" class="headerlink" title="3.2.13.3 SLI一致性"></a>3.2.13.3 SLI一致性</h3><p>在具有多个 GPU 的系统中，所有支持 CUDA 的 GPU 都可以通过 CUDA 驱动程序和运行时作为单独的设备进行访问。然而，当系统处于 SLI 模式时，有如下所述的特殊注意事项。</p>
<p>首先，在一个 GPU 上的一个 CUDA 设备中的分配将消耗其他 GPU 上的内存，这些 GPU 是 Direct3D 或 OpenGL 设备的 SLI 配置的一部分。因此，分配可能会比预期的更早失败。</p>
<p>其次，应用程序应该创建多个 CUDA 上下文，一个用于 SLI 配置中的每个 GPU。虽然这不是严格要求，但它避免了设备之间不必要的数据传输。应用程序可以将 <code>cudaD3D[9|10|11]GetDevices()</code>用于 Direct3D 和 <code>cudaGLGetDevices()</code> 用于 OpenGL 调用，以识别当前执行渲染的设备的 CUDA 设备句柄和下一帧。鉴于此信息，应用程序通常会选择适当的设备并将 Direct3D 或 OpenGL 资源映射到由 <code>cudaD3D[9|10|11]GetDevices()</code> 或当 <code>deviceList</code> 参数设置为 <code>cudaD3D[9|10 |11]DeviceListCurrentFrame</code> 或 <code>cudaGLDeviceListCurrentFrame</code>。</p>
<p>请注意，从 <code>cudaGraphicsD9D[9|10|11]RegisterResource</code> 和 <code>cudaGraphicsGLRegister[Buffer|Image]</code> 返回的资源只能在发生注册的设备上使用。因此，在 SLI 配置中，当在不同的 CUDA 设备上计算不同帧的数据时，有必要分别为每个设备注册资源。</p>
<p>有关 CUDA 运行时如何分别与 Direct3D 和 OpenGL 互操作的详细信息，请参阅 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#direct3d-interoperability">Direct3D 互操作性</a>和 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#opengl-interoperability">OpenGL 互操作性</a>。</p>
<h3 id="3-2-14-扩展资源一致性"><a href="#3-2-14-扩展资源一致性" class="headerlink" title="3.2.14 扩展资源一致性"></a>3.2.14 扩展资源一致性</h3><p>这里待定(实际上是作者不熟悉)</p>
<h3 id="3-2-15-CUDA用户对象"><a href="#3-2-15-CUDA用户对象" class="headerlink" title="3.2.15 CUDA用户对象"></a>3.2.15 CUDA用户对象</h3><p>CUDA 用户对象可用于帮助管理 CUDA 中异步工作所使用的资源的生命周期。 特别是，此功能对于 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-graphs">CUDA 图</a>和<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#creating-a-graph-using-stream-capture">流捕获</a>非常有用。</p>
<p>各种资源管理方案与 CUDA 图不兼容。 例如，考虑基于事件的池或同步创建、异步销毁方案。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Library API with pool allocation</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">libraryWork</span><span class="params">(cudaStream_t stream)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> &amp;resource = pool.<span class="built_in">claimTemporaryResource</span>();</span><br><span class="line">    resource.<span class="built_in">waitOnReadyEventInStream</span>(stream);</span><br><span class="line">    <span class="built_in">launchWork</span>(stream, resource);</span><br><span class="line">    resource.<span class="built_in">recordReadyEvent</span>(stream);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Library API with asynchronous resource deletion</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">libraryWork</span><span class="params">(cudaStream_t stream)</span> </span>&#123;</span><br><span class="line">    Resource *resource = <span class="keyword">new</span> <span class="built_in">Resource</span>(...);</span><br><span class="line">    <span class="built_in">launchWork</span>(stream, resource);</span><br><span class="line">    <span class="built_in">cudaStreamAddCallback</span>(</span><br><span class="line">        stream,</span><br><span class="line">        [](cudaStream_t, cudaError_t, <span class="type">void</span> *resource) &#123;</span><br><span class="line">            <span class="keyword">delete</span> <span class="built_in">static_cast</span>&lt;Resource *&gt;(resource);</span><br><span class="line">        &#125;,</span><br><span class="line">        resource,</span><br><span class="line">        <span class="number">0</span>);</span><br><span class="line">    <span class="comment">// Error handling considerations not shown</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>由于需要间接或图更新的资源的非固定指针或句柄，以及每次提交工作时需要同步 CPU 代码，这些方案对于 CUDA 图来说是困难的。如果这些注意事项对库的调用者隐藏，并且由于在捕获期间使用了不允许的 API，它们也不适用于流捕获。存在各种解决方案，例如将资源暴露给调用者。 CUDA 用户对象提供了另一种方法。</p>
<p>CUDA 用户对象将用户指定的析构函数回调与内部引用计数相关联，类似于 C++ <code>shared_ptr</code>。引用可能归 CPU 上的用户代码和 CUDA 图所有。请注意，对于用户拥有的引用，与 C++ 智能指针不同，没有代表引用的对象；用户必须手动跟踪用户拥有的引用。一个典型的用例是在创建用户对象后立即将唯一的用户拥有的引用移动到 CUDA 图。</p>
<p>当引用关联到 CUDA 图时，CUDA 将自动管理图操作。克隆的 <code>cudaGraph_t</code> 保留源 <code>cudaGraph_t</code> 拥有的每个引用的副本，具有相同的多重性。实例化的 <code>cudaGraphExec_t</code> 保留源 <code>cudaGraph_t</code> 中每个引用的副本。当 <code>cudaGraphExec_t</code> 在未同步的情况下被销毁时，引用将保留到执行完成。</p>
<p>这是一个示例用法。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">cudaGraph_t graph;  <span class="comment">// Preexisting graph</span></span><br><span class="line"></span><br><span class="line">Object *object = <span class="keyword">new</span> Object;  <span class="comment">// C++ object with possibly nontrivial destructor</span></span><br><span class="line">cudaUserObject_t cuObject;</span><br><span class="line"><span class="built_in">cudaUserObjectCreate</span>(</span><br><span class="line">    &amp;cuObject,</span><br><span class="line">    object,  <span class="comment">// Here we use a CUDA-provided template wrapper for this API,</span></span><br><span class="line">             <span class="comment">// which supplies a callback to delete the C++ object pointer</span></span><br><span class="line">    <span class="number">1</span>,  <span class="comment">// Initial refcount</span></span><br><span class="line">    cudaUserObjectNoDestructorSync  <span class="comment">// Acknowledge that the callback cannot be</span></span><br><span class="line">                                    <span class="comment">// waited on via CUDA</span></span><br><span class="line">);</span><br><span class="line"><span class="built_in">cudaGraphRetainUserObject</span>(</span><br><span class="line">    graph,</span><br><span class="line">    cuObject,</span><br><span class="line">    <span class="number">1</span>,  <span class="comment">// Number of references</span></span><br><span class="line">    cudaGraphUserObjectMove  <span class="comment">// Transfer a reference owned by the caller (do</span></span><br><span class="line">                             <span class="comment">// not modify the total reference count)</span></span><br><span class="line">);</span><br><span class="line"><span class="comment">// No more references owned by this thread; no need to call release API</span></span><br><span class="line">cudaGraphExec_t graphExec;</span><br><span class="line"><span class="built_in">cudaGraphInstantiate</span>(&amp;graphExec, graph, <span class="literal">nullptr</span>, <span class="literal">nullptr</span>, <span class="number">0</span>);  <span class="comment">// Will retain a</span></span><br><span class="line">                                                               <span class="comment">// new reference</span></span><br><span class="line"><span class="built_in">cudaGraphDestroy</span>(graph);  <span class="comment">// graphExec still owns a reference</span></span><br><span class="line"><span class="built_in">cudaGraphLaunch</span>(graphExec, <span class="number">0</span>);  <span class="comment">// Async launch has access to the user objects</span></span><br><span class="line"><span class="built_in">cudaGraphExecDestroy</span>(graphExec);  <span class="comment">// Launch is not synchronized; the release</span></span><br><span class="line">                                  <span class="comment">// will be deferred if needed</span></span><br><span class="line"><span class="built_in">cudaStreamSynchronize</span>(<span class="number">0</span>);  <span class="comment">// After the launch is synchronized, the remaining</span></span><br><span class="line">                           <span class="comment">// reference is released and the destructor will</span></span><br><span class="line">                           <span class="comment">// execute. Note this happens asynchronously.</span></span><br><span class="line"><span class="comment">// If the destructor callback had signaled a synchronization object, it would</span></span><br><span class="line"><span class="comment">// be safe to wait on it at this point.</span></span><br></pre></td></tr></table></figure>
<p>子图节点中的图所拥有的引用与子图相关联，而不是与父图相关联。如果更新或删除子图，则引用会相应更改。如果使用 <code>cudaGraphExecUpdate</code> 或 <code>cudaGraphExecChildGraphNodeSetParams</code> 更新可执行图或子图，则会克隆新源图中的引用并替换目标图中的引用。在任何一种情况下，如果先前的启动不同步，则将保留任何将被释放的引用，直到启动完成执行。</p>
<p>目前没有通过 CUDA API 等待用户对象析构函数的机制。用户可以从析构代码中手动发出同步对象的信号。另外，从析构函数调用 CUDA API 是不合法的，类似于对 <code>cudaLaunchHostFunc</code> 的限制。这是为了避免阻塞 CUDA 内部共享线程并阻止前进。如果依赖是一种方式并且执行调用的线程不能阻止 CUDA 工作的前进进度，则向另一个线程发出执行 API 调用的信号是合法的。</p>
<p>用户对象是使用 <code>cudaUserObjectCreate</code> 创建的，这是浏览相关 API 的一个很好的起点。</p>
<h2 id="3-3-版本和兼容性"><a href="#3-3-版本和兼容性" class="headerlink" title="3.3 版本和兼容性"></a>3.3 版本和兼容性</h2><p>开发人员在开发 CUDA 应用程序时应该关注两个版本号：描述计算设备的一般规范和特性的计算能力（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability">计算能力</a>）和描述受支持的特性的 CUDA 驱动程序 API 的版本。驱动程序 API 和运行时。</p>
<p>驱动程序 API 的版本在驱动程序头文件中定义为 <code>CUDA_VERSION</code>。它允许开发人员检查他们的应用程序是否需要比当前安装的设备驱动程序更新的设备驱动程序。这很重要，因为驱动 API 是向后兼容的，这意味着针对特定版本的驱动 API 编译的应用程序、插件和库（包括 CUDA 运行时）将继续在后续的设备驱动版本上工作，如下图所示. 驱动 API 不向前兼容，这意味着针对特定版本的驱动 API 编译的应用程序、插件和库（包括 CUDA 运行时）将不适用于以前版本的设备驱动。</p>
<p>需要注意的是，支持的版本的混合和匹配存在限制：</p>
<ul>
<li>由于系统上一次只能安装一个版本的 CUDA 驱动程序，因此安装的驱动程序必须与必须在已建成的系统其上运行的任何应用程序、插件或库所依据的最大驱动程序 API 版本相同或更高版本 。</li>
<li>应用程序使用的所有插件和库必须使用相同版本的 CUDA 运行时，除非它们静态链接到运行时，在这种情况下，运行时的多个版本可以共存于同一进程空间中。 请注意，如果使用 nvcc 链接应用程序，则默认使用静态版本的 CUDA Runtime 库，并且所有 CUDA Toolkit 库都针对 CUDA Runtime 静态链接。</li>
<li>应用程序使用的所有插件和库必须使用与使用运行时的任何库（例如 cuFFT、cuBLAS…）相同的版本，除非静态链接到这些库。</li>
</ul>
<p><img src="/img/compatibility-of-cuda-versions.png" alt="compatibility-of-cuda-versions.png"></p>
<p>对于 Tesla GPU 产品，CUDA 10 为 CUDA 驱动程序的用户模式组件引入了新的向前兼容升级路径。 此功能在 <a href="https://docs.nvidia.com/deploy/cuda-compatibility/index.html">CUDA 兼容性</a>中进行了描述。 此处描述的对 CUDA 驱动程序版本的要求适用于用户模式组件的版本。</p>
<h2 id="3-4-Compute-Modes"><a href="#3-4-Compute-Modes" class="headerlink" title="3.4 Compute Modes"></a>3.4 Compute Modes</h2><p>在运行 Windows Server 2008 及更高版本或 Linux 的 Tesla 解决方案上，可以使用 NVIDIA 的系统管理接口 (nvidia-smi) 将系统中的任何设备设置为以下三种模式之一，这是作为驱动程序一部分分发的工具：</p>
<ul>
<li>默认计算模式：多个主机线程可以同时使用该设备（通过在此设备上调用 <code>cudaSetDevice()</code>，当使用运行时 API 时，或通过使 current 成为与设备关联的上下文，当使用驱动程序 API 时）。</li>
<li>独占进程计算模式：在设备上只能在系统中的所有进程中创建一个 CUDA 上下文。 在创建该上下文的进程中，该上下文可以是当前任意数量的线程。</li>
<li>禁止的计算模式：不能在设备上创建 CUDA 上下文。</li>
</ul>
<p>这尤其意味着，如果设备 0 处于禁止模式或独占进程模式并被另一个设备使用，则使用运行时 API 而不显式调用 <code>cudaSetDevice()</code> 的主机线程可能与设备 0 以外的设备相关联过程。 <code>cudaSetValidDevices()</code> 可用于从设备的优先级列表中设置设备。</p>
<p>另请注意，对于采用 Pascal 架构（具有主要修订号 6 及更高版本的计算能力）的设备，存在对计算抢占的支持。这允许计算任务在指令级粒度上被抢占，而不是像以前的 Maxwell 和 Kepler GPU 架构中那样以线程块粒度进行抢占，其好处是可以防止具有长时间运行内核的应用程序垄断系统或超时。但是，将存在与计算抢占相关的上下文切换开销，它会在支持的设备上自动启用。具有属性 <code>cudaDevAttrComputePreemptionSupported</code> 的单个属性查询函数 <code>cudaDeviceGetAttribute()</code> 可用于确定正在使用的设备是否支持计算抢占。希望避免与不同进程相关的上下文切换开销的用户可以通过选择独占进程模式来确保在 GPU 上只有一个进程处于活动状态。</p>
<p>应用程序可以通过检查 <code>computeMode</code> 设备属性来查询设备的计算模式（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-enumeration">设备枚举</a>）。 </p>
<h2 id="3-5-模式切换"><a href="#3-5-模式切换" class="headerlink" title="3.5 模式切换"></a>3.5 模式切换</h2><p>具有显示输出的 GPU 将一些 DRAM 内存专用于所谓的主画面，用于刷新用户查看其输出的显示设备。当用户通过更改显示器的分辨率或位深度（使用 NVIDIA 控制面板或 Windows 上的显示控制面板）来启动显示器的模式切换时，主表面所需的内存量会发生变化。例如，如果用户将显示分辨率从 1280x1024x32 位更改为 1600x1200x32 位，则系统必须将 7.68 MB 专用于主画面，而不是 5.24 MB。 （在启用抗锯齿的情况下运行的全屏图形应用程序可能需要更多的主画面显示内存。）在 Windows 上，可能会启动显示模式切换的其他事件包括启动全屏 DirectX 应用程序，按 Alt+Tab 来完成任务从全屏 DirectX 应用程序切换，或按 Ctrl+Alt+Del 锁定计算机。</p>
<p>如果模式切换增加了主画面所需的内存量，系统可能不得不蚕食专用于 CUDA 应用程序的内存分配。因此，模式切换会导致对 CUDA 运行时的任何调用失败并返回无效的上下文错误。</p>
<h2 id="3-6-在Windows上的Tesla计算集群"><a href="#3-6-在Windows上的Tesla计算集群" class="headerlink" title="3.6 在Windows上的Tesla计算集群"></a>3.6 在Windows上的Tesla计算集群</h2><p>使用 NVIDIA 的系统管理界面 (nvidia-smi)，可以将 Windows 设备驱动程序置于 Tesla 和 Quadro 系列设备的 TCC（Tesla Compute Cluster）模式。</p>
<p>TCC 模式不支持任何图形功能。</p>
<h1 id="第四章-硬件实现"><a href="#第四章-硬件实现" class="headerlink" title="第四章 硬件实现"></a>第四章 硬件实现</h1><p>NVIDIA GPU 架构围绕可扩展的多线程流式多处理器 (SM: Streaming Multiprocessors) 阵列构建。当主机 CPU 上的 CUDA 程序调用内核网格时，网格的块被枚举并分发到具有可用执行能力的多处理器。一个线程块的线程在一个SM上并发执行，多个线程块可以在一个SM上并发执行。当线程块终止时，新块在空出的SM上启动。</p>
<p>SM旨在同时执行数百个线程。为了管理如此大量的线程，它采用了一种称为 SIMT（Single-Instruction, Multiple-Thread: 单指令，多线程）的独特架构，在 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture">SIMT 架构</a>中进行了描述。这些指令是流水线的，利用单个线程内的指令级并行性，以及通过同时硬件多线程处理的广泛线程级并行性，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-multithreading">硬件多线程</a>中详述。与 CPU 内核不同，它们是按顺序发出的，没有分支预测或推测执行。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture">SIMT 架构</a>和<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-multithreading">硬件多线程</a>描述了所有设备通用的流式多处理器的架构特性。 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-3-0">Compute Capability 3.x、Compute Capability 5.x、Compute Capability 6.x 和 Compute Capability 7.x</a> 分别为计算能力 3.x、5.x、6.x 和 7.x 的设备提供了详细信息。</p>
<p>NVIDIA GPU 架构使用 little-endian 表示。 </p>
<h2 id="4-1-SIMT-架构"><a href="#4-1-SIMT-架构" class="headerlink" title="4.1 SIMT 架构"></a>4.1 SIMT 架构</h2><p>多处理器以 32 个并行线程组（称为 warp）的形式创建、管理、调度和执行线程。组成 warp 的各个线程一起从同一个程序地址开始，但它们有自己的指令地址计数器和寄存器状态，因此可以自由地分支和独立执行。warp一词源于编织，这是第一个并行线程技术。半warp是warp的前半部分或后半部分。四分之一经线是warp的第一、第二、第三或第四四分之一。</p>
<p>当一个多处理器被赋予一个或多个线程块来执行时，它将它们划分为warp，并且每个warp都由warp调度程序调度以执行。一个块被分割成warp的方式总是一样的；每个warp包含连续的线程，增加线程ID，第一个warp包含线程0。<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy">线程层次结构</a>描述了线程ID如何与块中的线程索引相关。</p>
<p>一个 warp 一次执行一条公共指令，因此当一个 warp 的所有 32 个线程都同意它们的执行路径时，就可以实现完全的效率。如果 warp 的线程通过依赖于数据的条件分支发散，则 warp 执行所采用的每个分支路径，禁用不在该路径上的线程。分支分歧只发生在一个warp内；不同的 warp 独立执行，无论它们是执行公共的还是不相交的代码路径。</p>
<p>SIMT 体系结构类似于 SIMD（单指令多数据）向量组织，其中单指令控制多个处理元素。一个关键区别是 SIMD 矢量组织向软件公开了 SIMD 宽度，而 SIMT 指令指定单个线程的执行和分支行为。与 SIMD 向量机相比，SIMT 使程序员能够为独立的标量线程编写线程级并行代码，以及为协调线程编写数据并行代码。为了正确起见，程序员基本上可以忽略 SIMT 行为；但是，通过代码很少需要warp中的线程发散，可以实现显着的性能改进。在实践中，这类似于传统代码中缓存线的作用：在设计正确性时可以安全地忽略缓存线大小，但在设计峰值性能时必须在代码结构中考虑。另一方面，向量架构需要软件将负载合并到向量中并手动管理分歧。</p>
<p>在 Volta 之前，warp 使用在 warp 中的所有 32 个线程之间共享的单个程序计数器以及指定 warp 的活动线程的活动掩码。结果，来自不同区域或不同执行状态的同一warp的线程无法相互发送信号或交换数据，并且需要细粒度共享由锁或互斥锁保护的数据的算法很容易导致死锁，具体取决于来自哪个warp竞争线程。</p>
<p>从 Volta 架构开始，独立线程调度允许线程之间的完全并发，而不管 warp。使用独立线程调度，GPU 维护每个线程的执行状态，包括程序计数器和调用堆栈，并且可以在每个线程的粒度上产生执行，以便更好地利用执行资源或允许一个线程等待数据由他人生产。调度优化器确定如何将来自同一个 warp 的活动线程组合成 SIMT 单元。这保留了与先前 NVIDIA GPU 一样的 SIMT 执行的高吞吐量，但具有更大的灵活性：线程现在可以在 sub-warp 粒度上发散和重新收敛。</p>
<p>如果开发人员对先前硬件架构的 warp-synchronicity2 做出假设，独立线程调度可能会导致参与执行代码的线程集与预期的完全不同。特别是，应重新访问任何warp同步代码（例如无同步、内部warp减少），以确保与 Volta 及更高版本的兼容性。有关详细信息，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-7-x">计算能力 7.x</a>。</p>
<p><strong>注意:</strong></p>
<p><strong>参与当前指令的 warp 线程称为活动线程，而不在当前指令上的线程是非活动的（禁用）。线程可能由于多种原因而处于非活动状态，包括比其 warp 的其他线程更早退出，采用与 warp 当前执行的分支路径不同的分支路径，或者是线程数不是线程数的块的最后一个线程warp尺寸的倍数。</strong></p>
<p>如果 warp 执行的非原子指令为多个 warp 的线程写入全局或共享内存中的同一位置，则该位置发生的序列化写入次数取决于设备的计算能力（参见 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-3-0">Compute Capability 3.x、Compute Capability 5.x、Compute Capability 6.x 和 Compute Capability 7.x</a>），哪个线程执行最终写入是未定义的。</p>
<p>如果一个由 warp 执行的<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions">原子指令</a>读取、修改和写入全局内存中多个线程的同一位置，则对该位置的每次读取/修改/写入都会发生并且它们都被序列化，但是它们发生的顺序是不确定的。</p>
<h2 id="4-2-硬件多线程"><a href="#4-2-硬件多线程" class="headerlink" title="4.2 硬件多线程"></a>4.2 硬件多线程</h2><p>多处理器处理的每个 warp 的执行上下文（程序计数器、寄存器等）在 warp 的整个生命周期内都在芯片上维护。因此，从一个执行上下文切换到另一个执行上下文是没有成本的，并且在每个指令发出时，warp 调度程序都会选择一个线程准备好执行其下一条指令（warp 的<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture__notes">活动线程</a>）并将指令发布给这些线程.</p>
<p>特别是，每个多处理器都有一组 32 位寄存器，这些寄存器在 warp 之间进行分区，以及在线程块之间进行分区的并行数据缓存或共享内存。</p>
<p>对于给定内核，可以在多处理器上一起驻留和处理的块和warp的数量取决于内核使用的寄存器和共享内存的数量以及多处理器上可用的寄存器和共享内存的数量。每个多处理器也有最大数量的驻留块和驻留warp的最大数量。这些限制以及多处理器上可用的寄存器数量和共享内存是设备计算能力的函数，在附录<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities">计算能力</a>中给出。如果每个多处理器没有足够的寄存器或共享内存来处理至少一个块，内核将无法启动。</p>
<p>一个块中的warp总数如下：</p>
<p><img src="/img/number_of_warps.png" alt="number of warps.png"></p>
<p>为块分配的寄存器总数和共享内存总量记录在 CUDA 工具包中提供的 CUDA Occupancy Calculator中。</p>
<h1 id="第五章-性能指南"><a href="#第五章-性能指南" class="headerlink" title="第五章 性能指南"></a>第五章 性能指南</h1><h2 id="5-1-整体性能优化策略"><a href="#5-1-整体性能优化策略" class="headerlink" title="5.1 整体性能优化策略"></a>5.1 整体性能优化策略</h2><p>性能优化围绕四个基本策略：</p>
<ul>
<li>最大化并行执行以实现最大利用率；</li>
<li>优化内存使用，实现最大内存吞吐量；</li>
<li>优化指令使用，实现最大指令吞吐量；</li>
<li>尽量减少内存抖动。</li>
</ul>
<p>哪些策略将为应用程序的特定部分产生最佳性能增益取决于该部分的性能限值； 例如，优化主要受内存访问限制的内核的指令使用不会产生任何显着的性能提升。 因此，应该通过测量和监控性能限制来不断地指导优化工作，例如使用 CUDA 分析器。 此外，将特定内核的浮点运算吞吐量或内存吞吐量（以更有意义的为准）与设备的相应峰值理论吞吐量进行比较表明内核还有多少改进空间。</p>
<h2 id="5-2-最大化利用率"><a href="#5-2-最大化利用率" class="headerlink" title="5.2 最大化利用率"></a>5.2 最大化利用率</h2><p>为了最大限度地提高利用率，应用程序的结构应该尽可能多地暴露并行性，并有效地将这种并行性映射到系统的各个组件，以使它们大部分时间都处于忙碌状态。</p>
<h3 id="5-2-1-应用程序层次"><a href="#5-2-1-应用程序层次" class="headerlink" title="5.2.1 应用程序层次"></a>5.2.1 应用程序层次</h3><p>在高层次上，应用程序应该通过使用异步函数调用和<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution">异步并发执行</a>中描述的流来最大化主机、设备和将主机连接到设备的总线之间的并行执行。它应该为每个处理器分配它最擅长的工作类型：主机的串行工作负载；设备的并行工作负载。</p>
<p>对于并行工作负载，在算法中由于某些线程需要同步以相互共享数据而破坏并行性的点，有两种情况： 这些线程属于同一个块，在这种情况下，它们应该使用 <code>__syncthreads ()</code> 并在同一个内核调用中通过共享内存共享数据，或者它们属于不同的块，在这种情况下，它们必须使用两个单独的内核调用通过全局内存共享数据，一个用于写入，一个用于从全局内存中读取。第二种情况不太理想，因为它增加了额外内核调用和全局内存流量的开销。因此，应该通过将算法映射到 CUDA 编程模型以使需要线程间通信的计算尽可能在单个线程块内执行，从而最大限度地减少它的发生。</p>
<h3 id="5-2-2-设备层次"><a href="#5-2-2-设备层次" class="headerlink" title="5.2.2 设备层次"></a>5.2.2 设备层次</h3><p>在较低级别，应用程序应该最大化设备多处理器之间的并行执行。</p>
<p>多个内核可以在一个设备上并发执行，因此也可以通过使用流来启用足够多的内核来实现最大利用率，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#asynchronous-concurrent-execution">异步并发执行</a>中所述。</p>
<h3 id="5-2-3-多处理器层次"><a href="#5-2-3-多处理器层次" class="headerlink" title="5.2.3 多处理器层次"></a>5.2.3 多处理器层次</h3><p>在更低的层次上，应用程序应该最大化多处理器内不同功能单元之间的并行执行。</p>
<p>如硬件多线程中所述，GPU 多处理器主要依靠线程级并行性来最大限度地利用其功能单元。因此，利用率与常驻warp的数量直接相关。在每个指令发出时，warp 调度程序都会选择一条准备好执行的指令。该指令可以是同一warp的另一条独立指令，利用指令级并行性，或者更常见的是另一个warp的指令，利用线程级并行性。如果选择了准备执行指令，则将其发布到 warp 的活动线程。一个warp准备好执行其下一条指令所需的时钟周期数称为延迟，并且当所有warp调度程序在该延迟期间的每个时钟周期总是有一些指令要为某个warp发出一些指令时，就可以实现充分利用，或者换句话说，当延迟完全“隐藏”时。隐藏 L 个时钟周期延迟所​​需的指令数量取决于这些指令各自的吞吐量（有关各种算术指令的吞吐量，请参见<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions">算术指令</a>）。如果我们假设指令具有最大吞吐量，它等于： </p>
<ul>
<li>4L 用于计算能力 5.x、6.1、6.2、7.x 和 8.x 的设备，因为对于这些设备，多处理器在一个时钟周期内为每个 warp 发出一条指令，一次四个 warp，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities">计算能力</a>中所述。</li>
<li>2L 用于计算能力 6.0 的设备，因为对于这些设备，每个周期发出的两条指令是两条不同warp的一条指令。</li>
<li>8L 用于计算能力 3.x 的设备，因为对于这些设备，每个周期发出的八条指令是四对，用于四个不同的warp，每对都用于相同的warp。</li>
</ul>
<p>warp 未准备好执行其下一条指令的最常见原因是该指令的输入操作数尚不可用。</p>
<p>如果所有输入操作数都是寄存器，则延迟是由寄存器依赖性引起的，即，一些输入操作数是由一些尚未完成的先前指令写入的。在这种情况下，延迟等于前一条指令的执行时间，warp 调度程序必须在此期间调度其他 warp 的指令。执行时间因指令而异。在计算能力 7.x 的设备上，对于大多数算术指令，它通常是 4 个时钟周期。这意味着每个多处理器需要 16 个活动 warp（4 个周期，4 个 warp 调度程序）来隐藏算术指令延迟（假设 warp 以最大吞吐量执行指令，否则需要更少的 warp）。如果各个warp表现出指令级并行性，即在它们的指令流中有多个独立指令，则需要更少的warp，因为来自单个warp的多个独立指令可以背靠背发出。</p>
<p>如果某些输入操作数驻留在片外存储器中，则延迟要高得多：通常为数百个时钟周期。在如此高的延迟期间保持 warp 调度程序繁忙所需的 warp 数量取决于内核代码及其指令级并行度。一般来说，如果没有片外存储器操作数的指令（即大部分时间是算术指令）与具有片外存储器操作数的指令数量之比较低（这个比例通常是称为程序的算术强度）。 </p>
<p>warp 未准备好执行其下一条指令的另一个原因是它正在某个内存栅栏（<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-fence-functions">内存栅栏函数</a>）或同步点（<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#synchronization-functions">同步函数</a>）处等待。随着越来越多的warp等待同一块中的其他warp在同步点之前完成指令的执行，同步点可以强制多处理器空闲。在这种情况下，每个多处理器拥有多个常驻块有助于减少空闲，因为来自不同块的warp不需要在同步点相互等待。</p>
<p>对于给定的内核调用，驻留在每个多处理器上的块和warp的数量取决于调用的执行配置（<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration">执行配置</a>）、多处理器的内存资源以及内核的资源需求，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-multithreading">硬件多线程</a>中所述。使用 <code>--ptxas-options=-v</code> 选项编译时，编译器会报告寄存器和共享内存的使用情况。</p>
<p>一个块所需的共享内存总量等于静态分配的共享内存量和动态分配的共享内存量之和。</p>
<p>内核使用的寄存器数量会对驻留warp的数量产生重大影响。例如，对于计算能力为 6.x 的设备，如果内核使用 64 个寄存器并且每个块有 512 个线程并且需要很少的共享内存，那么两个块（即 32 个 warp）可以驻留在多处理器上，因为它们需要 2x512x64 个寄存器，它与多处理器上可用的寄存器数量完全匹配。但是一旦内核多使用一个寄存器，就只能驻留一个块（即 16 个 warp），因为两个块需要 2x512x65 个寄存器，这比多处理器上可用的寄存器多。因此，编译器会尽量减少寄存器的使用，同时保持寄存器溢出（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">设备内存访问</a>）和最少的指令数量。可以使用 <code>maxrregcount</code> 编译器选项或启动边界来控制寄存器的使用，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#launch-bounds">启动边界</a>中所述。</p>
<p>寄存器文件组织为 32 位寄存器。因此，存储在寄存器中的每个变量都需要至少一个 32 位寄存器，例如双精度变量使用两个 32 位寄存器。</p>
<p>对于给定的内核调用，执行配置对性能的影响通常取决于内核代码。因此建议进行实验。应用程序还可以根据寄存器文件大小和共享内存大小参数化执行配置，这取决于设备的计算能力，以及设备的多处理器数量和内存带宽，所有这些都可以使用运行时查询（参见参考手册）。</p>
<p>每个块的线程数应选择为 warp 大小的倍数，以避免尽可能多地在填充不足的 warp 上浪费计算资源。</p>
<h4 id="5-2-3-1-占用率计算"><a href="#5-2-3-1-占用率计算" class="headerlink" title="5.2.3.1 占用率计算"></a>5.2.3.1 占用率计算</h4><p>存在几个 API 函数来帮助程序员根据寄存器和共享内存要求选择线程块大小。</p>
<ul>
<li>占用计算器 API，<code>cudaOccupancyMaxActiveBlocksPerMultiprocessor</code>，可以根据内核的块大小和共享内存使用情况提供占用预测。此函数根据每个多处理器的并发线程块数报告占用情况。</li>
<li><ul>
<li><strong>请注意，此值可以转换为其他指标。乘以每个块的warp数得出每个多处理器的并发warp数；进一步将并发warp除以每个多处理器的最大warp得到占用率作为百分比。</strong></li>
</ul>
</li>
<li>基于占用率的启动配置器 API，<code>cudaOccupancyMaxPotentialBlockSize</code> 和 <code>cudaOccupancyMaxPotentialBlockSizeVariableSMem</code>，启发式地计算实现最大多处理器级占用率的执行配置。</li>
</ul>
<p>以下代码示例计算 MyKernel 的占用率。然后，它使用并发warp与每个多处理器的最大warp之间的比率报告占用率。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="function">Device code</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MyKernel</span><span class="params">(<span class="type">int</span> *d, <span class="type">int</span> *a, <span class="type">int</span> *b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> idx = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    d[idx] = a[idx] * b[idx];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Host code</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> numBlocks;        <span class="comment">// Occupancy in terms of active blocks</span></span><br><span class="line">    <span class="type">int</span> blockSize = <span class="number">32</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// These variables are used to convert occupancy to warps</span></span><br><span class="line">    <span class="type">int</span> device;</span><br><span class="line">    cudaDeviceProp prop;</span><br><span class="line">    <span class="type">int</span> activeWarps;</span><br><span class="line">    <span class="type">int</span> maxWarps;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaGetDevice</span>(&amp;device);</span><br><span class="line">    <span class="built_in">cudaGetDeviceProperties</span>(&amp;prop, device);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaOccupancyMaxActiveBlocksPerMultiprocessor</span>(</span><br><span class="line">        &amp;numBlocks,</span><br><span class="line">        MyKernel,</span><br><span class="line">        blockSize,</span><br><span class="line">        <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    activeWarps = numBlocks * blockSize / prop.warpSize;</span><br><span class="line">    maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Occupancy: &quot;</span> &lt;&lt; (<span class="type">double</span>)activeWarps / maxWarps * <span class="number">100</span> &lt;&lt; <span class="string">&quot;%&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下面的代码示例根据用户输入配置了一个基于占用率的内核启动MyKernel。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Device code</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MyKernel</span><span class="params">(<span class="type">int</span> *array, <span class="type">int</span> arrayCount)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> idx = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="keyword">if</span> (idx &lt; arrayCount) &#123;</span><br><span class="line">        array[idx] *= array[idx];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Host code</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">launchMyKernel</span><span class="params">(<span class="type">int</span> *array, <span class="type">int</span> arrayCount)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> blockSize;      <span class="comment">// The launch configurator returned block size</span></span><br><span class="line">    <span class="type">int</span> minGridSize;    <span class="comment">// The minimum grid size needed to achieve the</span></span><br><span class="line">                        <span class="comment">// maximum occupancy for a full device</span></span><br><span class="line">                        <span class="comment">// launch</span></span><br><span class="line">    <span class="type">int</span> gridSize;       <span class="comment">// The actual grid size needed, based on input</span></span><br><span class="line">                        <span class="comment">// size</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaOccupancyMaxPotentialBlockSize</span>(</span><br><span class="line">        &amp;minGridSize,</span><br><span class="line">        &amp;blockSize,</span><br><span class="line">        (<span class="type">void</span>*)MyKernel,</span><br><span class="line">        <span class="number">0</span>,</span><br><span class="line">        arrayCount);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Round up according to array size</span></span><br><span class="line">    gridSize = (arrayCount + blockSize - <span class="number">1</span>) / blockSize;</span><br><span class="line"></span><br><span class="line">    MyKernel&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(array, arrayCount);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If interested, the occupancy can be calculated with</span></span><br><span class="line">    <span class="comment">// cudaOccupancyMaxActiveBlocksPerMultiprocessor</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>CUDA 工具包还在 <code>&lt;CUDA_Toolkit_Path&gt;/include/cuda_occupancy.h</code> 中为任何不能依赖 CUDA 软件堆栈的用例提供了一个自记录的独立占用计算器和启动配置器实现。 还提供了占用计算器的电子表格版本。 电子表格版本作为一种学习工具特别有用，它可以可视化更改影响占用率的参数（块大小、每个线程的寄存器和每个线程的共享内存）的影响。</p>
<h2 id="5-3-最大化存储吞吐量"><a href="#5-3-最大化存储吞吐量" class="headerlink" title="5.3 最大化存储吞吐量"></a>5.3 最大化存储吞吐量</h2><p>最大化应用程序的整体内存吞吐量的第一步是最小化低带宽的数据传输。</p>
<p>这意味着最大限度地减少主机和设备之间的数据传输，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#data-transfer-between-host-and-device">主机和设备之间的数据传输</a>中所述，因为它们的带宽比全局内存和设备之间的数据传输低得多。</p>
<p>这也意味着通过最大化片上内存的使用来最小化全局内存和设备之间的数据传输：共享内存和缓存（即计算能力 2.x 及更高版本的设备上可用的 L1 缓存和 L2 缓存、纹理缓存和常量缓存 适用于所有设备）。</p>
<p>共享内存相当于用户管理的缓存：应用程序显式分配和访问它。 如 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-c-runtime">CUDA Runtime</a> 所示，典型的编程模式是将来自设备内存的数据暂存到共享内存中； 换句话说，拥有一个块的每个线程：</p>
<ul>
<li>将数据从设备内存加载到共享内存，</li>
<li>与块的所有其他线程同步，以便每个线程可以安全地读取由不同线程填充的共享内存位置，<br>  处理共享内存中的数据，</li>
<li>如有必要，再次同步以确保共享内存已使用结果更新，</li>
<li>将结果写回设备内存。</li>
</ul>
<p>对于某些应用程序（例如，全局内存访问模式依赖于数据），传统的硬件管理缓存更适合利用数据局部性。如 Compute Capability 3.x、Compute Capability 7.x 和 Compute Capability 8.x 中所述，对于计算能力 3.x、7.x 和 8.x 的设备，相同的片上存储器用于 L1 和共享内存，以及有多少专用于 L1 与共享内存，可针对每个内核调用进行配置。</p>
<p>内核访问内存的吞吐量可能会根据每种内存类型的访问模式而变化一个数量级。因此，最大化内存吞吐量的下一步是根据<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">设备内存访问</a>中描述的最佳内存访问模式尽可能优化地组织内存访问。这种优化对于全局内存访问尤为重要，因为与可用的片上带宽和算术指令吞吐量相比，全局内存带宽较低，因此非最佳全局内存访问通常会对性能产生很大影响。</p>
<h3 id="5-3-1-设备与主机之间的数据传输"><a href="#5-3-1-设备与主机之间的数据传输" class="headerlink" title="5.3.1 设备与主机之间的数据传输"></a>5.3.1 设备与主机之间的数据传输</h3><p>应用程序应尽量减少主机和设备之间的数据传输。 实现这一点的一种方法是将更多代码从主机移动到设备，即使这意味着运行的内核没有提供足够的并行性以在设备上全效率地执行。 中间数据结构可以在设备内存中创建，由设备操作，并在没有被主机映射或复制到主机内存的情况下销毁。</p>
<p>此外，由于与每次传输相关的开销，将许多小传输批处理为单个大传输总是比单独进行每个传输执行得更好。</p>
<p>在具有前端总线的系统上，主机和设备之间的数据传输的更高性能是通过使用页锁定主机内存来实现的，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#page-locked-host-memory">页锁定主机内存</a>中所述。</p>
<p>此外，在使用映射页锁定内存（<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#mapped-memory">Mapped Memory</a>）时，无需分配任何设备内存，也无需在设备和主机内存之间显式复制数据。 每次内核访问映射内存时都会隐式执行数据传输。 为了获得最佳性能，这些内存访问必须与对全局内存的访问合并（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">设备内存访问</a>）。 假设它们映射的内存只被读取或写入一次，使用映射的页面锁定内存而不是设备和主机内存之间的显式副本可以提高性能。</p>
<p>在设备内存和主机内存在物理上相同的集成系统上，主机和设备内存之间的任何拷贝都是多余的，应该使用映射的页面锁定内存。 应用程序可以通过检查集成设备属性（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-enumeration">设备枚举</a>）是否等于 1 来查询设备是否集成。</p>
<h3 id="5-3-2-设备内存访问"><a href="#5-3-2-设备内存访问" class="headerlink" title="5.3.2 设备内存访问"></a>5.3.2 设备内存访问</h3><p>访问可寻址内存（即全局、本地、共享、常量或纹理内存）的指令可能需要多次重新发出，具体取决于内存地址在 warp 内线程中的分布。 分布如何以这种方式影响指令吞吐量特定于每种类型的内存，在以下部分中进行描述。 例如，对于全局内存，一般来说，地址越分散，吞吐量就越低。</p>
<p><strong>全局内存</strong></p>
<p>全局内存驻留在设备内存中，设备内存通过 32、64 或 128 字节内存事务访问。这些内存事务必须自然对齐：只有32字节、64字节或128字节的设备内存段按其大小对齐(即，其第一个地址是其大小的倍数)才能被内存事务读取或写入。</p>
<p>当一个 warp 执行一条访问全局内存的指令时，它会将 warp 内的线程的内存访问合并为一个或多个内存事务，具体取决于每个线程访问的大小以及内存地址在整个线程中的分布。线程。一般来说，需要的事务越多，除了线程访问的字之外，传输的未使用字也越多，相应地降低了指令吞吐量。例如，如果为每个线程的 4 字节访问生成一个 32 字节的内存事务，则吞吐量除以 8。</p>
<p>需要多少事务以及最终影响多少吞吐量取决于设备的计算能力。 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-3-0">Compute Capability 3.x、Compute Capability 5.x、Compute Capability 6.x、Compute Capability 7.x 和 Compute Capability 8.x</a> 提供了有关如何为各种计算能力处理全局内存访问的更多详细信息。</p>
<p>为了最大化全局内存吞吐量，因此通过以下方式最大化合并非常重要：</p>
<ul>
<li>遵循基于 Compute Capability 3.x、Compute Capability 5.x、Compute Capability 6.x、Compute Capability 7.x 和 Compute Capability 8.x 的最佳访问模式</li>
<li>使用满足以下“尺寸和对齐要求”部分中详述的大小和对齐要求的数据类型，</li>
<li>在某些情况下填充数据，例如，在访问二维数组时，如下面的二维数组部分所述。</li>
</ul>
<p><strong>尺寸和对齐要求</strong></p>
<p>全局内存指令支持读取或写入大小等于 1、2、4、8 或 16 字节的字。 当且仅当数据类型的大小为 1、2、4、8 或 16 字节并且数据为 对齐（即，它的地址是该大小的倍数）。</p>
<p>如果未满足此大小和对齐要求，则访问将编译为具有交错访问模式的多个指令，从而阻止这些指令完全合并。 因此，对于驻留在全局内存中的数据，建议使用满足此要求的类型。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#built-in-vector-types">内置矢量类型</a>自动满足对齐要求。</p>
<p>对于结构，大小和对齐要求可以由编译器使用对齐说明符 <code>__align__(8)</code> 或 <code>__align__(16)</code> 强制执行，例如:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">__align__</span>(<span class="number">8</span>) &#123;</span><br><span class="line">    <span class="type">float</span> x;</span><br><span class="line">    <span class="type">float</span> y;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">__align__</span>(<span class="number">16</span>) &#123;</span><br><span class="line">    <span class="type">float</span> x;</span><br><span class="line">    <span class="type">float</span> y;</span><br><span class="line">    <span class="type">float</span> z;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>驻留在全局内存中, 或由驱动程序, 或运行时 API 的内存分配例程之一返回的变量的任何地址始终与至少 256 字节对齐。</p>
<p>读取非自然对齐的 8 字节或 16 字节字会产生不正确的结果（相差几个字），因此必须特别注意保持这些类型的任何值或数组值的起始地址对齐。 一个可能容易被忽视的典型情况是使用一些自定义全局内存分配方案时，其中多个数组的分配（多次调用 <code>cudaMalloc()</code> 或 <code>cuMemAlloc()</code>）被单个大块内存的分配所取代分区为多个数组，在这种情况下，每个数组的起始地址都与块的起始地址有偏移。</p>
<p><strong>二维数组</strong></p>
<p>一个常见的全局内存访问模式是当索引 (tx,ty) 的每个线程使用以下地址访问一个宽度为 width 的二维数组的一个元素时，位于 type* 类型的地址 BaseAddress （其中 type 满足最大化中描述的使用要求 ）：</p>
<p>BaseAddress + width * ty + tx</p>
<p>为了使这些访问完全合并，线程块的宽度和数组的宽度都必须是 warp 大小的倍数。</p>
<p>特别是，这意味着如果一个数组的宽度不是这个大小的倍数，如果它实际上分配了一个宽度向上舍入到这个大小的最接近的倍数并相应地填充它的行，那么访问它的效率会更高。 参考手册中描述的 <code>cudaMallocPitch()</code> 和 <code>cuMemAllocPitch()</code> 函数以及相关的内存复制函数使程序员能够编写不依赖于硬件的代码来分配符合这些约束的数组。</p>
<p><strong>本地内存</strong></p>
<p>本地内存访问仅发生在<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#variable-memory-space-specifiers">可变内存空间说明符</a>中提到的某些自动变量上。 编译器可能放置在本地内存中的变量是：</p>
<ul>
<li>无法确定它们是否以常数索引的数组，</li>
<li>会占用过多寄存器空间的大型结构或数组，</li>
<li>如果内核使用的寄存器多于可用寄存器（这也称为寄存器溢出），则为任何变量。</li>
</ul>
<p>检查 PTX 汇编代码（通过使用 <code>-ptx</code> 或 <code>-keep</code> 选项进行编译）将判断在第一个编译阶段是否已将变量放置在本地内存中，因为它将使用 <code>.local</code> 助记符声明并使用 ld 访问<code>.local</code> 和 <code>st.local</code> 助记符。即使没有，后续编译阶段可能仍会做出其他决定，但如果他们发现它为目标体系结构消耗了过多的寄存器空间：使用 <code>cuobjdump</code> 检查 <code>cubin</code> 对象将判断是否是这种情况。此外，当使用 <code>--ptxas-options=-v</code> 选项编译时，编译器会报告每个内核 (<code>lmem</code>) 的总本地内存使用量。请注意，某些数学函数具有可能访问本地内存的实现路径。</p>
<p>本地内存空间驻留在<strong>设备内存</strong>中，因此本地内存访问与全局内存访问具有相同的高延迟和低带宽，并且与设备内存访问中所述的内存合并要求相同。然而，本地存储器的组织方式是通过连续的线程 ID 访问连续的 32 位字。因此，只要一个 warp 中的所有线程访问相同的相对地址（例如，数组变量中的相同索引，结构变量中的相同成员），访问就会完全合并。</p>
<p>在某些计算能力 3.x 的设备上，本地内存访问始终缓存在 L1 和 L2 中，其方式与全局内存访问相同（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-3-0">计算能力 3.x</a>）。</p>
<p>在计算能力 5.x 和 6.x 的设备上，本地内存访问始终以与全局内存访问相同的方式缓存在 L2 中（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-5-x">计算能力 5.x 和计算能力 6.x</a>）。</p>
<p><strong>共享内存</strong></p>
<p>因为它是片上的，所以共享内存比本地或全局内存具有更高的带宽和更低的延迟。</p>
<p>为了实现高带宽，共享内存被分成大小相等的内存模块，称为banks，可以同时访问。因此，可以同时处理由落在 n 个不同存储器组中的 n 个地址构成的任何存储器读取或写入请求，从而产生的总带宽是单个模块带宽的 n 倍。</p>
<p>但是，如果一个内存请求的两个地址落在同一个内存 bank 中，就会发生 bank 冲突，访问必须串行化。硬件根据需要将具有bank冲突的内存请求拆分为多个单独的无冲突请求，从而将吞吐量降低等于单独内存请求数量的总数。如果单独的内存请求的数量为 n，则称初始内存请求会导致 n-way bank 冲突。</p>
<p>因此，为了获得最佳性能，重要的是要了解内存地址如何映射到内存组，以便调度内存请求，从而最大限度地减少内存组冲突。这在<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-3-0">计算能力 3.x、计算能力 5.x、计算能力 6.x、计算能力 7.x 和计算能力 8.x</a> 中针对计算能力 3.x、5.x、6.x 7.x 和 8.x 的设备分别进行了描述。</p>
<p><strong>常量内存</strong></p>
<p>常量内存空间驻留在设备内存中，并缓存在常量缓存中。</p>
<p>然后，一个请求被拆分为与初始请求中不同的内存地址一样多的单独请求，从而将吞吐量降低等于单独请求数量的总数。</p>
<p>然后在缓存命中的情况下以常量缓存的吞吐量为结果请求提供服务，否则以设备内存的吞吐量提供服务。</p>
<p><strong>纹理和表面记忆</strong>  </p>
<p>纹理和表面内存空间驻留在设备内存中并缓存在纹理缓存中，因此纹理提取或表面读取仅在缓存未命中时从设备内存读取一次内存，否则只需从纹理缓存读取一次。 纹理缓存针对 2D 空间局部性进行了优化，因此读取 2D 中地址靠近在一起的纹理或表面的同一 warp 的线程将获得最佳性能。 此外，它专为具有恒定延迟的流式提取而设计； 缓存命中会降低 DRAM 带宽需求，但不会降低获取延迟。</p>
<p>通过纹理或表面获取读取设备内存具有一些优势，可以使其成为从全局或常量内存读取设备内存的有利替代方案：</p>
<ul>
<li>如果内存读取不遵循全局或常量内存读取必须遵循以获得良好性能的访问模式，则可以实现更高的带宽，前提是纹理提取或表面读取中存在局部性；</li>
<li>寻址计算由专用单元在内核外部执行；</li>
<li>打包的数据可以在单个操作中广播到单独的变量；</li>
<li>8 位和 16 位整数输入数据可以选择转换为 [0.0, 1.0] 或 [-1.0, 1.0] 范围内的 32 位浮点值（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-memory">纹理内存</a>）。</li>
</ul>
<h2 id="5-4最大化指令吞吐量"><a href="#5-4最大化指令吞吐量" class="headerlink" title="5.4最大化指令吞吐量"></a>5.4最大化指令吞吐量</h2><p>为了最大化指令吞吐量，应用程序应该：</p>
<ul>
<li>尽量减少使用低吞吐量的算术指令； 这包括在不影响最终结果的情况下用精度换取速度，例如使用内部函数而不是常规函数（内部函数在<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#intrinsic-functions">内部函数</a>中列出），单精度而不是双精度，或者将非规范化数字刷新为零；</li>
<li>最大限度地减少由控制流指令引起的发散warp，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#control-flow-instructions">控制流指令</a>中所述</li>
<li>减少指令的数量，例如，尽可能优化同步点（如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#synchronization-instruction">同步指令</a>中所述）或使用受限指针（如 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#restrict"><strong>restrict</strong></a> 中所述）。</li>
</ul>
<p>在本节中，吞吐量以每个多处理器每个时钟周期的操作数给出。 对于 32 的 warp 大小，一条指令对应于 32 次操作，因此如果 N 是每个时钟周期的操作数，则指令吞吐量为每个时钟周期的 N/32 条指令。</p>
<p>所有吞吐量都是针对一个多处理器的。 它们必须乘以设备中的多处理器数量才能获得整个设备的吞吐量。</p>
<h3 id="5-4-1-算数指令"><a href="#5-4-1-算数指令" class="headerlink" title="5.4.1 算数指令"></a>5.4.1 算数指令</h3><p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions__throughput-native-arithmetic-instructions">如下图所示</a><br><img src="/img/Throughput.png" alt="Throughput.png"></p>
<p>其他指令和功能是在本机指令之上实现的。不同计算能力的设备实现可能不同，编译后的native指令的数量可能会随着编译器版本的不同而波动。对于复杂的函数，可以有多个代码路径，具体取决于输入。 <code>cuobjdump</code> 可用于检查 <code>cubin</code> 对象中的特定实现。</p>
<p>一些函数的实现在 CUDA 头文件（<code>math_functions.h、device_functions.h</code>、…）上很容易获得。</p>
<p>通常，使用 <code>-ftz=true</code> 编译的代码（非规范化数字刷新为零）往往比使用 <code>-ftz=false</code> 编译的代码具有更高的性能。类似地，使用 <code>-prec-div=false</code>（不太精确的除法）编译的代码往往比使用 <code>-prec-div=true</code> 编译的代码具有更高的性能，使用 <code>-prec-sqrt=false</code>（不太精确的平方根）编译的代码往往比使用 <code>-prec-sqrt=true</code> 编译的代码具有更高的性能。 nvcc 用户手册更详细地描述了这些编译标志。 </p>
<p><strong>Single-Precision Floating-Point Division</strong></p>
<p><code>__fdividef(x, y)</code>（参见<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#intrinsic-functions">内部函数</a>）提供比除法运算符更快的单精度浮点除法。</p>
<p><strong>Single-Precision Floating-Point Reciprocal Square Root</strong></p>
<p>为了保留 IEEE-754 语义，编译器可以将 1.0/sqrtf() 优化为 <code>rsqrtf()</code>，仅当倒数和平方根都是近似值时（即 <code>-prec-div=false</code> 和 <code>-prec-sqrt=false</code>）。 因此，建议在需要时直接调用 <code>rsqrtf()</code>。</p>
<p><strong>Single-Precision Floating-Point Square Root</strong></p>
<p>单精度浮点平方根被实现为倒数平方根后跟倒数，而不是倒数平方根后跟乘法，因此它可以为 0 和无穷大提供正确的结果。</p>
<p><strong>Sine and Cosine</strong></p>
<p>sinf(x)、cosf(x)、tanf(x)、sincosf(x) 和相应的双精度指令更昂贵，如果参数 x 的量级很大，则更是如此。</p>
<p>更准确地说，参数缩减代码（参见实现的<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#mathematical-functions">数学函数</a>）包括两个代码路径，分别称为快速路径和慢速路径。</p>
<p>快速路径用于大小足够小的参数，并且基本上由几个乘加运算组成。 慢速路径用于量级较大的参数，并且包含在整个参数范围内获得正确结果所需的冗长计算。</p>
<p>目前，三角函数的参数缩减代码为单精度函数选择幅度小于105615.0f，双精度函数小于2147483648.0的参数选择快速路径。</p>
<p>由于慢速路径比快速路径需要更多的寄存器，因此尝试通过在本地内存中存储一些中间变量来降低慢速路径中的寄存器压力，这可能会因为本地内存的高延迟和带宽而影响性能（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">设备内存访问</a>）。 目前单精度函数使用28字节的本地内存，双精度函数使用44字节。 但是，确切的数量可能会发生变化。</p>
<p>由于在慢路径中需要进行冗长的计算和使用本地内存，当需要进行慢路径缩减时，与快速路径缩减相比，这些三角函数的吞吐量要低一个数量级。</p>
<p><strong>Integer Arithmetic</strong></p>
<p>整数除法和模运算的成本很高，因为它们最多可编译为 20 条指令。 在某些情况下，它们可以用按位运算代替：如果 n 是 2 的幂，则 <code>(i/n)</code> 等价于 <code>(i&gt;&gt;log2(n))</code> 并且 <code>(i%n)</code> 等价于<code>(i&amp;(n- 1))</code>; 如果 n 是字母，编译器将执行这些转换。</p>
<p><code>__brev</code> 和 <code>__popc</code> 映射到一条指令，而 <code>__brevll</code> 和 <code>__popcll</code> 映射到几条指令。</p>
<p><code>__[u]mul24</code> 是不再有任何理由使用的遗留内部函数。</p>
<p><strong>Half Precision Arithmetic</strong></p>
<p>为了实现 16 位精度浮点加法、乘法或乘法加法的良好性能，建议将 half2 数据类型用于半精度，将 <code>__nv_bfloat162</code> 用于 <code>__nv_bfloat16</code> 精度。 然后可以使用向量内在函数（例如 <code>__hadd2、__hsub2、__hmul2、__hfma2</code>）在一条指令中执行两个操作。 使用 <code>half2</code> 或 <code>__nv_bfloat162</code> 代替使用 <code>half</code> 或 <code>__nv_bfloat16</code> 的两个调用也可能有助于其他内在函数的性能，例如warp shuffles。</p>
<p>提供了内在的 <code>__halves2half2</code> 以将两个半精度值转换为 <code>half2</code> 数据类型。</p>
<p>提供了内在的 <code>__halves2bfloat162</code> 以将两个 <code>__nv_bfloat</code> 精度值转换为 <code>__nv_bfloat162</code> 数据类型。</p>
<p><strong>Type Conversion</strong></p>
<p>有时，编译器必须插入转换指令，从而引入额外的执行周期。 情况如下：</p>
<ul>
<li>对 char 或 short 类型的变量进行操作的函数，其操作数通常需要转换为 int，</li>
<li>双精度浮点常量（即那些没有任何类型后缀定义的常量）用作单精度浮点计算的输入（由 C/C++ 标准规定）。</li>
</ul>
<p>最后一种情况可以通过使用单精度浮点常量来避免，这些常量使用 f 后缀定义，例如 3.141592653589793f、1.0f、0.5f。</p>
<h3 id="5-4-2-控制流指令"><a href="#5-4-2-控制流指令" class="headerlink" title="5.4.2 控制流指令"></a>5.4.2 控制流指令</h3><p>任何流控制指令（<code>if、switch、do、for、while</code>）都可以通过导致相同 warp 的线程发散（即遵循不同的执行路径）来显着影响有效指令吞吐量。如果发生这种情况，则必须对不同的执行路径进行序列化，从而增加为此 warp 执行的指令总数。</p>
<p>为了在控制流取决于线程 ID 的情况下获得最佳性能，应编写控制条件以最小化发散warp的数量。这是可能的，因为正如 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture">SIMT 架构</a>中提到的那样，整个块的warp分布是确定性的。一个简单的例子是当控制条件仅取决于 (threadIdx / warpSize) 时，warpSize 是warp大小。在这种情况下，由于控制条件与warp完全对齐，因此没有warp发散。</p>
<p>有时，编译器可能会展开循环，或者它可能会通过使用分支预测来优化短 if 或 switch 块，如下所述。在这些情况下，任何warp都不会发散。程序员还可以使用#<code>pragma unroll</code> 指令控制循环展开（参见<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#pragma-unroll">#pragma unroll</a>）。</p>
<p>当使用分支预测时，其执行取决于控制条件的任何指令都不会被跳过。相反，它们中的每一个都与基于控制条件设置为真或假的每线程条件代码或预测相关联，尽管这些指令中的每一个都被安排执行，但实际上只有具有真预测的指令被执行。带有错误预测的指令不写入结果，也不评估地址或读取操作数。</p>
<h3 id="5-4-3-同步指令"><a href="#5-4-3-同步指令" class="headerlink" title="5.4.3 同步指令"></a>5.4.3 同步指令</h3><p>对于计算能力为 3.x 的设备，<code>__syncthreads()</code> 的吞吐量为每个时钟周期 128 次操作，对于计算能力为 6.0 的设备，每个时钟周期为 32 次操作，对于计算能力为 7.x 和 8.x 的设备，每个时钟周期为 16 次操作。 对于计算能力为 5.x、6.1 和 6.2 的设备，每个时钟周期 64 次操作。</p>
<p>请注意，<code>__syncthreads()</code> 可以通过强制多处理器空闲来影响性能，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">设备内存访问</a>中所述。</p>
<h2 id="5-5最小化内存抖动"><a href="#5-5最小化内存抖动" class="headerlink" title="5.5最小化内存抖动"></a>5.5最小化内存抖动</h2><p>经常不断地分配和释放内存的应用程序可能会发现分配调用往往会随着时间的推移而变慢，直至达到极限。这通常是由于将内存释放回操作系统供其自己使用的性质而预期的。为了在这方面获得最佳性能，我们建议如下：</p>
<ul>
<li>尝试根据手头的问题调整分配大小。不要尝试使用 <code>cudaMalloc / cudaMallocHost / cuMemCreate</code> 分配所有可用内存，因为这会强制内存立即驻留并阻止其他应用程序能够使用该内存。这会给操作系统调度程序带来更大的压力，或者只是阻止使用相同 GPU 的其他应用程序完全运行。</li>
<li>尝试在应用程序的早期以适当大小分配内存，并且仅在应用程序没有任何用途时分配内存。减少应用程序中的 <code>cudaMalloc</code>+<code>cudaFree</code> 调用次数，尤其是在性能关键区域。</li>
<li>如果应用程序无法分配足够的设备内存，请考虑使用其他内存类型，例如 <code>cudaMallocHost</code> 或 <code>cudaMallocManaged</code>，它们的性能可能不高，但可以使应用程序取得进展。</li>
<li>对于支持该功能的平台，<code>cudaMallocManaged</code> 允许超额订阅，并且启用正确的 <code>cudaMemAdvise</code> 策略，将允许应用程序保留 <code>cudaMalloc</code> 的大部分（如果不是全部）性能。 <code>cudaMallocManaged</code> 也不会强制分配在需要或预取之前驻留，从而减少操作系统调度程序的整体压力并更好地启用多原则用例。</li>
</ul>
<h1 id="附录A-支持GPU设备列表"><a href="#附录A-支持GPU设备列表" class="headerlink" title="附录A 支持GPU设备列表"></a>附录A 支持GPU设备列表</h1><p><a href="https://developer.nvidia.com/cuda-gpus">https://developer.nvidia.com/cuda-gpus</a> 列出了所有支持 CUDA 的设备及其计算能力。</p>
<p>可以使用运行时查询计算能力、多处理器数量、时钟频率、设备内存总量和其他属性（参见参考手册）。</p>
<h1 id="附录B-对C-扩展的详细描述"><a href="#附录B-对C-扩展的详细描述" class="headerlink" title="附录B 对C++扩展的详细描述"></a>附录B 对C++扩展的详细描述</h1><h2 id="B-1-函数执行空间说明符"><a href="#B-1-函数执行空间说明符" class="headerlink" title="B.1 函数执行空间说明符"></a>B.1 函数执行空间说明符</h2><p>函数执行空间说明符表示函数是在主机上执行还是在设备上执行，以及它是可从主机调用还是从设备调用。</p>
<h3 id="B-1-1-global"><a href="#B-1-1-global" class="headerlink" title="B.1.1 __global__"></a>B.1.1 __global__</h3><p><code>__global__</code> 执行空间说明符将函数声明为内核。 它的功能是：</p>
<ul>
<li>在设备上执行，</li>
<li>可从主机调用，</li>
<li>可在计算能力为 3.2 或更高的设备调用（有关更多详细信息，请参阅 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-dynamic-parallelism">CUDA 动态并行性</a>）。<br>  <code>__global__</code> 函数必须具有 void 返回类型，并且不能是类的成员。</li>
</ul>
<p>对 <code>__global__</code> 函数的任何调用都必须指定其执行配置，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration">执行配置</a>中所述。</p>
<p>对 <code>__global__</code> 函数的调用是异步的，这意味着它在设备完成执行之前返回。</p>
<h3 id="B-1-2-device"><a href="#B-1-2-device" class="headerlink" title="B.1.2 __device__"></a>B.1.2 __device__</h3><p><code>__device__</code> 执行空间说明符声明了一个函数：</p>
<ul>
<li>在设备上执行，</li>
<li>只能从设备调用。<br>  <code>__global__</code> 和 <code>__device__</code> 执行空间说明符不能一起使用。</li>
</ul>
<h3 id="B-1-3-host"><a href="#B-1-3-host" class="headerlink" title="B.1.3 __host__"></a>B.1.3 __host__</h3><p><code>__host__</code> 执行空间说明符声明了一个函数：</p>
<ul>
<li>在主机上执行，</li>
<li>只能从主机调用。<br>  相当于声明一个函数只带有 <code>__host__</code> 执行空间说明符，或者声明它没有任何 <code>__host__</code> <code>、__device__</code> 或 <code>__global__</code> 执行空间说明符； 在任何一种情况下，该函数都仅为主机编译。</li>
</ul>
<p><code>__global__</code> 和 <code>__host__</code> 执行空间说明符不能一起使用。</p>
<p>但是， <code>__device__</code> 和 <code>__host__</code> 执行空间说明符可以一起使用，在这种情况下，该函数是为主机和设备编译的。 Application Compatibility 中引入的 <code>__CUDA_ARCH__</code>宏可用于区分主机和设备之间的代码路径：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ __device__ <span class="title">func</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="meta">#<span class="keyword">if</span> __CUDA_ARCH__ &gt;= 800</span></span><br><span class="line">   <span class="comment">// Device code path for compute capability 8.x</span></span><br><span class="line"><span class="meta">#<span class="keyword">elif</span> __CUDA_ARCH__ &gt;= 700</span></span><br><span class="line">   <span class="comment">// Device code path for compute capability 7.x</span></span><br><span class="line"><span class="meta">#<span class="keyword">elif</span> __CUDA_ARCH__ &gt;= 600</span></span><br><span class="line">   <span class="comment">// Device code path for compute capability 6.x</span></span><br><span class="line"><span class="meta">#<span class="keyword">elif</span> __CUDA_ARCH__ &gt;= 500</span></span><br><span class="line">   <span class="comment">// Device code path for compute capability 5.x</span></span><br><span class="line"><span class="meta">#<span class="keyword">elif</span> __CUDA_ARCH__ &gt;= 300</span></span><br><span class="line">   <span class="comment">// Device code path for compute capability 3.x</span></span><br><span class="line"><span class="meta">#<span class="keyword">elif</span> !defined(__CUDA_ARCH__) </span></span><br><span class="line">   <span class="comment">// Host code path</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="B-1-4-Undefined-behavior"><a href="#B-1-4-Undefined-behavior" class="headerlink" title="B.1.4 Undefined behavior"></a>B.1.4 Undefined behavior</h3><p>在以下情况下，“跨执行空间”调用具有未定义的行为：</p>
<ul>
<li><code>__CUDA_ARCH__</code> 定义了, 从 <code>__global__</code> 、 <code>__device__</code> 或 <code>__host__ __device__</code> 函数到 <code>__host__</code> 函数的调用。</li>
<li><code>__CUDA_ARCH__</code> 未定义，从 <code>__host__</code> 函数内部调用 <code>__device__</code> 函数。</li>
</ul>
<h4 id="B-1-5-noinline-and-forceinline"><a href="#B-1-5-noinline-and-forceinline" class="headerlink" title="B.1.5 __noinline__ and __forceinline__"></a>B.1.5 <code>__noinline__</code> and <code>__forceinline__</code></h4><p>编译器在认为合适时内联任何 <code>__device__</code> 函数。</p>
<p><code>__noinline__</code> 函数限定符可用作提示编译器尽可能不要内联函数。</p>
<p><code>__forceinline__</code> 函数限定符可用于强制编译器内联函数。</p>
<p><code>__noinline__</code> 和 <code>__forceinline__</code> 函数限定符不能一起使用，并且两个函数限定符都不能应用于内联函数。</p>
<h2 id="B-2-Variable-Memory-Space-Specifiers"><a href="#B-2-Variable-Memory-Space-Specifiers" class="headerlink" title="B.2 Variable Memory Space Specifiers"></a>B.2 Variable Memory Space Specifiers</h2><p>变量内存空间说明符表示变量在设备上的内存位置。</p>
<p>在设备代码中声明的没有本节中描述的任何 <code>__device__</code>、<code>__shared__</code> 和 <code>__constant__</code> 内存空间说明符的自动变量通常驻留在寄存器中。 但是，在某些情况下，编译器可能会选择将其放置在本地内存中，这可能会产生不利的性能后果，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">设备内存访问</a>中所述。</p>
<h3 id="B-2-1-device"><a href="#B-2-1-device" class="headerlink" title="B.2.1 __device__"></a>B.2.1 __device__</h3><p><code>__device__</code> 内存空间说明符声明了一个驻留在设备上的变量。</p>
<p>在接下来的三个部分中定义的其他内存空间说明符中最多有一个可以与 <code>__device__</code> 一起使用，以进一步表示变量属于哪个内存空间。 如果它们都不存在，则变量：</p>
<ul>
<li>驻留在全局内存空间中，</li>
<li>具有创建它的 CUDA 上下文的生命周期，</li>
<li>每个设备都有一个不同的对象，</li>
<li>可从网格内的所有线程和主机通过运行时库 (<code>cudaGetSymbolAddress() / cudaGetSymbolSize() / cudaMemcpyToSymbol() / cudaMemcpyFromSymbol()</code>) 访问。</li>
</ul>
<h3 id="B-2-2-constant"><a href="#B-2-2-constant" class="headerlink" title="B.2.2. __constant__"></a>B.2.2. __constant__</h3><p><code>__constant__</code> 内存空间说明符，可选择与 <code>__device__</code> 一起使用，声明一个变量：</p>
<ul>
<li>驻留在常量的内存空间中，</li>
<li>具有创建它的 CUDA 上下文的生命周期，</li>
<li>每个设备都有一个不同的对象，</li>
<li>可从网格内的所有线程和主机通过运行时库 (<code>cudaGetSymbolAddress() / cudaGetSymbolSize() / cudaMemcpyToSymbol() / cudaMemcpyFromSymbol()</code>) 访问。</li>
</ul>
<h3 id="B-2-3-shared"><a href="#B-2-3-shared" class="headerlink" title="B.2.3 __shared__"></a>B.2.3 __shared__</h3><p><code>__shared__</code> 内存空间说明符，可选择与 <code>__device__</code> 一起使用，声明一个变量：</p>
<ul>
<li>驻留在线程块的共享内存空间中，</li>
<li>具有块的生命周期，</li>
<li>每个块有一个不同的对象，</li>
<li>只能从块内的所有线程访问，</li>
<li>没有固定地址。</li>
</ul>
<p>将共享内存中的变量声明为外部数组时，例如:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> __shared__ <span class="type">float</span> shared[];</span><br></pre></td></tr></table></figure>
<p>数组的大小在启动时确定（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration">执行配置</a>）。 以这种方式声明的所有变量都从内存中的相同地址开始，因此必须通过偏移量显式管理数组中变量的布局。 例如，如果想要在动态分配的共享内存中等价于，</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">short</span> array0[<span class="number">128</span>];</span><br><span class="line"><span class="type">float</span> array1[<span class="number">64</span>];</span><br><span class="line"><span class="type">int</span>   array2[<span class="number">256</span>];</span><br></pre></td></tr></table></figure>
<p>可以通过以下方式声明和初始化数组：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> __shared__ <span class="type">float</span> array[];</span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">func</span><span class="params">()</span>      <span class="comment">// __device__ or __global__ function</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">short</span>* array0 = (<span class="type">short</span>*)array; </span><br><span class="line">    <span class="type">float</span>* array1 = (<span class="type">float</span>*)&amp;array0[<span class="number">128</span>];</span><br><span class="line">    <span class="type">int</span>*   array2 =   (<span class="type">int</span>*)&amp;array1[<span class="number">64</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>请注意，指针需要与它们指向的类型对齐，因此以下代码不起作用，因为 array1 未对齐到 4 个字节。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> __shared__ <span class="type">float</span> array[];</span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">func</span><span class="params">()</span>      <span class="comment">// __device__ or __global__ function</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">short</span>* array0 = (<span class="type">short</span>*)array; </span><br><span class="line">    <span class="type">float</span>* array1 = (<span class="type">float</span>*)&amp;array0[<span class="number">127</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#vector-types__alignment-requirements-in-device-code">表 4</a> 列出了内置向量类型的对齐要求。</p>
<h3 id="B-2-4-managed"><a href="#B-2-4-managed" class="headerlink" title="B.2.4. managed"></a>B.2.4. <strong>managed</strong></h3><p><code>__managed__</code> 内存空间说明符，可选择与 <code>__device__</code> 一起使用，声明一个变量：</p>
<ul>
<li>可以从设备和主机代码中引用，例如，可以获取其地址，也可以直接从设备或主机功能读取或写入。</li>
<li>具有应用程序的生命周期。<br>  有关更多详细信息，请参阅 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#managed-specifier"><code>__managed__</code> 内存空间</a>说明符。</li>
</ul>
<h3 id="B-2-5-restrict"><a href="#B-2-5-restrict" class="headerlink" title="B.2.5. restrict"></a>B.2.5. <strong>restrict</strong></h3><p>nvcc 通过 <code>__restrict__</code> 关键字支持受限指针。</p>
<p>C99中引入了受限指针，以缓解存在于c类型语言中的混叠问题，这种问题抑制了从代码重新排序到公共子表达式消除等各种优化。</p>
<p>下面是一个受混叠问题影响的例子，使用受限指针可以帮助编译器减少指令的数量：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* a,</span></span></span><br><span class="line"><span class="params"><span class="function">         <span class="type">const</span> <span class="type">float</span>* b,</span></span></span><br><span class="line"><span class="params"><span class="function">         <span class="type">float</span>* c)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    c[<span class="number">0</span>] = a[<span class="number">0</span>] * b[<span class="number">0</span>];</span><br><span class="line">    c[<span class="number">1</span>] = a[<span class="number">0</span>] * b[<span class="number">0</span>];</span><br><span class="line">    c[<span class="number">2</span>] = a[<span class="number">0</span>] * b[<span class="number">0</span>] * a[<span class="number">1</span>];</span><br><span class="line">    c[<span class="number">3</span>] = a[<span class="number">0</span>] * a[<span class="number">1</span>];</span><br><span class="line">    c[<span class="number">4</span>] = a[<span class="number">0</span>] * b[<span class="number">0</span>];</span><br><span class="line">    c[<span class="number">5</span>] = b[<span class="number">0</span>];</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此处的效果是减少了内存访问次数和减少了计算次数。 这通过由于“缓存”负载和常见子表达式而增加的寄存器压力来平衡。</p>
<p>由于寄存器压力在许多 CUDA 代码中是一个关键问题，因此由于占用率降低，使用受限指针会对 CUDA 代码产生负面性能影响。</p>
<h2 id="B-3-Built-in-Vector-Types"><a href="#B-3-Built-in-Vector-Types" class="headerlink" title="B.3. Built-in Vector Types"></a>B.3. Built-in Vector Types</h2><h3 id="B-3-1-char-short-int-long-longlong-float-double"><a href="#B-3-1-char-short-int-long-longlong-float-double" class="headerlink" title="B.3.1. char, short, int, long, longlong, float, double"></a>B.3.1. char, short, int, long, longlong, float, double</h3><p>这些是从基本整数和浮点类型派生的向量类型。 它们是结构，第一个、第二个、第三个和第四个组件可以分别通过字段 <code>x、y、z 和 w</code> 访问。 它们都带有 <code>make_&lt;type name&gt;</code>形式的构造函数； 例如，</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">int2 <span class="title">make_int2</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y)</span></span>;</span><br></pre></td></tr></table></figure>
<p>它创建了一个带有 <code>value(x, y)</code> 的 <code>int2</code> 类型的向量。<br>向量类型的对齐要求在下表中有详细说明。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Type</th>
<th>Alignment</th>
</tr>
</thead>
<tbody>
<tr>
<td>char1, uchar1</td>
<td>1</td>
</tr>
<tr>
<td>char2, uchar2</td>
<td>2</td>
</tr>
<tr>
<td>char3, uchar3</td>
<td>1</td>
</tr>
<tr>
<td>char4, uchar4</td>
<td>4</td>
</tr>
<tr>
<td>short1, ushort1</td>
<td>2</td>
</tr>
<tr>
<td>short2, ushort2</td>
<td>4</td>
</tr>
<tr>
<td>short3, ushort3</td>
<td>2</td>
</tr>
<tr>
<td>short4, ushort4</td>
<td>8</td>
</tr>
<tr>
<td>int1, uint1</td>
<td>4</td>
</tr>
<tr>
<td>int2, uint2</td>
<td>8</td>
</tr>
<tr>
<td>int3, uint3</td>
<td>4</td>
</tr>
<tr>
<td>int4, uint4</td>
<td>16</td>
</tr>
<tr>
<td>long1, ulong1</td>
<td>4 if sizeof(long) is equal to sizeof(int) 8, otherwise</td>
</tr>
<tr>
<td>long2, ulong2</td>
<td>8 if sizeof(long) is equal to sizeof(int), 16, otherwise</td>
</tr>
<tr>
<td>long3, ulong3</td>
<td>4 if sizeof(long) is equal to sizeof(int), 8, otherwise</td>
</tr>
<tr>
<td>long4, ulong4</td>
<td>16</td>
</tr>
<tr>
<td>longlong1, ulonglong1</td>
<td>8</td>
</tr>
<tr>
<td>longlong2, ulonglong2</td>
<td>16</td>
</tr>
<tr>
<td>longlong3, ulonglong3</td>
<td>8</td>
</tr>
<tr>
<td>longlong4, ulonglong4</td>
<td>16</td>
</tr>
<tr>
<td>float1</td>
<td>4</td>
</tr>
<tr>
<td>float2</td>
<td>8</td>
</tr>
<tr>
<td>float3</td>
<td>4</td>
</tr>
<tr>
<td>float4</td>
<td>16</td>
</tr>
<tr>
<td>double1</td>
<td>8</td>
</tr>
<tr>
<td>double2</td>
<td>16</td>
</tr>
<tr>
<td>double3</td>
<td>8</td>
</tr>
<tr>
<td>double4</td>
<td>16</td>
</tr>
</tbody>
</table>
</div>
<h3 id="B-3-2-dim3"><a href="#B-3-2-dim3" class="headerlink" title="B.3.2. dim3"></a>B.3.2. dim3</h3><p>此类型是基于 uint3 的整数向量类型，用于指定维度。 定义 dim3 类型的变量时，任何未指定的组件都将初始化为 1。</p>
<h2 id="B-4-Built-in-Variables"><a href="#B-4-Built-in-Variables" class="headerlink" title="B.4. Built-in Variables"></a>B.4. Built-in Variables</h2><h3 id="B-4-1-gridDim"><a href="#B-4-1-gridDim" class="headerlink" title="B.4.1. gridDim"></a>B.4.1. gridDim</h3><p>该变量的类型为 <code>dim3</code>（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#dim3"> dim3</a>）并包含网格的尺寸。</p>
<h3 id="B-4-2-blockIdx"><a href="#B-4-2-blockIdx" class="headerlink" title="B.4.2. blockIdx"></a>B.4.2. blockIdx</h3><p>该变量是 <code>uint3</code> 类型（请参见 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#vector-types">char、short、int、long、longlong、float、double</a>）并包含网格内的块索引。</p>
<h3 id="B-4-3-blockDim"><a href="#B-4-3-blockDim" class="headerlink" title="B.4.3. blockDim"></a>B.4.3. blockDim</h3><p>该变量的类型为 <code>dim3</code>（请参阅 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#dim3">dim3</a>）并包含块的尺寸。</p>
<h3 id="B-4-4-threadIdx"><a href="#B-4-4-threadIdx" class="headerlink" title="B.4.4. threadIdx"></a>B.4.4. threadIdx</h3><p>此变量是 <code>uint3</code> 类型（请参见 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#vector-types">char、short、int、long、longlong、float、double</a> ）并包含块内的线程索引。</p>
<h3 id="B-4-5-warpSize"><a href="#B-4-5-warpSize" class="headerlink" title="B.4.5. warpSize"></a>B.4.5. warpSize</h3><p>该变量是 <code>int</code> 类型，包含线程中的 <code>warp</code> 大小（有关 <code>warp</code> 的定义，请参见 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture">SIMT Architecture</a>）。</p>
<h2 id="B-5-Memory-Fence-Functions"><a href="#B-5-Memory-Fence-Functions" class="headerlink" title="B.5. Memory Fence Functions"></a>B.5. Memory Fence Functions</h2><p>CUDA 编程模型假设设备具有弱序内存模型，即 CUDA 线程将数据写入共享内存、全局内存、页面锁定主机内存或对等设备的内存的顺序不一定是 观察到数据被另一个 CUDA 或主机线程写入的顺序。 两个线程在没有同步的情况下读取或写入同一内存位置是未定义的行为。</p>
<p>在以下示例中，thread 1 执行 writeXY()，而thread 2 执行 readXY()。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">int</span> X = <span class="number">1</span>, Y = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">writeXY</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    X = <span class="number">10</span>;</span><br><span class="line">    Y = <span class="number">20</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">readXY</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> B = Y;</span><br><span class="line">    <span class="type">int</span> A = X;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>两个线程同时从相同的内存位置 X 和 Y 读取和写入。 任何数据竞争都是未定义的行为，并且没有定义的语义。 A 和 B 的结果值可以是任何值。</p>
<p>内存栅栏函数可用于强制对内存访问进行一些排序。 内存栅栏功能在强制执行排序的范围上有所不同，但它们独立于访问的内存空间（共享内存、全局内存、页面锁定的主机内存和对等设备的内存）。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence_block();</span><br></pre></td></tr></table></figure>
<p>请确保：</p>
<ul>
<li>线程在调用 <strong>threadfence_block() 之前对所有内存的所有写入都被线程的块中的所有线程观察到. 这发生在调用线程在调用 </strong>threadfence_block() 之后对内存的所有写入之前；</li>
<li>线程在调用 <strong>threadfence_block() 之前对所有内存进行的所有读取都排在线程在调用 </strong>threadfence_block() 之后对所有内存的所有读取之前。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence();</span><br></pre></td></tr></table></figure>
<p>充当调用线程块中所有线程的 <code>__threadfence_block()</code> 并且还确保在调用 <code>__threadfence()</code>之后调用线程对所有内存的写入不会被设备中的任何线程观察到在任何写入之前发生 调用线程在调用 __threadfence() 之前产生的所有内存。 请注意，要使这种排序保证为真，观察线程必须真正观察内存而不是它的缓存版本； 这可以通过使用 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#volatile-qualifier">volatile 限定符</a>中详述的 volatile 关键字来确保。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence_system()</span><br></pre></td></tr></table></figure>
<p>充当调用线程块中所有线程的 <code>__threadfence_block()</code>，并确保设备中的所有线程、主机线程和所有线程在调用 <code>__threadfence_system()</code> 之前对调用线程所做的所有内存的所有写入都被观察到 对等设备中的线程在调用 <code>__threadfence_system()</code> 之后调用线程对所有内存的所有写入之前发生。</p>
<p><code>__threadfence_system()</code> 仅受计算能力 2.x 及更高版本的设备支持。</p>
<p>在前面的代码示例中，我们可以在代码中插入栅栏，如下所示：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">int</span> X = <span class="number">1</span>, Y = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">writeXY</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    X = <span class="number">10</span>;</span><br><span class="line">    __threadfence();</span><br><span class="line">    Y = <span class="number">20</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">readXY</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> B = Y;</span><br><span class="line">    __threadfence();</span><br><span class="line">    <span class="type">int</span> A = X;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于此代码，可以观察到以下结果：</p>
<ul>
<li>A 等于 1，B 等于 2，</li>
<li>A 等于 10，B 等于 2，</li>
<li>A 等于 10，B 等于 20。</li>
</ul>
<p>第四种结果是不可能的，因为第一次写入必须在第二次写入之前可见。 如果线程 1 和 2 属于同一个块，使用 <strong>threadfence_block() 就足够了。 如果线程 1 和 2 不属于同一个块，如果它们是来自同一设备的 CUDA 线程，则必须使用 </strong>threadfence()，如果它们是来自两个不同设备的 CUDA 线程，则必须使用 __threadfence_system()。</p>
<p>一个常见的用例是当线程消耗由其他线程产生的一些数据时，如以下内核代码示例所示，该内核在一次调用中计算 N 个数字的数组的总和。 每个块首先对数组的一个子集求和，并将结果存储在全局内存中。 当所有块都完成后，最后一个完成的块从全局内存中读取这些部分和中的每一个，并将它们相加以获得最终结果。 为了确定哪个块最后完成，每个块自动递增一个计数器以表示它已完成计算和存储其部分和（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions">原子函数</a>关于原子函数）。 最后一个块是接收等于 <code>gridDim.x-1</code> 的计数器值的块。 如果在存储部分和和递增计数器之间没有设置栅栏，则计数器可能会在存储部分和之前递增，因此可能会到达 gridDim.x-1 并让最后一个块在实际更新之前在Global Memory中开始读取部分和 。</p>
<p>作者添加: 开发者指南中原文介绍threadfence的时候,比较长比较绕,可能对于新手开发朋友来说比较难理解.作者觉得,可以简单的理解为一种等待行为.让Warp中线程运行到threadfence这里等一下, 不然可能产生上面的还没写完,下面的就开始读的问题. 这种写后读,可能会读到错误的数据.</p>
<p>内存栅栏函数只影响线程内存操作的顺序； 它们不确保这些内存操作对其他线程可见（就像 <code>__syncthreads()</code> 对块内的线程所做的那样（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#synchronization-functions">同步函数</a>））。 在下面的代码示例中，通过将结果变量声明为volatile 来确保对结果变量的内存操作的可见性（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#volatile-qualifier">volatile 限定符</a>）。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">unsigned</span> <span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line">__shared__ <span class="type">bool</span> isLastBlockDone;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">sum</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* array, <span class="type">unsigned</span> <span class="type">int</span> N,</span></span></span><br><span class="line"><span class="params"><span class="function">                    <span class="keyword">volatile</span> <span class="type">float</span>* result)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Each block sums a subset of the input array.</span></span><br><span class="line">    <span class="type">float</span> partialSum = <span class="built_in">calculatePartialSum</span>(array, N);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Thread 0 of each block stores the partial sum</span></span><br><span class="line">        <span class="comment">// to global memory. The compiler will use </span></span><br><span class="line">        <span class="comment">// a store operation that bypasses the L1 cache</span></span><br><span class="line">        <span class="comment">// since the &quot;result&quot; variable is declared as</span></span><br><span class="line">        <span class="comment">// volatile. This ensures that the threads of</span></span><br><span class="line">        <span class="comment">// the last block will read the correct partial</span></span><br><span class="line">        <span class="comment">// sums computed by all other blocks.</span></span><br><span class="line">        result[blockIdx.x] = partialSum;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Thread 0 makes sure that the incrementation</span></span><br><span class="line">        <span class="comment">// of the &quot;count&quot; variable is only performed after</span></span><br><span class="line">        <span class="comment">// the partial sum has been written to global memory.</span></span><br><span class="line">        __threadfence();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Thread 0 signals that it is done.</span></span><br><span class="line">        <span class="type">unsigned</span> <span class="type">int</span> value = <span class="built_in">atomicInc</span>(&amp;count, gridDim.x);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Thread 0 determines if its block is the last</span></span><br><span class="line">        <span class="comment">// block to be done.</span></span><br><span class="line">        isLastBlockDone = (value == (gridDim.x - <span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Synchronize to make sure that each thread reads</span></span><br><span class="line">    <span class="comment">// the correct value of isLastBlockDone.</span></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (isLastBlockDone) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// The last block sums the partial sums</span></span><br><span class="line">        <span class="comment">// stored in result[0 .. gridDim.x-1]</span></span><br><span class="line">        <span class="type">float</span> totalSum = <span class="built_in">calculateTotalSum</span>(result);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Thread 0 of last block stores the total sum</span></span><br><span class="line">            <span class="comment">// to global memory and resets the count</span></span><br><span class="line">            <span class="comment">// varialble, so that the next kernel call</span></span><br><span class="line">            <span class="comment">// works properly.</span></span><br><span class="line">            result[<span class="number">0</span>] = totalSum;</span><br><span class="line">            count = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="B-6-Synchronization-Functions"><a href="#B-6-Synchronization-Functions" class="headerlink" title="B.6. Synchronization Functions"></a>B.6. Synchronization Functions</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncthreads();</span><br></pre></td></tr></table></figure>
<p>等待直到线程块中的所有线程都达到这一点，并且这些线程在 <code>__syncthreads()</code> 之前进行的所有全局和共享内存访问对块中的所有线程都是可见的。</p>
<p><code>__syncthreads()</code> 用于协调同一块的线程之间的通信。 当块中的某些线程访问共享或全局内存中的相同地址时，对于其中一些内存访问，可能存在先读后写、先读后写或先写后写的风险。 通过在这些访问之间同步线程可以避免这些数据危害。</p>
<p><code>__syncthreads()</code> 允许在条件代码中使用，但前提是条件在整个线程块中的计算结果相同，否则代码执行可能会挂起或产生意外的副作用。</p>
<p>计算能力 2.x 及更高版本的设备支持以下描述的三种 __syncthreads() 变体。</p>
<p><code>int __syncthreads_count(int predicate)</code>与 __syncthreads() 相同，其附加功能是它为块的所有线程评估predicate并返回predicate评估为非零的线程数。</p>
<p><code>int __syncthreads_and(int predicate)</code> 与 __syncthreads() 相同，其附加功能是它为块的所有线程计算predicate，并且当且仅当predicate对所有线程的计算结果都为非零时才返回非零。</p>
<p><code>int __syncthreads_or(int predicate)</code> 与 __syncthreads() 相同，其附加功能是它为块的所有线程评估predicate，并且当且仅当predicate对其中任何一个线程评估为非零时才返回非零。</p>
<p><code>void __syncwarp(unsigned mask=0xffffffff)</code> 将导致正在执行的线程等待，直到 mask 中命名的所有 warp 通道都执行了 <strong>syncwarp()（具有相同的掩码），然后再恢复执行。 掩码中命名的所有未退出线程必须执行具有相同掩码的相应 </strong>syncwarp()，否则结果未定义。</p>
<p>执行 <strong>syncwarp() 保证参与屏障的线程之间的内存排序。 因此，warp 中希望通过内存进行通信的线程可以存储到内存，执行 </strong>syncwarp()，然后安全地读取 warp 中其他线程存储的值。</p>
<p>注意：对于 .target sm_6x 或更低版本，mask 中的所有线程在收敛时必须执行相同的 __syncwarp()，并且 mask 中所有值的并集必须等于活动掩码。 否则，行为未定义。</p>
<h2 id="B-7-Mathematical-Functions"><a href="#B-7-Mathematical-Functions" class="headerlink" title="B.7. Mathematical Functions"></a>B.7. Mathematical Functions</h2><p>参考手册列出了设备代码支持的所有 C/C++ 标准库数学函数和仅设备代码支持的所有内部函数。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#mathematical-functions-appendix">数学函数</a>为其中一些函数提供精度信息。</p>
<h2 id="B-8-Texture-Functions"><a href="#B-8-Texture-Functions" class="headerlink" title="B.8. Texture Functions"></a>B.8. Texture Functions</h2><p>纹理对象在 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-object-api">Texture Object API</a> 中描述</p>
<p>纹理引用在 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-reference-api">[[DEPRECATED]] 纹理引用 API</a> 中描述</p>
<p>纹理提取在<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-fetching">纹理提取</a>中进行了描述。</p>
<h3 id="B-8-1-Texture-Object-API"><a href="#B-8-1-Texture-Object-API" class="headerlink" title="B.8.1. Texture Object API"></a>B.8.1. Texture Object API</h3><h4 id="B-8-1-1-tex1Dfetch"><a href="#B-8-1-1-tex1Dfetch" class="headerlink" title="B.8.1.1. tex1Dfetch()"></a>B.8.1.1. tex1Dfetch()</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">template&lt;class T&gt;</span><br><span class="line">T <span class="title function_">tex1Dfetch</span><span class="params">(cudaTextureObject_t texObj, <span class="type">int</span> x)</span>;</span><br></pre></td></tr></table></figure>
<p>从使用整数纹理坐标 x 的一维纹理对象 texObj 指定的线性内存区域中获取。 tex1Dfetch() 仅适用于非归一化坐标，因此仅支持边界和钳位寻址模式。 它不执行任何纹理过滤。 对于整数类型，它可以选择将整数提升为单精度浮点数。</p>
<h4 id="B-8-1-2。-tex1D"><a href="#B-8-1-2。-tex1D" class="headerlink" title="B.8.1.2。 tex1D()"></a>B.8.1.2。 tex1D()</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">template&lt;class T&gt;</span><br><span class="line">T <span class="title function_">tex1D</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x)</span>;</span><br></pre></td></tr></table></figure>
<p>从使用纹理坐标 x 的一维纹理对象 texObj 指定的 CUDA 数组中获取。</p>
<h4 id="B-8-1-3。-tex1DLod"><a href="#B-8-1-3。-tex1DLod" class="headerlink" title="B.8.1.3。 tex1DLod()"></a>B.8.1.3。 tex1DLod()</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">template&lt;class T&gt;</span><br><span class="line">T <span class="title function_">tex1DLod</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">float</span> level)</span>;</span><br></pre></td></tr></table></figure>
<p>使用细节级别的纹理坐标 x 从一维纹理对象 texObj 指定的 CUDA 数组中获取。</p>
<h4 id="B-8-1-4。-tex1DGrad"><a href="#B-8-1-4。-tex1DGrad" class="headerlink" title="B.8.1.4。 tex1DGrad()"></a>B.8.1.4。 tex1DGrad()</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">template&lt;class T&gt;</span><br><span class="line">T <span class="title function_">tex1DGrad</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">float</span> dx, <span class="type">float</span> dy)</span>;</span><br></pre></td></tr></table></figure>
<p>从使用纹理坐标 x 的一维纹理对象 texObj 指定的 CUDA 数组中获取。细节层次来源于 X 梯度 dx 和 Y 梯度 dy。</p>
<h4 id="B-8-1-5。-tex2D"><a href="#B-8-1-5。-tex2D" class="headerlink" title="B.8.1.5。 tex2D()"></a>B.8.1.5。 tex2D()</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">template&lt;class T&gt;</span><br><span class="line">T <span class="title function_">tex2D</span><span class="params">(cudaTextureObject_t texObj, 浮点 x, 浮点 y)</span>;</span><br></pre></td></tr></table></figure>
<p>从 CUDA 数组或由二维纹理对象 texObj 使用纹理坐标 (x,y) 指定的线性内存区域获取。</p>
<h4 id="B-8-1-6。-tex2DLod"><a href="#B-8-1-6。-tex2DLod" class="headerlink" title="B.8.1.6。 tex2DLod()"></a>B.8.1.6。 tex2DLod()</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">template&lt;class T&gt;</span><br><span class="line"><span class="title function_">tex2DLod</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">float</span> y, <span class="type">float</span> level)</span>;</span><br></pre></td></tr></table></figure>
<p>从 CUDA 数组或二维纹理对象 texObj 指定的线性内存区域中获取，使用细节级别的纹理坐标 (x,y)。</p>
<h4 id="B-8-1-7。-tex2DGrad"><a href="#B-8-1-7。-tex2DGrad" class="headerlink" title="B.8.1.7。 tex2DGrad()"></a>B.8.1.7。 tex2DGrad()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">tex2DGrad</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">float</span> y,</span></span></span><br><span class="line"><span class="params"><span class="function">            float2 dx，float2 dy）；</span></span></span><br></pre></td></tr></table></figure>
<p>使用纹理坐标 (x,y) 从二维纹理对象 texObj 指定的 CUDA 数组中获取。细节层次来源于 dx 和 dy 梯度。</p>
<h4 id="B-8-1-8。-tex3D"><a href="#B-8-1-8。-tex3D" class="headerlink" title="B.8.1.8。 tex3D()"></a>B.8.1.8。 tex3D()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">tex3D</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">float</span> y, <span class="type">float</span> z)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用纹理坐标 (x,y,z) 从三维纹理对象 texObj 指定的 CUDA 数组中获取。</p>
<h4 id="B-8-1-9。-tex3DLod"><a href="#B-8-1-9。-tex3DLod" class="headerlink" title="B.8.1.9。 tex3DLod()"></a>B.8.1.9。 tex3DLod()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">tex3DLod</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">float</span> y, <span class="type">float</span> z, <span class="type">float</span> level)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用细节级别的纹理坐标 <code>(x,y,z)</code>从 CUDA 数组或由三维纹理对象 <code>texObj</code> 指定的线性内存区域获取。</p>
<h4 id="B-8-1-10。-tex3DGrad"><a href="#B-8-1-10。-tex3DGrad" class="headerlink" title="B.8.1.10。 tex3DGrad()"></a>B.8.1.10。 tex3DGrad()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">tex3DGrad</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">float</span> y, <span class="type">float</span> z,</span></span></span><br><span class="line"><span class="params"><span class="function">            float4 dx，float4 dy）；</span></span></span><br></pre></td></tr></table></figure>
<p>从由三维纹理对象 <code>texObj</code> 指定的 CUDA 数组中获取，使用纹理坐标 (x,y,z) 在从 <code>X</code> 和 <code>Y</code> 梯度 <code>dx</code> 和 <code>dy</code> 派生的细节级别。</p>
<h4 id="B-8-1-11。-tex1DLlayered"><a href="#B-8-1-11。-tex1DLlayered" class="headerlink" title="B.8.1.11。 tex1DLlayered()"></a>B.8.1.11。 tex1DLlayered()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">tex1DLayered</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">int</span> layer)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用纹理坐标 <code>x</code>和索<code>layer</code>从一维纹理对象 <code>texObj</code> 指定的 CUDA 数组中获取，如分层纹理中所述</p>
<h4 id="B-8-1-12。-tex1DLlayeredLod"><a href="#B-8-1-12。-tex1DLlayeredLod" class="headerlink" title="B.8.1.12。 tex1DLlayeredLod()"></a>B.8.1.12。 tex1DLlayeredLod()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">tex1DLayeredLod</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">int</span> layer, <span class="type">float</span> level)</span></span>;</span><br></pre></td></tr></table></figure>
<p>从使用纹理坐标 <code>x</code> 和细节级别级别的图层 <code>layer</code> 的一维分层纹理指定的 CUDA 数组中获取。</p>
<h4 id="B-8-1-13。-tex1DLlayeredGrad"><a href="#B-8-1-13。-tex1DLlayeredGrad" class="headerlink" title="B.8.1.13。 tex1DLlayeredGrad()"></a>B.8.1.13。 tex1DLlayeredGrad()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">tex1DLayeredGrad</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">int</span> layer,</span></span></span><br><span class="line"><span class="params"><span class="function">                   <span class="type">float</span> dx, <span class="type">float</span> dy)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用纹理坐标 <code>x</code> 和从 <code>dx</code> 和 <code>dy</code> 梯度派生的细节层次从 <code>layer</code> 的一维分层纹理指定的 CUDA 数组中获取。</p>
<h4 id="B-8-1-14。-tex2DLlayered"><a href="#B-8-1-14。-tex2DLlayered" class="headerlink" title="B.8.1.14。 tex2DLlayered()"></a>B.8.1.14。 tex2DLlayered()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">tex2DLayered</span><span class="params">(cudaTextureObject_t texObj,</span></span></span><br><span class="line"><span class="params"><span class="function">               <span class="type">float</span> <span class="type">float</span> y，<span class="type">int</span> layer）；</span></span></span><br></pre></td></tr></table></figure>
<p>使用纹理坐标 <code>(x,y)</code> 和索引层从二维纹理对象 <code>texObj</code> 指定的 CUDA 数组中获取，如分层纹理中所述。</p>
<h4 id="B-8-1-15。-tex2DLlayeredLod"><a href="#B-8-1-15。-tex2DLlayeredLod" class="headerlink" title="B.8.1.15。 tex2DLlayeredLod()"></a>B.8.1.15。 tex2DLlayeredLod()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">tex2DLayeredLod</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">float</span> y, <span class="type">int</span> layer,</span></span></span><br><span class="line"><span class="params"><span class="function">                  <span class="type">float</span> level）；</span></span></span><br></pre></td></tr></table></figure>
<p>使用纹理坐标 <code>(x,y)</code> 从 <code>layer</code>  的二维分层纹理指定的 CUDA 数组中获取。</p>
<h4 id="B-8-1-16。-tex2DLlayeredGrad"><a href="#B-8-1-16。-tex2DLlayeredGrad" class="headerlink" title="B.8.1.16。 tex2DLlayeredGrad()"></a>B.8.1.16。 tex2DLlayeredGrad()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">tex2DLayeredGrad</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">float</span> y, <span class="type">int</span> layer,</span></span></span><br><span class="line"><span class="params"><span class="function">                   float2 dx，float2 dy）；</span></span></span><br></pre></td></tr></table></figure>
<p>使用纹理坐标 <code>(x,y)</code> 和从 <code>dx</code> 和 <code>dy</code>  梯度派生的细节层次从 layer  的二维分层纹理指定的 CUDA 数组中获取。</p>
<h4 id="B-8-1-17。-texCubemap"><a href="#B-8-1-17。-texCubemap" class="headerlink" title="B.8.1.17。 texCubemap()"></a>B.8.1.17。 texCubemap()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">texCubemap</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">float</span> y, <span class="type">float</span> z)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用纹理坐标 <code>(x,y,z)</code> 获取由立方体纹理对象 <code>texObj</code> 指定的 CUDA 数组，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cubemap-textures">立方体纹理</a>中所述。</p>
<h4 id="B-8-1-18。-texCubemapLod"><a href="#B-8-1-18。-texCubemapLod" class="headerlink" title="B.8.1.18。 texCubemapLod()"></a>B.8.1.18。 texCubemapLod()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">texCubemapLod</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">float</span>, y, <span class="type">float</span> z,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">float</span> level）；</span></span></span><br></pre></td></tr></table></figure>
<p>使用<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cubemap-textures">立方体纹理</a>中描述的纹理坐标 (x,y,z) 从立方体纹理对象 <code>texObj</code> 指定的 CUDA 数组中获取。使用的详细级别由<code>level</code>给出。</p>
<h4 id="B-8-1-19。-texCubemapLayered"><a href="#B-8-1-19。-texCubemapLayered" class="headerlink" title="B.8.1.19。 texCubemapLayered()"></a>B.8.1.19。 texCubemapLayered()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">texCubemapLayered</span><span class="params">(cudaTextureObject_t texObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                    <span class="type">float</span> x，<span class="type">float</span> y，<span class="type">float</span> z，<span class="type">int</span> layer）；</span></span></span><br></pre></td></tr></table></figure>
<p>使用纹理坐标 (x,y,z) 和索引层从立方体分层纹理对象 <code>texObj</code> 指定的 CUDA 数组中获取，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cubemap-layered-textures">立方体分层纹理</a>中所述。</p>
<h4 id="B-8-1-20。-texCubemapLayeredLod"><a href="#B-8-1-20。-texCubemapLayeredLod" class="headerlink" title="B.8.1.20。 texCubemapLayeredLod()"></a>B.8.1.20。 texCubemapLayeredLod()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">texCubemapLayeredLod</span><span class="params">(cudaTextureObject_t texObj, <span class="type">float</span> x, <span class="type">float</span> y, <span class="type">float</span> z,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">int</span> layer，<span class="type">float</span> level）；</span></span></span><br></pre></td></tr></table></figure>
<p>使用纹理坐标 (x,y,z) 和索引层从立方体分层纹理对象 <code>texObj</code> 指定的 CUDA 数组中获取，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cubemap-layered-textures">立方体分层纹理</a>中所述，在细节级别级别。</p>
<h4 id="B-8-1-21。-tex2Dgather"><a href="#B-8-1-21。-tex2Dgather" class="headerlink" title="B.8.1.21。 tex2Dgather()"></a>B.8.1.21。 tex2Dgather()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">tex2Dgather</span><span class="params">(cudaTextureObject_t texObj,</span></span></span><br><span class="line"><span class="params"><span class="function">              <span class="type">float</span> x，<span class="type">float</span> y，<span class="type">int</span> comp = <span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure>
<p>从 2D 纹理对象 <code>texObj</code> 指定的 CUDA 数组中获取，使用纹理坐标 x 和 y 以及<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-gather">纹理采集</a>中描述的 <code>comp</code> 参数。</p>
<h3 id="B-8-2-Texture-Reference-API"><a href="#B-8-2-Texture-Reference-API" class="headerlink" title="B.8.2. Texture Reference API"></a>B.8.2. Texture Reference API</h3><h4 id="B-8-2-1-tex1Dfetch"><a href="#B-8-2-1-tex1Dfetch" class="headerlink" title="B.8.2.1. tex1Dfetch()"></a>B.8.2.1. tex1Dfetch()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> DataType&gt;</span></span><br><span class="line"><span class="function">Type <span class="title">tex1Dfetch</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">   texture&lt;DataType, cudaTextureType1D,</span></span></span><br><span class="line"><span class="params"><span class="function">           cudaReadModeElementType&gt; texRef,</span></span></span><br><span class="line"><span class="params"><span class="function">   <span class="type">int</span> x)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">tex1Dfetch</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">   texture&lt;<span class="type">unsigned</span> <span class="type">char</span>, cudaTextureType1D,</span></span></span><br><span class="line"><span class="params"><span class="function">           cudaReadModeNormalizedFloat&gt; texRef,</span></span></span><br><span class="line"><span class="params"><span class="function">   <span class="type">int</span> x)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">tex1Dfetch</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">   texture&lt;<span class="type">signed</span> <span class="type">char</span>, cudaTextureType1D,</span></span></span><br><span class="line"><span class="params"><span class="function">           cudaReadModeNormalizedFloat&gt; texRef,</span></span></span><br><span class="line"><span class="params"><span class="function">   <span class="type">int</span> x)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">tex1Dfetch</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">   texture&lt;<span class="type">unsigned</span> <span class="type">short</span>, cudaTextureType1D,</span></span></span><br><span class="line"><span class="params"><span class="function">           cudaReadModeNormalizedFloat&gt; texRef,</span></span></span><br><span class="line"><span class="params"><span class="function">   <span class="type">int</span> x)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">tex1Dfetch</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">   texture&lt;<span class="type">signed</span> <span class="type">short</span>, cudaTextureType1D,</span></span></span><br><span class="line"><span class="params"><span class="function">           cudaReadModeNormalizedFloat&gt; texRef,</span></span></span><br><span class="line"><span class="params"><span class="function">   <span class="type">int</span> x)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用整数纹理坐标 x 从绑定到一维纹理引用 <code>texRef</code> 的线性内存区域中获取。 <code>tex1Dfetch()</code> 仅适用于非归一化坐标，因此仅支持边界和钳位寻址模式。 它不执行任何纹理过滤。 对于整数类型，它可以选择将整数提升为单精度浮点数。</p>
<p>除了上面显示的功能外，还支持 2 元组和 4 元组； 例如：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">float4 <span class="title">tex1Dfetch</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">   texture&lt;uchar4, cudaTextureType1D,</span></span></span><br><span class="line"><span class="params"><span class="function">           cudaReadModeNormalizedFloat&gt; texRef,</span></span></span><br><span class="line"><span class="params"><span class="function">   <span class="type">int</span> x)</span></span>;</span><br></pre></td></tr></table></figure>
<h2 id="B-9-Surface-Functions"><a href="#B-9-Surface-Functions" class="headerlink" title="B.9. Surface Functions"></a>B.9. Surface Functions</h2><p>Surface 函数仅受计算能力 2.0 及更高版本的设备支持。</p>
<p>Surface 对象在 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#surface-object-api-appendix">Surface Object API</a> 中描述</p>
<p>Surface引用在<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#surface-reference-api-appendix">Surface引用 API</a> 中描述。</p>
<p>在下面的部分中，<code>boundaryMode</code> 指定了边界模式，即如何处理超出范围的表面坐标； 它等于 <code>cudaBoundaryModeClamp</code>，在这种情况下，超出范围的坐标被限制到有效范围，或 <code>cudaBoundaryModeZero</code>，在这种情况下，超出范围的读取返回零并且忽略超出范围的写入，或者 <code>cudaBoundaryModeTrap</code>， 在这种情况下，超出范围的访问会导致内核执行失败。</p>
<h3 id="B-9-1-Surface-Object-API"><a href="#B-9-1-Surface-Object-API" class="headerlink" title="B.9.1. Surface Object API"></a>B.9.1. Surface Object API</h3><h4 id="B-9-1-1-surf1Dread"><a href="#B-9-1-1-surf1Dread" class="headerlink" title="B.9.1.1. surf1Dread()"></a>B.9.1.1. surf1Dread()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">surf1Dread</span><span class="params">(cudaSurfaceObject_t surfObj, <span class="type">int</span> x,</span></span></span><br><span class="line"><span class="params"><span class="function">               boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用坐标 x 读取由一维surface对象 <code>surfObj</code> 指定的 CUDA 数组。</p>
<h4 id="B-9-1-2-surf1Dwrite"><a href="#B-9-1-2-surf1Dwrite" class="headerlink" title="B.9.1.2. surf1Dwrite"></a>B.9.1.2. surf1Dwrite</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf1Dwrite</span><span class="params">(T data,</span></span></span><br><span class="line"><span class="params"><span class="function">                  cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                  <span class="type">int</span> x,</span></span></span><br><span class="line"><span class="params"><span class="function">                  boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>将数据写入由坐标 x 处的一维surface对象 <code>surfObj</code> 指定的 CUDA 数组。</p>
<h4 id="B-9-1-3-surf2Dread"><a href="#B-9-1-3-surf2Dread" class="headerlink" title="B.9.1.3. surf2Dread()"></a>B.9.1.3. surf2Dread()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">surf2Dread</span><span class="params">(cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">              <span class="type">int</span> x, <span class="type">int</span> y,</span></span></span><br><span class="line"><span class="params"><span class="function">              boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf2Dread</span><span class="params">(T* data,</span></span></span><br><span class="line"><span class="params"><span class="function">                 cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">int</span> x, <span class="type">int</span> y,</span></span></span><br><span class="line"><span class="params"><span class="function">                 boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用坐标 x 和 y 读取二维surface对象 <code>surfObj</code> 指定的 CUDA 数组。</p>
<h4 id="B-9-1-4-surf2Dwrite"><a href="#B-9-1-4-surf2Dwrite" class="headerlink" title="B.9.1.4 surf2Dwrite()"></a>B.9.1.4 surf2Dwrite()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf2Dwrite</span><span class="params">(T data,</span></span></span><br><span class="line"><span class="params"><span class="function">                  cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                  <span class="type">int</span> x, <span class="type">int</span> y,</span></span></span><br><span class="line"><span class="params"><span class="function">                  boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>将值数据写入由坐标 x 和 y 处的二维surface对象 <code>surfObj</code> 指定的 CUDA 数组。</p>
<h4 id="B-9-1-5-surf3Dread"><a href="#B-9-1-5-surf3Dread" class="headerlink" title="B.9.1.5. surf3Dread()"></a>B.9.1.5. surf3Dread()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">surf3Dread</span><span class="params">(cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">              <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> z,</span></span></span><br><span class="line"><span class="params"><span class="function">              boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf3Dread</span><span class="params">(T* data,</span></span></span><br><span class="line"><span class="params"><span class="function">                 cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> z,</span></span></span><br><span class="line"><span class="params"><span class="function">                 boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用坐标 x、y 和 z 读取由三维surface对象 <code>surfObj</code> 指定的 CUDA 数组。</p>
<h4 id="B-9-1-6-surf3Dwrite"><a href="#B-9-1-6-surf3Dwrite" class="headerlink" title="B.9.1.6. surf3Dwrite()"></a>B.9.1.6. surf3Dwrite()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf3Dwrite</span><span class="params">(T data,</span></span></span><br><span class="line"><span class="params"><span class="function">                  cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                  <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> z,</span></span></span><br><span class="line"><span class="params"><span class="function">                  boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>将值数据写入由坐标 x、y 和 z 处的三维surface对象 <code>surfObj</code> 指定的 CUDA 数组。</p>
<h4 id="B-9-1-7-surf1DLayeredread"><a href="#B-9-1-7-surf1DLayeredread" class="headerlink" title="B.9.1.7. surf1DLayeredread()"></a>B.9.1.7. surf1DLayeredread()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">surf1DLayeredread</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">                 cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">int</span> x, <span class="type">int</span> layer,</span></span></span><br><span class="line"><span class="params"><span class="function">                 boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf1DLayeredread</span><span class="params">(T data,</span></span></span><br><span class="line"><span class="params"><span class="function">                 cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">int</span> x, <span class="type">int</span> layer,</span></span></span><br><span class="line"><span class="params"><span class="function">                 boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用坐标 x 和索引层读取一维分层surface对象 <code>surfObj</code> 指定的 CUDA 数组。</p>
<h4 id="B-9-1-8-surf1DLayeredwrite"><a href="#B-9-1-8-surf1DLayeredwrite" class="headerlink" title="B.9.1.8. surf1DLayeredwrite()"></a>B.9.1.8. surf1DLayeredwrite()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf1DLayeredwrite</span><span class="params">(T data,</span></span></span><br><span class="line"><span class="params"><span class="function">                 cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">int</span> x, <span class="type">int</span> layer,</span></span></span><br><span class="line"><span class="params"><span class="function">                 boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>将值数据写入坐标 x 和索引层的二维分层surface对象 <code>surfObj</code> 指定的 CUDA 数组。</p>
<h4 id="B-9-1-9-surf2DLayeredread"><a href="#B-9-1-9-surf2DLayeredread" class="headerlink" title="B.9.1.9. surf2DLayeredread()"></a>B.9.1.9. surf2DLayeredread()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">surf2DLayeredread</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">                 cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> layer,</span></span></span><br><span class="line"><span class="params"><span class="function">                 boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf2DLayeredread</span><span class="params">(T data,</span></span></span><br><span class="line"><span class="params"><span class="function">                         cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> layer,	</span></span></span><br><span class="line"><span class="params"><span class="function">                         boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用坐标 x 和 y 以及索引层读取二维分层surface对象 <code>surfObj</code> 指定的 CUDA 数组。</p>
<h4 id="B-9-1-10-surf2DLayeredwrite"><a href="#B-9-1-10-surf2DLayeredwrite" class="headerlink" title="B.9.1.10. surf2DLayeredwrite()"></a>B.9.1.10. surf2DLayeredwrite()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf2DLayeredwrite</span><span class="params">(T data,</span></span></span><br><span class="line"><span class="params"><span class="function">                          cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> layer,</span></span></span><br><span class="line"><span class="params"><span class="function">                          boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>将数据写入由坐标 x 和 y 处的一维分层surface对象 <code>surfObj</code> 和索引层指定的 CUDA 数组。</p>
<h4 id="B-9-1-11-surfCubemapread"><a href="#B-9-1-11-surfCubemapread" class="headerlink" title="B.9.1.11. surfCubemapread()"></a>B.9.1.11. surfCubemapread()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">surfCubemapread</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">                 cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> face,</span></span></span><br><span class="line"><span class="params"><span class="function">                 boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surfCubemapread</span><span class="params">(T data,</span></span></span><br><span class="line"><span class="params"><span class="function">                 cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> face,</span></span></span><br><span class="line"><span class="params"><span class="function">                 boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用坐标 x 和 y 以及面索引 face 读取立方体surface对象 <code>surfObj</code> 指定的 CUDA 数组。</p>
<h4 id="B-9-1-12-surfCubemapwrite"><a href="#B-9-1-12-surfCubemapwrite" class="headerlink" title="B.9.1.12. surfCubemapwrite()"></a>B.9.1.12. surfCubemapwrite()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surfCubemapwrite</span><span class="params">(T data,</span></span></span><br><span class="line"><span class="params"><span class="function">                 cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> face,</span></span></span><br><span class="line"><span class="params"><span class="function">                 boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>将数据写入由立方体对象 surfObj 在坐标 x 和 y 以及面索引 face 处指定的 CUDA 数组。 </p>
<h4 id="B-9-1-13-surfCubemapLayeredread"><a href="#B-9-1-13-surfCubemapLayeredread" class="headerlink" title="B.9.1.13. surfCubemapLayeredread()"></a>B.9.1.13. surfCubemapLayeredread()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function">T <span class="title">surfCubemapLayeredread</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">             cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">             <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> layerFace,</span></span></span><br><span class="line"><span class="params"><span class="function">             boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surfCubemapLayeredread</span><span class="params">(T data,</span></span></span><br><span class="line"><span class="params"><span class="function">             cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">             <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> layerFace,</span></span></span><br><span class="line"><span class="params"><span class="function">             boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用坐标 x 和 y 以及索引 <code>layerFace</code> 读取由立方体分层surface对象 <code>surfObj</code> 指定的 CUDA 数组。</p>
<h4 id="B-9-1-14-surfCubemapLayeredwrite"><a href="#B-9-1-14-surfCubemapLayeredwrite" class="headerlink" title="B.9.1.14. surfCubemapLayeredwrite()"></a>B.9.1.14. surfCubemapLayeredwrite()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> T&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surfCubemapLayeredwrite</span><span class="params">(T data,</span></span></span><br><span class="line"><span class="params"><span class="function">             cudaSurfaceObject_t surfObj,</span></span></span><br><span class="line"><span class="params"><span class="function">             <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> layerFace,</span></span></span><br><span class="line"><span class="params"><span class="function">             boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>将数据写入由立方体分层对象 <code>surfObj</code> 在坐标 x 和 y 以及索引 layerFace 指定的 CUDA 数组。</p>
<h3 id="B-9-2-Surface-Reference-API"><a href="#B-9-2-Surface-Reference-API" class="headerlink" title="B.9.2. Surface Reference API"></a>B.9.2. Surface Reference API</h3><h4 id="B-9-2-1-surf1Dread"><a href="#B-9-2-1-surf1Dread" class="headerlink" title="B.9.2.1. surf1Dread()"></a>B.9.2.1. surf1Dread()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function">Type <span class="title">surf1Dread</span><span class="params">(surface&lt;<span class="type">void</span>, cudaSurfaceType1D&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf1Dread</span><span class="params">(Type data,</span></span></span><br><span class="line"><span class="params"><span class="function">                surface&lt;<span class="type">void</span>, cudaSurfaceType1D&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用坐标 x 读取绑定到一维surface引用 <code>surfRef</code> 的 CUDA 数组。</p>
<h4 id="B-9-2-2-surf1Dwrite"><a href="#B-9-2-2-surf1Dwrite" class="headerlink" title="B.9.2.2. surf1Dwrite"></a>B.9.2.2. surf1Dwrite</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf1Dwrite</span><span class="params">(Type data,</span></span></span><br><span class="line"><span class="params"><span class="function">                 surface&lt;<span class="type">void</span>, cudaSurfaceType1D&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">int</span> x,</span></span></span><br><span class="line"><span class="params"><span class="function">                 boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<h4 id="B-9-2-3-surf2Dread"><a href="#B-9-2-3-surf2Dread" class="headerlink" title="B.9.2.3. surf2Dread()"></a>B.9.2.3. surf2Dread()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function">Type <span class="title">surf2Dread</span><span class="params">(surface&lt;<span class="type">void</span>, cudaSurfaceType2D&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x, <span class="type">int</span> y,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf2Dread</span><span class="params">(Type* data,</span></span></span><br><span class="line"><span class="params"><span class="function">                surface&lt;<span class="type">void</span>, cudaSurfaceType2D&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x, <span class="type">int</span> y,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用坐标 x 和 y 读取绑定到二维surface引用 <code>surfRef</code> 的 CUDA 数组。</p>
<h4 id="B-9-2-4-surf2Dwrite"><a href="#B-9-2-4-surf2Dwrite" class="headerlink" title="B.9.2.4. surf2Dwrite()"></a>B.9.2.4. surf2Dwrite()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf3Dwrite</span><span class="params">(Type data,</span></span></span><br><span class="line"><span class="params"><span class="function">                 surface&lt;<span class="type">void</span>, cudaSurfaceType3D&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> z,</span></span></span><br><span class="line"><span class="params"><span class="function">                 boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>将值数据写入绑定到坐标 x 和 y 处的二维surface引用 <code>surfRef</code> 的 CUDA 数组。 </p>
<h4 id="B-9-2-5-surf3Dread"><a href="#B-9-2-5-surf3Dread" class="headerlink" title="B.9.2.5. surf3Dread()"></a>B.9.2.5. surf3Dread()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function">Type <span class="title">surf3Dread</span><span class="params">(surface&lt;<span class="type">void</span>, cudaSurfaceType3D&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> z,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf3Dread</span><span class="params">(Type* data,</span></span></span><br><span class="line"><span class="params"><span class="function">                surface&lt;<span class="type">void</span>, cudaSurfaceType3D&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> z,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用坐标 x、y 和 z 读取绑定到三维surface引用 <code>surfRef</code> 的 CUDA 数组。</p>
<h4 id="B-9-2-6-surf3Dwrite"><a href="#B-9-2-6-surf3Dwrite" class="headerlink" title="B.9.2.6. surf3Dwrite()"></a>B.9.2.6. surf3Dwrite()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf3Dwrite</span><span class="params">(Type data,</span></span></span><br><span class="line"><span class="params"><span class="function">                 surface&lt;<span class="type">void</span>, cudaSurfaceType3D&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> z,</span></span></span><br><span class="line"><span class="params"><span class="function">                 boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>将数据写入绑定到坐标 x、y 和 z 处的surface引用 <code>surfRef</code> 的 CUDA 数组。</p>
<h4 id="B-9-2-7-surf1DLayeredread"><a href="#B-9-2-7-surf1DLayeredread" class="headerlink" title="B.9.2.7. surf1DLayeredread()"></a>B.9.2.7. surf1DLayeredread()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function">Type <span class="title">surf1DLayeredread</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">                surface&lt;<span class="type">void</span>, cudaSurfaceType1DLayered&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x, <span class="type">int</span> layer,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf1DLayeredread</span><span class="params">(Type data,</span></span></span><br><span class="line"><span class="params"><span class="function">                surface&lt;<span class="type">void</span>, cudaSurfaceType1DLayered&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x, <span class="type">int</span> layer,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用坐标 x 和索引层读取绑定到一维分层surface引用 <code>surfRef</code> 的 CUDA 数组。</p>
<h4 id="B-9-2-8-surf1DLayeredwrite"><a href="#B-9-2-8-surf1DLayeredwrite" class="headerlink" title="B.9.2.8. surf1DLayeredwrite()"></a>B.9.2.8. surf1DLayeredwrite()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf1DLayeredwrite</span><span class="params">(Type data,</span></span></span><br><span class="line"><span class="params"><span class="function">                surface&lt;<span class="type">void</span>, cudaSurfaceType1DLayered&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x, <span class="type">int</span> layer,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>将数据写入绑定到坐标 x 和索引层的二维分层surface引用 <code>surfRef</code> 的 CUDA 数组。</p>
<h4 id="B-9-2-9-surf2DLayeredread"><a href="#B-9-2-9-surf2DLayeredread" class="headerlink" title="B.9.2.9. surf2DLayeredread()"></a>B.9.2.9. surf2DLayeredread()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function">Type <span class="title">surf2DLayeredread</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">                surface&lt;<span class="type">void</span>, cudaSurfaceType2DLayered&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> layer,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf2DLayeredread</span><span class="params">(Type data,</span></span></span><br><span class="line"><span class="params"><span class="function">                surface&lt;<span class="type">void</span>, cudaSurfaceType2DLayered&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> layer,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用坐标 x 和 y 以及索引层读取绑定到二维分层surface引用 <code>surfRef</code> 的 CUDA 数组。</p>
<h4 id="B-9-2-10-surf2DLayeredwrite"><a href="#B-9-2-10-surf2DLayeredwrite" class="headerlink" title="B.9.2.10. surf2DLayeredwrite()"></a>B.9.2.10. surf2DLayeredwrite()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surf2DLayeredwrite</span><span class="params">(Type data,</span></span></span><br><span class="line"><span class="params"><span class="function">                surface&lt;<span class="type">void</span>, cudaSurfaceType2DLayered&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> layer,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>将数据写入绑定到坐标 x 和 y 处的一维分层surface引用 <code>surfRef</code> 和索引层的 CUDA 数组。</p>
<h4 id="B-9-2-11-surfCubemapread"><a href="#B-9-2-11-surfCubemapread" class="headerlink" title="B.9.2.11. surfCubemapread()"></a>B.9.2.11. surfCubemapread()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function">Type <span class="title">surfCubemapread</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">                surface&lt;<span class="type">void</span>, cudaSurfaceTypeCubemap&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> face,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surfCubemapread</span><span class="params">(Type data,</span></span></span><br><span class="line"><span class="params"><span class="function">                surface&lt;<span class="type">void</span>, cudaSurfaceTypeCubemap&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> face,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用坐标 x 和 y 以及面索引 <code>face</code> 读取绑定到立方体surface引用 <code>surfRef</code> 的 CUDA 数组。</p>
<h4 id="B-9-2-12-surfCubemapwrite"><a href="#B-9-2-12-surfCubemapwrite" class="headerlink" title="B.9.2.12. surfCubemapwrite()"></a>B.9.2.12. surfCubemapwrite()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surfCubemapwrite</span><span class="params">(Type data,</span></span></span><br><span class="line"><span class="params"><span class="function">                surface&lt;<span class="type">void</span>, cudaSurfaceTypeCubemap&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> face,</span></span></span><br><span class="line"><span class="params"><span class="function">                boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>将数据写入绑定到位于坐标 x , y 和面索引 face 处的立方体引用 <code>surfRef</code> 的 CUDA 数组。</p>
<p>B.9.2.13. surfCubemapLayeredread()</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function">Type <span class="title">surfCubemapLayeredread</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">            surface&lt;<span class="type">void</span>, cudaSurfaceTypeCubemapLayered&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">            <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> layerFace,</span></span></span><br><span class="line"><span class="params"><span class="function">            boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surfCubemapLayeredread</span><span class="params">(Type data,</span></span></span><br><span class="line"><span class="params"><span class="function">            surface&lt;<span class="type">void</span>, cudaSurfaceTypeCubemapLayered&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">            <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> layerFace,</span></span></span><br><span class="line"><span class="params"><span class="function">            boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>使用坐标 x 和 y 以及索引 layerFace 读取绑定到立方体分层surface引用 <code>surfRef</code> 的 CUDA 数组。</p>
<h4 id="B-9-2-14-surfCubemapLayeredwrite"><a href="#B-9-2-14-surfCubemapLayeredwrite" class="headerlink" title="B.9.2.14. surfCubemapLayeredwrite()"></a>B.9.2.14. surfCubemapLayeredwrite()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> Type&gt;</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">surfCubemapLayeredwrite</span><span class="params">(Type data,</span></span></span><br><span class="line"><span class="params"><span class="function">            surface&lt;<span class="type">void</span>, cudaSurfaceTypeCubemapLayered&gt; surfRef,</span></span></span><br><span class="line"><span class="params"><span class="function">            <span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> layerFace,</span></span></span><br><span class="line"><span class="params"><span class="function">            boundaryMode = cudaBoundaryModeTrap)</span></span>;</span><br></pre></td></tr></table></figure>
<p>将数据写入绑定到位于坐标 x , y 和索引 layerFace处的立方体分层引用 <code>surfRef</code>  的 CUDA 数组。</p>
<h2 id="B-10-Read-Only-Data-Cache-Load-Function"><a href="#B-10-Read-Only-Data-Cache-Load-Function" class="headerlink" title="B.10. Read-Only Data Cache Load Function"></a>B.10. Read-Only Data Cache Load Function</h2><p>只读数据缓存加载功能仅支持计算能力3.5及以上的设备。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">T __ldg(<span class="type">const</span> T* address);</span><br></pre></td></tr></table></figure>
<p>返回位于地址<code>address</code>的 T 类型数据，其中 T 为 <code>char、signed char、short、int、long、long longunsigned char、unsigned short、unsigned int、unsigned long、unsigned long long、char2、char4、short2、short4、 int2、int4、longlong2uchar2、uchar4、ushort2、ushort4、uint2、uint4、ulonglong2float、float2、float4、double</code> 或 <code>double2</code>. 包含 <code>cuda_fp16.h</code> 头文件，T 可以是 <code>__half</code> 或 <code>__half2</code>。 同样，包含 cuda_bf16.h 头文件后，T 也可以是 <code>__nv_bfloat16</code> 或 <code>__nv_bfloat162</code>。 该操作缓存在只读数据缓存中（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#global-memory-3-0">全局内存</a>）。</p>
<h2 id="B-11-Load-Functions-Using-Cache-Hints"><a href="#B-11-Load-Functions-Using-Cache-Hints" class="headerlink" title="B.11. Load Functions Using Cache Hints"></a>B.11. Load Functions Using Cache Hints</h2><p>这些加载功能仅受计算能力 3.5 及更高版本的设备支持。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">T __ldcg(<span class="type">const</span> T* address);</span><br><span class="line">T __ldca(<span class="type">const</span> T* address);</span><br><span class="line">T __ldcs(<span class="type">const</span> T* address);</span><br><span class="line">T __ldlu(<span class="type">const</span> T* address);</span><br><span class="line">T __ldcv(<span class="type">const</span> T* address);</span><br></pre></td></tr></table></figure>
<p>返回位于地址<code>address</code>的 T 类型数据，其中 T 为 <code>char、signed char、short、int、long、long longunsigned char、unsigned short、unsigned int、unsigned long、unsigned long long、char2、char4、short2、short4、 int2、int4、longlong2uchar2、uchar4、ushort2、ushort4、uint2、uint4、ulonglong2float、float2、float4、double 或 double2</code>。 包含 <code>cuda_fp16.h</code> 头文件，T 可以是 <code>__half</code> 或 <code>__half2</code>。 同样，包含 cuda_bf16.h 头文件后，T 也可以是 <code>__nv_bfloat16</code> 或 <code>__nv_bfloat162</code>。 该操作正在使用相应的缓存运算符（请参阅 <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#cache-operators">PTX ISA</a>）</p>
<h2 id="B-12-Store-Functions-Using-Cache-Hints"><a href="#B-12-Store-Functions-Using-Cache-Hints" class="headerlink" title="B.12. Store Functions Using Cache Hints"></a>B.12. Store Functions Using Cache Hints</h2><p>这些存储功能仅受计算能力 3.5 及更高版本的设备支持。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __stwb(T* address, T value);</span><br><span class="line"><span class="type">void</span> __stcg(T* address, T value);</span><br><span class="line"><span class="type">void</span> __stcs(T* address, T value);</span><br><span class="line"><span class="type">void</span> __stwt(T* address, T value);</span><br></pre></td></tr></table></figure>
<p>将类型 T 的<code>value</code>参数存储到地址 <code>address</code> 的位置，其中 T 是 <code>char、signed char、short、int、long、long longunsigned char、unsigned short、unsigned int、unsigned long、unsigned long long、char2、char4、short2 、short4、int2、int4、longlong2uchar2、uchar4、ushort2、ushort4、uint2、uint4、ulonglong2float、float2、float4、double 或 double2</code>。 包含 <code>cuda_fp16.h</code> 头文件，T 可以是 <code>__half</code> 或 <code>__half2</code>。 同样，包含 cuda_bf16.h 头文件后，T 也可以是 <code>__nv_bfloat16</code> 或 <code>__nv_bfloat162</code>。 该操作正在使用相应的缓存运算符（请参阅 <a href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#cache-operators">PTX ISA</a>）</p>
<h2 id="B-13-Time-Function"><a href="#B-13-Time-Function" class="headerlink" title="B.13. Time Function"></a>B.13. Time Function</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">clock_t</span> <span class="title">clock</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="type">long</span> <span class="type">long</span> <span class="type">int</span> <span class="title">clock64</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure>
<p>在设备代码中执行时，返回每个时钟周期递增的每个多处理器计数器的值。 在内核开始和结束时对该计数器进行采样，获取两个样本的差异，并记录每个线程的结果，为每个线程提供设备完全执行线程所花费的时钟周期数的度量， 但不是设备实际执行线程指令所花费的时钟周期数。 前一个数字大于后者，因为线程是时间切片的。</p>
<h2 id="B-14-Atomic-Functions"><a href="#B-14-Atomic-Functions" class="headerlink" title="B.14. Atomic Functions"></a>B.14. Atomic Functions</h2><p>原子函数对驻留在全局或共享内存中的一个 32 位或 64 位字执行读-修改-写原子操作。 例如，<code>atomicAdd()</code> 在全局或共享内存中的某个地址读取一个字，向其中加一个数字，然后将结果写回同一地址。 该操作是原子的，因为它保证在不受其他线程干扰的情况下执行。 换句话说，在操作完成之前，没有其他线程可以访问该地址。 原子函数不充当内存栅栏，也不意味着内存操作的同步或排序约束（有关内存栅栏的更多详细信息，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-fence-functions">内存栅栏函数</a>）。 原子函数只能在设备函数中使用。</p>
<p>原子函数仅相对于特定集合的线程执行的其他操作是原子的：</p>
<ul>
<li>系统范围的原子：当前程序中所有线程的原子操作，包括系统中的其他 CPU 和 GPU。 这些以 <code>_system</code> 为后缀，例如 <code>atomicAdd_system</code>。</li>
<li>设备范围的原子：当前程序中所有 CUDA 线程的原子操作，在与当前线程相同的计算设备中执行。 这些没有后缀，只是以操作命名，例如 <code>atomicAdd</code>。</li>
<li>Block-wide atomics：当前程序中所有 CUDA 线程的原子操作，在与当前线程相同的线程块中执行。 这些以 _block 为后缀，例如 <code>atomicAdd_block</code>。</li>
</ul>
<p>在以下示例中，CPU 和 GPU 都以原子方式更新地址 <code>addr</code> 处的整数值：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">mykernel</span><span class="params">(<span class="type">int</span> *addr)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">atomicAdd_system</span>(addr, <span class="number">10</span>);       <span class="comment">// only available on devices with compute capability 6.x</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> *addr;</span><br><span class="line">  <span class="built_in">cudaMallocManaged</span>(&amp;addr, <span class="number">4</span>);</span><br><span class="line">  *addr = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">   mykernel&lt;&lt;&lt;...&gt;&gt;&gt;(addr);</span><br><span class="line">   __sync_fetch_and_add(addr, <span class="number">10</span>);  <span class="comment">// CPU atomic operation</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>请注意，任何原子操作都可以基于 <code>atomicCAS()</code>（Compare And Swap）来实现。 例如，用于双精度浮点数的 atomicAdd() 在计算能力低于 6.0 的设备上不可用，但可以按如下方式实现：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">if</span> __CUDA_ARCH__ &lt; 600</span></span><br><span class="line"><span class="function">__device__ <span class="type">double</span> <span class="title">atomicAdd</span><span class="params">(<span class="type">double</span>* address, <span class="type">double</span> val)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span>* address_as_ull =</span><br><span class="line">                              (<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span>*)address;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> old = *address_as_ull, assumed;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        assumed = old;</span><br><span class="line">        old = <span class="built_in">atomicCAS</span>(address_as_ull, assumed,</span><br><span class="line">                        __double_as_longlong(val +</span><br><span class="line">                               __longlong_as_double(assumed)));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Note: uses integer comparison to avoid hang in case of NaN (since NaN != NaN)</span></span><br><span class="line">    &#125; <span class="keyword">while</span> (assumed != old);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> __longlong_as_double(old);</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br></pre></td></tr></table></figure>
<p>以下设备范围的原子 API 有系统范围和块范围的变体，但以下情况除外：</p>
<ul>
<li>计算能力低于 6.0 的设备只支持设备范围的原子操作，</li>
<li>计算能力低于 7.2 的 Tegra 设备不支持系统范围的原子操作。</li>
</ul>
<h3 id="B-14-1-Arithmetic-Functions"><a href="#B-14-1-Arithmetic-Functions" class="headerlink" title="B.14.1. Arithmetic Functions"></a>B.14.1. Arithmetic Functions</h3><h4 id="B-14-1-1-atomicAdd"><a href="#B-14-1-1-atomicAdd" class="headerlink" title="B.14.1.1. atomicAdd()"></a>B.14.1.1. atomicAdd()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">atomicAdd</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">int</span> <span class="title">atomicAdd</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">unsigned</span> <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> <span class="title">atomicAdd</span><span class="params">(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">atomicAdd</span><span class="params">(<span class="type">float</span>* address, <span class="type">float</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">atomicAdd</span><span class="params">(<span class="type">double</span>* address, <span class="type">double</span> val)</span></span>;</span><br><span class="line"><span class="function">__half2 <span class="title">atomicAdd</span><span class="params">(__half2 *address, __half2 val)</span></span>;</span><br><span class="line"><span class="function">__half <span class="title">atomicAdd</span><span class="params">(__half *address, __half val)</span></span>;</span><br><span class="line"><span class="function">__nv_bfloat162 <span class="title">atomicAdd</span><span class="params">(__nv_bfloat162 *address, __nv_bfloat162 val)</span></span>;</span><br><span class="line"><span class="function">__nv_bfloat16 <span class="title">atomicAdd</span><span class="params">(__nv_bfloat16 *address, __nv_bfloat16 val)</span></span>;</span><br></pre></td></tr></table></figure>
<p>读取位于全局或共享内存中地址 <code>address</code> 的 16 位、32 位或 64 位字 <code>old</code>，计算 <code>(old + val)</code>，并将结果存储回同一地址的内存中。这三个操作在一个原子事务中执行。该函数返回<code>old</code>。</p>
<p><code>atomicAdd()</code> 的 32 位浮点版本仅受计算能力 2.x 及更高版本的设备支持。</p>
<p><code>atomicAdd()</code> 的 64 位浮点版本仅受计算能力 6.x 及更高版本的设备支持。</p>
<p><code>atomicAdd()</code> 的 32 位 <code>__half2</code> 浮点版本仅受计算能力 6.x 及更高版本的设备支持。 <code>__half2</code> 或 <code>__nv_bfloat162</code> 加法操作的原子性分别保证两个 <code>__half</code> 或 <code>__nv_bfloat16</code> 元素中的每一个；不保证整个 <code>__half2</code> 或 <code>__nv_bfloat162</code> 作为单个 32 位访问是原子的。</p>
<p><code>atomicAdd()</code> 的 16 位 <code>__half</code> 浮点版本仅受计算能力 7.x 及更高版本的设备支持。</p>
<p><code>atomicAdd()</code> 的 16 位 <code>__nv_bfloat16</code> 浮点版本仅受计算能力 8.x 及更高版本的设备支持。</p>
<h4 id="B-14-1-2-atomicSub"><a href="#B-14-1-2-atomicSub" class="headerlink" title="B.14.1.2. atomicSub()"></a>B.14.1.2. atomicSub()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">atomicSub</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">int</span> <span class="title">atomicSub</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">unsigned</span> <span class="type">int</span> val)</span></span>;</span><br></pre></td></tr></table></figure>
<p>读取位于全局或共享内存中地址<code>address</code>的 32 位字 <code>old</code>，计算 <code>(old - val)</code>，并将结果存储回同一地址的内存中。 这三个操作在一个原子事务中执行。 该函数返回<code>old</code>。</p>
<h4 id="B-14-1-3-atomicExch"><a href="#B-14-1-3-atomicExch" class="headerlink" title="B.14.1.3. atomicExch()"></a>B.14.1.3. atomicExch()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">atomicExch</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">int</span> <span class="title">atomicExch</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="type">unsigned</span> <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> <span class="title">atomicExch</span><span class="params">(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">atomicExch</span><span class="params">(<span class="type">float</span>* address, <span class="type">float</span> val)</span></span>;</span><br></pre></td></tr></table></figure>
<p>读取位于全局或共享内存中地址address的 32 位或 64 位字 <code>old</code> 并将 <code>val</code> 存储回同一地址的内存中。 这两个操作在一个原子事务中执行。 该函数返回<code>old</code>。</p>
<h4 id="B-14-1-4-atomicMin"><a href="#B-14-1-4-atomicMin" class="headerlink" title="B.14.1.4. atomicMin()"></a>B.14.1.4. atomicMin()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">atomicMin</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">int</span> <span class="title">atomicMin</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">unsigned</span> <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> <span class="title">atomicMin</span><span class="params">(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">long</span> <span class="type">long</span> <span class="type">int</span> <span class="title">atomicMin</span><span class="params">(<span class="type">long</span> <span class="type">long</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> val)</span></span>;</span><br></pre></td></tr></table></figure>
<p>读取位于全局或共享内存中地址<code>address</code>的 32 位或 64 位字 <code>old</code>，计算 <code>old</code> 和 <code>val</code> 的最小值，并将结果存储回同一地址的内存中。 这三个操作在一个原子事务中执行。 该函数返回<code>old</code>。</p>
<p><code>atomicMin()</code> 的 64 位版本仅受计算能力 3.5 及更高版本的设备支持。</p>
<h4 id="B-14-1-5-atomicMax"><a href="#B-14-1-5-atomicMax" class="headerlink" title="B.14.1.5. atomicMax()"></a>B.14.1.5. atomicMax()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">atomicMax</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">int</span> <span class="title">atomicMax</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">unsigned</span> <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> <span class="title">atomicMax</span><span class="params">(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">long</span> <span class="type">long</span> <span class="type">int</span> <span class="title">atomicMax</span><span class="params">(<span class="type">long</span> <span class="type">long</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> val)</span></span>;</span><br></pre></td></tr></table></figure>
<p>读取位于全局或共享内存中地址<code>address</code>的 32 位或 64 位字 <code>old</code>，计算 <code>old</code> 和 <code>val</code> 的最大值，并将结果存储回同一地址的内存中。 这三个操作在一个原子事务中执行。 该函数返回<code>old</code>。</p>
<p><code>atomicMax()</code> 的 64 位版本仅受计算能力 3.5 及更高版本的设备支持。</p>
<h4 id="B-14-1-6-atomicInc"><a href="#B-14-1-6-atomicInc" class="headerlink" title="B.14.1.6. atomicInc()"></a>B.14.1.6. atomicInc()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">int</span> <span class="title">atomicInc</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">unsigned</span> <span class="type">int</span> val)</span></span>;</span><br></pre></td></tr></table></figure>
<p>读取位于全局或共享内存中地址<code>address</code>的 32 位字 <code>old</code>，计算 <code>((old &gt;= val) ? 0 : (old+1))</code>，并将结果存储回同一地址的内存中。 这三个操作在一个原子事务中执行。 该函数返回<code>old</code>。</p>
<h4 id="B-14-1-7-atomicDec"><a href="#B-14-1-7-atomicDec" class="headerlink" title="B.14.1.7. atomicDec()"></a>B.14.1.7. atomicDec()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">int</span> <span class="title">atomicDec</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">unsigned</span> <span class="type">int</span> val)</span></span>;</span><br></pre></td></tr></table></figure>
<p>读取位于全局或共享内存中地址<code>address</code>的 32 位字 <code>old</code>，计算 <code>(((old == 0) || (old &gt; val)) ? val : (old-1) )</code>，并将结果存储回同一个地址的内存。 这三个操作在一个原子事务中执行。 该函数返回<code>old</code>。</p>
<h4 id="B-14-1-8-atomicCAS"><a href="#B-14-1-8-atomicCAS" class="headerlink" title="B.14.1.8. atomicCAS()"></a>B.14.1.8. atomicCAS()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">atomicCAS</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> compare, <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">int</span> <span class="title">atomicCAS</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">unsigned</span> <span class="type">int</span> compare,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">unsigned</span> <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> <span class="title">atomicCAS</span><span class="params">(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> compare,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">short</span> <span class="type">int</span> <span class="title">atomicCAS</span><span class="params">(<span class="type">unsigned</span> <span class="type">short</span> <span class="type">int</span> *address, </span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">unsigned</span> <span class="type">short</span> <span class="type">int</span> compare, </span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">unsigned</span> <span class="type">short</span> <span class="type">int</span> val)</span></span>;</span><br></pre></td></tr></table></figure>
<p>读取位于全局或共享内存中地址<code>address</code>的 16 位、32 位或 64 位字 <code>old</code>，计算 <code>(old == compare ? val : old)</code> ，并将结果存储回同一地址的内存中。 这三个操作在一个原子事务中执行。 该函数返回<code>old</code>（Compare And Swap）。</p>
<h3 id="B-14-2-Bitwise-Functions"><a href="#B-14-2-Bitwise-Functions" class="headerlink" title="B.14.2. Bitwise Functions"></a>B.14.2. Bitwise Functions</h3><h4 id="B-14-2-1-atomicAnd"><a href="#B-14-2-1-atomicAnd" class="headerlink" title="B.14.2.1. atomicAnd()"></a>B.14.2.1. atomicAnd()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">atomicAnd</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">int</span> <span class="title">atomicAnd</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">unsigned</span> <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> <span class="title">atomicAnd</span><span class="params">(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> val)</span></span>;</span><br></pre></td></tr></table></figure>
<p>读取位于全局或共享内存中地址<code>address</code>的 32 位或 64 位字 <code>old</code>，计算 <code>(old &amp; val)</code>，并将结果存储回同一地址的内存中。 这三个操作在一个原子事务中执行。 该函数返回<code>old</code>。</p>
<p><code>atomicAnd()</code> 的 64 位版本仅受计算能力 3.5 及更高版本的设备支持。</p>
<h4 id="B-14-2-2-atomicOr"><a href="#B-14-2-2-atomicOr" class="headerlink" title="B.14.2.2. atomicOr()"></a>B.14.2.2. atomicOr()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">atomicOr</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">int</span> <span class="title">atomicOr</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                      <span class="type">unsigned</span> <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> <span class="title">atomicOr</span><span class="params">(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> val)</span></span>;</span><br></pre></td></tr></table></figure>
<p>读取位于全局或共享内存中地址<code>address</code>的 32 位或 64 位字 <code>old</code>，计算 <code>(old | val)</code>，并将结果存储回同一地址的内存中。 这三个操作在一个原子事务中执行。 该函数返回<code>old</code>。</p>
<p><code>atomicOr()</code> 的 64 位版本仅受计算能力 3.5 及更高版本的设备支持。</p>
<h4 id="B-14-2-3-atomicXor"><a href="#B-14-2-3-atomicXor" class="headerlink" title="B.14.2.3. atomicXor()"></a>B.14.2.3. atomicXor()</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">atomicXor</span><span class="params">(<span class="type">int</span>* address, <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">int</span> <span class="title">atomicXor</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">unsigned</span> <span class="type">int</span> val)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> <span class="title">atomicXor</span><span class="params">(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span>* address,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 <span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> val)</span></span>;</span><br></pre></td></tr></table></figure>
<p>读取位于全局或共享内存中地址<code>address</code>的 32 位或 64 位字 <code>old</code>，计算 <code>(old ^ val)</code>，并将结果存储回同一地址的内存中。 这三个操作在一个原子事务中执行。 该函数返回<code>old</code>。</p>
<p><code>atomicXor()</code> 的 64 位版本仅受计算能力 3.5 及更高版本的设备支持。</p>
<h2 id="B-15-Address-Space-Predicate-Functions"><a href="#B-15-Address-Space-Predicate-Functions" class="headerlink" title="B.15. Address Space Predicate Functions"></a>B.15. Address Space Predicate Functions</h2><p>如果参数是空指针，则本节中描述的函数具有未指定的行为。</p>
<h3 id="B-15-1-isGlobal"><a href="#B-15-1-isGlobal" class="headerlink" title="B.15.1. __isGlobal()"></a>B.15.1. __isGlobal()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">unsigned</span> <span class="type">int</span> __isGlobal(<span class="type">const</span> <span class="type">void</span> *ptr);</span><br></pre></td></tr></table></figure>
<p>如果 <code>ptr</code> 包含全局内存空间中对象的通用地址，则返回 1，否则返回 0。</p>
<h3 id="B-15-2-isShared"><a href="#B-15-2-isShared" class="headerlink" title="B.15.2. __isShared()"></a>B.15.2. __isShared()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">unsigned</span> <span class="type">int</span> __isShared(<span class="type">const</span> <span class="type">void</span> *ptr);</span><br></pre></td></tr></table></figure>
<p>如果 <code>ptr</code> 包含共享内存空间中对象的通用地址，则返回 1，否则返回 0。</p>
<h3 id="B-15-3-isConstant"><a href="#B-15-3-isConstant" class="headerlink" title="B.15.3. __isConstant()"></a>B.15.3. __isConstant()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">unsigned</span> <span class="type">int</span> __isConstant(<span class="type">const</span> <span class="type">void</span> *ptr);</span><br></pre></td></tr></table></figure>
<p>如果 <code>ptr</code> 包含常量内存空间中对象的通用地址，则返回 1，否则返回 0。</p>
<h3 id="B-15-4-isLocal"><a href="#B-15-4-isLocal" class="headerlink" title="B.15.4. __isLocal()"></a>B.15.4. __isLocal()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">unsigned</span> <span class="type">int</span> __isLocal(<span class="type">const</span> <span class="type">void</span> *ptr);</span><br></pre></td></tr></table></figure>
<p>如果 <code>ptr</code> 包含本地内存空间中对象的通用地址，则返回 1，否则返回 0。</p>
<h2 id="B-16-Address-Space-Conversion-Functions"><a href="#B-16-Address-Space-Conversion-Functions" class="headerlink" title="B.16. Address Space Conversion Functions"></a>B.16. Address Space Conversion Functions</h2><h3 id="B-16-1-cvta-generic-to-global"><a href="#B-16-1-cvta-generic-to-global" class="headerlink" title="B.16.1. __cvta_generic_to_global()"></a>B.16.1. __cvta_generic_to_global()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">size_t</span> __cvta_generic_to_global(<span class="type">const</span> <span class="type">void</span> *ptr);</span><br></pre></td></tr></table></figure>
<p>返回对 <code>ptr</code> 表示的通用地址执行 PTX <code>cvta.to.global</code> 指令的结果。</p>
<h3 id="B-16-2-cvta-generic-to-shared"><a href="#B-16-2-cvta-generic-to-shared" class="headerlink" title="B.16.2. __cvta_generic_to_shared()"></a>B.16.2. __cvta_generic_to_shared()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">size_t</span> __cvta_generic_to_shared(<span class="type">const</span> <span class="type">void</span> *ptr);</span><br></pre></td></tr></table></figure>
<p>返回对 <code>ptr</code> 表示的通用地址执行 PTX <code>cvta.to.shared</code> 指令的结果。</p>
<h3 id="B-16-3-cvta-generic-to-constant"><a href="#B-16-3-cvta-generic-to-constant" class="headerlink" title="B.16.3. __cvta_generic_to_constant()"></a>B.16.3. __cvta_generic_to_constant()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">size_t</span> __cvta_generic_to_constant(<span class="type">const</span> <span class="type">void</span> *ptr);</span><br></pre></td></tr></table></figure>
<p>返回对 <code>ptr</code> 表示的通用地址执行 PTX <code>cvta.to.const</code> 指令的结果。</p>
<h3 id="B-16-4-cvta-generic-to-local"><a href="#B-16-4-cvta-generic-to-local" class="headerlink" title="B.16.4. __cvta_generic_to_local()"></a>B.16.4. __cvta_generic_to_local()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">void</span> * __cvta_global_to_generic(<span class="type">size_t</span> rawbits);</span><br></pre></td></tr></table></figure>
<p>返回通过对 <code>rawbits</code> 提供的值执行 PTX <code>cvta.to.local</code> 指令获得的通用指针。</p>
<h3 id="B-16-5-cvta-global-to-generic"><a href="#B-16-5-cvta-global-to-generic" class="headerlink" title="B.16.5. __cvta_global_to_generic()"></a>B.16.5. __cvta_global_to_generic()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">void</span> * __cvta_global_to_generic(<span class="type">size_t</span> rawbits);</span><br></pre></td></tr></table></figure>
<p>返回通过对 <code>rawbits</code> 提供的值执行 PTX <code>cvta.global</code> 指令获得的通用指针。</p>
<h3 id="B-16-6-cvta-shared-to-generic"><a href="#B-16-6-cvta-shared-to-generic" class="headerlink" title="B.16.6. __cvta_shared_to_generic()"></a>B.16.6. __cvta_shared_to_generic()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">void</span> * __cvta_shared_to_generic(<span class="type">size_t</span> rawbits);</span><br></pre></td></tr></table></figure>
<p>返回通过对 <code>rawbits</code> 提供的值执行 PTX <code>cvta.shared</code> 指令获得的通用指针。</p>
<h3 id="B-16-7-cvta-constant-to-generic"><a href="#B-16-7-cvta-constant-to-generic" class="headerlink" title="B.16.7. __cvta_constant_to_generic()"></a>B.16.7. __cvta_constant_to_generic()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">void</span> * __cvta_constant_to_generic(<span class="type">size_t</span> rawbits);</span><br></pre></td></tr></table></figure>
<p>返回通过对 <code>rawbits</code> 提供的值执行 PTX <code>cvta.const</code> 指令获得的通用指针。</p>
<h3 id="B-16-8-cvta-local-to-generic"><a href="#B-16-8-cvta-local-to-generic" class="headerlink" title="B.16.8. __cvta_local_to_generic()"></a>B.16.8. __cvta_local_to_generic()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">void</span> * __cvta_local_to_generic(<span class="type">size_t</span> rawbits);</span><br></pre></td></tr></table></figure>
<p>返回通过对 <code>rawbits</code> 提供的值执行 PTX <code>cvta.local</code> 指令获得的通用指针。</p>
<h2 id="B-17-Alloca-Function"><a href="#B-17-Alloca-Function" class="headerlink" title="B.17. Alloca Function"></a>B.17. Alloca Function</h2><h3 id="B-17-1-Synopsis"><a href="#B-17-1-Synopsis" class="headerlink" title="B.17.1. Synopsis"></a>B.17.1. Synopsis</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ __device__ <span class="type">void</span> * <span class="title">alloca</span><span class="params">(<span class="type">size_t</span> size)</span></span>;</span><br></pre></td></tr></table></figure>
<h3 id="B-17-2-Description"><a href="#B-17-2-Description" class="headerlink" title="B.17.2. Description"></a>B.17.2. Description</h3><p><code>alloca()</code> 函数在调用者的堆栈帧(stack frame)中分配 <code>size</code> 个字节的内存。 返回值是一个指向分配内存的指针，当从设备代码调用函数时，内存的开头是 16 字节对齐的。 当 <code>alloca()</code> 的调用者返回时，分配的内存会自动释放。</p>
<p>注意：在 Windows 平台上，在使用 <code>alloca()</code> 之前必须包含 <code>&lt;malloc.h&gt;</code>。 使用 <code>alloca()</code> 可能会导致堆栈溢出，用户需要相应地调整堆栈大小。</p>
<p>它受计算能力 5.2 或更高版本的支持。</p>
<h3 id="B-17-3-Example"><a href="#B-17-3-Example" class="headerlink" title="B.17.3. Example"></a>B.17.3. Example</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span> num)</span> </span>&#123;</span><br><span class="line">	int4 *ptr = (int4 *)<span class="built_in">alloca</span>(num * <span class="built_in">sizeof</span>(int4));</span><br><span class="line">	<span class="comment">// use of ptr</span></span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="B-18-Compiler-Optimization-Hint-Functions"><a href="#B-18-Compiler-Optimization-Hint-Functions" class="headerlink" title="B.18. Compiler Optimization Hint Functions"></a>B.18. Compiler Optimization Hint Functions</h2><p>本节中描述的函数可用于向编译器优化器提供附加信息。</p>
<h3 id="B-18-1-builtin-assume-aligned"><a href="#B-18-1-builtin-assume-aligned" class="headerlink" title="B.18.1. __builtin_assume_aligned()"></a>B.18.1. __builtin_assume_aligned()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> * __builtin_assume_aligned (<span class="type">const</span> <span class="type">void</span> *exp, <span class="type">size_t</span> align)</span><br></pre></td></tr></table></figure>
<p>允许编译器假定参数指针至少对齐<code>align</code>字节，并返回参数指针。</p>
<p>Example:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> *res = __builtin_assume_aligned(ptr, <span class="number">32</span>); <span class="comment">// compiler can assume &#x27;res&#x27; is</span></span><br><span class="line">                                               <span class="comment">// at least 32-byte aligned</span></span><br></pre></td></tr></table></figure>
<p>三个参数版本:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> * __builtin_assume_aligned (<span class="type">const</span> <span class="type">void</span> *exp, <span class="type">size_t</span> align, </span><br><span class="line">                                 &lt;integral type&gt; offset)</span><br></pre></td></tr></table></figure>
<p>允许编译器假设 <code>(char *)exp - offset</code> 至少对齐<code>align</code>字节，并返回参数指针。</p>
<p>Example:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> *res = __builtin_assume_aligned(ptr, <span class="number">32</span>, <span class="number">8</span>); <span class="comment">// compiler can assume </span></span><br><span class="line">                                                  <span class="comment">// &#x27;(char *)res - 8&#x27; is</span></span><br><span class="line">                                                  <span class="comment">// at least 32-byte aligned.</span></span><br></pre></td></tr></table></figure>
<h3 id="B-18-2-builtin-assume"><a href="#B-18-2-builtin-assume" class="headerlink" title="B.18.2. __builtin_assume()"></a>B.18.2. __builtin_assume()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __builtin_assume(<span class="type">bool</span> exp)</span><br></pre></td></tr></table></figure>
<p>允许编译器假定布尔参数为真。 如果参数在运行时不为真，则行为未定义。 该参数没有被评估，因此任何副作用都将被丢弃。</p>
<p>Example:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> <span class="function">__device__ <span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> *ptr, <span class="type">int</span> idx)</span> </span>&#123;</span><br><span class="line">   __builtin_assume(idx &lt;= <span class="number">2</span>);</span><br><span class="line">   <span class="keyword">return</span> ptr[idx];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="B-18-3-assume"><a href="#B-18-3-assume" class="headerlink" title="B.18.3. __assume()"></a>B.18.3. __assume()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __assume(<span class="type">bool</span> exp)</span><br></pre></td></tr></table></figure>
<p>允许编译器假定布尔参数为真。 如果参数在运行时不为真，则行为未定义。 该参数没有被评估，因此任何副作用都将被丢弃。</p>
<p>Example:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> <span class="function">__device__ <span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> *ptr, <span class="type">int</span> idx)</span> </span>&#123;</span><br><span class="line">   __assume(idx &lt;= <span class="number">2</span>);</span><br><span class="line">   <span class="keyword">return</span> ptr[idx];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="B-18-4-builtin-expect"><a href="#B-18-4-builtin-expect" class="headerlink" title="B.18.4. __builtin_expect()"></a>B.18.4. __builtin_expect()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">long</span> __builtin_expect (<span class="type">long</span> exp, <span class="type">long</span> c)</span><br></pre></td></tr></table></figure>
<p>向编译器指示期望 <code>exp == c</code>，并返回 <code>exp</code> 的值。 通常用于向编译器指示分支预测信息。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Example:</span><br><span class="line"></span><br><span class="line">    <span class="comment">// indicate to the compiler that likely &quot;var == 0&quot;, </span></span><br><span class="line">    <span class="comment">// so the body of the if-block is unlikely to be</span></span><br><span class="line">    <span class="comment">// executed at run time.</span></span><br><span class="line">    <span class="keyword">if</span> (__builtin_expect (var, <span class="number">0</span>))</span><br><span class="line">      <span class="built_in">doit</span> ();</span><br></pre></td></tr></table></figure>
<h3 id="B-18-5-builtin-unreachable"><a href="#B-18-5-builtin-unreachable" class="headerlink" title="B.18.5. __builtin_unreachable()"></a>B.18.5. __builtin_unreachable()</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __builtin_unreachable(<span class="type">void</span>)</span><br></pre></td></tr></table></figure>
<p>向编译器指示控制流永远不会到达调用此函数的位置。 如果控制流在运行时确实到达了这一点，则程序具有未定义的行为。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Example:</span><br><span class="line"></span><br><span class="line">    <span class="comment">// indicates to the compiler that the default case label is never reached.</span></span><br><span class="line">    <span class="keyword">switch</span> (in) &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>: <span class="keyword">return</span> <span class="number">4</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">2</span>: <span class="keyword">return</span> <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">default</span>: __builtin_unreachable();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="B-18-6-Restrictions"><a href="#B-18-6-Restrictions" class="headerlink" title="B.18.6. Restrictions"></a>B.18.6. Restrictions</h3><p><code>__assume()</code> 仅在使用 cl.exe 主机编译器时受支持。 所有平台都支持其他功能，但受以下限制：</p>
<ul>
<li>如果Host编译器支持该函数，则可以从translation unit中的任何位置调用该函数。</li>
<li>否则，必须从 <code>__device__/__global__</code> 函数的主体中调用该函数，或者仅在定义 <code>__CUDA_ARCH__</code> 宏时调用。</li>
</ul>
<h2 id="B-19-Warp-Vote-Functions"><a href="#B-19-Warp-Vote-Functions" class="headerlink" title="B.19. Warp Vote Functions"></a>B.19. Warp Vote Functions</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> __all_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> predicate);</span><br><span class="line"><span class="type">int</span> __any_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> predicate);</span><br><span class="line"><span class="type">unsigned</span> __ballot_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> predicate);</span><br><span class="line"><span class="type">unsigned</span> __activemask();</span><br></pre></td></tr></table></figure>
<p>弃用通知：<code>__any、__all 和 __ballot</code> 在 CUDA 9.0 中已针对所有设备弃用。</p>
<p>删除通知：当面向具有 7.x 或更高计算能力的设备时，<code>__any、__all 和 __ballot</code> 不再可用，而应使用它们的同步变体。</p>
<p>warp 投票功能允许给定 warp 的线程执行缩减和广播操作。 这些函数将来自warp中每个线程的<code>int predicate</code>作为输入，并将这些值与零进行比较。 比较的结果通过以下方式之一在 warp 的活动线程中组合（减少），向每个参与线程广播单个返回值：</p>
<ul>
<li><code>__all_sync(unsigned mask, predicate):</code><br>  评估<code>mask</code>中所有未退出线程的<code>predicate</code>，当且仅当<code>predicate</code>对所有线程的评估结果都为非零时，才返回非零值。</li>
<li><code>__any_sync(unsigned mask, predicate):</code><br>  评估<code>mask</code>中所有未退出线程的<code>predicate</code>，当且仅当<code>predicate</code>对其中任何一个的评估为非零时才返回非零。</li>
<li><code>__ballot_sync(unsigned mask, predicate):</code><br>  当且仅当 <code>predicate</code> 对 warp 的第 N 个线程计算为非零并且第 N 个线程处于活动状态时，为 <code>mask</code> 中所有未退出的线程计算<code>predicate</code>并返回一个其第 N 位被设置的整型。</li>
<li><strong>activemask():<br>  返回调用 warp 中所有当前活动线程的 32 位整数掩码。如果调用 `</strong>activemask()<code>时，warp 中的第 N 条通道处于活动状态，则设置第 N 位。非活动线程由返回掩码中的 0 位表示。退出程序的线程总是被标记为非活动的。请注意，在</code>__activemask()` 调用中收敛的线程不能保证在后续指令中收敛，除非这些指令正在同步 warp 内置函数。</li>
</ul>
<p>对于<code>__all_sync、__any_sync 和 __ballot_sync</code>，必须传递一个掩码(<code>mask</code>)来指定参与调用的线程。 必须为每个参与线程设置一个表示线程通道 ID 的位，以确保它们在硬件执行内部函数之前正确收敛。 掩码中命名的所有活动线程必须使用相同的掩码执行相同的内部线程，否则结果未定义。</p>
<h2 id="B-20-Warp-Match-Functions"><a href="#B-20-Warp-Match-Functions" class="headerlink" title="B.20. Warp Match Functions"></a>B.20. Warp Match Functions</h2><p><code>__match_any_sync</code> 和 <code>__match_all_sync</code> 在 warp 中的线程之间执行变量的广播和比较操作。</p>
<p>由计算能力 7.x 或更高版本的设备支持。</p>
<h3 id="B-20-1-Synopsis"><a href="#B-20-1-Synopsis" class="headerlink" title="B.20.1. Synopsis"></a>B.20.1. Synopsis</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> __match_any_sync(<span class="type">unsigned</span> mask, T value);</span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> __match_all_sync(<span class="type">unsigned</span> mask, T value, <span class="type">int</span> *pred);</span><br></pre></td></tr></table></figure>
<p><code>T</code> 可以是 <code>int、unsigned int、long、unsigned long、long long、unsigned long long、float 或 double</code>。</p>
<h3 id="B-20-2-Description"><a href="#B-20-2-Description" class="headerlink" title="B.20.2. Description"></a>B.20.2. Description</h3><p><code>__match_sync()</code>的intrinsics允许在对<code>mask</code>中命名的线程进行同步之后，在不同的线程之间广播和比较一个值。</p>
<p><code>__match_any_sync</code></p>
<p>返回<code>mask</code>中具有相同<code>value</code>的线程掩码</p>
<p><code>__match_all_sync</code></p>
<p>如果掩码中的所有线程的<code>value</code>值都相同，则返回<code>mask</code>； 否则返回 0。 如果 <code>mask</code> 中的所有线程具有相同的 <code>value</code> 值，则 <code>pred</code> 设置为 <code>true</code>； 否则predicate设置为假。</p>
<p>新的 <code>*_sync</code> 匹配内在函数采用一个掩码，指示参与调用的线程。 必须为每个参与线程设置一个表示线程通道 ID 的位，以确保它们在硬件执行内部函数之前正确收敛。 掩码中命名的所有非退出线程必须使用相同的掩码执行相同的内在函数，否则结果未定义。</p>
<h2 id="B-21-Warp-Reduce-Functions"><a href="#B-21-Warp-Reduce-Functions" class="headerlink" title="B.21. Warp Reduce Functions"></a>B.21. Warp Reduce Functions</h2><p><code>__reduce_sync(unsigned mask, T value)</code> 内在函数在同步 <code>mask</code> 中命名的线程后对 <code>value</code> 中提供的数据执行归约操作。 <code>T</code> 对于 <code>&#123;add, min, max&#125;</code> 可以是无符号的或有符号的，并且仅对于 <code>&#123;and, or, xor&#125;</code> 操作是无符号的。</p>
<p>由计算能力 8.x 或更高版本的设备支持。</p>
<h3 id="B-21-1-Synopsis"><a href="#B-21-1-Synopsis" class="headerlink" title="B.21.1. Synopsis"></a>B.21.1. Synopsis</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// add/min/max</span></span><br><span class="line"><span class="type">unsigned</span> __reduce_add_sync(<span class="type">unsigned</span> mask, <span class="type">unsigned</span> value);</span><br><span class="line"><span class="type">unsigned</span> __reduce_min_sync(<span class="type">unsigned</span> mask, <span class="type">unsigned</span> value);</span><br><span class="line"><span class="type">unsigned</span> __reduce_max_sync(<span class="type">unsigned</span> mask, <span class="type">unsigned</span> value);</span><br><span class="line"><span class="type">int</span> __reduce_add_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> value);</span><br><span class="line"><span class="type">int</span> __reduce_min_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> value);</span><br><span class="line"><span class="type">int</span> __reduce_max_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> value);</span><br><span class="line"></span><br><span class="line"><span class="comment">// and/or/xor</span></span><br><span class="line"><span class="type">unsigned</span> __reduce_and_sync(<span class="type">unsigned</span> mask, <span class="type">unsigned</span> value);</span><br><span class="line"><span class="type">unsigned</span> __reduce_or_sync(<span class="type">unsigned</span> mask, <span class="type">unsigned</span> value);</span><br><span class="line"><span class="type">unsigned</span> __reduce_xor_sync(<span class="type">unsigned</span> mask, <span class="type">unsigned</span> value);</span><br></pre></td></tr></table></figure>
<h3 id="B-21-2-Description"><a href="#B-21-2-Description" class="headerlink" title="B.21.2. Description"></a>B.21.2. Description</h3><p><code>__reduce_add_sync、__reduce_min_sync、__reduce_max_sync</code></p>
<p>返回对 <code>mask</code> 中命名的每个线程在 <code>value</code> 中提供的值应用算术加法、最小或最大规约操作的结果。</p>
<p><code>__reduce_and_sync、__reduce_or_sync、__reduce_xor_sync</code></p>
<p>返回对 <code>mask</code> 中命名的每个线程在 <code>value</code> 中提供的值应用逻辑 <code>AND、OR 或 XOR</code> 规约操作的结果。</p>
<h2 id="B-22-Warp-Shuffle-Functions"><a href="#B-22-Warp-Shuffle-Functions" class="headerlink" title="B.22. Warp Shuffle Functions"></a>B.22. Warp Shuffle Functions</h2><p><code>__shfl_sync、__shfl_up_sync、__shfl_down_sync 和 __shfl_xor_sync</code> 在 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture">warp</a> 内的线程之间交换变量。</p>
<p>由计算能力 3.x 或更高版本的设备支持。</p>
<p>弃用通知：<code>__shfl、__shfl_up、__shfl_down 和 __shfl_xor</code> 在 CUDA 9.0 中已针对所有设备弃用。</p>
<p>删除通知：当面向具有 7.x 或更高计算能力的设备时，<code>__shfl、__shfl_up、__shfl_down 和 __shfl_xor</code> 不再可用，而应使用它们的同步变体。</p>
<p>作者添加:这里可能大家对接下来会提到的threadIndex, warpIdx, laneIndex会比较混淆.那么我用下图来说明.</p>
<p><img src="/img/ID.png" alt="ID.png"></p>
<h3 id="B-22-1-Synopsis"><a href="#B-22-1-Synopsis" class="headerlink" title="B.22.1. Synopsis"></a>B.22.1. Synopsis</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">T __shfl_sync(<span class="type">unsigned</span> mask, T var, <span class="type">int</span> srcLane, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_up_sync(<span class="type">unsigned</span> mask, T var, <span class="type">unsigned</span> <span class="type">int</span> delta, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_down_sync(<span class="type">unsigned</span> mask, T var, <span class="type">unsigned</span> <span class="type">int</span> delta, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_xor_sync(<span class="type">unsigned</span> mask, T var, <span class="type">int</span> laneMask, <span class="type">int</span> width=warpSize);</span><br></pre></td></tr></table></figure>
<p><code>T</code> 可以是 <code>int、unsigned int、long、unsigned long、long long、unsigned long long、float 或 double</code>。 包含 <code>cuda_fp16.h</code> 头文件后，<code>T</code> 也可以是 <code>__half 或 __half2</code>。 同样，包含 cuda_bf16.h 头文件后，T 也可以是 <code>__nv_bfloat16 或 __nv_bfloat162</code>。</p>
<h3 id="B-22-2-Description"><a href="#B-22-2-Description" class="headerlink" title="B.22.2. Description"></a>B.22.2. Description</h3><p><code>__shfl_sync()</code> 内在函数允许在 warp 内的线程之间交换变量，而无需使用共享内存。 交换同时发生在 warp 中的所有活动线程（并以<code>mask</code>命名），根据类型移动每个线程 4 或 8 个字节的数据。</p>
<p>warp 中的线程称为通道(lanes)，并且可能具有介于 0 和 warpSize-1（包括）之间的索引。 支持四种源通道(source-lane)寻址模式：</p>
<p><code>__shfl_sync()</code></p>
<p>从索引通道直接复制</p>
<p><code>__shfl_up_sync()</code></p>
<p>从相对于调用者 ID 较低的通道复制</p>
<p><code>__shfl_down_sync()</code></p>
<p>从相对于调用者具有更高 ID 的通道复制</p>
<p><code>__shfl_xor_sync()</code></p>
<p>基于自身通道 ID 的按位<code>异或</code>从通道复制</p>
<p>线程只能从积极参与 <code>__shfl_sync()</code> 命令的另一个线程读取数据。 如果目标线程处于非活动状态，则检索到的值未定义。</p>
<p>所有 <code>__shfl_sync()</code> 内在函数都采用一个可选的宽度参数，该参数会改变内在函数的行为。 <code>width</code> 的值必须是 2 的幂； 如果 <code>width</code> 不是 2 的幂，或者是大于 <code>warpSize</code> 的数字，则结果未定义。</p>
<p><code>__shfl_sync()</code> 返回由 <code>srcLane</code> 给定 ID 的线程持有的 <code>var</code> 的值。 如果 <code>width</code> 小于 <code>warpSize</code>，则 warp 的每个子部分都表现为一个单独的实体，其起始逻辑通道 ID 为 0。如果 <code>srcLane</code> 超出范围 [0:width-1]，则返回的值对应于通过 <code>srcLane</code> srcLane modulo width所持有的 <code>var</code> 的值 （即在同一部分内）。</p>
<p>作者添加:这里原本中说的有点绕,我还是用图来说明比较好.注意下面四个图均由作者制作,如果有问题,仅仅是作者水平问题-_-!.</p>
<p><img src="/img/shfl.png" alt="shfl.png"></p>
<p><code>__shfl_up_sync()</code> 通过从调用者的通道 ID 中减去 delta 来计算源通道 ID。 返回由生成的通道 ID 保存的 <code>var</code> 的值：实际上， <code>var</code> 通过 <code>delta</code> 通道向上移动。 如果宽度小于 <code>warpSize</code>，则warp的每个子部分都表现为一个单独的实体，起始逻辑通道 ID 为 0。源通道索引不会环绕宽度值，因此实际上较低的 <code>delta</code> 通道将保持不变。<br><img src="/img/shfl_up.png" alt="shfl_up.png"></p>
<p><code>__shfl_down_sync()</code> 通过将 delta 加调用者的通道 ID 来计算源通道 ID。 返回由生成的通道 ID 保存的 <code>var</code> 的值：这具有将 <code>var</code> 向下移动 <code>delta</code> 通道的效果。 如果 <code>width</code> 小于 warpSize，则 warp 的每个子部分都表现为一个单独的实体，起始逻辑通道 ID 为 0。至于 <code>__shfl_up_sync()</code>，源通道的 ID 号不会环绕宽度值，因此 upper delta lanes将保持不变。<br><img src="/img/shfl_down.png" alt="shfl_down.png"></p>
<p><code>__shfl_xor_sync()</code> 通过对调用者的通道 ID 与 <code>laneMask</code> 执行按位异或来计算源通道 ID：返回结果通道 ID 所持有的 <code>var</code> 的值。 如果宽度小于warpSize，那么每组宽度连续的线程都能够访问早期线程组中的元素，但是如果它们尝试访问后面线程组中的元素，则将返回他们自己的<code>var</code>值。 这种模式实现了一种蝶式寻址模式，例如用于树规约和广播。<br><img src="/img/shufl_xor.png" alt="shufl_xor.png"></p>
<p>新的 <code>*_sync shfl</code> 内部函数采用一个掩码，指示参与调用的线程。 必须为每个参与线程设置一个表示线程通道 ID 的位，以确保它们在硬件执行内部函数之前正确收敛。 掩码中命名的所有非退出线程必须使用相同的掩码执行相同的内在函数，否则结果未定义。</p>
<h3 id="B-22-3-Notes"><a href="#B-22-3-Notes" class="headerlink" title="B.22.3. Notes"></a>B.22.3. Notes</h3><p>线程只能从积极参与 __shfl_sync() 命令的另一个线程读取数据。 如果目标线程处于非活动状态，则检索到的值未定义。</p>
<p>宽度必须是 2 的幂（即 2、4、8、16 或 32）。 未指定其他值的结果。</p>
<h3 id="B-22-4-Examples"><a href="#B-22-4-Examples" class="headerlink" title="B.22.4. Examples"></a>B.22.4. Examples</h3><h4 id="B-22-4-1-Broadcast-of-a-single-value-across-a-warp"><a href="#B-22-4-1-Broadcast-of-a-single-value-across-a-warp" class="headerlink" title="B.22.4.1. Broadcast of a single value across a warp"></a>B.22.4.1. Broadcast of a single value across a warp</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">bcast</span><span class="params">(<span class="type">int</span> arg)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> laneId = threadIdx.x &amp; <span class="number">0x1f</span>;</span><br><span class="line">    <span class="type">int</span> value;</span><br><span class="line">    <span class="keyword">if</span> (laneId == <span class="number">0</span>)        <span class="comment">// Note unused variable for</span></span><br><span class="line">        value = arg;        <span class="comment">// all threads except lane 0</span></span><br><span class="line">    value = __shfl_sync(<span class="number">0xffffffff</span>, value, <span class="number">0</span>);   <span class="comment">// Synchronize all threads in warp, and get &quot;value&quot; from lane 0</span></span><br><span class="line">    <span class="keyword">if</span> (value != arg)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Thread %d failed.\n&quot;</span>, threadIdx.x);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    bcast&lt;&lt;&lt; <span class="number">1</span>, <span class="number">32</span> &gt;&gt;&gt;(<span class="number">1234</span>);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>B.22.4.2. Inclusive plus-scan across sub-partitions of 8 threads</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">scan4</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> laneId = threadIdx.x &amp; <span class="number">0x1f</span>;</span><br><span class="line">    <span class="comment">// Seed sample starting value (inverse of lane ID)</span></span><br><span class="line">    <span class="type">int</span> value = <span class="number">31</span> - laneId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Loop to accumulate scan within my partition.</span></span><br><span class="line">    <span class="comment">// Scan requires log2(n) == 3 steps for 8 threads</span></span><br><span class="line">    <span class="comment">// It works by an accumulated sum up the warp</span></span><br><span class="line">    <span class="comment">// by 1, 2, 4, 8 etc. steps.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">1</span>; i&lt;=<span class="number">4</span>; i*=<span class="number">2</span>) &#123;</span><br><span class="line">        <span class="comment">// We do the __shfl_sync unconditionally so that we</span></span><br><span class="line">        <span class="comment">// can read even from threads which won&#x27;t do a</span></span><br><span class="line">        <span class="comment">// sum, and then conditionally assign the result.</span></span><br><span class="line">        <span class="type">int</span> n = __shfl_up_sync(<span class="number">0xffffffff</span>, value, i, <span class="number">8</span>);</span><br><span class="line">        <span class="keyword">if</span> ((laneId &amp; <span class="number">7</span>) &gt;= i)</span><br><span class="line">            value += n;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Thread %d final value = %d\n&quot;</span>, threadIdx.x, value);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    scan4&lt;&lt;&lt; <span class="number">1</span>, <span class="number">32</span> &gt;&gt;&gt;();</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="B-22-4-3-Reduction-across-a-warp"><a href="#B-22-4-3-Reduction-across-a-warp" class="headerlink" title="B.22.4.3. Reduction across a warp"></a>B.22.4.3. Reduction across a warp</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">warpReduce</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> laneId = threadIdx.x &amp; <span class="number">0x1f</span>;</span><br><span class="line">    <span class="comment">// Seed starting value as inverse lane ID</span></span><br><span class="line">    <span class="type">int</span> value = <span class="number">31</span> - laneId;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Use XOR mode to perform butterfly reduction</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">16</span>; i&gt;=<span class="number">1</span>; i/=<span class="number">2</span>)</span><br><span class="line">        value += __shfl_xor_sync(<span class="number">0xffffffff</span>, value, i, <span class="number">32</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// &quot;value&quot; now contains the sum across all threads</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Thread %d final value = %d\n&quot;</span>, threadIdx.x, value);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    warpReduce&lt;&lt;&lt; <span class="number">1</span>, <span class="number">32</span> &gt;&gt;&gt;();</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="B-23-Nanosleep-Function"><a href="#B-23-Nanosleep-Function" class="headerlink" title="B.23. Nanosleep Function"></a>B.23. Nanosleep Function</h2><h3 id="B-23-1-Synopsis"><a href="#B-23-1-Synopsis" class="headerlink" title="B.23.1. Synopsis"></a>B.23.1. Synopsis</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">T __nanosleep(<span class="type">unsigned</span> ns);</span><br></pre></td></tr></table></figure>
<h3 id="B-23-2-Description"><a href="#B-23-2-Description" class="headerlink" title="B.23.2. Description"></a>B.23.2. Description</h3><p><code>__nanosleep(ns)</code> 将线程挂起大约接近延迟 ns 的睡眠持续时间，以纳秒为单位指定。</p>
<p>它受计算能力 7.0 或更高版本的支持。</p>
<h2 id="B-23-3-Example"><a href="#B-23-3-Example" class="headerlink" title="B.23.3. Example"></a>B.23.3. Example</h2><p>以下代码实现了一个具有指数回退的互斥锁。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">mutex_lock</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span> *mutex)</span> </span>&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> ns = <span class="number">8</span>;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">atomicCAS</span>(mutex, <span class="number">0</span>, <span class="number">1</span>) == <span class="number">1</span>) &#123;</span><br><span class="line">        __nanosleep(ns);</span><br><span class="line">        <span class="keyword">if</span> (ns &lt; <span class="number">256</span>) &#123;</span><br><span class="line">            ns *= <span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">mutex_unlock</span><span class="params">(<span class="type">unsigned</span> <span class="type">int</span> *mutex)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">atomicExch</span>(mutex, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="B-24-Warp-matrix-functions"><a href="#B-24-Warp-matrix-functions" class="headerlink" title="B.24. Warp matrix functions"></a>B.24. Warp matrix functions</h2><p>C++ warp矩阵运算利用Tensor Cores来加速 <code>D=A*B+C</code> 形式的矩阵问题。 计算能力 7.0 或更高版本的设备的混合精度浮点数据支持这些操作。 这需要一个warp中所有线程的合作。 此外，仅当条件在整个 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture">warp</a> 中的计算结果相同时，才允许在条件代码中执行这些操作，否则代码执行可能会挂起。</p>
<h3 id="B-24-1-Description"><a href="#B-24-1-Description" class="headerlink" title="B.24.1. Description"></a>B.24.1. Description</h3><p>以下所有函数和类型都在命名空间 <code>nvcuda::wmma</code> 中定义。 Sub-byte操作被视为预览版，即它们的数据结构和 API 可能会发生变化，并且可能与未来版本不兼容。 这个额外的功能在 nvcuda::wmma::experimental 命名空间中定义。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Use, <span class="type">int</span> m, <span class="type">int</span> n, <span class="type">int</span> k, <span class="keyword">typename</span> T, <span class="keyword">typename</span> Layout=<span class="type">void</span>&gt; <span class="keyword">class</span> fragment;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">load_matrix_sync</span><span class="params">(fragment&lt;...&gt; &amp;a, <span class="type">const</span> T* mptr, <span class="type">unsigned</span> ldm)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">load_matrix_sync</span><span class="params">(fragment&lt;...&gt; &amp;a, <span class="type">const</span> T* mptr, <span class="type">unsigned</span> ldm, <span class="type">layout_t</span> layout)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">store_matrix_sync</span><span class="params">(T* mptr, <span class="type">const</span> fragment&lt;...&gt; &amp;a, <span class="type">unsigned</span> ldm, <span class="type">layout_t</span> layout)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">fill_fragment</span><span class="params">(fragment&lt;...&gt; &amp;a, <span class="type">const</span> T&amp; v)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">mma_sync</span><span class="params">(fragment&lt;...&gt; &amp;d, <span class="type">const</span> fragment&lt;...&gt; &amp;a, <span class="type">const</span> fragment&lt;...&gt; &amp;b, <span class="type">const</span> fragment&lt;...&gt; &amp;c, <span class="type">bool</span> satf=<span class="literal">false</span>)</span></span>;</span><br></pre></td></tr></table></figure>
<p><code>fragment</code>:</p>
<p>包含矩阵的一部分的重载类，分布在warp中的所有线程中。 矩阵元素到<code>fragment</code>内部存储的映射是未指定的，并且在未来的架构中可能会发生变化。</p>
<p>只允许模板参数的某些组合。 第一个模板参数指定片段将如何参与矩阵运算。 可接受的使用值是：</p>
<ul>
<li><code>matrix_a</code> 当<code>fragment</code> 用作第一个被乘数时，A</li>
<li><code>matrix_b</code> 当<code>fragment</code>用作第二个被乘数时，B</li>
<li>当<code>fragment</code>用作源或目标累加器（分别为 C 或 D）时的累加器。</li>
</ul>
<p><code>m、n 和 k</code> 大小描述了参与乘法累加操作的warp-wide矩阵块的形状。 每个tile的尺寸取决于它的作用。 对于 <code>matrix_a</code>，图块的尺寸为 <code>m x k</code>； 对于 <code>matrix_b</code>，维度是 <code>k x n</code>，累加器块是 <code>m x n</code>。</p>
<p>对于被乘数，数据类型 <code>T</code> 可以是 <code>double、float、__half、__nv_bfloat16、char 或 unsigned char</code>，对于累加器，可以是 <code>double、float、int 或 __half</code>。 如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#wmma-type-sizes">元素类型和矩阵大小</a>中所述，支持累加器和被乘数类型的有限组合。 必须为 <code>matrix_a</code> 和 <code>matrix_b</code> 片段指定 <code>Layout</code> 参数。 <code>row_major</code> 或 <code>col_major</code> 分别表示矩阵<strong><em>行或列</em></strong>中的元素在内存中是连续的。 累加器矩阵的 <code>Layout</code> 参数应保留默认值 <code>void</code>。 仅当按如下所述加载或存储累加器时才指定行或列布局。</p>
<p><code>load_matrix_sync</code>:</p>
<p>等到所有warp通道(lanes)都到达 <code>load_matrix_sync</code>，然后从内存中加载矩阵片段 <code>a</code>。 <code>mptr</code> 必须是一个 256 位对齐的指针，指向内存中矩阵的第一个元素。 <code>ldm</code> 描述连续行（对于行主序）或列（对于列主序）之间的元素跨度，对于 <code>__half</code> 元素类型必须是 8 的倍数，对于浮点元素类型必须是 4 的倍数。 （即，两种情况下都是 16 字节的倍数）。 如果<code>fragment</code>是累加器，则布局参数必须指定为 <code>mem_row_major</code> 或 <code>mem_col_major</code>。 对于 <code>matrix_a</code> 和 <code>matrix_b</code> 片段，<code>Layout</code>是从<code>fragment</code>的<code>Layout</code>参数中推断出来的。 a 的 <code>mptr、ldm、layout</code> 和所有模板参数的值对于 warp 中的所有线程必须相同。 这个函数必须被warp中的所有线程调用，否则结果是未定义的。</p>
<p><code>store_matrix_sync</code>:</p>
<p>等到所有warp通道都到达 <code>store_matrix_sync</code>，然后将矩阵片段 a 存储到内存中。 <code>mptr</code> 必须是一个 256 位对齐的指针，指向内存中矩阵的第一个元素。 <code>ldm</code> 描述连续行（对于行主序）或列（对于列主序）之间的元素跨度，对于<code>__half</code> 元素类型必须是 8 的倍数，对于浮点元素类型必须是 4 的倍数。 （即，两种情况下都是 16 字节的倍数）。 输出矩阵的布局必须指定为 <code>mem_row_major</code> 或 <code>mem_col_major</code>。 a 的 <code>mptr、ldm、layout</code> 和所有模板参数的值对于 warp 中的所有线程必须相同。</p>
<p><code>fill_fragment</code>:</p>
<p>用常量 v 填充矩阵片段。由于未指定矩阵元素到每个片段的映射，因此该函数通常由 warp 中的所有线程调用，并具有共同的 v 值。</p>
<p><code>mma_sync</code>:</p>
<p>等到所有<code>warp lanes</code>都到达<code>mma_sync</code>，然后执行warp同步的矩阵乘法累加操作<code>D=A*B+C</code>。 还支持原位(in-place)操作，<code>C=A*B+C</code>。 对于 warp 中的所有线程，每个矩阵片段的 <code>satf</code> 和模板参数的值必须相同。 此外，模板参数 <code>m、n 和 k</code> 必须在片段 <code>A、B、C 和 D</code> 之间匹配。该函数必须由 warp 中的所有线程调用，否则结果未定义。</p>
<p>如果 <code>satf</code>（饱和到有限值—saturate to finite value）模式为真，则以下附加数值属性适用于目标累加器：</p>
<ul>
<li>如果元素结果为+Infinity，则相应的累加器将包含+MAX_NORM</li>
<li>如果元素结果为 -Infinity，则相应的累加器将包含 -MAX_NORM</li>
<li>如果元素结果为 NaN，则对应的累加器将包含 +0</li>
</ul>
<p>由于未指定矩阵元素到每个线程片段的映射，因此必须在调用 <code>store_matrix_sync</code> 后从内存（共享或全局）访问单个矩阵元素。 在 warp 中的所有线程将对所有片段元素统一应用元素操作的特殊情况下，可以使用以下<code>fragment</code>类成员实现直接元素访问。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> <span class="title class_">fragment</span>&lt;Use, m, n, k, T, Layout&gt;::num_elements;</span><br><span class="line">T fragment&lt;Use, m, n, k, T, Layout&gt;::x[num_elements];</span><br></pre></td></tr></table></figure>
<p>例如，以下代码将累加器矩阵缩小一半。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wmma::fragment&lt;wmma::accumulator, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="type">float</span>&gt; frag;</span><br><span class="line"><span class="type">float</span> alpha = <span class="number">0.5f</span>; <span class="comment">// Same value for all threads in warp</span></span><br><span class="line"><span class="comment">/*...*/</span></span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> t=<span class="number">0</span>; t&lt;frag.num_elements; t++)</span><br><span class="line">frag.x[t] *= alpha; </span><br></pre></td></tr></table></figure>
<h3 id="B-24-2-Alternate-Floating-Point"><a href="#B-24-2-Alternate-Floating-Point" class="headerlink" title="B.24.2. Alternate Floating Point"></a>B.24.2. Alternate Floating Point</h3><p>Tensor Core 支持在具有 8.0 及更高计算能力的设备上进行替代类型的浮点运算。</p>
<p><code>__nv_bfloat16</code>:</p>
<p>此数据格式是另一种 <code>fp16</code>格式，其范围与 <code>f32</code> 相同，但精度降低（7 位）。 您可以直接将此数据格式与 <code>cuda_bf16.h</code> 中提供的 <code>__nv_bfloat16</code> 类型一起使用。 具有 <code>__nv_bfloat16</code> 数据类型的矩阵片段需要与浮点类型的累加器组合。 支持的形状和操作与 <code>__half</code> 相同。</p>
<p><code>tf32</code>:</p>
<p>这种数据格式是 <code>Tensor Cores</code> 支持的特殊浮点格式，范围与 f32 相同，但精度降低（&gt;=10 位）。这种格式的内部布局是实现定义的。为了在 <code>WMMA</code> 操作中使用这种浮点格式，输入矩阵必须手动转换为 <code>tf32</code> 精度。</p>
<p>为了便于转换，提供了一个新的内联函数 <code>__float_to_tf32</code>。虽然内联函数的输入和输出参数是浮点类型，但输出将是 <code>tf32</code>。这个新精度仅适用于张量核心，如果与其他浮点类型操作混合使用，结果的精度和范围将是未定义的。</p>
<p>一旦输入矩阵（<code>matrix_a</code> 或 <code>matrix_b</code>）被转换为 <code>tf32</code> 精度，具有<code>precision::tf32</code> 精度的片段和<code>load_matrix_sync</code> 的<code>float</code> 数据类型的组合将利用此新功能。两个累加器片段都必须具有浮点数据类型。唯一支持的矩阵大小是 <code>16x16x8 (m-n-k)</code>。</p>
<p>片段的元素表示为浮点数，因此从 <code>element_type&lt;T&gt;</code> 到 <code>storage_element_type&lt;T&gt;</code> 的映射是：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">precision::tf32 -&gt; <span class="type">float</span></span><br></pre></td></tr></table></figure>
<h3 id="B-24-3-Double-Precision"><a href="#B-24-3-Double-Precision" class="headerlink" title="B.24.3. Double Precision"></a>B.24.3. Double Precision</h3><p><code>Tensor Core</code> 支持计算能力 8.0 及更高版本的设备上的双精度浮点运算。 要使用这个新功能，必须使用具有 <code>double</code> 类型的片段。 <code>mma_sync</code> 操作将使用 <code>.rn</code>（四舍五入到最接近的偶数）舍入修饰符执行。</p>
<h3 id="B-24-4-Sub-byte-Operations"><a href="#B-24-4-Sub-byte-Operations" class="headerlink" title="B.24.4. Sub-byte Operations"></a>B.24.4. Sub-byte Operations</h3><p>Sub-byte <code>WMMA</code> 操作提供了一种访问 Tensor Core 的低精度功能的方法。 它们被视为预览功能，即它们的数据结构和 API 可能会发生变化，并且可能与未来版本不兼容。 此功能可通过 <code>nvcuda::wmma::experimental</code> 命名空间获得：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> experimental &#123; </span><br><span class="line">    <span class="keyword">namespace</span> precision &#123; </span><br><span class="line">        <span class="keyword">struct</span> <span class="title class_">u4</span>; <span class="comment">// 4-bit unsigned </span></span><br><span class="line">        <span class="keyword">struct</span> <span class="title class_">s4</span>; <span class="comment">// 4-bit signed </span></span><br><span class="line">        <span class="keyword">struct</span> <span class="title class_">b1</span>; <span class="comment">// 1-bit </span></span><br><span class="line">   &#125; </span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">bmmaBitOp</span> &#123;</span><br><span class="line">        bmmaBitOpXOR = <span class="number">1</span>, <span class="comment">// compute_75 minimum</span></span><br><span class="line">        bmmaBitOpAND = <span class="number">2</span>  <span class="comment">// compute_80 minimum</span></span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">bmmaAccumulateOp</span> &#123; bmmaAccumulateOpPOPC = <span class="number">1</span> &#125;; </span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>对于 4 位精度，可用的 API 保持不变，但您必须指定 <code>experimental::precision::u4</code> 或 <code>experimental::precision::s4</code> 作为片段数据类型。 由于片段的元素被打包在一起，<code>num_storage_elements</code> 将小于该片段的 <code>num_elements</code>。 Sub-byte片段的 <code>num_elements</code> 变量，因此返回<code>Sub-byte</code>类型 <code>element_type&lt;T&gt;</code> 的元素数。 对于单位精度也是如此，在这种情况下，从 <code>element_type&lt;T&gt;</code> 到 <code>storage_element_type&lt;T&gt;</code> 的映射如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">experimental::precision::u4 -&gt; <span class="built_in">unsigned</span> (<span class="number">8</span> elements in <span class="number">1</span> storage element) </span><br><span class="line">experimental::precision::s4 -&gt; <span class="built_in">int</span> (<span class="number">8</span> elements in <span class="number">1</span> storage element) </span><br><span class="line">experimental::precision::b1 -&gt; <span class="built_in">unsigned</span> (<span class="number">32</span> elements in <span class="number">1</span> storage element) </span><br><span class="line">T -&gt; T  <span class="comment">//all other types</span></span><br></pre></td></tr></table></figure>
<p>Sub-byte片段的允许布局始终为 <code>matrix_a</code> 的 <code>row_major</code> 和 <code>matrix_b</code>的 <code>col_major</code>。</p>
<p>对于子字节操作，<code>load_matrix_sync</code> 中 <code>ldm</code> 的值对于元素类型 <code>experimental::precision::u4</code> 和 <code>Experimental::precision::s4</code> 应该是 32 的倍数，或者对于元素类型 <code>experimental::precision::b1</code> 应该是 128 的倍数 （即，两种情况下都是 16 字节的倍数）。</p>
<p><code>bmma_sync</code>:<br>等到所有warp lane都执行了<code>bmma_sync</code>，然后执行warp同步位矩阵乘法累加运算<code>D = (A op B) + C</code>，其中op由逻辑运算<code>bmmaBitOp</code>和<code>bmmaAccumulateOp</code>定义的累加组成。 可用的操作有：</p>
<ul>
<li><code>bmmaBitOpXOR</code>，<code>matrix_a</code> 中的一行与 <code>matrix_b</code> 的 128 位列的 128 位 XOR</li>
<li><code>bmmaBitOpAND</code>，<code>matrix_a</code> 中的一行与 <code>matrix_b</code> 的 128 位列的 128 位 AND，可用于计算能力 8.0 及更高版本的设备。</li>
</ul>
<p>累积操作始终是 <code>bmmaAccumulateOpPOPC</code>，它计算设置位的数量。</p>
<h3 id="B-24-5-Restrictions"><a href="#B-24-5-Restrictions" class="headerlink" title="B.24.5. Restrictions"></a>B.24.5. Restrictions</h3><p>对于每个主要和次要设备架构，tensor cores所需的特殊格式可能不同。 由于线程仅持有整个矩阵的片段（不透明的架构特定的 ABI 数据结构），因此开发人员不允许对如何将各个参数映射到参与矩阵乘法累加的寄存器做出假设，这使情况变得更加复杂。</p>
<p>由于片段是特定于体系结构的，如果函数已针对不同的链接兼容体系结构编译并链接在一起成为相同的设备可执行文件，则将它们从函数 A 传递到函数 B 是不安全的。 在这种情况下，片段的大小和布局将特定于一种架构，而在另一种架构中使用 <code>WMMA API</code> 将导致不正确的结果或潜在的损坏。</p>
<p>片段布局不同的两个链接兼容架构的示例是 sm_70 和 sm_75。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fragA.cu: <span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123; wmma::fragment&lt;...&gt; mat_a; <span class="built_in">bar</span>(&amp;mat_a); &#125;</span><br><span class="line">fragB.cu: <span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">(wmma::fragment&lt;...&gt; *mat_a)</span> </span>&#123; <span class="comment">// operate on mat_a &#125;  </span></span><br></pre></td></tr></table></figure>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sm_70 fragment layout</span></span><br><span class="line">$&gt; nvcc -dc -arch=compute_70 -code=sm_70 fragA.cu -o fragA.o</span><br><span class="line"><span class="comment">// sm_75 fragment layout</span></span><br><span class="line">$&gt; nvcc -dc -arch=compute_75 -code=sm_75 fragB.cu -o fragB.o</span><br><span class="line"><span class="comment">// Linking the two together</span></span><br><span class="line">$&gt; nvcc -dlink -arch=sm_75 fragA.o fragB.o -o frag.o   </span><br></pre></td></tr></table></figure>
<p>这种未定义的行为在编译时和运行时的工具也可能无法检测到，因此需要格外小心以确保片段的布局是一致的。 当与既为不同的链接兼容架构构建并期望传递 WMMA 片段的遗留库链接时，最有可能出现这种链接危险。</p>
<p>请注意，在弱链接的情况下（例如，CUDA C++ 内联函数），链接器可能会选择任何可用的函数定义，这可能会导致编译单元之间的隐式传递。</p>
<p>为避免此类问题，矩阵应始终存储到内存中以通过外部接口传输（例如 <code>wmma::store_matrix_sync(dst, ...)</code>;），然后可以安全地将其作为指针类型传递给 <code>bar()</code> [ 例如 <code>float *dst</code>]。</p>
<p>请注意，由于 sm_70 可以在 sm_75 上运行，因此可以将上述示例 sm_75 代码更改为 sm_70 并在 sm_75 上正确运行。 但是，当与其他 sm_75 单独编译的二进制文件链接时，建议在您的应用程序中包含 sm_75 本机代码。</p>
<h3 id="B-24-6-Element-Types-amp-Matrix-Sizes"><a href="#B-24-6-Element-Types-amp-Matrix-Sizes" class="headerlink" title="B.24.6. Element Types &amp; Matrix Sizes"></a>B.24.6. Element Types &amp; Matrix Sizes</h3><p>张量核心支持多种元素类型和矩阵大小。 下表显示了支持的 <code>matrix_a、matrix_b</code> 和<code>accumulator</code>矩阵的各种组合：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Matrix A</th>
<th>Matrix B</th>
<th>Accumulator</th>
<th>Matrix Size (m-n-k)</th>
</tr>
</thead>
<tbody>
<tr>
<td>__half</td>
<td>__half</td>
<td>float</td>
<td>16x16x16</td>
</tr>
<tr>
<td>__half</td>
<td>__half</td>
<td>float</td>
<td>32x8x16</td>
</tr>
<tr>
<td>__half</td>
<td>__half</td>
<td>float</td>
<td>8x32x16</td>
</tr>
<tr>
<td>__half</td>
<td>__half</td>
<td>__half</td>
<td>16x16x16</td>
</tr>
<tr>
<td>__half</td>
<td>__half</td>
<td>__half</td>
<td>32x8x16</td>
</tr>
<tr>
<td>__half</td>
<td>__half</td>
<td>__half</td>
<td>8x32x16</td>
</tr>
<tr>
<td>unsigned char</td>
<td>unsigned char</td>
<td>int</td>
<td>16x16x16</td>
</tr>
<tr>
<td>unsigned char</td>
<td>unsigned char</td>
<td>int</td>
<td>32x8x16</td>
</tr>
<tr>
<td>unsigned char</td>
<td>unsigned char</td>
<td>int</td>
<td>8x32x16</td>
</tr>
<tr>
<td>signed char</td>
<td>signed char</td>
<td>int</td>
<td>16x16x16</td>
</tr>
<tr>
<td>signed char</td>
<td>signed char</td>
<td>int</td>
<td>32x8x16</td>
</tr>
<tr>
<td>signed char</td>
<td>signed char</td>
<td>int</td>
<td>8x32x16</td>
</tr>
</tbody>
</table>
</div>
<p>备用浮点支持：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Matrix A</th>
<th>Matrix B</th>
<th>Accumulator</th>
<th>Matrix Size (m-n-k)</th>
</tr>
</thead>
<tbody>
<tr>
<td>__nv_bfloat16</td>
<td>__nv_bfloat16</td>
<td>float</td>
<td>16x16x16</td>
</tr>
<tr>
<td>__nv_bfloat16</td>
<td>__nv_bfloat16</td>
<td>float</td>
<td>32x8x16</td>
</tr>
<tr>
<td>__nv_bfloat16</td>
<td>__nv_bfloat16</td>
<td>float</td>
<td>8x32x16</td>
</tr>
<tr>
<td>precision::tf32</td>
<td>precision::tf32</td>
<td>float</td>
<td>16x16x8</td>
</tr>
</tbody>
</table>
</div>
<p>双精支持:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Matrix A</th>
<th>Matrix B</th>
<th>Accumulator</th>
<th>Matrix Size (m-n-k)</th>
</tr>
</thead>
<tbody>
<tr>
<td>double</td>
<td>double</td>
<td>double</td>
<td>8x8x4</td>
</tr>
</tbody>
</table>
</div>
<p>对sub-byte操作的实验性支持：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Matrix A</th>
<th>Matrix B</th>
<th>Accumulator</th>
<th>Matrix Size (m-n-k)</th>
</tr>
</thead>
<tbody>
<tr>
<td>precision::u4</td>
<td>precision::u4</td>
<td>int</td>
<td>8x8x32</td>
</tr>
<tr>
<td>precision::s4</td>
<td>precision::s4</td>
<td>int</td>
<td>8x8x32</td>
</tr>
<tr>
<td>precision::b1</td>
<td>precision::b1</td>
<td>int</td>
<td>8x8x128</td>
</tr>
</tbody>
</table>
</div>
<h3 id="B-24-7-Example"><a href="#B-24-7-Example" class="headerlink" title="B.24.7. Example"></a>B.24.7. Example</h3><p>以下代码在单个warp中实现 16x16x16 矩阵乘法:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;mma.h&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvcuda;</span><br><span class="line">      </span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">wmma_ker</span><span class="params">(half *a, half *b, <span class="type">float</span> *c)</span> </span>&#123;</span><br><span class="line">   <span class="comment">// Declare the fragments</span></span><br><span class="line">   wmma::fragment&lt;wmma::matrix_a, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, half, wmma::col_major&gt; a_frag;</span><br><span class="line">   wmma::fragment&lt;wmma::matrix_b, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, half, wmma::row_major&gt; b_frag;</span><br><span class="line">   wmma::fragment&lt;wmma::accumulator, <span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>, <span class="type">float</span>&gt; c_frag;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Initialize the output to zero</span></span><br><span class="line">   wmma::<span class="built_in">fill_fragment</span>(c_frag, <span class="number">0.0f</span>);</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Load the inputs</span></span><br><span class="line">   wmma::<span class="built_in">load_matrix_sync</span>(a_frag, a, <span class="number">16</span>);</span><br><span class="line">   wmma::<span class="built_in">load_matrix_sync</span>(b_frag, b, <span class="number">16</span>);</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Perform the matrix multiplication</span></span><br><span class="line">   wmma::<span class="built_in">mma_sync</span>(c_frag, a_frag, b_frag, c_frag);</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Store the output</span></span><br><span class="line">   wmma::<span class="built_in">store_matrix_sync</span>(c, c_frag, <span class="number">16</span>, wmma::mem_row_major);</span><br><span class="line">&#125;   </span><br></pre></td></tr></table></figure>
<h2 id="B-25-Asynchronous-Barrier"><a href="#B-25-Asynchronous-Barrier" class="headerlink" title="B.25. Asynchronous Barrier"></a>B.25. Asynchronous Barrier</h2><p>NVIDIA C++ 标准库引入了 <a href="https://nvidia.github.io/libcudacxx/extended_api/synchronization_primitives/barrier.html"><code>std::barrier</code></a> 的 GPU 实现。 除了 <code>std::barrier</code>的实现，该库还提供允许用户指定屏障对象范围的扩展。 屏障 API 范围记录在 <a href="https://nvidia.github.io/libcudacxx/extended_api/thread_scopes.html">Thread Scopes</a> 下。 计算能力 8.0 或更高版本的设备为屏障操作和这些屏障与 memcpy_async 功能的集成提供硬件加速。 在计算能力低于 8.0 但从 7.0 开始的设备上，这些障碍在没有硬件加速的情况下可用</p>
<p><code>nvcuda::experimental::awbarrier</code>被弃用，取而代之的是<code>cuda::barrier</code>。</p>
<h3 id="B-25-1-Simple-Synchronization-Pattern"><a href="#B-25-1-Simple-Synchronization-Pattern" class="headerlink" title="B.25.1. Simple Synchronization Pattern"></a>B.25.1. Simple Synchronization Pattern</h3><p>在没有到达/等待障碍的情况下，使用 <code>__syncthreads()</code>（同步块中的所有线程）或 <code>group.sync()</code> 使用<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cooperative-groups">协作组</a>时实现同步。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">simple_sync</span><span class="params">(<span class="type">int</span> iteration_count)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> block = cooperative_groups::<span class="built_in">this_thread_block</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; iteration_count; ++i) &#123;</span><br><span class="line">        <span class="comment">/* code before arrive */</span></span><br><span class="line">        block.<span class="built_in">sync</span>(); <span class="comment">/* wait for all threads to arrive here */</span></span><br><span class="line">        <span class="comment">/* code after wait */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>线程在同步点（<code>block.sync()</code>）被阻塞，直到所有线程都到达同步点。 此外，同步点之前发生的内存更新保证对同步点之后块中的所有线程可见，即等效于 <code>__threadfence_block()</code> 以及<code>sync</code>。</p>
<p>这种模式分为三个阶段：</p>
<ul>
<li>同步前的代码执行将在同步后读取的内存更新。</li>
<li>同步点</li>
<li>同步点之后的代码，具有同步点之前发生的内存更新的可见性。</li>
</ul>
<h3 id="B-25-2-Temporal-Splitting-and-Five-Stages-of-Synchronization"><a href="#B-25-2-Temporal-Splitting-and-Five-Stages-of-Synchronization" class="headerlink" title="B.25.2. Temporal Splitting and Five Stages of Synchronization"></a>B.25.2. Temporal Splitting and Five Stages of Synchronization</h3><p>使用 std::barrier 的时间分割同步模式如下。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda/barrier&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">compute</span><span class="params">(<span class="type">float</span>* data, <span class="type">int</span> curr_iteration)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">split_arrive_wait</span><span class="params">(<span class="type">int</span> iteration_count, <span class="type">float</span> *data)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">using</span> barrier = cuda::barrier&lt;cuda::thread_scope_block&gt;;</span><br><span class="line">    __shared__  barrier bar;</span><br><span class="line">    <span class="keyword">auto</span> block = cooperative_groups::<span class="built_in">this_thread_block</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (block.<span class="built_in">thread_rank</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="built_in">init</span>(&amp;bar, block.<span class="built_in">size</span>()); <span class="comment">// Initialize the barrier with expected arrival count</span></span><br><span class="line">    &#125;</span><br><span class="line">    block.<span class="built_in">sync</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> curr_iter = <span class="number">0</span>; curr_iter &lt; iteration_count; ++curr_iter) &#123;</span><br><span class="line">        <span class="comment">/* code before arrive */</span></span><br><span class="line">       barrier::arrival_token token = bar.<span class="built_in">arrive</span>(); <span class="comment">/* this thread arrives. Arrival does not block a thread */</span></span><br><span class="line">       <span class="built_in">compute</span>(data, curr_iter); </span><br><span class="line">       bar.<span class="built_in">wait</span>(std::<span class="built_in">move</span>(token)); <span class="comment">/* wait for all threads participating in the barrier to complete bar.arrive()*/</span></span><br><span class="line">        <span class="comment">/* code after wait */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>在此模式中，同步点 (<code>block.sync()</code>) 分为到达点 (<code>bar.arrive()</code>) 和等待点 (<code>bar.wait(std::move(token))</code>)。 一个线程通过第一次调用 <code>bar.arrive()</code> 开始参与 <code>cuda::barrier</code>。 当一个线程调用 <code>bar.wait(std::move(token))</code> 时，它将被阻塞，直到参与线程完成 <code>bar.arrive()</code> 的预期次数，该次数由传递给 <code>init()</code>的预期到达计数参数指定。 在参与线程调用 <code>bar.arrive()</code>之前发生的内存更新保证在参与线程调用 <code>bar.wait(std::move(token))</code> 之后对参与线程可见。 请注意，对 <code>bar.arrive()</code> 的调用不会阻塞线程，它可以继续其他不依赖于在其他参与线程调用 <code>bar.arrive()</code> 之前发生的内存更新的工作。</p>
<p><code>arrive</code> 然后<code>wait</code>模式有五个阶段，可以反复重复：</p>
<ul>
<li>到达之前的代码执行将在等待后读取的内存更新。</li>
<li>带有隐式内存栅栏的到达点（即，相当于 <code>__threadfence_block()</code>）。</li>
<li>到达和等待之间的代码。</li>
<li>等待点。</li>
<li>等待后的代码，可以看到在到达之前执行的更新。</li>
</ul>
<h3 id="B-25-3-Bootstrap-Initialization-Expected-Arrival-Count-and-Participation"><a href="#B-25-3-Bootstrap-Initialization-Expected-Arrival-Count-and-Participation" class="headerlink" title="B.25.3. Bootstrap Initialization, Expected Arrival Count, and Participation"></a>B.25.3. Bootstrap Initialization, Expected Arrival Count, and Participation</h3><p>必须在任何线程开始参与 <code>cuda::barrier</code> 之前进行初始化。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda/barrier&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">init_barrier</span><span class="params">()</span> </span>&#123; </span><br><span class="line">    __shared__ cuda::barrier&lt;cuda::thread_scope_block&gt; bar;</span><br><span class="line">    <span class="keyword">auto</span> block = cooperative_groups::<span class="built_in">this_thread_block</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (block.<span class="built_in">thread_rank</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="built_in">init</span>(&amp;bar, block.<span class="built_in">size</span>()); <span class="comment">// Single thread initializes the total expected arrival count.</span></span><br><span class="line">    &#125;</span><br><span class="line">    block.<span class="built_in">sync</span>();         </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在任何线程可以参与 <code>cuda::barrier</code> 之前，必须使用带有预期到达计数的 <code>init()</code> 初始化屏障，在本例中为 <code>block.size()</code>。 必须在任何线程调用 <code>bar.arrive()</code> 之前进行初始化。 这带来了一个引导挑战，因为线程必须在参与 <code>cuda::barrier</code> 之前进行同步，但是线程正在创建 <code>cuda::barrier</code> 以进行同步。 在此示例中，将参与的线程是协作组的一部分，并使用 <code>block.sync()</code> 来引导初始化。 在此示例中，整个线程块参与初始化，因此也可以使用 <code>__syncthreads()</code>。</p>
<p><code>init()</code> 的第二个参数是预期到达计数，即参与线程在解除对 <code>bar.wait(std::move(token)</code> 的调用之前将调用 <code>bar.arrive()</code> 的次数 ））。 在前面的示例中，<code>cuda::barrier</code> 使用线程块中的线程数进行初始化，即，<code>cooperative_groups::this_thread_block().size()</code>，并且线程块中的所有线程都参与了屏障。</p>
<p><code>cuda::barrier</code>可以灵活地指定线程如何参与（拆分到达/等待）以及哪些线程参与。 相比之下，来自协作组的 <code>this_thread_block.sync()</code> 或 <code>__syncthreads()</code> 适用于整个线程块，而 <code>__syncwarp(mask)</code> 是 warp 的指定子集。 如果用户的意图是同步一个完整的线程块或一个完整的warp，出于性能原因，我们建议分别使用 <code>__syncthreads()</code> 和 <code>__syncwarp(mask)</code>。</p>
<h3 id="B-25-4-A-Barrier’s-Phase-Arrival-Countdown-Completion-and-Reset"><a href="#B-25-4-A-Barrier’s-Phase-Arrival-Countdown-Completion-and-Reset" class="headerlink" title="B.25.4. A Barrier’s Phase: Arrival, Countdown, Completion, and Reset"></a>B.25.4. A Barrier’s Phase: Arrival, Countdown, Completion, and Reset</h3><p>当参与线程调用 <code>bar.arrive()</code> 时，<code>cuda::barrier</code> 从预期到达计数倒数到零。当倒计时达到零时，当前阶段的 <code>cuda::barrier</code> 就完成了。当最后一次调用 <code>bar.arrive()</code> 导致倒计时归零时，倒计时会自动自动重置。重置将倒计时分配给预期到达计数，并将 <code>cuda::barrier</code> 移动到下一阶段。</p>
<p>从 <code>token=bar.arrive()</code>返回的 <code>cuda::barrier::arrival_token</code> 类的<code>token</code>对象与屏障的当前阶段相关联。当 <code>cuda::barrier</code> 处于当前阶段时，对 <code>bar.wait(std::move(token))</code> 的调用会阻塞调用线程，即，当与<code>token</code>关联的阶段与 <code>cuda::barrier</code> 的阶段匹配时。如果在调用 <code>bar.wait(std::move(token))</code> 之前阶段提前（因为倒计时达到零），则线程不会阻塞；如果在 <code>bar.wait(std::move(token))</code> 中线程被阻塞时阶段提前，则线程被解除阻塞。</p>
<p>了解何时可能发生或不可能发生重置至关重要，尤其是在到达/等待同步模式中。</p>
<ul>
<li>线程对 <code>token=bar.arrive()</code> 和 <code>bar.wait(std::move(token))</code> 的调用必须按顺序进行，以便 <code>token=bar.arrive()</code> 在 <code>cuda::barrier</code> 的当前阶段发生，并且 <code>bar.wait (std::move(token))</code> 发生在同一阶段或下一阶段。</li>
<li>当屏障的计数器非零时，线程对 <code>bar.arrive()</code> 的调用必须发生。 在屏障初始化之后，如果线程对 <code>bar.arrive()</code> 的调用导致倒计时达到零，则必须先调用 <code>bar.wait(std::move(token))</code>，然后才能将屏障重新用于对 <code>bar.arrive()</code> 的后续调用。</li>
<li><code>bar.wait()</code> 只能使用当前阶段或前一个阶段的<code>token</code>对象调用。 对于<code>token</code>对象的任何其他值，行为是未定义的。<br>  对于简单的到达/等待同步模式，遵守这些使用规则很简单。 </li>
</ul>
<h3 id="B-25-5-Spatial-Partitioning-also-known-as-Warp-Specialization"><a href="#B-25-5-Spatial-Partitioning-also-known-as-Warp-Specialization" class="headerlink" title="B.25.5. Spatial Partitioning (also known as Warp Specialization)"></a>B.25.5. Spatial Partitioning (also known as Warp Specialization)</h3><p>线程块可以在空间上进行分区，以便warp专门用于执行独立计算。 空间分区用于生产者或消费者模式，其中一个线程子集产生的数据由另一个（不相交的）线程子集同时使用。</p>
<p>生产者/消费者空间分区模式需要两个单侧同步来管理生产者和消费者之间的数据缓冲区。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Producer</th>
<th>Consumer</th>
</tr>
</thead>
<tbody>
<tr>
<td>wait for buffer to be ready to be filled</td>
<td>signal buffer is ready to be filled</td>
</tr>
<tr>
<td>produce data and fill the buffer</td>
<td></td>
</tr>
<tr>
<td>signal buffer is filled</td>
<td>wait for buffer to be filled</td>
</tr>
<tr>
<td></td>
<td>consume data in filled buffer</td>
</tr>
</tbody>
</table>
</div>
<p> 生产者线程等待消费者线程发出缓冲区已准备好填充的信号； 但是，消费者线程不会等待此信号。 消费者线程等待生产者线程发出缓冲区已满的信号； 但是，生产者线程不会等待此信号。 对于完整的生产者/消费者并发，此模式具有（至少）双缓冲，其中每个缓冲区需要两个 <code>cuda::barriers</code>。</p>
 <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"> <span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda/barrier&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> barrier = cuda::barrier&lt;cuda::thread_scope_block&gt;;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">producer</span><span class="params">(barrier ready[], barrier filled[], <span class="type">float</span>* buffer, <span class="type">float</span>* in, <span class="type">int</span> N, <span class="type">int</span> buffer_len)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; (N/buffer_len); ++i) &#123;</span><br><span class="line">        ready[i%<span class="number">2</span>].<span class="built_in">arrive_and_wait</span>(); <span class="comment">/* wait for buffer_(i%2) to be ready to be filled */</span></span><br><span class="line">        <span class="comment">/* produce, i.e., fill in, buffer_(i%2)  */</span></span><br><span class="line">        barrier::arrival_token token = filled[i%<span class="number">2</span>].<span class="built_in">arrive</span>(); <span class="comment">/* buffer_(i%2) is filled */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">consumer</span><span class="params">(barrier ready[], barrier filled[], <span class="type">float</span>* buffer, <span class="type">float</span>* out, <span class="type">int</span> N, <span class="type">int</span> buffer_len)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    barrier::arrival_token token1 = ready[<span class="number">0</span>].<span class="built_in">arrive</span>(); <span class="comment">/* buffer_0 is ready for initial fill */</span></span><br><span class="line">    barrier::arrival_token token2 = ready[<span class="number">1</span>].<span class="built_in">arrive</span>(); <span class="comment">/* buffer_1 is ready for initial fill */</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; (N/buffer_len); ++i) &#123;</span><br><span class="line">        filled[i%<span class="number">2</span>].<span class="built_in">arrive_and_wait</span>(); <span class="comment">/* wait for buffer_(i%2) to be filled */</span></span><br><span class="line">        <span class="comment">/* consume buffer_(i%2) */</span></span><br><span class="line">        barrier::arrival_token token = ready[i%<span class="number">2</span>].<span class="built_in">arrive</span>(); <span class="comment">/* buffer_(i%2) is ready to be re-filled */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//N is the total number of float elements in arrays in and out</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">producer_consumer_pattern</span><span class="params">(<span class="type">int</span> N, <span class="type">int</span> buffer_len, <span class="type">float</span>* in, <span class="type">float</span>* out)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Shared memory buffer declared below is of size 2 * buffer_len</span></span><br><span class="line">    <span class="comment">// so that we can alternatively work between two buffers. </span></span><br><span class="line">    <span class="comment">// buffer_0 = buffer and buffer_1 = buffer + buffer_len</span></span><br><span class="line">    __shared__ <span class="keyword">extern</span> <span class="type">float</span> buffer[];</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// bar[0] and bar[1] track if buffers buffer_0 and buffer_1 are ready to be filled, </span></span><br><span class="line">    <span class="comment">// while bar[2] and bar[3] track if buffers buffer_0 and buffer_1 are filled-in respectively</span></span><br><span class="line">    __shared__ barrier bar[<span class="number">4</span>];</span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> block = cooperative_groups::<span class="built_in">this_thread_block</span>();</span><br><span class="line">    <span class="keyword">if</span> (block.<span class="built_in">thread_rank</span>() &lt; <span class="number">4</span>)</span><br><span class="line">        <span class="built_in">init</span>(bar + block.<span class="built_in">thread_rank</span>(), block.<span class="built_in">size</span>());</span><br><span class="line">    block.<span class="built_in">sync</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (block.<span class="built_in">thread_rank</span>() &lt; warpSize)</span><br><span class="line">        <span class="built_in">producer</span>(bar, bar<span class="number">+2</span>, buffer, in, N, buffer_len);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">consumer</span>(bar, bar<span class="number">+2</span>, buffer, out, N, buffer_len);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这个例子中，第一个 warp 被专门为生产者，其余的 warp 被专门为消费者。 所有生产者和消费者线程都参与（调用<code>bar.arrive()</code> 或 <code>bar.arrive_and_wait()</code>）四个 <code>cuda::barriers</code> 中的每一个，因此预期到达计数等于 <code>block.size()</code>。</p>
<p>生产者线程等待消费者线程发出可以填充共享内存缓冲区的信号。 为了等待 <code>cuda::barrier</code>，生产者线程必须首先到达 <code>ready[i%2].arrive()</code> 以获取<code>token</code>，然后使用该<code>token</code> <code>ready[i%2].wait(token)</code>。 为简单起见，<code>ready[i%2].arrive_and_wait()</code> 结合了这些操作。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bar.<span class="built_in">arrive_and_wait</span>();</span><br><span class="line"><span class="comment">/* is equivalent to */</span></span><br><span class="line">bar.<span class="built_in">wait</span>(bar.<span class="built_in">arrive</span>());</span><br></pre></td></tr></table></figure>
<p>生产者线程计算并填充ready缓冲区，然后它们通过到达填充屏障来表示缓冲区已填充，<code>filled[i%2].arrive()</code>。 生产者线程此时不会等待，而是等待下一次迭代的缓冲区（双缓冲）准备好被填充。</p>
<p>消费者线程首先发出两个缓冲区已准备好填充的信号。 消费者线程此时不等待，而是等待此迭代的缓冲区被填充，<code>filled[i%2].arrive_and_wait()</code>。 在消费者线程消耗完缓冲区后，它们会发出信号表明缓冲区已准备好再次填充，<code>ready[i%2].arrive()</code>，然后等待下一次迭代的缓冲区被填充。</p>
<h3 id="B-25-6-Early-Exit-Dropping-out-of-Participation"><a href="#B-25-6-Early-Exit-Dropping-out-of-Participation" class="headerlink" title="B.25.6. Early Exit (Dropping out of Participation)"></a>B.25.6. Early Exit (Dropping out of Participation)</h3><p>当参与同步序列的线程必须提前退出该序列时，该线程必须在退出之前显式退出参与。 其余参与线程可以正常进行后续的 <code>cuda::barrier</code> 到达和等待操作。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda/barrier&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">bool</span> <span class="title">condition_check</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">early_exit_kernel</span><span class="params">(<span class="type">int</span> N)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">using</span> barrier = cuda::barrier&lt;cuda::thread_scope_block&gt;;</span><br><span class="line">    __shared__ barrier bar;</span><br><span class="line">    <span class="keyword">auto</span> block = cooperative_groups::<span class="built_in">this_thread_block</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (block.<span class="built_in">thread_rank</span>() == <span class="number">0</span>)</span><br><span class="line">        <span class="built_in">init</span>(&amp;bar , block.<span class="built_in">size</span>());</span><br><span class="line">    block.<span class="built_in">sync</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; ++i) &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">condition_check</span>()) &#123;</span><br><span class="line">          bar.<span class="built_in">arrive_and_drop</span>();</span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/* other threads can proceed normally */</span></span><br><span class="line">        barrier::arrival_token token = bar.<span class="built_in">arrive</span>();</span><br><span class="line">        <span class="comment">/* code between arrive and wait */</span></span><br><span class="line">        bar.<span class="built_in">wait</span>(std::<span class="built_in">move</span>(token)); <span class="comment">/* wait for all threads to arrive */</span></span><br><span class="line">        <span class="comment">/* code after wait */</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此操作到达 <code>cuda::barrier</code> 以履行参与线程到达当前阶段的义务，然后减少下一阶段的预期到达计数，以便不再期望该线程到达屏障。</p>
<h3 id="B-25-7-Memory-Barrier-Primitives-Interface"><a href="#B-25-7-Memory-Barrier-Primitives-Interface" class="headerlink" title="B.25.7. Memory Barrier Primitives Interface"></a>B.25.7. Memory Barrier Primitives Interface</h3><p>内存屏障原语是 <code>cuda::barrier</code> 功能的 C类型(C-like) 接口。 这些原语可通过包含 <code>&lt;cuda_awbarrier_primitives.h&gt;</code> 头文件获得。</p>
<h4 id="B-25-7-1-Data-Types"><a href="#B-25-7-1-Data-Types" class="headerlink" title="B.25.7.1. Data Types"></a>B.25.7.1. Data Types</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="comment">/* implementation defined */</span> <span class="type">__mbarrier_t</span>;</span><br><span class="line"><span class="keyword">typedef</span> <span class="comment">/* implementation defined */</span> <span class="type">__mbarrier_token_t</span>;     </span><br></pre></td></tr></table></figure>
<h4 id="B-25-7-2-Memory-Barrier-Primitives-API"><a href="#B-25-7-2-Memory-Barrier-Primitives-API" class="headerlink" title="B.25.7.2. Memory Barrier Primitives API"></a>B.25.7.2. Memory Barrier Primitives API</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">uint32_t</span> __mbarrier_maximum_count();</span><br><span class="line"><span class="type">void</span> __mbarrier_init(<span class="type">__mbarrier_t</span>* bar, <span class="type">uint32_t</span> expected_count); </span><br></pre></td></tr></table></figure>
<ul>
<li><code>bar</code> 必须是指向 <code>__shared__</code> 内存的指针。</li>
<li><code>expected_count &lt;= __mbarrier_maximum_count()</code></li>
<li>将当前和下一阶段的 <code>*bar</code> 预期到达计数初始化为 <code>expected_count</code>。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __mbarrier_inval(<span class="type">__mbarrier_t</span>* bar); </span><br></pre></td></tr></table></figure>
<ul>
<li><code>bar</code> 必须是指向共享内存中的 <code>mbarrier</code> 对象的指针。</li>
<li>在重新使用相应的共享内存之前，需要使 <code>*bar</code> 无效。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">__mbarrier_token_t</span> __mbarrier_arrive(<span class="type">__mbarrier_t</span>* bar);    </span><br></pre></td></tr></table></figure>
<ul>
<li><code>*bar</code> 的初始化必须在此调用之前发生。</li>
<li>待处理计数不得为零。</li>
<li>原子地减少屏障当前阶段的挂起计数。</li>
<li>在递减之前返回与屏障状态关联的到达token。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">__mbarrier_token_t</span> __mbarrier_arrive_and_drop(<span class="type">__mbarrier_t</span>* bar);   </span><br></pre></td></tr></table></figure>
<ul>
<li><code>*bar</code> 的初始化必须在此调用之前发生。</li>
<li>待处理计数不得为零。</li>
<li>原子地减少当前阶段的未决计数和屏障下一阶段的预期计数。</li>
<li>在递减之前返回与屏障状态关联的到达token。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">bool</span> __mbarrier_test_wait(<span class="type">__mbarrier_t</span>* bar, <span class="type">__mbarrier_token_t</span> token);  </span><br></pre></td></tr></table></figure>
<ul>
<li>token必须与 <code>*this</code> 的前一个阶段或当前阶段相关联。</li>
<li>如果 token 与 <code>*bar</code> 的前一个阶段相关联，则返回 true，否则返回 false。</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Note: This API has been deprecated in CUDA 11.1</span></span><br><span class="line"><span class="type">uint32_t</span> __mbarrier_pending_count(<span class="type">__mbarrier_token_t</span> token);    </span><br></pre></td></tr></table></figure>
<h2 id="B-26-Asynchronous-Data-Copies"><a href="#B-26-Asynchronous-Data-Copies" class="headerlink" title="B.26. Asynchronous Data Copies"></a>B.26. Asynchronous Data Copies</h2><p>CUDA 11 引入了带有 <code>memcpy_async</code> API 的异步数据操作，以允许设备代码显式管理数据的异步复制。 <code>memcpy_async</code> 功能使 CUDA 内核能够将计算与数据传输重叠。</p>
<h3 id="B-26-1-memcpy-async-API"><a href="#B-26-1-memcpy-async-API" class="headerlink" title="B.26.1. memcpy_async API"></a>B.26.1. memcpy_async API</h3><p><code>memcpy_async</code> API 在 <code>cuda/barrier、cuda/pipeline</code> 和<code>cooperative_groups/memcpy_async.h</code> 头文件中提供。</p>
<p><code>cuda::memcpy_async</code> API 与 <code>cuda::barrier</code> 和 <code>cuda::pipeline</code> 同步原语一起使用，而<code>cooperative_groups::memcpy_async</code> 使用 <code>coopertive_groups::wait</code> 进行同步。</p>
<p>这些 API 具有非常相似的语义：将对象从 <code>src</code> 复制到 <code>dst</code>，就好像由另一个线程执行一样，在完成复制后，可以通过 <code>cuda::pipeline、cuda::barrier</code> 或<code>cooperative_groups::wait</code> 进行同步。</p>
<p><a href="https://nvidia.github.io/libcudacxx"><code>libcudacxx</code></a> API 文档和一些示例中提供了 <code>cuda::barrier</code> 和 <code>cuda::pipeline</code> 的 <code>cuda::memcpy_async</code> 重载的完整 API 文档。</p>
<p><code>Cooperation_groups::memcpy_async</code> 的 API 文档在<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cooperative-groups">文档的合作组部分</a>中提供。</p>
<p>使用 <code>cuda::barrier</code> 和 <code>cuda::pipeline</code> 的 <code>memcpy_async</code> API 需要 7.0 或更高的计算能力。在具有 8.0 或更高计算能力的设备上，从全局内存到共享内存的 <code>memcpy_async</code> 操作可以受益于硬件加速。</p>
<h3 id="B-26-2-Copy-and-Compute-Pattern-Staging-Data-Through-Shared-Memory"><a href="#B-26-2-Copy-and-Compute-Pattern-Staging-Data-Through-Shared-Memory" class="headerlink" title="B.26.2. Copy and Compute Pattern - Staging Data Through Shared Memory"></a>B.26.2. Copy and Compute Pattern - Staging Data Through Shared Memory</h3><p>CUDA 应用程序通常采用一种<strong><em>copy and compute</em></strong> 模式：</p>
<ul>
<li>从全局内存中获取数据，</li>
<li>将数据存储到共享内存中，</li>
<li>对共享内存数据执行计算，并可能将结果写回全局内存。</li>
</ul>
<p>以下部分说明了如何在使用和不使用<code>memcpy_async</code> 功能的情况下表达此模式：</p>
<ul>
<li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#without_memcpy_async">没有 memcpy_async</a> 部分介绍了一个不与数据移动重叠计算并使用中间寄存器复制数据的示例。</li>
<li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#with_memcpy_async">使用 memcpy_async</a> 部分改进了前面的示例，引入了<code>cooperation_groups::memcpy_async</code> 和 <code>cuda::memcpy_async</code> API 直接将数据从全局复制到共享内存，而不使用中间寄存器。</li>
<li>使用 <code>cuda::barrier</code> 的<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memcpy_async_barrier">异步数据拷贝</a>部分显示了带有协作组和屏障的 memcpy</li>
<li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#with-memcpy_async-pipeline-pattern-single">单步异步数据拷贝</a>展示了利用单步<code>cuda::pipeline</code>的memcpy</li>
<li><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#with-memcpy_async-pipeline-pattern-multi">多步异步数据拷贝</a>展示了使用<code>cuda::pipeline</code>多步memcpy</li>
</ul>
<h3 id="B-26-3-Without-memcpy-async"><a href="#B-26-3-Without-memcpy-async" class="headerlink" title="B.26.3. Without memcpy_async"></a>B.26.3. Without memcpy_async</h3><p>如果没有 <code>memcpy_async</code>，复制和计算模式的复制阶段表示为 <code>shared[local_idx] = global[global_idx]</code>。 这种全局到共享内存的复制被扩展为从全局内存读取到寄存器，然后从寄存器写入共享内存。</p>
<p>当这种模式出现在迭代算法中时，每个线程块需要在 <code>shared[local_idx] = global[global_idx]</code> 分配之后进行同步，以确保在计算阶段开始之前对共享内存的所有写入都已完成。 线程块还需要在计算阶段之后再次同步，以防止在所有线程完成计算之前覆盖共享内存。 此模式在以下代码片段中进行了说明。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">compute</span><span class="params">(<span class="type">int</span>* global_out, <span class="type">int</span> <span class="type">const</span>* shared_in)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Computes using all values of current batch from shared memory.</span></span><br><span class="line">    <span class="comment">// Stores this thread&#x27;s result back to global memory.</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">without_memcpy_async</span><span class="params">(<span class="type">int</span>* global_out, <span class="type">int</span> <span class="type">const</span>* global_in, <span class="type">size_t</span> size, <span class="type">size_t</span> batch_sz)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> grid = cooperative_groups::<span class="built_in">this_grid</span>();</span><br><span class="line">  <span class="keyword">auto</span> block = cooperative_groups::<span class="built_in">this_thread_block</span>();</span><br><span class="line">  <span class="built_in">assert</span>(size == batch_sz * grid.<span class="built_in">size</span>()); <span class="comment">// Exposition: input size fits batch_sz * grid_size</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">extern</span> __shared__ <span class="type">int</span> shared[]; <span class="comment">// block.size() * sizeof(int) bytes</span></span><br><span class="line"></span><br><span class="line">  <span class="type">size_t</span> local_idx = block.<span class="built_in">thread_rank</span>();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">size_t</span> batch = <span class="number">0</span>; batch &lt; batch_sz; ++batch) &#123;</span><br><span class="line">    <span class="comment">// Compute the index of the current batch for this block in global memory:</span></span><br><span class="line">    <span class="type">size_t</span> block_batch_idx = block.<span class="built_in">group_index</span>().x * block.<span class="built_in">size</span>() + grid.<span class="built_in">size</span>() * batch;</span><br><span class="line">    <span class="type">size_t</span> global_idx = block_batch_idx + threadIdx.x;</span><br><span class="line">    shared[local_idx] = global_in[global_idx];</span><br><span class="line"></span><br><span class="line">    block.<span class="built_in">sync</span>(); <span class="comment">// Wait for all copies to complete</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">compute</span>(global_out + block_batch_idx, shared); <span class="comment">// Compute and write result to global memory</span></span><br><span class="line"></span><br><span class="line">    block.<span class="built_in">sync</span>(); <span class="comment">// Wait for compute using shared memory to finish</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<h3 id="B-26-4-With-memcpy-async"><a href="#B-26-4-With-memcpy-async" class="headerlink" title="B.26.4. With memcpy_async"></a>B.26.4. With memcpy_async</h3><p>使用 <code>memcpy_async</code>，从全局内存中分配共享内存</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shared[local_idx] = global_in[global_idx];</span><br></pre></td></tr></table></figure>
<p>替换为来自<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cooperative-groups">合作组</a>的异步复制操作</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cooperative_groups::<span class="built_in">memcpy_async</span>(group, shared, global_in + batch_idx, <span class="built_in">sizeof</span>(<span class="type">int</span>) * block.<span class="built_in">size</span>());</span><br></pre></td></tr></table></figure>
<p><code>cooperation_groups::memcpy_async</code> API 将 <code>sizeof(int) * block.size()</code> 字节从 <code>global_in + batch_idx</code> 开始的全局内存复制到共享数据。 这个操作就像由另一个线程执行一样发生，在复制完成后，它与当前线程对<code>cooperative_groups::wait</code> 的调用同步。 在复制操作完成之前，修改全局数据或读取写入共享数据会引入数据竞争。</p>
<p>在具有 8.0 或更高计算能力的设备上，从全局内存到共享内存的 <code>memcpy_async</code> 传输可以受益于硬件加速，从而避免通过中间寄存器传输数据。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups/memcpy_async.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">compute</span><span class="params">(<span class="type">int</span>* global_out, <span class="type">int</span> <span class="type">const</span>* shared_in)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">with_memcpy_async</span><span class="params">(<span class="type">int</span>* global_out, <span class="type">int</span> <span class="type">const</span>* global_in, <span class="type">size_t</span> size, <span class="type">size_t</span> batch_sz)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> grid = cooperative_groups::<span class="built_in">this_grid</span>();</span><br><span class="line">  <span class="keyword">auto</span> block = cooperative_groups::<span class="built_in">this_thread_block</span>();</span><br><span class="line">  <span class="built_in">assert</span>(size == batch_sz * grid.<span class="built_in">size</span>()); <span class="comment">// Exposition: input size fits batch_sz * grid_size</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">extern</span> __shared__ <span class="type">int</span> shared[]; <span class="comment">// block.size() * sizeof(int) bytes</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">size_t</span> batch = <span class="number">0</span>; batch &lt; batch_sz; ++batch) &#123;</span><br><span class="line">    <span class="type">size_t</span> block_batch_idx = block.<span class="built_in">group_index</span>().x * block.<span class="built_in">size</span>() + grid.<span class="built_in">size</span>() * batch;</span><br><span class="line">    <span class="comment">// Whole thread-group cooperatively copies whole batch to shared memory:</span></span><br><span class="line">    cooperative_groups::<span class="built_in">memcpy_async</span>(block, shared, global_in + block_batch_idx, <span class="built_in">sizeof</span>(<span class="type">int</span>) * block.<span class="built_in">size</span>());</span><br><span class="line"></span><br><span class="line">    cooperative_groups::<span class="built_in">wait</span>(block); <span class="comment">// Joins all threads, waits for all copies to complete</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">compute</span>(global_out + block_batch_idx, shared);</span><br><span class="line"></span><br><span class="line">    block.<span class="built_in">sync</span>();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;&#125;      </span><br></pre></td></tr></table></figure>
<h3 id="B-26-5-Asynchronous-Data-Copies-using-cuda-barrier"><a href="#B-26-5-Asynchronous-Data-Copies-using-cuda-barrier" class="headerlink" title="B.26.5. Asynchronous Data Copies using cuda::barrier"></a>B.26.5. Asynchronous Data Copies using cuda::barrier</h3><p><code>cuda::memcpy_async</code> 的 <code>cuda::barrier</code> 重载允许使用屏障同步异步数据传输。 此重载执行复制操作，就好像由绑定到屏障的另一个线程执行：在创建时增加当前阶段的预期计数，并在完成复制操作时减少它，这样屏障的阶段只会前进, 当所有参与屏障的线程都已到达，并且绑定到屏障当前阶段的所有 memcpy_async 都已完成时。 以下示例使用block范围的屏障，所有块线程都参与其中，并将等待操作与屏障到达和等待交换，同时提供与前一个示例相同的功能：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda/barrier&gt;</span></span></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">compute</span><span class="params">(<span class="type">int</span>* global_out, <span class="type">int</span> <span class="type">const</span>* shared_in)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">with_barrier</span><span class="params">(<span class="type">int</span>* global_out, <span class="type">int</span> <span class="type">const</span>* global_in, <span class="type">size_t</span> size, <span class="type">size_t</span> batch_sz)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> grid = cooperative_groups::<span class="built_in">this_grid</span>();</span><br><span class="line">  <span class="keyword">auto</span> block = cooperative_groups::<span class="built_in">this_thread_block</span>();</span><br><span class="line">  <span class="built_in">assert</span>(size == batch_sz * grid.<span class="built_in">size</span>()); <span class="comment">// Assume input size fits batch_sz * grid_size</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">extern</span> __shared__ <span class="type">int</span> shared[]; <span class="comment">// block.size() * sizeof(int) bytes</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create a synchronization object (C++20 barrier)</span></span><br><span class="line">  __shared__ cuda::barrier&lt;cuda::thread_scope::thread_scope_block&gt; barrier;</span><br><span class="line">  <span class="keyword">if</span> (block.<span class="built_in">thread_rank</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">init</span>(&amp;barrier, block.<span class="built_in">size</span>()); <span class="comment">// Friend function initializes barrier</span></span><br><span class="line">  &#125;</span><br><span class="line">  block.<span class="built_in">sync</span>();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">size_t</span> batch = <span class="number">0</span>; batch &lt; batch_sz; ++batch) &#123;</span><br><span class="line">    <span class="type">size_t</span> block_batch_idx = block.<span class="built_in">group_index</span>().x * block.<span class="built_in">size</span>() + grid.<span class="built_in">size</span>() * batch;</span><br><span class="line">    cuda::<span class="built_in">memcpy_async</span>(block, shared, global_in + block_batch_idx, <span class="built_in">sizeof</span>(<span class="type">int</span>) * block.<span class="built_in">size</span>(), barrier);</span><br><span class="line"></span><br><span class="line">    barrier.<span class="built_in">arrive_and_wait</span>(); <span class="comment">// Waits for all copies to complete</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">compute</span>(global_out + block_batch_idx, shared);</span><br><span class="line"></span><br><span class="line">    block.<span class="built_in">sync</span>();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="B-26-6-Performance-Guidance-for-memcpy-async"><a href="#B-26-6-Performance-Guidance-for-memcpy-async" class="headerlink" title="B.26.6. Performance Guidance for memcpy_async"></a>B.26.6. Performance Guidance for memcpy_async</h3><p>对于计算能力 8.x，pipeline机制在同一 CUDA warp中的 CUDA 线程之间共享。 这种共享会导致成批的 memcpy_async 纠缠在warp中，这可能会在某些情况下影响性能。</p>
<p>本节重点介绍 warp-entanglement 对提交、等待和到达操作的影响。 有关各个操作的概述，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#pipeline-interface">pipeline接口</a>和<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#pipeline-primitives-interface">pipeline基元接口</a>。</p>
<h4 id="B-26-6-1-Alignment"><a href="#B-26-6-1-Alignment" class="headerlink" title="B.26.6.1. Alignment"></a>B.26.6.1. Alignment</h4><p>在具有计算能力 8.0 的设备上，<code>cp.async</code> 系列指令允许将数据从全局异步复制到共享内存。 这些指令支持一次复制 4、8 和 16 个字节。 如果提供给 <code>memcpy_async</code> 的大小是 4、8 或 16 的倍数，并且传递给 <code>memcpy_async</code> 的两个指针都对齐到 4、8 或 16 对齐边界，则可以使用专门的异步内存操作来实现 <code>memcpy_async</code>。</p>
<p>此外，为了在使用 <code>memcpy_async</code> API 时获得最佳性能，需要为共享内存和全局内存对齐 128 字节。</p>
<p>对于指向对齐要求为 1 或 2 的类型值的指针，通常无法证明指针始终对齐到更高的对齐边界。 确定是否可以使用 <code>cp.async</code> 指令必须延迟到运行时。 执行这样的运行时对齐检查会增加代码大小并增加运行时开销。</p>
<p><code>cuda::aligned_size_t&lt;size_t Align&gt;(size_t size)Shape</code>可用于证明传递给 <code>memcpy_async</code>的两个指针都与 <code>Align</code> 边界对齐，并且大小是 <code>Align</code> 的倍数，方法是将其作为参数传递，其中 <code>memcpy_async</code> API 需要一个 <code>Shape</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cuda::<span class="built_in">memcpy_async</span>(group, dst, src, cuda::<span class="built_in">aligned_size_t</span>&lt;<span class="number">16</span>&gt;(N * block.<span class="built_in">size</span>()), pipeline);</span><br></pre></td></tr></table></figure>
<p>如果验证不正确，则行为未定义。 </p>
<h4 id="B-26-6-2-Trivially-copyable"><a href="#B-26-6-2-Trivially-copyable" class="headerlink" title="B.26.6.2. Trivially copyable"></a>B.26.6.2. Trivially copyable</h4><p>在具有计算能力 8.0 的设备上，<code>cp.async</code> 系列指令允许将数据从全局异步复制到共享内存。 如果传递给 <code>memcpy_async</code> 的指针类型不指向 <code>TriviallyCopyable</code> 类型，则需要调用每个输出元素的复制构造函数，并且这些指令不能用于加速 <code>memcpy_async</code>。</p>
<h4 id="B-26-6-3-Warp-Entanglement-Commit"><a href="#B-26-6-3-Warp-Entanglement-Commit" class="headerlink" title="B.26.6.3. Warp Entanglement - Commit"></a>B.26.6.3. Warp Entanglement - Commit</h4><p><code>memcpy_async</code> 批处理的序列在 warp 中共享。 提交操作被合并，使得对于调用提交操作的所有聚合线程，序列增加一次。 如果warp完全收敛，则序列加1； 如果warp完全发散，则序列增加 32。</p>
<ul>
<li><p>设 PB 为 warp-shared pipeline的实际批次序列. </p>
<p>  <code>PB = &#123;BP0, BP1, BP2, …, BPL&#125;</code></p>
</li>
<li><p>令 TB 为线程感知的批次序列，就好像该序列仅由该线程调用提交操作增加。</p>
<p>  <code>TB = &#123;BT0, BT1, BT2, …, BTL&#125;</code></p>
<p>  <code>pipeline::producer_commit()</code> 返回值来自线程感知的批处理序列。</p>
</li>
<li><p>线程感知序列中的索引始终与实际warp共享序列中的相等或更大的索引对齐。 仅当从聚合线程调用所有提交操作时，序列才相等。</p>
<p>  <code>BTn ≡ BPm 其中 n &lt;= m</code></p>
</li>
</ul>
<p>例如，当warp完全发散时：</p>
<ul>
<li><p>warp共享pipeline的实际顺序是：PB = {0, 1, 2, 3, …, 31} (PL=31)。</p>
</li>
<li><p>该warp的每个线程的感知顺序将是：</p>
<ul>
<li><p><code>Thread 0: TB = &#123;0&#125; (TL=0)</code></p>
</li>
<li><p><code>Thread 1: TB = &#123;0&#125; (TL=0)</code></p>
<p>  <code>…</code></p>
</li>
<li><p><code>Thread 31: TB = &#123;0&#125; (TL=0)</code></p>
</li>
</ul>
</li>
</ul>
<h4 id="B-26-6-4-Warp-Entanglement-Wait"><a href="#B-26-6-4-Warp-Entanglement-Wait" class="headerlink" title="B.26.6.4. Warp Entanglement - Wait"></a>B.26.6.4. Warp Entanglement - Wait</h4><p>CUDA 线程调用 <code>pipeline_consumer_wait_prior&lt;N&gt;()</code> 或 <code>pipeline::consumer_wait()</code> 以等待感知序列 TB 中的批次完成。 注意 <code>pipeline::consumer_wait()</code> 等价于 <code>pipeline_consumer_wait_prior&lt;N&gt;()</code>，其中 <code>N = PL</code>。</p>
<p><code>pipeline_consumer_wait_prior&lt;N&gt;()</code> 函数等待实际序列中的批次，至少达到并包括 <code>PL-N</code>。 由于 <code>TL &lt;= PL</code>，等待批次达到并包括 <code>PL-N</code> 包括等待批次 <code>TL-N</code>。 因此，当 <code>TL &lt; PL</code> 时，线程将无意中等待更多的、更新的批次。</p>
<p>在上面的极端完全发散的warp示例中，每个线程都可以等待所有 32 个批次。</p>
<h4 id="B-26-6-5-Warp-Entanglement-Arrive-On"><a href="#B-26-6-5-Warp-Entanglement-Arrive-On" class="headerlink" title="B.26.6.5. Warp Entanglement - Arrive-On"></a>B.26.6.5. Warp Entanglement - Arrive-On</h4><p><code>Warp-divergence</code> 影响到达 <code>on(bar)</code> 操作更新障碍的次数。 如果调用 warp 完全收敛，则屏障更新一次。 如果调用 warp 完全发散，则将 32 个单独的更新应用于屏障。</p>
<h4 id="B-26-6-6-Keep-Commit-and-Arrive-On-Operations-Converged"><a href="#B-26-6-6-Keep-Commit-and-Arrive-On-Operations-Converged" class="headerlink" title="B.26.6.6. Keep Commit and Arrive-On Operations Converged"></a>B.26.6.6. Keep Commit and Arrive-On Operations Converged</h4><p>建议提交和到达调用由聚合线程进行：</p>
<ul>
<li>通过保持线程的感知批次序列与实际序列对齐，不要过度等待，并且</li>
<li>以最小化对屏障对象的更新。</li>
</ul>
<p>当这些操作之前的代码分支线程时，应该在调用提交或到达操作之前通过 <code>__syncwarp</code> 重新收敛warp。</p>
<h2 id="B-27-Asynchronous-Data-Copies-using-cuda-pipeline"><a href="#B-27-Asynchronous-Data-Copies-using-cuda-pipeline" class="headerlink" title="B.27. Asynchronous Data Copies using cuda::pipeline"></a>B.27. Asynchronous Data Copies using cuda::pipeline</h2><p>CUDA 提供 <code>cuda::pipeline</code> 同步对象来管理异步数据移动并将其与计算重叠。</p>
<p><code>cuda::pipeline</code> 的 API 文档在 <a href="https://nvidia.github.io/libcudacxx">libcudacxx API</a> 中提供。 流水线对象是一个具有头尾的双端 N 阶段队列，用于按照先进先出 (FIFO) 的顺序处理工作。 管道对象具有以下成员函数来管理管道的各个阶段。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Pipeline Class Member Function</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>producer_acquire</code></td>
<td>Acquires an available stage in the pipeline internal queue.</td>
</tr>
<tr>
<td><code>producer_commit</code></td>
<td>Commits the asynchronous operations issued after the producer_acquire call on the currently acquired stage of the pipeline.</td>
</tr>
<tr>
<td><code>consumer_wait</code></td>
<td>Wait for completion of all asynchronous operations on the oldest stage of the pipeline.</td>
</tr>
<tr>
<td><code>consumer_release</code></td>
<td>Release the oldest stage of the pipeline to the pipeline object for reuse. The released stage can be then acquired by the producer.</td>
</tr>
</tbody>
</table>
</div>
<h3 id="B-27-1-Single-Stage-Asynchronous-Data-Copies-using-cuda-pipeline"><a href="#B-27-1-Single-Stage-Asynchronous-Data-Copies-using-cuda-pipeline" class="headerlink" title="B.27.1. Single-Stage Asynchronous Data Copies using cuda::pipeline"></a>B.27.1. Single-Stage Asynchronous Data Copies using <code>cuda::pipeline</code></h3><p>在前面的示例中，我们展示了如何使用<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#collectives-cg-wait"><code>cooperative_groups</code></a>和 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#aw-barrier"><code>cuda::barrier</code></a> 进行异步数据传输。 在本节中，我们将使用带有单个阶段的 <code>cuda::pipeline</code> API 来调度异步拷贝。 稍后我们将扩展此示例以显示多阶段重叠计算和复制。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups/memcpy_async.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda/pipeline&gt;</span></span></span><br><span class="line">        </span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">compute</span><span class="params">(<span class="type">int</span>* global_out, <span class="type">int</span> <span class="type">const</span>* shared_in)</span></span>;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">with_single_stage</span><span class="params">(<span class="type">int</span>* global_out, <span class="type">int</span> <span class="type">const</span>* global_in, <span class="type">size_t</span> size, <span class="type">size_t</span> batch_sz)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> grid = cooperative_groups::<span class="built_in">this_grid</span>();</span><br><span class="line">    <span class="keyword">auto</span> block = cooperative_groups::<span class="built_in">this_thread_block</span>();</span><br><span class="line">    <span class="built_in">assert</span>(size == batch_sz * grid.<span class="built_in">size</span>()); <span class="comment">// Assume input size fits batch_sz * grid_size</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">size_t</span> stages_count = <span class="number">1</span>; <span class="comment">// Pipeline with one stage</span></span><br><span class="line">    <span class="comment">// One batch must fit in shared memory:</span></span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="type">int</span> shared[];  <span class="comment">// block.size() * sizeof(int) bytes</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Allocate shared storage for a two-stage cuda::pipeline:</span></span><br><span class="line">    __shared__ cuda::pipeline_shared_state&lt;</span><br><span class="line">        cuda::thread_scope::thread_scope_block,</span><br><span class="line">        stages_count</span><br><span class="line">    &gt; shared_state;</span><br><span class="line">    <span class="keyword">auto</span> pipeline = cuda::<span class="built_in">make_pipeline</span>(block, &amp;shared_state);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Each thread processes `batch_sz` elements.</span></span><br><span class="line">    <span class="comment">// Compute offset of the batch `batch` of this thread block in global memory:</span></span><br><span class="line">    <span class="keyword">auto</span> block_batch = [&amp;](<span class="type">size_t</span> batch) -&gt; <span class="type">int</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> block.<span class="built_in">group_index</span>().x * block.<span class="built_in">size</span>() + grid.<span class="built_in">size</span>() * batch;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> batch = <span class="number">0</span>; batch &lt; batch_sz; ++batch) &#123;</span><br><span class="line">        <span class="type">size_t</span> global_idx = <span class="built_in">block_batch</span>(batch);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Collectively acquire the pipeline head stage from all producer threads:</span></span><br><span class="line">        pipeline.<span class="built_in">producer_acquire</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Submit async copies to the pipeline&#x27;s head stage to be</span></span><br><span class="line">        <span class="comment">// computed in the next loop iteration</span></span><br><span class="line">        cuda::<span class="built_in">memcpy_async</span>(block, shared, global_in + global_idx, <span class="built_in">sizeof</span>(<span class="type">int</span>) * block.<span class="built_in">size</span>(), pipeline);</span><br><span class="line">        <span class="comment">// Collectively commit (advance) the pipeline&#x27;s head stage</span></span><br><span class="line">        pipeline.<span class="built_in">producer_commit</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Collectively wait for the operations committed to the</span></span><br><span class="line">        <span class="comment">// previous `compute` stage to complete:</span></span><br><span class="line">        pipeline.<span class="built_in">consumer_wait</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Computation overlapped with the memcpy_async of the &quot;copy&quot; stage:</span></span><br><span class="line">        <span class="built_in">compute</span>(global_out + global_idx, shared);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Collectively release the stage resources</span></span><br><span class="line">        pipeline.<span class="built_in">consumer_release</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>B.27.2. Multi-Stage Asynchronous Data Copies using <code>cuda::pipeline</code></p>
<p>在前面带有<code>cooperative_groups::wait</code> 和<code>cuda::barrier</code> 的示例中，内核线程立即等待数据传输到共享内存完成。 这避免了数据从全局内存传输到寄存器，但不会通过重叠计算隐藏 <code>memcpy_async</code> 操作的延迟。</p>
<p>为此，我们在以下示例中使用 CUDA <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#pipeline-interface"><strong><em>pipeline</em></strong></a> 功能。 它提供了一种管理 <code>memcpy_async</code> 批处理序列的机制，使 CUDA 内核能够将内存传输与计算重叠。 以下示例实现了一个将数据传输与计算重叠的两级管道。 它：</p>
<ul>
<li>初始化管道共享状态（更多下文）</li>
<li>通过为第一批调度 <code>memcpy_async</code> 来启动管道。</li>
<li>循环所有批次：它为下一个批次安排 <code>memcpy_async</code>，在完成上一个批次的 <code>memcpy_async</code> 时阻塞所有线程，然后将上一个批次的计算与下一个批次的内存的异步副本重叠。</li>
<li>最后，它通过对最后一批执行计算来排空管道。</li>
</ul>
<p>请注意，为了与 <code>cuda::pipeline</code> 的互操作性，此处使用来自 <code>cuda/pipeline</code> 头文件的 <code>cuda::memcpy_async</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups/memcpy_async.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda/pipeline&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">compute</span><span class="params">(<span class="type">int</span>* global_out, <span class="type">int</span> <span class="type">const</span>* shared_in)</span></span>;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">with_staging</span><span class="params">(<span class="type">int</span>* global_out, <span class="type">int</span> <span class="type">const</span>* global_in, <span class="type">size_t</span> size, <span class="type">size_t</span> batch_sz)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> grid = cooperative_groups::<span class="built_in">this_grid</span>();</span><br><span class="line">    <span class="keyword">auto</span> block = cooperative_groups::<span class="built_in">this_thread_block</span>();</span><br><span class="line">    <span class="built_in">assert</span>(size == batch_sz * grid.<span class="built_in">size</span>()); <span class="comment">// Assume input size fits batch_sz * grid_size</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">size_t</span> stages_count = <span class="number">2</span>; <span class="comment">// Pipeline with two stages</span></span><br><span class="line">    <span class="comment">// Two batches must fit in shared memory:</span></span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="type">int</span> shared[];  <span class="comment">// stages_count * block.size() * sizeof(int) bytes</span></span><br><span class="line">    <span class="type">size_t</span> shared_offset[stages_count] = &#123; <span class="number">0</span>, block.<span class="built_in">size</span>() &#125;; <span class="comment">// Offsets to each batch</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate shared storage for a two-stage cuda::pipeline:</span></span><br><span class="line">    __shared__ cuda::pipeline_shared_state&lt;</span><br><span class="line">        cuda::thread_scope::thread_scope_block,</span><br><span class="line">        stages_count</span><br><span class="line">    &gt; shared_state;</span><br><span class="line">    <span class="keyword">auto</span> pipeline = cuda::<span class="built_in">make_pipeline</span>(block, &amp;shared_state);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Each thread processes `batch_sz` elements.</span></span><br><span class="line">    <span class="comment">// Compute offset of the batch `batch` of this thread block in global memory:</span></span><br><span class="line">    <span class="keyword">auto</span> block_batch = [&amp;](<span class="type">size_t</span> batch) -&gt; <span class="type">int</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> block.<span class="built_in">group_index</span>().x * block.<span class="built_in">size</span>() + grid.<span class="built_in">size</span>() * batch;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialize first pipeline stage by submitting a `memcpy_async` to fetch a whole batch for the block:</span></span><br><span class="line">    <span class="keyword">if</span> (batch_sz == <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">    pipeline.<span class="built_in">producer_acquire</span>();</span><br><span class="line">    cuda::<span class="built_in">memcpy_async</span>(block, shared + shared_offset[<span class="number">0</span>], global_in + <span class="built_in">block_batch</span>(<span class="number">0</span>), <span class="built_in">sizeof</span>(<span class="type">int</span>) * block.<span class="built_in">size</span>(), pipeline);</span><br><span class="line">    pipeline.<span class="built_in">producer_commit</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Pipelined copy/compute:</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> batch = <span class="number">1</span>; batch &lt; batch_sz; ++batch) &#123;</span><br><span class="line">        <span class="comment">// Stage indices for the compute and copy stages:</span></span><br><span class="line">        <span class="type">size_t</span> compute_stage_idx = (batch - <span class="number">1</span>) % <span class="number">2</span>;</span><br><span class="line">        <span class="type">size_t</span> copy_stage_idx = batch % <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">size_t</span> global_idx = <span class="built_in">block_batch</span>(batch);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Collectively acquire the pipeline head stage from all producer threads:</span></span><br><span class="line">        pipeline.<span class="built_in">producer_acquire</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Submit async copies to the pipeline&#x27;s head stage to be</span></span><br><span class="line">        <span class="comment">// computed in the next loop iteration</span></span><br><span class="line">        cuda::<span class="built_in">memcpy_async</span>(block, shared + shared_offset[copy_stage_idx], global_in + global_idx, <span class="built_in">sizeof</span>(<span class="type">int</span>) * block.<span class="built_in">size</span>(), pipeline);</span><br><span class="line">        <span class="comment">// Collectively commit (advance) the pipeline&#x27;s head stage</span></span><br><span class="line">        pipeline.<span class="built_in">producer_commit</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Collectively wait for the operations commited to the</span></span><br><span class="line">        <span class="comment">// previous `compute` stage to complete:</span></span><br><span class="line">        pipeline.<span class="built_in">consumer_wait</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Computation overlapped with the memcpy_async of the &quot;copy&quot; stage:</span></span><br><span class="line">        <span class="built_in">compute</span>(global_out + global_idx, shared + shared_offset[compute_stage_idx]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Collectively release the stage resources</span></span><br><span class="line">        pipeline.<span class="built_in">consumer_release</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compute the data fetch by the last iteration</span></span><br><span class="line">    pipeline.<span class="built_in">consumer_wait</span>();</span><br><span class="line">    <span class="built_in">compute</span>(global_out + <span class="built_in">block_batch</span>(batch_sz<span class="number">-1</span>), shared + shared_offset[(batch_sz - <span class="number">1</span>) % <span class="number">2</span>]);</span><br><span class="line">    pipeline.<span class="built_in">consumer_release</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#pipeline-interface"><strong><em>pipeline</em></strong></a> 对象是一个带有头尾的双端队列，用于按照先进先出 (FIFO) 的顺序处理工作。 生产者线程将工作提交到管道的头部，而消费者线程从管道的尾部提取工作。 在上面的示例中，所有线程都是生产者和消费者线程。 线程首先提交 <code>memcpy_async</code> 操作以获取下一批，同时等待上一批 <code>memcpy_async</code> 操作完成。</p>
<ul>
<li>将工作提交到pipeline阶段包括：<ul>
<li>使用 <code>pipeline.producer_acquire()</code> 从一组生产者线程中集体获取pipeline头。</li>
<li>将 <code>memcpy_async</code> 操作提交到pipeline头。</li>
<li>使用 <code>pipeline.producer_commit()</code> 共同提交（推进）pipeline头。</li>
</ul>
</li>
<li>使用先前提交的阶段包括：<ul>
<li>共同等待阶段完成，例如，使用 pipeline.consumer_wait() 等待尾部（最旧）阶段。</li>
<li>使用 <code>pipeline.consumer_release()</code> 集体释放阶段。</li>
</ul>
</li>
</ul>
<p><code>cuda::pipeline_shared_state&lt;scope, count&gt;</code>封装了允许管道处理多达 <code>count</code> 个并发阶段的有限资源。 如果所有资源都在使用中，则 <code>pipeline.producer_acquire()</code> 会阻塞生产者线程，直到消费者线程释放下一个管道阶段的资源。<br>通过将循环的 <code>prolog</code> 和 <code>epilog</code> 与循环本身合并，可以以更简洁的方式编写此示例，如下所示：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">size_t</span> stages_count = <span class="number">2</span> <span class="comment">/* Pipeline with stages_count stages */</span>&gt;</span><br><span class="line">__global__ <span class="type">void</span> <span class="built_in">with_staging_unified</span>(<span class="type">int</span>* global_out, <span class="type">int</span> <span class="type">const</span>* global_in, <span class="type">size_t</span> size, <span class="type">size_t</span> batch_sz) &#123;</span><br><span class="line">    <span class="keyword">auto</span> grid = cooperative_groups::<span class="built_in">this_grid</span>();</span><br><span class="line">    <span class="keyword">auto</span> block = cooperative_groups::<span class="built_in">this_thread_block</span>();</span><br><span class="line">    <span class="built_in">assert</span>(size == batch_sz * grid.<span class="built_in">size</span>()); <span class="comment">// Assume input size fits batch_sz * grid_size</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="type">int</span> shared[]; <span class="comment">// stages_count * block.size() * sizeof(int) bytes</span></span><br><span class="line">    <span class="type">size_t</span> shared_offset[stages_count];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> s = <span class="number">0</span>; s &lt; stages_count; ++s) shared_offset[s] = s * block.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">    __shared__ cuda::pipeline_shared_state&lt;</span><br><span class="line">        cuda::thread_scope::thread_scope_block,</span><br><span class="line">        stages_count</span><br><span class="line">    &gt; shared_state;</span><br><span class="line">    <span class="keyword">auto</span> pipeline = cuda::<span class="built_in">make_pipeline</span>(block, &amp;shared_state);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> block_batch = [&amp;](<span class="type">size_t</span> batch) -&gt; <span class="type">int</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> block.<span class="built_in">group_index</span>().x * block.<span class="built_in">size</span>() + grid.<span class="built_in">size</span>() * batch;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// compute_batch: next batch to process</span></span><br><span class="line">    <span class="comment">// fetch_batch:  next batch to fetch from global memory</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> compute_batch = <span class="number">0</span>, fetch_batch = <span class="number">0</span>; compute_batch &lt; batch_sz; ++compute_batch) &#123;</span><br><span class="line">        <span class="comment">// The outer loop iterates over the computation of the batches</span></span><br><span class="line">        <span class="keyword">for</span> (; fetch_batch &lt; batch_sz &amp;&amp; fetch_batch &lt; (compute_batch + stages_count); ++fetch_batch) &#123;</span><br><span class="line">            <span class="comment">// This inner loop iterates over the memory transfers, making sure that the pipeline is always full</span></span><br><span class="line">            pipeline.<span class="built_in">producer_acquire</span>();</span><br><span class="line">            <span class="type">size_t</span> shared_idx = fetch_batch % stages_count;</span><br><span class="line">            <span class="type">size_t</span> batch_idx = fetch_batch;</span><br><span class="line">            <span class="type">size_t</span> block_batch_idx = <span class="built_in">block_batch</span>(batch_idx);</span><br><span class="line">            cuda::<span class="built_in">memcpy_async</span>(block, shared + shared_offset[shared_idx], global_in + block_batch_idx, <span class="built_in">sizeof</span>(<span class="type">int</span>) * block.<span class="built_in">size</span>(), pipeline);</span><br><span class="line">            pipeline.<span class="built_in">producer_commit</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        pipeline.<span class="built_in">consumer_wait</span>();</span><br><span class="line">        <span class="type">int</span> shared_idx = compute_batch % stages_count;</span><br><span class="line">        <span class="type">int</span> batch_idx = compute_batch;</span><br><span class="line">        <span class="built_in">compute</span>(global_out + <span class="built_in">block_batch</span>(batch_idx), shared + shared_offset[shared_idx]);</span><br><span class="line">        pipeline.<span class="built_in">consumer_release</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面使用的 <code>pipeline&lt;thread_scope_block&gt;</code> 原语非常灵活，并且支持我们上面的示例未使用的两个特性：块中的任意线程子集都可以参与管道，并且从参与的线程中，任何子集都可以成为生产者 ，消费者，或两者兼而有之。 在以下示例中，具有“偶数”线程等级的线程是生产者，而其他线程是消费者：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">compute</span><span class="params">(<span class="type">int</span>* global_out, <span class="type">int</span> shared_in)</span></span>; </span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="type">size_t</span> stages_count = <span class="number">2</span>&gt;</span><br><span class="line">__global__ <span class="type">void</span> <span class="built_in">with_specialized_staging_unified</span>(<span class="type">int</span>* global_out, <span class="type">int</span> <span class="type">const</span>* global_in, <span class="type">size_t</span> size, <span class="type">size_t</span> batch_sz) &#123;</span><br><span class="line">    <span class="keyword">auto</span> grid = cooperative_groups::<span class="built_in">this_grid</span>();</span><br><span class="line">    <span class="keyword">auto</span> block = cooperative_groups::<span class="built_in">this_thread_block</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// In this example, threads with &quot;even&quot; thread rank are producers, while threads with &quot;odd&quot; thread rank are consumers:</span></span><br><span class="line">    <span class="type">const</span> cuda::pipeline_role thread_role </span><br><span class="line">      = block.<span class="built_in">thread_rank</span>() % <span class="number">2</span> == <span class="number">0</span>? cuda::pipeline_role::producer : cuda::pipeline_role::consumer;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Each thread block only has half of its threads as producers:</span></span><br><span class="line">    <span class="keyword">auto</span> producer_threads = block.<span class="built_in">size</span>() / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Map adjacent even and odd threads to the same id:</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> thread_idx = block.<span class="built_in">thread_rank</span>() / <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> elements_per_batch = size / batch_sz;</span><br><span class="line">    <span class="keyword">auto</span> elements_per_batch_per_block = elements_per_batch / grid.<span class="built_in">group_dim</span>().x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="type">int</span> shared[]; <span class="comment">// stages_count * elements_per_batch_per_block * sizeof(int) bytes</span></span><br><span class="line">    <span class="type">size_t</span> shared_offset[stages_count];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> s = <span class="number">0</span>; s &lt; stages_count; ++s) shared_offset[s] = s * elements_per_batch_per_block;</span><br><span class="line"></span><br><span class="line">    __shared__ cuda::pipeline_shared_state&lt;</span><br><span class="line">        cuda::thread_scope::thread_scope_block,</span><br><span class="line">        stages_count</span><br><span class="line">    &gt; shared_state;</span><br><span class="line">    cuda::pipeline pipeline = cuda::<span class="built_in">make_pipeline</span>(block, &amp;shared_state, thread_role);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Each thread block processes `batch_sz` batches.</span></span><br><span class="line">    <span class="comment">// Compute offset of the batch `batch` of this thread block in global memory:</span></span><br><span class="line">    <span class="keyword">auto</span> block_batch = [&amp;](<span class="type">size_t</span> batch) -&gt; <span class="type">int</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> elements_per_batch * batch + elements_per_batch_per_block * blockIdx.x;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> compute_batch = <span class="number">0</span>, fetch_batch = <span class="number">0</span>; compute_batch &lt; batch_sz; ++compute_batch) &#123;</span><br><span class="line">        <span class="comment">// The outer loop iterates over the computation of the batches</span></span><br><span class="line">        <span class="keyword">for</span> (; fetch_batch &lt; batch_sz &amp;&amp; fetch_batch &lt; (compute_batch + stages_count); ++fetch_batch) &#123;</span><br><span class="line">            <span class="comment">// This inner loop iterates over the memory transfers, making sure that the pipeline is always full</span></span><br><span class="line">            <span class="keyword">if</span> (thread_role == cuda::pipeline_role::producer) &#123;</span><br><span class="line">                <span class="comment">// Only the producer threads schedule asynchronous memcpys:</span></span><br><span class="line">                pipeline.<span class="built_in">producer_acquire</span>();</span><br><span class="line">                <span class="type">size_t</span> shared_idx = fetch_batch % stages_count;</span><br><span class="line">                <span class="type">size_t</span> batch_idx = fetch_batch;</span><br><span class="line">                <span class="type">size_t</span> global_batch_idx = <span class="built_in">block_batch</span>(batch_idx) + thread_idx;</span><br><span class="line">                <span class="type">size_t</span> shared_batch_idx = shared_offset[shared_idx] + thread_idx;</span><br><span class="line">                cuda::<span class="built_in">memcpy_async</span>(shared + shared_batch_idx, global_in + global_batch_idx, <span class="built_in">sizeof</span>(<span class="type">int</span>), pipeline);</span><br><span class="line">                pipeline.<span class="built_in">producer_commit</span>();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (thread_role == cuda::pipeline_role::consumer) &#123;</span><br><span class="line">            <span class="comment">// Only the consumer threads compute:</span></span><br><span class="line">            pipeline.<span class="built_in">consumer_wait</span>();</span><br><span class="line">            <span class="type">size_t</span> shared_idx = compute_batch % stages_count;</span><br><span class="line">            <span class="type">size_t</span> global_batch_idx = <span class="built_in">block_batch</span>(compute_batch) + thread_idx;</span><br><span class="line">            <span class="type">size_t</span> shared_batch_idx = shared_offset[shared_idx] + thread_idx;</span><br><span class="line">            <span class="built_in">compute</span>(global_out + global_batch_idx, *(shared + shared_batch_idx));</span><br><span class="line">            pipeline.<span class="built_in">consumer_release</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>管道执行了一些优化，例如，当所有线程既是生产者又是消费者时，但总的来说，支持所有这些特性的成本不能完全消除。 例如，流水线在共享内存中存储并使用一组屏障进行同步，如果块中的所有线程都参与流水线，这并不是真正必要的。</p>
<p>对于块中的所有线程都参与管道的特殊情况，我们可以通过使用<code>pipeline&lt;thread_scope_thread&gt;</code> 结合 <code>__syncthreads()</code> 做得比<code>pipeline&lt;thread_scope_block&gt;</code> 更好：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="type">size_t</span> stages_count&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">with_staging_scope_thread</span><span class="params">(<span class="type">int</span>* global_out, <span class="type">int</span> <span class="type">const</span>* global_in, <span class="type">size_t</span> size, <span class="type">size_t</span> batch_sz)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> grid = cooperative_groups::<span class="built_in">this_grid</span>();</span><br><span class="line">    <span class="keyword">auto</span> block = cooperative_groups::<span class="built_in">this_thread_block</span>();</span><br><span class="line">    <span class="keyword">auto</span> thread = cooperative_groups::<span class="built_in">this_thread</span>();</span><br><span class="line">    <span class="built_in">assert</span>(size == batch_sz * grid.<span class="built_in">size</span>()); <span class="comment">// Assume input size fits batch_sz * grid_size</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="type">int</span> shared[]; <span class="comment">// stages_count * block.size() * sizeof(int) bytes</span></span><br><span class="line">    <span class="type">size_t</span> shared_offset[stages_count];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> s = <span class="number">0</span>; s &lt; stages_count; ++s) shared_offset[s] = s * block.<span class="built_in">size</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// No pipeline::shared_state needed</span></span><br><span class="line">    cuda::pipeline&lt;cuda::thread_scope_thread&gt; pipeline = cuda::<span class="built_in">make_pipeline</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> block_batch = [&amp;](<span class="type">size_t</span> batch) -&gt; <span class="type">int</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> block.<span class="built_in">group_index</span>().x * block.<span class="built_in">size</span>() + grid.<span class="built_in">size</span>() * batch;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> compute_batch = <span class="number">0</span>, fetch_batch = <span class="number">0</span>; compute_batch &lt; batch_sz; ++compute_batch) &#123;</span><br><span class="line">        <span class="keyword">for</span> (; fetch_batch &lt; batch_sz &amp;&amp; fetch_batch &lt; (compute_batch + stages_count); ++fetch_batch) &#123;</span><br><span class="line">            pipeline.<span class="built_in">producer_acquire</span>();</span><br><span class="line">            <span class="type">size_t</span> shared_idx = fetch_batch % stages_count;</span><br><span class="line">            <span class="type">size_t</span> batch_idx = fetch_batch;</span><br><span class="line">            <span class="comment">// Each thread fetches its own data:</span></span><br><span class="line">            <span class="type">size_t</span> thread_batch_idx = <span class="built_in">block_batch</span>(batch_idx) + threadIdx.x;</span><br><span class="line">            <span class="comment">// The copy is performed by a single `thread` and the size of the batch is now that of a single element:</span></span><br><span class="line">            cuda::<span class="built_in">memcpy_async</span>(thread, shared + shared_offset[shared_idx] + threadIdx.x, global_in + thread_batch_idx, <span class="built_in">sizeof</span>(<span class="type">int</span>), pipeline);</span><br><span class="line">            pipeline.<span class="built_in">producer_commit</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        pipeline.<span class="built_in">consumer_wait</span>();</span><br><span class="line">        block.<span class="built_in">sync</span>(); <span class="comment">// __syncthreads: All memcpy_async of all threads in the block for this stage have completed here</span></span><br><span class="line">        <span class="type">int</span> shared_idx = compute_batch % stages_count;</span><br><span class="line">        <span class="type">int</span> batch_idx = compute_batch;</span><br><span class="line">        <span class="built_in">compute</span>(global_out + <span class="built_in">block_batch</span>(batch_idx), shared + shared_offset[shared_idx]);</span><br><span class="line">        pipeline.<span class="built_in">consumer_release</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果计算操作只读取与当前线程在同一 warp 中的其他线程写入的共享内存，则 <code>__syncwarp()</code> 就足够了。</p>
<h3 id="B-27-3-Pipeline-Interface"><a href="#B-27-3-Pipeline-Interface" class="headerlink" title="B.27.3. Pipeline Interface"></a>B.27.3. Pipeline Interface</h3><p><a href="https://nvidia.github.io/libcudacxx">libcudacxx</a> API 文档中提供了 <code>cuda::memcpy_async</code> 的完整 API 文档以及一些示例。</p>
<p><code>pipeline</code>接口需要</p>
<ul>
<li>至少 CUDA 11.0，</li>
<li>至少与 ISO C++ 2011 兼容，例如，使用 -std=c++11 编译，</li>
<li><code>#include &lt;cuda/pipeline&gt;</code>。<br>  对于类似 C 的接口，在不兼容 ISO C++ 2011 的情况下进行编译时，请参阅 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#pipeline-primitives-interface">Pipeline Primitives Interface</a>。</li>
</ul>
<h3 id="B-27-4-Pipeline-Primitives-Interface"><a href="#B-27-4-Pipeline-Primitives-Interface" class="headerlink" title="B.27.4. Pipeline Primitives Interface"></a>B.27.4. Pipeline Primitives Interface</h3><p><code>pipeline</code>原语是用于 <code>memcpy_async</code> 功能的类 C 接口。 通过包含 <cuda_pipeline.h> 头文件，可以使用<code>pipeline</code>原语接口。 在不兼容 ISO C++ 2011 的情况下进行编译时，请包含 <code>&lt;cuda_pipeline_primitives.h&gt;</code> 头文件。</p>
<h3 id="B-27-4-1-memcpy-async-Primitive"><a href="#B-27-4-1-memcpy-async-Primitive" class="headerlink" title="B.27.4.1. memcpy_async Primitive"></a>B.27.4.1. memcpy_async Primitive</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __pipeline_memcpy_async(<span class="type">void</span>* __restrict__ dst_shared,</span><br><span class="line">                             <span class="type">const</span> <span class="type">void</span>* __restrict__ src_global,</span><br><span class="line">                             <span class="type">size_t</span> size_and_align,</span><br><span class="line">                             <span class="type">size_t</span> zfill=<span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<ul>
<li>请求提交以下操作以进行异步评估：</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">size_t</span> i = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (; i &lt; size_and_align - zfill; ++i) ((<span class="type">char</span>*)dst_shared)[i] = ((<span class="type">char</span>*)src_shared)[i]; <span class="comment">/* copy */</span></span><br><span class="line"><span class="keyword">for</span> (; i &lt; size_and_align; ++i) ((<span class="type">char</span>*)dst_shared)[i] = <span class="number">0</span>; <span class="comment">/* zero-fill */</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>需要:</p>
<ul>
<li><code>dst_shared</code> 必须是指向 <code>memcpy_async</code> 的共享内存目标的指针。</li>
<li><code>src_global</code> 必须是指向 <code>memcpy_async</code> 的全局内存源的指针。</li>
<li><code>size_and_align</code> 必须为 4、8 或 16。</li>
<li><code>zfill &lt;= size_and_align</code>.</li>
<li><code>size_and_align</code> 必须是 <code>dst_shared</code> 和 <code>src_global</code> 的对齐方式。</li>
</ul>
</li>
<li><p>任何线程在等待 <code>memcpy_async</code> 操作完成之前修改源内存或观察目标内存都是一种竞争条件。 在提交 <code>memcpy_async</code> 操作和等待其完成之间，以下任何操作都会引入竞争条件：</p>
<ul>
<li>从 <code>dst_shared</code> 加载。</li>
<li>存储到 <code>dst_shared</code> 或 <code>src_global。</code></li>
<li>对 <code>dst_shared</code> 或 <code>src_global</code> 应用原子更新。</li>
</ul>
</li>
</ul>
<h4 id="B-27-4-2-Commit-Primitive"><a href="#B-27-4-2-Commit-Primitive" class="headerlink" title="B.27.4.2. Commit Primitive"></a>B.27.4.2. Commit Primitive</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __pipeline_commit();</span><br></pre></td></tr></table></figure>
<ul>
<li>将提交的 <code>memcpy_async</code> 作为当前批次提交到管道。</li>
</ul>
<h4 id="B-27-4-3-Wait-Primitive"><a href="#B-27-4-3-Wait-Primitive" class="headerlink" title="B.27.4.3. Wait Primitive"></a>B.27.4.3. Wait Primitive</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __pipeline_wait_prior(<span class="type">size_t</span> N);</span><br></pre></td></tr></table></figure>
<ul>
<li>令 <code>&#123;0, 1, 2, ..., L&#125;</code> 为与给定线程调用 <code>__pipeline_commit()</code> 相关联的索引序列。</li>
<li>等待批次完成，至少包括 <code>L-N</code>。</li>
</ul>
<h4 id="B-27-4-4-Arrive-On-Barrier-Primitive"><a href="#B-27-4-4-Arrive-On-Barrier-Primitive" class="headerlink" title="B.27.4.4. Arrive On Barrier Primitive"></a>B.27.4.4. Arrive On Barrier Primitive</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __pipeline_arrive_on(<span class="type">__mbarrier_t</span>* bar);</span><br></pre></td></tr></table></figure>
<ul>
<li><code>bar</code> 指向共享内存中的屏障。</li>
<li>将屏障到达计数加一，当在此调用之前排序的所有 <code>memcpy_async</code> 操作已完成时，到达计数减一，因此对到达计数的净影响为零。 用户有责任确保到达计数的增量不超过 <code>__mbarrier_maximum_count()</code>。</li>
</ul>
<h2 id="B-28-Profiler-Counter-Function"><a href="#B-28-Profiler-Counter-Function" class="headerlink" title="B.28. Profiler Counter Function"></a>B.28. Profiler Counter Function</h2><p>每个多处理器都有一组 16 个硬件计数器，应用程序可以通过调用 <code>__prof_trigger()</code> 函数用一条指令递增这些计数器。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __prof_trigger(<span class="type">int</span> counter);</span><br></pre></td></tr></table></figure>
<p>索引计数器的每个多处理器硬件计数器每warp增加一。 计数器 8 到 15 是保留的，不应由应用程序使用。</p>
<p>计数器 0, 1, …, 7 的值可以通过 <code>nvprof --events prof_trigger_0x</code> 通过 <code>nvprof</code> 获得，其中 <code>x</code> 为 0, 1, …, 7。所有计数器在每次内核启动之前都会重置（注意，在收集 计数器，内核启动是同步的，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#concurrent-execution-host-device">主机和设备之间的并发执行</a>中所述）。</p>
<h2 id="B-29-Assertion"><a href="#B-29-Assertion" class="headerlink" title="B.29. Assertion"></a>B.29. Assertion</h2><p>只有计算能力 2.x 及更高版本的设备才支持Assertion。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">assert</span><span class="params">(<span class="type">int</span> expression)</span></span>;</span><br></pre></td></tr></table></figure>
<p>如果表达式等于 0，停止内核执行。 如果程序在调试器中运行，则会触发断点，并且调试器可用于检查设备的当前状态。 否则，表达式等于 0 的每个线程在通过 <code>cudaDeviceSynchronize()</code>、<code>cudaStreamSynchronize()</code> 或 <code>cudaEventSynchronize()</code> 与主机同步后向 <code>stderr</code> 打印一条消息。 该消息的格式如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;filename&gt;:&lt;line number&gt;:&lt;function&gt;:</span><br><span class="line">block: [blockId.x,blockId.x,blockIdx.z],</span><br><span class="line">thread: [threadIdx.x,threadIdx.y,threadIdx.z]</span><br><span class="line">Assertion `&lt;expression&gt;` failed.</span><br></pre></td></tr></table></figure>
<p>对同一设备进行的任何后续主机端同步调用都将返回 <code>cudaErrorAssert</code>。 在调用 <code>cudaDeviceReset()</code> 重新初始化设备之前，不能再向该设备发送命令。</p>
<p>如果<code>expression</code>不为零，则内核执行不受影响。</p>
<p>例如，源文件 <code>test.cu</code>中的以下程序</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;assert.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">testAssert</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> is_one = <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> should_be_one = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// This will have no effect</span></span><br><span class="line">    <span class="built_in">assert</span>(is_one);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// This will halt kernel execution</span></span><br><span class="line">    <span class="built_in">assert</span>(should_be_one);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>* argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    testAssert&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>将会输出:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test.cu:19: void testAssert(): block: [0,0,0], thread: [0,0,0] Assertion `should_be_one` failed.</span><br></pre></td></tr></table></figure>
<p>Assertion用于调试目的。 它们会影响性能，因此建议在产品代码中禁用它们。 它们可以在编译时通过在包含 <code>assert.h</code> 之前定义 <code>NDEBUG</code> 预处理器宏来禁用。 请注意，表达式不应是具有副作用的表达式（例如 (<code>++i &gt; 0</code>)），否则禁用Assertion将影响代码的功能。</p>
<h2 id="B-30-Trap-function"><a href="#B-30-Trap-function" class="headerlink" title="B.30. Trap function"></a>B.30. Trap function</h2><p>可以通过从任何设备线程调用 <code>__trap()</code> 函数来启动trap操作。 </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __trap();</span><br></pre></td></tr></table></figure>
<p>内核的执行被中止并在主机程序中引发中断。</p>
<h2 id="B-31-Breakpoint-Function"><a href="#B-31-Breakpoint-Function" class="headerlink" title="B.31. Breakpoint Function"></a>B.31. Breakpoint Function</h2><p>可以通过从任何设备线程调用 <code>__brkpt()</code> 函数来暂停内核函数的执行。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __brkpt();</span><br></pre></td></tr></table></figure>
<h2 id="B-32-Formatted-Output"><a href="#B-32-Formatted-Output" class="headerlink" title="B.32. Formatted Output"></a>B.32. Formatted Output</h2><p>格式化输出仅受计算能力 2.x 及更高版本的设备支持。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">printf</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *format[, arg, ...])</span></span>;</span><br></pre></td></tr></table></figure>
<p>将来自内核的格式化输出打印到主机端输出流。</p>
<p>内核中的 <code>printf()</code>函数的行为方式与标准 C 库 <code>printf()</code> 函数类似，用户可以参考主机系统的手册以获取有关 <code>printf()</code> 行为的完整描述。本质上，作为格式传入的字符串输出到主机上的流，在遇到格式说明符的任何地方都会从参数列表中进行替换。下面列出了支持的格式说明符。</p>
<p><code>printf()</code> 命令作为任何其他设备端函数执行：每个线程，并且在调用线程的上下文中。对于多线程内核，这意味着每个线程都将使用指定的线程数据执行对 <code>printf()</code> 的直接调用。然后，输出字符串的多个版本将出现在主机流中，每个遇到 <code>printf()</code> 的做线程一次。</p>
<p>如果只需要单个输出字符串，则由程序员将输出限制为单个线程（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#examples-per-thread"><code>示例</code></a>以获取说明性示例）。</p>
<p>与返回打印字符数的 C 标准 <code>printf()</code> 不同，CUDA 的 <code>printf()</code> 返回已解析参数的数量。如果格式字符串后面没有参数，则返回 0。如果格式字符串为 NULL，则返回 -1。如果发生内部错误，则返回 -2。</p>
<h3 id="B-32-1-Format-Specifiers"><a href="#B-32-1-Format-Specifiers" class="headerlink" title="B.32.1. Format Specifiers"></a>B.32.1. Format Specifiers</h3><p>对于标准 <code>printf()</code>，格式说明符采用以下形式：<code>%[flags][width][.precision][size]type</code></p>
<p>支持以下字段（有关所有行为的完整描述，请参阅广泛可用的文档）：</p>
<ul>
<li>Flags: <code>&#39;#&#39; &#39; &#39; &#39;0&#39; &#39;+&#39; &#39;-&#39;</code></li>
<li>Width: <code>&#39;*&#39; &#39;0-9&#39;</code></li>
<li>Precision: <code>&#39;0-9&#39;</code></li>
<li>Size: <code>&#39;h&#39; &#39;l&#39; &#39;ll&#39;</code></li>
<li>Type: <code>&quot;%cdiouxXpeEfgGaAs&quot;</code></li>
</ul>
<p>请注意，CUDA 的 <code>printf()</code> 将接受标志、宽度、精度、大小和类型的任何组合，无论它们总体上是否构成有效的格式说明符。 换句话说，<code>“%hd”</code>将被接受，并且 <code>printf</code> 将接受参数列表中相应位置的双精度变量。</p>
<h3 id="B-32-2-Limitations"><a href="#B-32-2-Limitations" class="headerlink" title="B.32.2. Limitations"></a>B.32.2. Limitations</h3><p><code>printf()</code> 输出的最终格式化发生在主机系统上。 这意味着主机系统的编译器和 C 库必须能够理解格式字符串。 已尽一切努力确保 CUDA 的 printf 函数支持的格式说明符形成来自最常见主机编译器的通用子集，但确切的行为将取决于主机操作系统。</p>
<p>如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#format-specifiers">格式说明符</a>中所述， <code>printf()</code> 将接受有效标志和类型的所有组合。 这是因为它无法确定在最终输出被格式化的主机系统上什么是有效的，什么是无效的。 这样做的效果是，如果程序发出包含无效组合的格式字符串，则输出可能未定义。</p>
<p>除了格式字符串之外，<code>printf()</code> 命令最多可以接受 32 个参数。 除此之外的其他参数将被忽略，格式说明符按原样输出。</p>
<p>由于在 64 位 Windows 平台上 <code>long</code> 类型的大小不同（在 64 位 Windows 平台上为 4 个字节，在其他 64 位平台上为 8 个字节），在非 Windows 64 位机器上编译的内核但在 win64 机器上运行将看到包含“%ld”的所有格式字符串的损坏输出。 建议编译平台与执行平台相匹配，以确保安全。</p>
<p><code>printf()</code> 的输出缓冲区在内核启动之前设置为固定大小（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#associated-host-side-api">关联的主机端 API</a>）。 它是循环的，如果在内核执行期间产生的输出多于缓冲区可以容纳的输出，则旧的输出将被覆盖。 仅当执行以下操作之一时才会刷新：</p>
<ul>
<li>通过 <code>&lt;&lt;&lt;&gt;&gt;&gt;</code> 或 <code>cuLaunchKernel()</code> 启动内核（在启动开始时，如果 <code>CUDA_LAUNCH_BLOCKING</code> 环境变量设置为 1，也在启动结束时），</li>
<li>通过 <code>cudaDeviceSynchronize()</code>、<code>cuCtxSynchronize()</code>、<code>cudaStreamSynchronize()</code>、<code>cuStreamSynchronize()</code>、<code>cudaEventSynchronize()</code> 或 <code>cuEventSynchronize()</code> 进行同步，</li>
<li>通过任何阻塞版本的 <code>cudaMemcpy*()</code> 或 <code>cuMemcpy*()</code> 进行内存复制，</li>
<li>通过 <code>cuModuleLoad()</code> 或 <code>cuModuleUnload()</code> 加载/卸载模块，</li>
<li>通过 <code>cudaDeviceReset()</code> 或 <code>cuCtxDestroy()</code> 销毁上下文。</li>
<li>在执行由 <code>cudaStreamAddCallback</code> 或 <code>cuStreamAddCallback</code> 添加的流回调之前。</li>
</ul>
<p>请注意，程序退出时缓冲区不会自动刷新。 用户必须显式调用 <code>cudaDeviceReset()</code> 或 <code>cuCtxDestroy()</code>，如下例所示。</p>
<p><code>printf()</code>在内部使用共享数据结构，因此调用 <code>printf()</code> 可能会改变线程的执行顺序。 特别是，调用 <code>printf()</code> 的线程可能比不调用 <code>printf()</code> 的线程花费更长的执行路径，并且该路径长度取决于 <code>printf()</code> 的参数。 但是请注意，除了显式 <code>__syncthreads()</code> 障碍外，<strong><em>CUDA 不保证线程执行顺序</em></strong>，因此无法判断执行顺序是否已被 <code>printf()</code> 或硬件中的其他调度行为修改。</p>
<h3 id="B-32-3-Associated-Host-Side-API"><a href="#B-32-3-Associated-Host-Side-API" class="headerlink" title="B.32.3. Associated Host-Side API"></a>B.32.3. Associated Host-Side API</h3><p>以下 API 函数获取和设置用于将 printf() 参数和内部元数据传输到主机的缓冲区大小（默认为 1 兆字节）：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaDeviceGetLimit</span>(<span class="type">size_t</span>* size,cudaLimitPrintfFifoSize)</span><br><span class="line"><span class="built_in">cudaDeviceSetLimit</span>(cudaLimitPrintfFifoSize, <span class="type">size_t</span> size)</span><br></pre></td></tr></table></figure>
<h3 id="B-32-4-Examples"><a href="#B-32-4-Examples" class="headerlink" title="B.32.4. Examples"></a>B.32.4. Examples</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">helloCUDA</span><span class="params">(<span class="type">float</span> f)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Hello thread %d, f=%f\n&quot;</span>, threadIdx.x, f);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    helloCUDA&lt;&lt;&lt;<span class="number">1</span>, <span class="number">5</span>&gt;&gt;&gt;(<span class="number">1.2345f</span>);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的代码将会输出:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Hello thread <span class="number">2</span>, f=<span class="number">1.2345</span></span><br><span class="line">Hello thread <span class="number">1</span>, f=<span class="number">1.2345</span></span><br><span class="line">Hello thread <span class="number">4</span>, f=<span class="number">1.2345</span></span><br><span class="line">Hello thread <span class="number">0</span>, f=<span class="number">1.2345</span></span><br><span class="line">Hello thread <span class="number">3</span>, f=<span class="number">1.2345</span></span><br></pre></td></tr></table></figure>
<p>注意每个线程如何遇到 <code>printf()</code> 命令，因此输出行数与网格中启动的线程数一样多。 正如预期的那样，全局值（即 <code>float f</code>）在所有线程之间是通用的，而局部值（即 <code>threadIdx.x</code>）在每个线程中是不同的。</p>
<p>下面的代码:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">helloCUDA</span><span class="params">(<span class="type">float</span> f)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Hello thread %d, f=%f\n&quot;</span>, threadIdx.x, f) ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    helloCUDA&lt;&lt;&lt;<span class="number">1</span>, <span class="number">5</span>&gt;&gt;&gt;(<span class="number">1.2345f</span>);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>将会输出:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello thread <span class="number">0</span>, f=<span class="number">1.2345</span></span><br></pre></td></tr></table></figure>
<p>不言而喻，<code>if()</code> 语句限制了哪些线程将调用 <code>printf</code>，因此只能看到一行输出。</p>
<h2 id="B-33-Dynamic-Global-Memory-Allocation-and-Operations"><a href="#B-33-Dynamic-Global-Memory-Allocation-and-Operations" class="headerlink" title="B.33. Dynamic Global Memory Allocation and Operations"></a>B.33. Dynamic Global Memory Allocation and Operations</h2><p>动态全局内存分配和操作仅受计算能力 2.x 及更高版本的设备支持。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ __device__ <span class="type">void</span>* <span class="title">malloc</span><span class="params">(<span class="type">size_t</span> size)</span></span>;</span><br><span class="line">__device__ <span class="type">void</span> *__nv_aligned_device_malloc(<span class="type">size_t</span> size, <span class="type">size_t</span> align);</span><br><span class="line"><span class="function">__host__ __device__  <span class="type">void</span> <span class="title">free</span><span class="params">(<span class="type">void</span>* ptr)</span></span>;</span><br></pre></td></tr></table></figure>
<p>从全局内存中的固定大小的堆中动态分配和释放内存。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ __device__ <span class="type">void</span>* <span class="title">memcpy</span><span class="params">(<span class="type">void</span>* dest, <span class="type">const</span> <span class="type">void</span>* src, <span class="type">size_t</span> size)</span></span>;</span><br></pre></td></tr></table></figure>
<p>从 <code>src</code> 指向的内存位置复制 <code>size</code> 个字节到 <code>dest</code> 指向的内存位置。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ __device__ <span class="type">void</span>* <span class="title">memset</span><span class="params">(<span class="type">void</span>* ptr, <span class="type">int</span> value, <span class="type">size_t</span> size)</span></span>;</span><br></pre></td></tr></table></figure>
<p>将 <code>ptr</code> 指向的内存块的 <code>size</code> 字节设置为 <code>value</code>（解释为无符号字符）。</p>
<p>CUDA 内核中的 <code>malloc()</code> 函数从设备堆中分配至少 <code>size</code> 个字节，并返回一个指向已分配内存的指针，如果没有足够的内存来满足请求，则返回 NULL。返回的指针保证与 16 字节边界对齐。</p>
<p>内核中的 CUDA <code>__nv_aligned_device_malloc()</code> 函数从设备堆中分配至少 <code>size</code> 个字节，并返回一个指向已分配内存的指针，如果内存不足以满足请求的大小或对齐，则返回 NULL。分配内存的地址将是 <code>align</code> 的倍数。 <code>align</code> 必须是 2 的非零幂。</p>
<p>CUDA 内核中的 <code>free()</code> 函数释放 <code>ptr</code> 指向的内存，该内存必须由先前对 <code>malloc()</code> 或 <code>__nv_aligned_device_malloc()</code> 的调用返回。如果 <code>ptr</code> 为 NULL，则忽略对 <code>free()</code> 的调用。使用相同的 <code>ptr</code> 重复调用 <code>free()</code> 具有未定义的行为。</p>
<p>给定 CUDA 线程通过 <code>malloc()</code> 或 <code>__nv_aligned_device_malloc()</code> 分配的内存在 CUDA 上下文的生命周期内保持分配状态，或者直到通过调用 <code>free()</code> 显式释放。它可以被任何其他 CUDA 线程使用，即使在随后的内核启动时也是如此。任何 CUDA 线程都可以释放由另一个线程分配的内存，但应注意确保不会多次释放同一指针。</p>
<h3 id="B-33-1-Heap-Memory-Allocation"><a href="#B-33-1-Heap-Memory-Allocation" class="headerlink" title="B.33.1. Heap Memory Allocation"></a>B.33.1. Heap Memory Allocation</h3><p>设备内存堆具有固定大小，必须在任何使用 <code>malloc()、__nv_aligned_device_malloc() 或 free()</code> 的程序加载到上下文之前指定该大小。 如果任何程序在没有明确指定堆大小的情况下使用 <code>malloc() 或 __nv_aligned_device_malloc()</code> ，则会分配 8 MB 的默认堆。</p>
<p>以下 API 函数获取和设置堆大小：</p>
<ul>
<li><code>cudaDeviceGetLimit(size_t* size, cudaLimitMallocHeapSize)</code></li>
<li><code>cudaDeviceSetLimit(cudaLimitMallocHeapSize, size_t size)</code></li>
</ul>
<p>授予的堆大小至少为 <code>size</code> 个字节。 <code>cuCtxGetLimit() 和 cudaDeviceGetLimit()</code> 返回当前请求的堆大小。</p>
<p>当模块被加载到上下文中时，堆的实际内存分配发生，或者显式地通过 CUDA 驱动程序 API（参见<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#module">模块</a>），或者隐式地通过 CUDA 运行时 API（参见 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-c-runtime">CUDA 运行时</a>）。 如果内存分配失败，模块加载会产生 <code>CUDA_ERROR_SHARED_OBJECT_INIT_FAILED</code> 错误。</p>
<p>一旦发生模块加载，堆大小就无法更改，并且不会根据需要动态调整大小。</p>
<p>除了通过主机端 CUDA API 调用（例如 <code>cudaMalloc()</code>）分配为设备堆保留的内存之外。</p>
<h3 id="B-33-2-Interoperability-with-Host-Memory-API"><a href="#B-33-2-Interoperability-with-Host-Memory-API" class="headerlink" title="B.33.2. Interoperability with Host Memory API"></a>B.33.2. Interoperability with Host Memory API</h3><p>通过设备 <code>malloc()</code> 或 <code>__nv_aligned_device_malloc()</code> 分配的内存不能使用运行时释放（即，通过从设备内存调用任何空闲内存函数）。</p>
<p>同样，通过运行时分配的内存（即，通过从设备内存调用任何内存分配函数）不能通过 <code>free()</code> 释放。</p>
<p>此外，在设备代码中调用 <code>malloc()</code> 或 <code>__nv_aligned_device_malloc()</code> 分配的内存不能用于任何运行时或驱动程序 API 调用（即 <code>cudaMemcpy</code>、<code>cudaMemset</code> 等）。</p>
<h3 id="B-33-3-Examples"><a href="#B-33-3-Examples" class="headerlink" title="B.33.3. Examples"></a>B.33.3. Examples</h3><h4 id="B-33-3-1-Per-Thread-Allocation"><a href="#B-33-3-1-Per-Thread-Allocation" class="headerlink" title="B.33.3.1. Per Thread Allocation"></a>B.33.3.1. Per Thread Allocation</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">mallocTest</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">size_t</span> size = <span class="number">123</span>;</span><br><span class="line">    <span class="type">char</span>* ptr = (<span class="type">char</span>*)<span class="built_in">malloc</span>(size);</span><br><span class="line">    <span class="built_in">memset</span>(ptr, <span class="number">0</span>, size);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Thread %d got pointer: %p\n&quot;</span>, threadIdx.x, ptr);</span><br><span class="line">    <span class="built_in">free</span>(ptr);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Set a heap size of 128 megabytes. Note that this must</span></span><br><span class="line">    <span class="comment">// be done before any kernel is launched.</span></span><br><span class="line">    <span class="built_in">cudaDeviceSetLimit</span>(cudaLimitMallocHeapSize, <span class="number">128</span>*<span class="number">1024</span>*<span class="number">1024</span>);</span><br><span class="line">    mallocTest&lt;&lt;&lt;<span class="number">1</span>, <span class="number">5</span>&gt;&gt;&gt;();</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的代码将会输出:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Thread 0 got pointer: 00057020</span><br><span class="line">Thread 1 got pointer: 0005708c</span><br><span class="line">Thread 2 got pointer: 000570f8</span><br><span class="line">Thread 3 got pointer: 00057164</span><br><span class="line">Thread 4 got pointer: 000571d0</span><br></pre></td></tr></table></figure>
<p>注意每个线程如何遇到 <code>malloc()</code> 和 <code>memset()</code> 命令，从而接收和初始化自己的分配。 （确切的指针值会有所不同：这些是说明性的。）</p>
<h4 id="B-33-3-2-Per-Thread-Block-Allocation"><a href="#B-33-3-2-Per-Thread-Block-Allocation" class="headerlink" title="B.33.3.2. Per Thread Block Allocation"></a>B.33.3.2. Per Thread Block Allocation</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">mallocTest</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span>* data;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The first thread in the block does the allocation and then</span></span><br><span class="line">    <span class="comment">// shares the pointer with all other threads through shared memory,</span></span><br><span class="line">    <span class="comment">// so that access can easily be coalesced.</span></span><br><span class="line">    <span class="comment">// 64 bytes per thread are allocated.</span></span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="type">size_t</span> size = blockDim.x * <span class="number">64</span>;</span><br><span class="line">        data = (<span class="type">int</span>*)<span class="built_in">malloc</span>(size);</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Check for failure</span></span><br><span class="line">    <span class="keyword">if</span> (data == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Threads index into the memory, ensuring coalescence</span></span><br><span class="line">    <span class="type">int</span>* ptr = data;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">64</span>; ++i)</span><br><span class="line">        ptr[i * blockDim.x + threadIdx.x] = threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Ensure all threads complete before freeing </span></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Only one thread may free the memory!</span></span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>)</span><br><span class="line">        <span class="built_in">free</span>(data);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">cudaDeviceSetLimit</span>(cudaLimitMallocHeapSize, <span class="number">128</span>*<span class="number">1024</span>*<span class="number">1024</span>);</span><br><span class="line">    mallocTest&lt;&lt;&lt;<span class="number">10</span>, <span class="number">128</span>&gt;&gt;&gt;();</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="B-33-3-3-Allocation-Persisting-Between-Kernel-Launches"><a href="#B-33-3-3-Allocation-Persisting-Between-Kernel-Launches" class="headerlink" title="B.33.3.3. Allocation Persisting Between Kernel Launches"></a>B.33.3.3. Allocation Persisting Between Kernel Launches</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NUM_BLOCKS 20</span></span><br><span class="line"></span><br><span class="line">__device__ <span class="type">int</span>* dataptr[NUM_BLOCKS]; <span class="comment">// Per-block pointer</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">allocmem</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Only the first thread in the block does the allocation</span></span><br><span class="line">    <span class="comment">// since we want only one allocation per block.</span></span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>)</span><br><span class="line">        dataptr[blockIdx.x] = (<span class="type">int</span>*)<span class="built_in">malloc</span>(blockDim.x * <span class="number">4</span>);</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Check for failure</span></span><br><span class="line">    <span class="keyword">if</span> (dataptr[blockIdx.x] == <span class="literal">NULL</span>)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Zero the data with all threads in parallel</span></span><br><span class="line">    dataptr[blockIdx.x][threadIdx.x] = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Simple example: store thread ID into each element</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">usemem</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span>* ptr = dataptr[blockIdx.x];</span><br><span class="line">    <span class="keyword">if</span> (ptr != <span class="literal">NULL</span>)</span><br><span class="line">        ptr[threadIdx.x] += threadIdx.x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print the content of the buffer before freeing it</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">freemem</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span>* ptr = dataptr[blockIdx.x];</span><br><span class="line">    <span class="keyword">if</span> (ptr != <span class="literal">NULL</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Block %d, Thread %d: final value = %d\n&quot;</span>,</span><br><span class="line">                      blockIdx.x, threadIdx.x, ptr[threadIdx.x]);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Only free from one thread!</span></span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>)</span><br><span class="line">        <span class="built_in">free</span>(ptr);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">cudaDeviceSetLimit</span>(cudaLimitMallocHeapSize, <span class="number">128</span>*<span class="number">1024</span>*<span class="number">1024</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate memory</span></span><br><span class="line">    allocmem&lt;&lt;&lt; NUM_BLOCKS, <span class="number">10</span> &gt;&gt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Use memory</span></span><br><span class="line">    usemem&lt;&lt;&lt; NUM_BLOCKS, <span class="number">10</span> &gt;&gt;&gt;();</span><br><span class="line">    usemem&lt;&lt;&lt; NUM_BLOCKS, <span class="number">10</span> &gt;&gt;&gt;();</span><br><span class="line">    usemem&lt;&lt;&lt; NUM_BLOCKS, <span class="number">10</span> &gt;&gt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Free memory</span></span><br><span class="line">    freemem&lt;&lt;&lt; NUM_BLOCKS, <span class="number">10</span> &gt;&gt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="B-34-Execution-Configuration"><a href="#B-34-Execution-Configuration" class="headerlink" title="B.34. Execution Configuration"></a>B.34. Execution Configuration</h2><p>对 <code>__global__</code>函数的任何调用都必须指定该调用的执行配置。执行配置定义了将用于在设备上执行功能的网格和块的维度，以及关联的流（有关流的描述，请参见 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-c-runtime">CUDA 运行时</a>）。</p>
<p>通过在函数名称和带括号的参数列表之间插入 <code>&lt;&lt;&lt; Dg, Db, Ns, S &gt;&gt;&gt;</code> 形式的表达式来指定执行配置，其中：</p>
<ul>
<li>Dg 是 dim3 类型（参见 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#dim3">dim3</a>），并指定网格的维度和大小，使得 Dg.x <em> Dg.y </em> Dg.z 等于正在启动的块数；</li>
<li>Db 是 dim3 类型（参见 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#dim3">dim3</a>），并指定每个块的维度和大小，使得 Db.x <em> Db.y </em> Db.z 等于每个块的线程数；</li>
<li>Ns 是 <code>size_t</code> 类型，指定除了静态分配的内存之外，每个块动态分配的共享内存中的字节数；这个动态分配的内存被声明为外部数组的任何变量使用，如 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared"><code>__shared__</code></a> 中所述； Ns 是一个可选参数，默认为 0；</li>
<li>S 是 <code>cudaStream_t</code> 类型并指定关联的流； S 是一个可选参数，默认为 0。</li>
</ul>
<p>例如，一个函数声明为:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">Func</span><span class="params">(<span class="type">float</span>* parameter)</span></span>;</span><br></pre></td></tr></table></figure>
<p>必须这样调用：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Func&lt;&lt;&lt; Dg, Db, Ns &gt;&gt;&gt;(parameter);</span><br></pre></td></tr></table></figure>
<p>执行配置的参数在实际函数参数之前进行评估。</p>
<p>如果 <code>Dg</code> 或 <code>Db</code> 大于 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities">Compute Capabilities</a> 中指定的设备允许的最大数，或者 Ns 大于设备上可用的最大共享内存量减去静态分配所需的共享内存量，则函数调用将失败 。</p>
<h2 id="B-35-Launch-Bounds"><a href="#B-35-Launch-Bounds" class="headerlink" title="B.35. Launch Bounds"></a>B.35. Launch Bounds</h2><p>正如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#multiprocessor-level">多处理器级别</a>中详细讨论的那样，内核使用的寄存器越少，多处理器上可能驻留的线程和线程块就越多，这可以提高性能。</p>
<p>因此，编译器使用启发式方法来最大限度地减少寄存器使用量，同时将寄存器溢出（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">设备内存访问</a>）和指令计数保持在最低限度。 应用程序可以选择性地通过以启动边界的形式向编译器提供附加信息来帮助这些启发式方法，这些信息使用<code>__global__</code> 函数定义中的 <code>__launch_bounds__()</code> 限定符指定：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span></span><br><span class="line">__launch_bounds__(maxThreadsPerBlock, minBlocksPerMultiprocessor)</span><br><span class="line"><span class="built_in">MyKernel</span>(...)</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>maxThreadsPerBlock</code> 指定应用程序启动 <code>MyKernel()</code> 的每个块的最大线程数； 它编译为 <code>.maxntidPTX</code> 指令；</li>
<li><code>minBlocksPerMultiprocessor</code> 是可选的，指定每个多处理器所需的最小驻留块数； 它编译为 .<code>minnctapersmPTX</code> 指令。</li>
</ul>
<p>如果指定了启动边界，编译器首先从它们推导出内核应该使用的寄存器数量的上限 L，以确保 <code>maxThreadsPerBlock</code> 线程的 <code>minBlocksPerMultiprocessor</code> 块（或单个块，如果未指定 <code>minBlocksPerMultiprocessor</code>）可以驻留在多处理器上（ 有关内核使用的寄存器数量与每个块分配的寄存器数量之间的关系，请参见<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-multithreading">硬件多线程</a>）。 然后编译器通过以下方式优化寄存器使用：</p>
<ul>
<li>如果初始寄存器使用量高于 L，编译器会进一步减少它，直到它变得小于或等于 L，通常以更多的本地内存使用或更多的指令为代价；</li>
<li>如果初始寄存器使用率低于 L<ul>
<li>如果指定了 <code>maxThreadsPerBlock</code> 而未指定 <code>minBlocksPerMultiprocessor</code>，则编译器使用 <code>maxThreadsPerBlock</code> 来确定 <code>n</code> 和 <code>n+1</code> 个常驻块之间转换的寄存器使用阈值（即，当使用较少的寄存器时，可以为额外的常驻块腾出空间，如 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#multiprocessor-level">多处理器级别</a>），然后应用与未指定启动边界时类似的启发式方法；</li>
<li>如果同时指定了 <code>minBlocksPerMultiprocessor</code> 和 <code>maxThreadsPerBlock</code>，编译器可能会将寄存器使用率提高到 <code>L</code> 以减少指令数量并更好地隐藏单线程指令延迟。</li>
</ul>
</li>
</ul>
<p>如果每个块执行的线程数超过其启动限制 <code>maxThreadsPerBlock</code>，则内核将无法启动。</p>
<p>CUDA 内核所需的每个线程资源可能会以不希望的方式限制最大块数量。为了保持对未来硬件和工具包的前向兼容性，并确保至少一个线程块可以在 <code>SM</code> 上运行，开发人员应该包含单个参数 <code>__launch_bounds__(maxThreadsPerBlock)</code>，它指定内核将启动的最大块大小。不这样做可能会导致“请求启动的资源过多”错误。在某些情况下，提供 <code>__launch_bounds__(maxThreadsPerBlock,minBlocksPerMultiprocessor)</code> 的两个参数版本可以提高性能。 <code>minBlocksPerMultiprocessor</code> 的正确值应使用详细的每个内核分析来确定。</p>
<p>给定内核的最佳启动范围通常会因主要架构修订版而异。下面的示例代码显示了通常如何使用应用程序兼容性中引入的 <code>__CUDA_ARCH__</code> 宏在设备代码中处理此问题</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> THREADS_PER_BLOCK          256</span></span><br><span class="line"><span class="meta">#<span class="keyword">if</span> __CUDA_ARCH__ &gt;= 200</span></span><br><span class="line">    <span class="meta">#<span class="keyword">define</span> MY_KERNEL_MAX_THREADS  (2 * THREADS_PER_BLOCK)</span></span><br><span class="line">    <span class="meta">#<span class="keyword">define</span> MY_KERNEL_MIN_BLOCKS   3</span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    <span class="meta">#<span class="keyword">define</span> MY_KERNEL_MAX_THREADS  THREADS_PER_BLOCK</span></span><br><span class="line">    <span class="meta">#<span class="keyword">define</span> MY_KERNEL_MIN_BLOCKS   2</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Device code</span></span><br><span class="line">__global__ <span class="type">void</span></span><br><span class="line">__launch_bounds__(MY_KERNEL_MAX_THREADS, MY_KERNEL_MIN_BLOCKS)</span><br><span class="line"><span class="built_in">MyKernel</span>(...)</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在使用每个块的最大线程数（指定为 <code>__launch_bounds__()</code> 的第一个参数）调用 <code>MyKernel</code> 的常见情况下，很容易在执行配置中使用 <code>MY_KERNEL_MAX_THREADS</code> 作为每个块的线程数：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Host code</span></span><br><span class="line">MyKernel&lt;&lt;&lt;blocksPerGrid, THREADS_PER_BLOCK&gt;&gt;&gt;(...);</span><br></pre></td></tr></table></figure>
<p>或者在运行时基于计算能力:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Host code</span></span><br><span class="line"><span class="built_in">cudaGetDeviceProperties</span>(&amp;deviceProp, device);</span><br><span class="line"><span class="type">int</span> threadsPerBlock =</span><br><span class="line">          (deviceProp.major &gt;= <span class="number">2</span> ?</span><br><span class="line">                    <span class="number">2</span> * THREADS_PER_BLOCK : THREADS_PER_BLOCK);</span><br><span class="line">MyKernel&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;(...);</span><br></pre></td></tr></table></figure>
<p>寄存器使用情况由 <code>--ptxas-options=-v</code> 编译器选项报告。 驻留块的数量可以从 CUDA 分析器报告的占用率中得出（有关占用率的定义，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses">设备内存访问</a>）。</p>
<p>还可以使用 <code>maxrregcount</code> 编译器选项控制文件中所有 <code>__global__</code> 函数的寄存器使用。 对于具有启动界限的函数，会忽略 <code>maxrregcount</code> 的值。</p>
<h2 id="B-36-pragma-unroll"><a href="#B-36-pragma-unroll" class="headerlink" title="B.36. #pragma unroll"></a>B.36. #pragma unroll</h2><p>默认情况下，编译器展开具有已知行程计数的小循环。 然而，<code>#pragma unroll</code> 指令可用于控制任何给定循环的展开。 它必须放在紧接在循环之前，并且仅适用于该循环。 可选地后跟一个整数常量表达式<code>ICE</code> 。 如果 <code>ICE</code> 不存在，如果其行程计数恒定，则循环将完全展开。 如果 ICE 计算结果为 1，编译器将不会展开循环。 如果 ICE 计算结果为非正整数或大于 int 数据类型可表示的最大值的整数，则该 <code>pragma</code> 将被忽略。</p>
<p>示例:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123; <span class="type">static</span> <span class="type">const</span> <span class="type">int</span> value = <span class="number">4</span>; &#125;;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="type">int</span> X, <span class="keyword">typename</span> T2&gt;</span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">int</span> *p1, <span class="type">int</span> *p2)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// no argument specified, loop will be completely unrolled</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">12</span>; ++i) </span><br><span class="line">  p1[i] += p2[i]*<span class="number">2</span>;</span><br><span class="line">  </span><br><span class="line"><span class="comment">// unroll value = 8</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll (X+1)</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">12</span>; ++i) </span><br><span class="line">  p1[i] += p2[i]*<span class="number">4</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// unroll value = 1, loop unrolling disabled</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll 1</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">12</span>; ++i) </span><br><span class="line">  p1[i] += p2[i]*<span class="number">8</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// unroll value = 4</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> unroll (T2::value)</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">12</span>; ++i) </span><br><span class="line">  p1[i] += p2[i]*<span class="number">16</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">bar</span><span class="params">(<span class="type">int</span> *p1, <span class="type">int</span> *p2)</span> </span>&#123;</span><br><span class="line"><span class="built_in">foo</span>&lt;<span class="number">7</span>, S1_t&gt;(p1, p2);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="B-37-SIMD-Video-Instructions"><a href="#B-37-SIMD-Video-Instructions" class="headerlink" title="B.37. SIMD Video Instructions"></a>B.37. SIMD Video Instructions</h2><p><code>PTX ISA 3.0</code> 版包括 <code>SIMD</code>（Single Instruction, Multiple Data）视频指令，可对 一对16 位值和 四个8 位值进行操作。 这些在计算能力 3.0 的设备上可用。</p>
<p>SIMD 视频指令如下：</p>
<ul>
<li>vadd2, vadd4</li>
<li>vsub2, vsub4</li>
<li>vavrg2, vavrg4</li>
<li>vabsdiff2, vabsdiff4</li>
<li>vmin2, vmin4</li>
<li>vmax2, vmax4</li>
<li>vset2, vset4</li>
</ul>
<p>PTX 指令，例如 SIMD 视频指令，可以通过汇编程序 <code>asm()</code> 语句包含在 CUDA 程序中。</p>
<p><code>asm()</code> 语句的基本语法是：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">asm</span>(<span class="string">&quot;template-string&quot;</span> : <span class="string">&quot;constraint&quot;</span>(output) : <span class="string">&quot;constraint&quot;</span>(input)<span class="string">&quot;));</span></span><br></pre></td></tr></table></figure>
<p>使用 vabsdiff4 PTX 指令的示例是：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">asm</span>(<span class="string">&quot;vabsdiff4.u32.u32.u32.add&quot;</span> <span class="string">&quot; %0, %1, %2, %3;&quot;</span>: <span class="string">&quot;=r&quot;</span> (result):<span class="string">&quot;r&quot;</span> (A), <span class="string">&quot;r&quot;</span> (B), <span class="string">&quot;r&quot;</span> (C));</span><br></pre></td></tr></table></figure>
<p>这使用 <code>vabsdiff4</code> 指令来计算整数四字节 <code>SIMD</code> 绝对差的和。 以 <code>SIMD</code> 方式为无符号整数 A 和 B 的每个字节计算绝对差值。 可选的累积操作 (<code>.add</code>) 被指定为对这些差值求和。</p>
<p>有关在代码中使用汇编语句的详细信息，请参阅文档“Using Inline PTX Assembly in CUDA”。 有关您正在使用的 PTX 版本的 PTX 指令的详细信息，请参阅 PTX ISA 文档（例如“Parallel Thread Execution ISA Version 3.0”）。</p>
<h2 id="B-38-Diagnostic-Pragmas"><a href="#B-38-Diagnostic-Pragmas" class="headerlink" title="B.38. Diagnostic Pragmas"></a>B.38. Diagnostic Pragmas</h2><p>以下 pragma 可用于控制发出给定诊断消息时使用的错误严重性。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> nv_diag_suppress</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> nv_diag_warning</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> nv_diag_error</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> nv_diag_default</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> nv_diag_once</span></span><br></pre></td></tr></table></figure>
<p>这些 pragma 的用法具有以下形式：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> nv_diag_xxx error_number, error_number ...</span></span><br></pre></td></tr></table></figure>
<p>使用警告消息中显示的错误号指定受影响的诊断。 任何诊断都可能被覆盖为错误，但只有警告的严重性可能被抑制或在升级为错误后恢复为警告。 <code>nv_diag_default pragma</code> 用于将诊断的严重性返回到在发出任何 pragma 之前有效的严重性（即，由任何命令行选项修改的消息的正常严重性）。 下面的示例禁止在声明 <code>foo</code> 时出现“已声明但从未引用”警告：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> nv_diag_suppress 177</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> i=<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> nv_diag_default 177</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> i=<span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以下 pragma 可用于保存和恢复当前诊断 pragma 状态：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> nv_diagnostic push</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> nv_diagnostic pop</span></span><br></pre></td></tr></table></figure>
<p>示例:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> nv_diagnostic push</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> nv_diag_suppress 177</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> i=<span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> nv_diagnostic pop</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> i=<span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>请注意，编译指示仅影响 <code>nvcc</code> CUDA 前端编译器； 它们对主机编译器没有影响。</p>
<p>注意：NVCC 也实现了没有 <code>nv_</code> 前缀的诊断 <code>pragma</code>，例如 <code>#pragma diag_suppress</code>，但它们已被弃用，并将从未来的版本中删除，使用这些诊断 <code>pragma</code> 将收到如下消息警告：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pragma <span class="string">&quot;diag_suppress&quot;</span> is deprecated, use <span class="string">&quot;nv_diag_suppress&quot;</span> instead </span><br></pre></td></tr></table></figure>
<h1 id="附录C-协作组"><a href="#附录C-协作组" class="headerlink" title="附录C 协作组"></a>附录C 协作组</h1><h2 id="C-1-Introduction"><a href="#C-1-Introduction" class="headerlink" title="C.1. Introduction"></a>C.1. Introduction</h2><p>Cooperative Groups 是 CUDA 9 中引入的 CUDA 编程模型的扩展，用于组织通信线程组。协作组允许开发人员表达线程通信的粒度，帮助他们表达更丰富、更有效的并行分解。</p>
<p>从历史上看，CUDA 编程模型为同步协作线程提供了一个单一、简单的构造：线程块的所有线程之间的屏障，如使用 <code>__syncthreads()</code> 内部函数实现的那样。但是，程序员希望以其他粒度定义和同步线程组，以“集体”组范围功能接口的形式实现更高的性能、设计灵活性和软件重用。为了表达更广泛的并行交互模式，许多面向性能的程序员已经求助于编写自己的临时和不安全的原语来同步单个 warp 中的线程，或者跨运行在单个 GPU 上的线程块集。虽然实现的性能改进通常很有价值，但这导致了越来越多的脆弱代码集合，随着时间的推移和跨 GPU 架构的不同，这些代码的编写、调整和维护成本很高。合作组通过提供安全且面向未来的机制来启用高性能代码来解决这个问题。</p>
<h2 id="C-2-What’s-New-in-CUDA-11-0"><a href="#C-2-What’s-New-in-CUDA-11-0" class="headerlink" title="C.2. What’s New in CUDA 11.0"></a>C.2. What’s New in CUDA 11.0</h2><ul>
<li>使用网格范围的组不再需要单独编译，并且同步该组的速度现在提高了 <code>30%</code>。此外，我们在最新的 Windows 平台上启用了协作启动，并在 MPS 下运行时增加了对它们的支持。</li>
<li><code>grid_group</code>现在可以转换为 <code>thread_group</code>。</li>
<li>线程块切片和合并组的新集合：<code>reduce</code> 和 <code>memcpy_async</code>。</li>
<li>线程块切片和合并组的新分区操作：<code>labeled_pa​​rtition</code> 和 <code>binary_partition</code>。</li>
<li>新的 API，<code>meta_group_rank</code> 和 <code>meta_group_size</code>，它们提供有关导致创建该组的分区的信息。</li>
<li>线程块<code>tile</code>现在可以在类型中编码其父级，这允许对发出的代码进行更好的编译时优化。</li>
<li>接口更改：<code>grid_group</code> 必须在声明时使用 <code>this_grid()</code> 构造。默认构造函数被删除。</li>
</ul>
<p>注意：在此版本中，我们正朝着要求 C++11 提供新功能的方向发展。在未来的版本中，所有现有 API 都需要这样做。</p>
<h2 id="C-3-Programming-Model-Concept"><a href="#C-3-Programming-Model-Concept" class="headerlink" title="C.3. Programming Model Concept"></a>C.3. Programming Model Concept</h2><p>协作组编程模型描述了 CUDA 线程块内和跨线程块的同步模式。 它为应用程序提供了定义它们自己的线程组的方法，以及同步它们的接口。 它还提供了强制执行某些限制的新启动 API，因此可以保证同步正常工作。 这些原语在 CUDA 内启用了新的协作并行模式，包括生产者-消费者并行、机会并行和整个网格的全局同步。</p>
<p>合作组编程模型由以下元素组成：</p>
<ul>
<li>表示协作线程组的数据类型；</li>
<li>获取由 CUDA 启动 API 定义的隐式组的操作（例如，线程块）；</li>
<li>将现有群体划分为新群体的集体；</li>
<li>用于数据移动和操作的集体算法（例如 <code>memcpy_async、reduce、scan</code>）；</li>
<li>同步组内所有线程的操作；</li>
<li>检查组属性的操作；</li>
<li>公开低级别、特定于组且通常是硬件加速的操作的集合。</li>
</ul>
<p>协作组中的主要概念是对象命名作为其中一部分的线程集的对象。 这种将组表示为一等程序对象的方式改进了软件组合，因为集合函数可以接收表示参与线程组的显式对象。 该对象还明确了程序员的意图，从而消除了不合理的架构假设，这些假设会导致代码脆弱、对编译器优化的不良限制以及与新一代 GPU 的更好兼容性。</p>
<p>为了编写高效的代码，最好使用专门的组（通用会失去很多编译时优化），并通过引用打算以某种协作方式使用这些线程的函数来传递这些组对象。</p>
<p>合作组需要 CUDA 9.0 或更高版本。 要使用合作组，请包含头文件：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Primary header is compatible with pre-C++11, collective algorithm headers require C++11</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"><span class="comment">// Optionally include for memcpy_async() collective</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups/memcpy_async.h&gt;</span></span></span><br><span class="line"><span class="comment">// Optionally include for reduce() collective</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups/reduce.h&gt;</span></span></span><br><span class="line"><span class="comment">// Optionally include for inclusive_scan() and exclusive_scan() collectives</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups/scan.h&gt;</span></span></span><br></pre></td></tr></table></figure>
<p>并使用合作组命名空间：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cooperative_groups;</span><br><span class="line"><span class="comment">// Alternatively use an alias to avoid polluting the namespace with collective algorithms</span></span><br><span class="line"><span class="keyword">namespace</span> cg = cooperative_groups;</span><br></pre></td></tr></table></figure>
<p>可以使用 nvcc 以正常方式编译代码，但是如果您希望使用 <code>memcpy_async、reduce</code> 或 <code>scan</code> 功能并且您的主机编译器的默认不是 C++11 或更高版本，那么您必须添加 <code>--std=c++11</code>到命令行。</p>
<h3 id="C-3-1-Composition-Example"><a href="#C-3-1-Composition-Example" class="headerlink" title="C.3.1. Composition Example"></a>C.3.1. Composition Example</h3><p>为了说明组的概念，此示例尝试执行块范围的求和。 以前，编写此代码时对实现存在隐藏的约束：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__device__ <span class="type">int</span> <span class="title">sum</span><span class="params">(<span class="type">int</span> *x, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">    <span class="keyword">return</span> total;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">parallel_kernel</span><span class="params">(<span class="type">float</span> *x)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// Entire thread block must call sum</span></span><br><span class="line">    <span class="built_in">sum</span>(x, n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>线程块中的所有线程都必须到达<code>__syncthreads()</code> 屏障，但是，对于可能想要使用 <code>sum(...)</code> 的开发人员来说，这个约束是隐藏的。 对于合作组，更好的编写方式是：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__device__ <span class="type">int</span> <span class="title">sum</span><span class="params">(<span class="type">const</span> thread_block&amp; g, <span class="type">int</span> *x, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    g.<span class="built_in">sync</span>()</span><br><span class="line">    <span class="keyword">return</span> total;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">parallel_kernel</span><span class="params">(...)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// Entire thread block must call sum</span></span><br><span class="line">    thread_block tb = <span class="built_in">this_thread_block</span>();</span><br><span class="line">    <span class="built_in">sum</span>(tb, x, n);</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="C-4-Group-Types"><a href="#C-4-Group-Types" class="headerlink" title="C.4. Group Types"></a>C.4. Group Types</h2><h3 id="C-4-1-Implicit-Groups"><a href="#C-4-1-Implicit-Groups" class="headerlink" title="C.4.1. Implicit Groups"></a>C.4.1. Implicit Groups</h3><p>隐式组代表内核的启动配置。不管你的内核是如何编写的，它总是有一定数量的线程、块和块尺寸、单个网格和网格尺寸。另外，如果使用多设备协同启动API，它可以有多个网格（每个设备一个网格）。这些组为分解为更细粒度的组提供了起点，这些组通常是硬件加速的，并且更专门针对开发人员正在解决的问题。</p>
<p>尽管您可以在代码中的任何位置创建隐式组，但这样做很危险。为隐式组创建句柄是一项集体操作——组中的所有线程都必须参与。如果组是在并非所有线程都到达的条件分支中创建的，则可能导致死锁或数据损坏。出于这个原因，建议您预先为隐式组创建一个句柄（尽可能早，在任何分支发生之前）并在整个内核中使用该句柄。出于同样的原因，必须在声明时初始化组句柄（没有默认构造函数），并且不鼓励复制构造它们。</p>
<h4 id="C-4-1-1-Thread-Block-Group"><a href="#C-4-1-1-Thread-Block-Group" class="headerlink" title="C.4.1.1. Thread Block Group"></a>C.4.1.1. Thread Block Group</h4><p>任何 CUDA 程序员都已经熟悉某一组线程：线程块。 Cooperative Groups 扩展引入了一个新的数据类型 <code>thread_block</code>，以在内核中明确表示这个概念。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">thread_block</span>;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">thread_block g = <span class="built_in">this_thread_block</span>();</span><br></pre></td></tr></table></figure>
<p>公开成员函数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>static void sync()</code>:</th>
<th>Synchronize the threads named in the group</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>static unsigned int thread_rank()</code>:</td>
<td>Rank of the calling thread within [0, num_threads)</td>
</tr>
<tr>
<td><code>static dim3 group_index()</code>:</td>
<td>3-Dimensional index of the block within the launched grid</td>
</tr>
<tr>
<td><code>static dim3 thread_index()</code>:</td>
<td>3-Dimensional index of the thread within the launched block</td>
</tr>
<tr>
<td><code>static dim3 dim_threads()</code>:</td>
<td>Dimensions of the launched block in units of threads</td>
</tr>
<tr>
<td><code>static unsigned int num_threads()</code>:</td>
<td>Total number of threads in the group</td>
</tr>
</tbody>
</table>
</div>
<p>旧版成员函数（别名）:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>static unsigned int size()</code>:</th>
<th>Total number of threads in the group (alias of num_threads())</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>static dim3 group_dim()</code>:</td>
<td>Dimensions of the launched block (alias of dim_threads())</td>
</tr>
</tbody>
</table>
</div>
<p>示例:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/// Loading an integer from global into shared memory</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(<span class="type">int</span> *globalInput)</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span> x;</span><br><span class="line">    thread_block g = <span class="built_in">this_thread_block</span>();</span><br><span class="line">    <span class="comment">// Choose a leader in the thread block</span></span><br><span class="line">    <span class="keyword">if</span> (g.<span class="built_in">thread_rank</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// load from global into shared for all threads to work with</span></span><br><span class="line">        x = (*globalInput);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// After loading data into shared memory, you want to synchronize</span></span><br><span class="line">    <span class="comment">// if all threads in your thread block need to see it</span></span><br><span class="line">    g.<span class="built_in">sync</span>(); <span class="comment">// equivalent to __syncthreads();</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意：组中的所有线程都必须参与集体操作，否则行为未定义。</p>
<p>相关：<code>thread_block</code> 数据类型派生自更通用的 <code>thread_group</code> 数据类型，可用于表示更广泛的组类。</p>
<h4 id="C-4-1-2-Grid-Group"><a href="#C-4-1-2-Grid-Group" class="headerlink" title="C.4.1.2. Grid Group"></a>C.4.1.2. Grid Group</h4><p>该组对象表示在单个网格中启动的所有线程。 除了 <code>sync()</code> 之外的 API 始终可用，但要能够跨网格同步，您需要使用协作启动 API。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">grid_group</span>;</span><br><span class="line">grid_group g = <span class="built_in">this_grid</span>();</span><br></pre></td></tr></table></figure>
<p>公开成员函数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>bool is_valid() const:</code></th>
<th>Returns whether the grid_group can synchronize</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>void sync() const:</code></td>
<td>Synchronize the threads named in the group</td>
</tr>
<tr>
<td><code>static unsigned long long thread_rank():</code></td>
<td>Rank of the calling thread within [0, num_threads)</td>
</tr>
<tr>
<td><code>static unsigned long long block_rank():</code></td>
<td>Rank of the calling block within [0, num_blocks)</td>
</tr>
<tr>
<td><code>static unsigned long long num_threads():</code></td>
<td>Total number of threads in the group</td>
</tr>
<tr>
<td><code>static unsigned long long num_blocks():</code></td>
<td>Total number of blocks in the group</td>
</tr>
<tr>
<td><code>static dim3 dim_blocks():</code></td>
<td>Dimensions of the launched grid in units of blocks</td>
</tr>
<tr>
<td><code>static dim3 block_index():</code></td>
<td>3-Dimensional index of the block within the launched grid</td>
</tr>
</tbody>
</table>
</div>
<p>旧版成员函数（别名）:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>static unsigned long long size():</code></th>
<th>Total number of threads in the group (alias of num_threads())</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>static dim3 group_dim():</code></td>
<td>Dimensions of the launched grid (alias of dim_blocks())</td>
</tr>
</tbody>
</table>
</div>
<h3 id="C-4-1-3-Multi-Grid-Group"><a href="#C-4-1-3-Multi-Grid-Group" class="headerlink" title="C.4.1.3. Multi Grid Group"></a>C.4.1.3. Multi Grid Group</h3><p>该组对象表示跨设备协作组启动的所有设备启动的所有线程。 与 <code>grid.group</code> 不同，所有 API 都要求您使用适当的启动 API。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">multi_grid_group</span>;</span><br></pre></td></tr></table></figure>
<p>通过一下方式构建:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Kernel must be launched with the cooperative multi-device API</span></span><br><span class="line">multi_grid_group g = <span class="built_in">this_multi_grid</span>();</span><br></pre></td></tr></table></figure>
<p>公开成员函数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>bool is_valid() const</code>:</th>
<th>Returns whether the multi_grid_group can be used</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>void sync() const</code>:</td>
<td>Synchronize the threads named in the group</td>
</tr>
<tr>
<td><code>unsigned long long num_threads() const</code>:</td>
<td>Total number of threads in the group</td>
</tr>
<tr>
<td><code>unsigned long long thread_rank() const</code>:</td>
<td>Rank of the calling thread within [0, num_threads)</td>
</tr>
<tr>
<td><code>unsigned int grid_rank() const</code>:</td>
<td>Rank of the grid within [0,num_grids]</td>
</tr>
<tr>
<td><code>unsigned int num_grids() const</code>:</td>
<td>Total number of grids launched</td>
</tr>
</tbody>
</table>
</div>
<p>旧版成员函数（别名）:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>unsigned long long size() const</code>:</th>
<th>Total number of threads in the group (alias of <code>num_threads()</code>)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="C-4-2-Explicit-Groups"><a href="#C-4-2-Explicit-Groups" class="headerlink" title="C.4.2. Explicit Groups"></a>C.4.2. Explicit Groups</h3><h4 id="C-4-2-1-Thread-Block-Tile"><a href="#C-4-2-1-Thread-Block-Tile" class="headerlink" title="C.4.2.1. Thread Block Tile"></a>C.4.2.1. Thread Block Tile</h4><p>tile组的模板版本，其中模板参数用于指定tile的大小 - 在编译时已知这一点，有可能实现更优化的执行。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> Size, <span class="keyword">typename</span> ParentT = <span class="type">void</span>&gt;</span><br><span class="line"><span class="keyword">class</span> thread_block_tile;</span><br></pre></td></tr></table></figure>
<p>通过以下构建:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> Size, <span class="keyword">typename</span> ParentT&gt;</span><br><span class="line"><span class="function">_CG_QUALIFIER thread_block_tile&lt;Size, ParentT&gt; <span class="title">tiled_partition</span><span class="params">(<span class="type">const</span> ParentT&amp; g)</span></span></span><br></pre></td></tr></table></figure>
<p><code>Size</code>必须是 2 的幂且小于或等于 32。</p>
<p><code>ParentT</code> 是从其中划分该组的父类型。 它是自动推断的，但是 void 的值会将此信息存储在组句柄中而不是类型中。</p>
<p>公开成员函数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>void sync() const</code>:</th>
<th>Synchronize the threads named in the group</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>unsigned long long num_threads() const</code>:</td>
<td>Total number of threads in the group</td>
</tr>
<tr>
<td><code>unsigned long long thread_rank() const</code>:</td>
<td>Rank of the calling thread within [0, num_threads)</td>
</tr>
<tr>
<td>u<code>nsigned long long meta_group_size() const</code>:</td>
<td>Returns the number of groups created when the parent group was partitioned.</td>
</tr>
<tr>
<td><code>unsigned long long meta_group_rank() const</code>:</td>
<td>Linear rank of the group within the set of tiles partitioned from a parent group (bounded by meta_group_size)</td>
</tr>
<tr>
<td><code>T shfl(T var, unsigned int src_rank) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions">Warp Shuffle Functions</a></td>
</tr>
<tr>
<td><code>T shfl_up(T var, int delta) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions">Warp Shuffle Functions</a></td>
</tr>
<tr>
<td><code>T shfl_down(T var, int delta) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions">Warp Shuffle Functions</a></td>
</tr>
<tr>
<td><code>T shfl_xor(T var, int delta) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions">Warp Shuffle Functions</a></td>
</tr>
<tr>
<td><code>T any(int predicate) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-vote-functions">Warp Vote Functions</a></td>
</tr>
<tr>
<td><code>T all(int predicate) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-vote-functions">Warp Vote Functions</a></td>
</tr>
<tr>
<td><code>T ballot(int predicate) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-vote-functions">Warp Vote Functions</a></td>
</tr>
<tr>
<td><code>T match_any(T val) const</code>:</td>
<td>Refer to Warp <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-match-functions">Match Functions</a></td>
</tr>
<tr>
<td><code>T match_all(T val, int &amp;pred) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-match-functions">Match Functions</a></td>
</tr>
</tbody>
</table>
</div>
<p>旧版成员函数（别名）:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>unsigned long long size() const</code>:</th>
<th>Total number of threads in the group (alias of num_threads())</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
</div>
<h4 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h4><p><code>shfl、shfl_up、shfl_down 和 shfl_xor</code> 函数在使用 C++11 或更高版本编译时接受任何类型的对象。 这意味着只要满足以下约束，就可以对非整数类型进行shuffle ：</p>
<ul>
<li>符合普通可复制的条件，即<br>  <code>is_trivially_copyable&lt;T&gt;::value == true</code></li>
<li><code>sizeof(T) &lt;= 32</code></li>
</ul>
<p>示例:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/// The following code will create two sets of tiled groups, of size 32 and 4 respectively:</span></span><br><span class="line"><span class="comment">/// The latter has the provenance encoded in the type, while the first stores it in the handle</span></span><br><span class="line">thread_block block = <span class="built_in">this_thread_block</span>();</span><br><span class="line">thread_block_tile&lt;<span class="number">32</span>&gt; tile32 = <span class="built_in">tiled_partition</span>&lt;<span class="number">32</span>&gt;(block);</span><br><span class="line">thread_block_tile&lt;<span class="number">4</span>, thread_block&gt; tile4 = <span class="built_in">tiled_partition</span>&lt;<span class="number">4</span>&gt;(block);</span><br></pre></td></tr></table></figure>
<p>注意：这里使用的是 thread_block_tile 模板化数据结构，并且组的大小作为模板参数而不是参数传递给 tiled_partition 调用。</p>
<h4 id="C-4-2-1-1-Warp-Synchronous-Code-Pattern"><a href="#C-4-2-1-1-Warp-Synchronous-Code-Pattern" class="headerlink" title="C.4.2.1.1. Warp-Synchronous Code Pattern"></a>C.4.2.1.1. Warp-Synchronous Code Pattern</h4><p>开发人员可能拥有他们之前对 warp 大小做出隐含假设并围绕该数字进行编码的 warp 同步代码。 现在这需要明确指定。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">cooperative_kernel</span><span class="params">(...)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// obtain default &quot;current thread block&quot; group</span></span><br><span class="line">    thread_block my_block = <span class="built_in">this_thread_block</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// subdivide into 32-thread, tiled subgroups</span></span><br><span class="line">    <span class="comment">// Tiled subgroups evenly partition a parent group into</span></span><br><span class="line">    <span class="comment">// adjacent sets of threads - in this case each one warp in size</span></span><br><span class="line">    <span class="keyword">auto</span> my_tile = <span class="built_in">tiled_partition</span>&lt;<span class="number">32</span>&gt;(my_block);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// This operation will be performed by only the</span></span><br><span class="line">    <span class="comment">// first 32-thread tile of each block</span></span><br><span class="line">    <span class="keyword">if</span> (my_tile.<span class="built_in">meta_group_rank</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">        my_tile.<span class="built_in">sync</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="C-4-2-1-2-Single-thread-group"><a href="#C-4-2-1-2-Single-thread-group" class="headerlink" title="C.4.2.1.2. Single thread group"></a>C.4.2.1.2. Single thread group</h5><p>可以从 this_thread 函数中获取代表当前线程的组：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">thread_block_tile&lt;1&gt; <span class="title">this_thread</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure>
<p>以下 <code>memcpy_async</code> API 使用 <code>thread_group</code> 将 <code>int</code> 元素从源复制到目标：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups/memcpy_async.h&gt;</span></span></span><br><span class="line"></span><br><span class="line">cooperative_groups::<span class="built_in">memcpy_async</span>(cooperative_groups::<span class="built_in">this_thread</span>(), dest, src, <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br></pre></td></tr></table></figure>
<p>可以在<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#with-memcpy_async-pipeline-pattern-single">使用 <code>cuda::pipeline</code> 的单阶段异步数据拷贝</a>和<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#with-memcpy_async-pipeline-pattern-multi">使用 <code>cuda::pipeline</code> 的多阶段异步数据拷贝</a>部分中找到使用 <code>this_thread</code> 执行异步复制的更详细示例。</p>
<h5 id="C-4-2-1-3-Thread-Block-Tile-of-size-larger-than-32"><a href="#C-4-2-1-3-Thread-Block-Tile-of-size-larger-than-32" class="headerlink" title="C.4.2.1.3. Thread Block Tile of size larger than 32"></a>C.4.2.1.3. Thread Block Tile of size larger than 32</h5><p>使用<code>cooperative_groups::experimental</code> 命名空间中的新API 可以获得大小为<code>64、128、256 或512</code> 的<code>thread_block_tile</code>。 要使用它，<code>_CG_ABI_EXPERIMENTAL</code> 必须在源代码中定义。 在分区之前，必须为 <code>thread_block_tile</code> 保留少量内存。 这可以使用必须驻留在共享或全局内存中的<code>cooperative_groups::experimental::block_tile_memory</code> 结构模板来完成。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> TileCommunicationSize = <span class="number">8</span>, <span class="type">unsigned</span> <span class="type">int</span> MaxBlockSize = <span class="number">1024</span>&gt;</span><br><span class="line"><span class="keyword">struct</span> block_tile_memory;</span><br></pre></td></tr></table></figure>
<p><code>TileCommunicationSize</code> 确定为集体操作保留多少内存。 如果对大于指定通信大小的大小类型执行此类操作，则集合可能涉及多次传输并需要更长的时间才能完成。</p>
<p><code>MaxBlockSize</code> 指定当前线程块中的最大线程数。 此参数可用于最小化仅以较小线程数启动的内核中 <code>block_tile_memory</code> 的共享内存使用量。</p>
<p>然后这个 <code>block_tile_memory</code> 需要被传递到<code>cooperative_groups::experimental::this_thread_block</code>，允许将生成的 <code>thread_block</code> 划分为大小大于 <code>32</code> 的tile。 <code>this_thread_block</code> 接受 <code>block_tile_memory</code> 参数的重载是一个集体操作，必须与所有线程一起调用 线程块。 返回的线程块可以使用<code>experimental::tiled_partition</code> 函数模板进行分区，该模板接受与常规<code>tiled_partition</code> 相同的参数。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> _CG_ABI_EXPERIMENTAL <span class="comment">// enable experimental API</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">cooperative_kernel</span><span class="params">(...)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// reserve shared memory for thread_block_tile usage.</span></span><br><span class="line">    __shared__ experimental::block_tile_memory&lt;<span class="number">4</span>, <span class="number">256</span>&gt; shared;</span><br><span class="line">    thread_block thb = experimental::<span class="built_in">this_thread_block</span>(shared);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> tile = experimental::<span class="built_in">tiled_partition</span>&lt;<span class="number">128</span>&gt;(thb);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>公开成员函数:  </p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>void sync() const</code>:</th>
<th>Synchronize the threads named in the group</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>unsigned long long num_threads() const</code>:</td>
<td>Total number of threads in the group</td>
</tr>
<tr>
<td><code>unsigned long long thread_rank() const</code>:</td>
<td>Rank of the calling thread within [0, num_threads)</td>
</tr>
<tr>
<td><code>unsigned long long meta_group_size() const</code>:</td>
<td>Returns the number of groups created when the parent group was partitioned.</td>
</tr>
<tr>
<td><code>unsigned long long meta_group_rank() const</code>:</td>
<td>Linear rank of the group within the set of tiles partitioned from a parent group (bounded by meta_group_size)</td>
</tr>
<tr>
<td><code>T shfl(T var, unsigned int src_rank) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions">Warp Shuffle Functions</a>, Note: All threads in the group have to specify the same src_rank, otherwise the behavior is undefined.</td>
</tr>
<tr>
<td><code>T any(int predicate) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-vote-functions">Warp Vote Functions</a></td>
</tr>
<tr>
<td><code>T all(int predicate) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-vote-functions">Warp Vote Functions</a></td>
</tr>
</tbody>
</table>
</div>
<p>旧版成员函数（别名）:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>unsigned long long size() const:</code></th>
<th>Total number of threads in the group (alias of num_threads())</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
</div>
<h4 id="C-4-2-2-Coalesced-Groups"><a href="#C-4-2-2-Coalesced-Groups" class="headerlink" title="C.4.2.2. Coalesced Groups"></a>C.4.2.2. Coalesced Groups</h4><p>在 CUDA 的 SIMT 架构中，在硬件级别，多处理器以 32 个一组的线程执行线程，称为 warp。 如果应用程序代码中存在依赖于数据的条件分支，使得 warp 中的线程发散，那么 warp 会串行执行每个分支，禁用不在该路径上的线程。 在路径上保持活动的线程称为合并。 协作组具有发现和创建包含所有合并线程的组的功能。</p>
<p>通过 <code>coalesced_threads()</code> 构造组句柄是伺机的(opportunistic)。 它在那个时间点返回一组活动线程，并且不保证返回哪些线程（只要它们是活动的）或者它们在整个执行过程中保持合并（它们将被重新组合在一起以执行一个集合，但之后可以再次发散）。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">coalesced_group</span>;</span><br></pre></td></tr></table></figure>
<p>通过以下重构:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">coalesced_group active = <span class="built_in">coalesced_threads</span>();</span><br></pre></td></tr></table></figure>
<p>公开成员函数: </p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>void sync() const</code>:</th>
<th>Synchronize the threads named in the group</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>unsigned long long num_threads() const</code>:</td>
<td>Total number of threads in the group</td>
</tr>
<tr>
<td><code>unsigned long long thread_rank() const</code>:</td>
<td>Rank of the calling thread within [0, num_threads)</td>
</tr>
<tr>
<td><code>unsigned long long meta_group_size() const</code>:</td>
<td>Returns the number of groups created when the parent group was partitioned. If this group was created by querying the set of active threads, e.g. coalesced_threads() the value of meta_group_size() will be 1.</td>
</tr>
<tr>
<td><code>unsigned long long meta_group_rank() const</code>:</td>
<td>Linear rank of the group within the set of tiles partitioned from a parent group (bounded by meta_group_size). If this group was created by querying the set of active threads, e.g. coalesced_threads() the value of meta_group_rank() will always be 0.</td>
</tr>
<tr>
<td><code>T shfl(T var, unsigned int src_rank) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions">Warp Shuffle Functions</a></td>
</tr>
<tr>
<td><code>T shfl_up(T var, int delta) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions">Warp Shuffle Functions</a></td>
</tr>
<tr>
<td><code>T shfl_down(T var, int delta) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-shuffle-functions">Warp Shuffle Functions</a></td>
</tr>
<tr>
<td><code>T any(int predicate) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-vote-functions">Warp Vote Functions</a></td>
</tr>
<tr>
<td><code>T all(int predicate) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-vote-functions">Warp Vote Functions</a></td>
</tr>
<tr>
<td><code>T ballot(int predicate) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-vote-functions">Warp Vote Functions</a></td>
</tr>
<tr>
<td><code>T match_any(T val) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-match-functions">Warp Match Functions</a></td>
</tr>
<tr>
<td><code>T match_all(T val, int &amp;pred) const</code>:</td>
<td>Refer to <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#warp-match-functions">Warp Match Functions</a></td>
</tr>
</tbody>
</table>
</div>
<p>旧版成员函数（别名）:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>unsigned long long size() const</code>:</th>
<th>Total number of threads in the group (alias of <code>num_threads()</code>)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>注意：<code>shfl、shfl_up 和 shfl_down</code> 函数在使用 C++11 或更高版本编译时接受任何类型的对象。 这意味着只要满足以下约束，就可以对非整数类型进行洗牌：</p>
<ul>
<li>符合普通可复制的条件，即<code>is_trivially_copyable&lt;T&gt;::value == true</code></li>
<li><code>sizeof(T) &lt;= 32</code></li>
</ul>
<p>示例:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/// Consider a situation whereby there is a branch in the</span></span><br><span class="line"><span class="comment">/// code in which only the 2nd, 4th and 8th threads in each warp are</span></span><br><span class="line"><span class="comment">/// active. The coalesced_threads() call, placed in that branch, will create (for each</span></span><br><span class="line"><span class="comment">/// warp) a group, active, that has three threads (with</span></span><br><span class="line"><span class="comment">/// ranks 0-2 inclusive).</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(<span class="type">int</span> *globalInput)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Lets say globalInput says that threads 2, 4, 8 should handle the data</span></span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x == *globalInput) &#123;</span><br><span class="line">        coalesced_group active = <span class="built_in">coalesced_threads</span>();</span><br><span class="line">        <span class="comment">// active contains 0-2 inclusive</span></span><br><span class="line">        active.<span class="built_in">sync</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="C-4-2-2-1-Discovery-Pattern"><a href="#C-4-2-2-1-Discovery-Pattern" class="headerlink" title="C.4.2.2.1. Discovery Pattern"></a>C.4.2.2.1. Discovery Pattern</h4><p>通常，开发人员需要使用当前活动的线程集。 不对存在的线程做任何假设，而是开发人员使用碰巧存在的线程。 这可以在以下“在warp中跨线程聚合原子增量”示例中看到（使用正确的 CUDA 9.0 内在函数集编写）：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> writemask = __activemask();</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> total = __popc(writemask);</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> prefix = __popc(writemask &amp; __lanemask_lt());</span><br><span class="line">    <span class="comment">// Find the lowest-numbered active lane</span></span><br><span class="line">    <span class="type">int</span> elected_lane = __ffs(writemask) - <span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> base_offset = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (prefix == <span class="number">0</span>) &#123;</span><br><span class="line">        base_offset = <span class="built_in">atomicAdd</span>(p, total);</span><br><span class="line">    &#125;</span><br><span class="line">    base_offset = __shfl_sync(writemask, base_offset, elected_lane);</span><br><span class="line">    <span class="type">int</span> thread_offset = prefix + base_offset;</span><br><span class="line">    <span class="keyword">return</span> thread_offset;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这可以用Cooperative Groups重写如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    cg::coalesced_group g = cg::<span class="built_in">coalesced_threads</span>();</span><br><span class="line">    <span class="type">int</span> prev;</span><br><span class="line">    <span class="keyword">if</span> (g.<span class="built_in">thread_rank</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">        prev = <span class="built_in">atomicAdd</span>(p, g.<span class="built_in">num_threads</span>());</span><br><span class="line">    &#125;</span><br><span class="line">    prev = g.<span class="built_in">thread_rank</span>() + g.<span class="built_in">shfl</span>(prev, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">return</span> prev;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="C-5-Group-Partitioning"><a href="#C-5-Group-Partitioning" class="headerlink" title="C.5. Group Partitioning"></a>C.5. Group Partitioning</h2><h3 id="C-5-1-tiled-partition"><a href="#C-5-1-tiled-partition" class="headerlink" title="C.5.1. tiled_partition"></a>C.5.1. tiled_partition</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> Size, <span class="keyword">typename</span> ParentT&gt;</span><br><span class="line"><span class="function">thread_block_tile&lt;Size, ParentT&gt; <span class="title">tiled_partition</span><span class="params">(<span class="type">const</span> ParentT&amp; g)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">thread_group <span class="title">tiled_partition</span><span class="params">(<span class="type">const</span> thread_group&amp; parent, <span class="type">unsigned</span> <span class="type">int</span> tilesz)</span></span>;</span><br></pre></td></tr></table></figure>
<p><code>tiled_partition</code> 方法是一种集体操作，它将父组划分为一维、行主序的子组平铺。 总共将创建 <code>((size(parent)/tilesz)</code> 子组，因此父组大小必须能被 <code>Size</code> 整除。允许的父组是 <code>thread_block</code> 或 <code>thread_block_tile</code>。</p>
<p>该实现可能导致调用线程在恢复执行之前等待，直到父组的所有成员都调用了该操作。功能仅限于本地硬件大小，<code>1/2/4/8/16/32</code>和<code>cg::size(parent)</code>必须大于size参数。<code>cooperative_groups::experimental</code>命名空间的实验版本支持<code>64/128/256/512</code>大小。</p>
<p>Codegen 要求：计算能力 3.5 最低，C++11 用于大于 32 的<code>size</code></p>
<p>示例:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/// The following code will create a 32-thread tile</span></span><br><span class="line">thread_block block = <span class="built_in">this_thread_block</span>();</span><br><span class="line">thread_block_tile&lt;<span class="number">32</span>&gt; tile32 = <span class="built_in">tiled_partition</span>&lt;<span class="number">32</span>&gt;(block);</span><br></pre></td></tr></table></figure>
<p>我们可以将这些组中的每一个分成更小的组，每个组的大小为 4 个线程：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> tile4 = <span class="built_in">tiled_partition</span>&lt;<span class="number">4</span>&gt;(tile32);</span><br><span class="line"><span class="comment">// or using a general group</span></span><br><span class="line"><span class="comment">// thread_group tile4 = tiled_partition(tile32, 4);</span></span><br></pre></td></tr></table></figure>
<p>例如，如果我们要包含以下代码行：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (tile<span class="number">4.</span><span class="built_in">thread_rank</span>()==<span class="number">0</span>) <span class="built_in">printf</span>(“Hello from tile4 rank <span class="number">0</span>\n”);</span><br></pre></td></tr></table></figure>
<p>那么该语句将由块中的每四个线程打印：每个 tile4 组中排名为 0 的线程，它们对应于块组中排名为 0、4、8、12.. 的那些线程。</p>
<h3 id="C-5-2-labeled-partition"><a href="#C-5-2-labeled-partition" class="headerlink" title="C.5.2. labeled_partition"></a>C.5.2. labeled_partition</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">coalesced_group <span class="title">labeled_partition</span><span class="params">(<span class="type">const</span> coalesced_group&amp; g, <span class="type">int</span> label)</span></span>;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> Size&gt;</span><br><span class="line"><span class="function">coalesced_group <span class="title">labeled_partition</span><span class="params">(<span class="type">const</span> thread_block_tile&lt;Size&gt;&amp; g, <span class="type">int</span> label)</span></span>;</span><br></pre></td></tr></table></figure>
<p><code>labeled_partition</code> 方法是一种集体操作，它将父组划分为一维子组，线程在这些子组中合并。 该实现将评估条件标签并将具有相同标签值的线程分配到同一组中。</p>
<p>该实现可能会导致调用线程在恢复执行之前等待直到父组的所有成员都调用了该操作。</p>
<p>注意：此功能仍在评估中，将来可能会略有变化。</p>
<p>Codegen 要求：计算能力 7.0 最低，C++11</p>
<h3 id="C-5-3-binary-partition"><a href="#C-5-3-binary-partition" class="headerlink" title="C.5.3. binary_partition"></a>C.5.3. binary_partition</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">coalesced_group <span class="title">binary_partition</span><span class="params">(<span class="type">const</span> coalesced_group&amp; g, <span class="type">bool</span> pred)</span></span>;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> Size&gt;</span><br><span class="line"><span class="function">coalesced_group <span class="title">binary_partition</span><span class="params">(<span class="type">const</span> thread_block_tile&lt;Size&gt;&amp; g, <span class="type">bool</span> pred)</span></span>;</span><br></pre></td></tr></table></figure>
<p><code>binary_partition()</code> 方法是一种集体操作，它将父组划分为一维子组，线程在其中合并。 该实现将评估predicate并将具有相同值的线程分配到同一组中。 这是<code>labeled_partition()</code> 的一种特殊形式，其中<code>label</code> 只能是0 或1。</p>
<p>该实现可能会导致调用线程在恢复执行之前等待直到父组的所有成员都调用了该操作。</p>
<p>注意：此功能仍在评估中，将来可能会略有变化。</p>
<p>Codegen 要求：计算能力 7.0 最低，C++11</p>
<p>示例:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/// This example divides a 32-sized tile into a group with odd</span></span><br><span class="line"><span class="comment">/// numbers and a group with even numbers</span></span><br><span class="line"><span class="function">_global__ <span class="type">void</span> <span class="title">oddEven</span><span class="params">(<span class="type">int</span> *inputArr)</span> </span>&#123;</span><br><span class="line">    cg::thread_block cta = cg::<span class="built_in">this_thread_block</span>();</span><br><span class="line">    cg::thread_block_tile&lt;<span class="number">32</span>&gt; tile32 = cg::<span class="built_in">tiled_partition</span>&lt;<span class="number">32</span>&gt;(cta);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// inputArr contains random integers</span></span><br><span class="line">    <span class="type">int</span> elem = inputArr[cta.<span class="built_in">thread_rank</span>()];</span><br><span class="line">    <span class="comment">// after this, tile32 is split into 2 groups,</span></span><br><span class="line">    <span class="comment">// a subtile where elem&amp;1 is true and one where its false</span></span><br><span class="line">    <span class="keyword">auto</span> subtile = cg::<span class="built_in">binary_partition</span>(tile32, (elem &amp; <span class="number">1</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="C-6-Group-Collectives"><a href="#C-6-Group-Collectives" class="headerlink" title="C.6. Group Collectives"></a>C.6. Group Collectives</h2><h3 id="C-6-1-Synchronization"><a href="#C-6-1-Synchronization" class="headerlink" title="C.6.1. Synchronization"></a>C.6.1. Synchronization</h3><h4 id="C-6-1-1-sync"><a href="#C-6-1-1-sync" class="headerlink" title="C.6.1.1. sync"></a>C.6.1.1. sync</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cooperative_groups::<span class="built_in">sync</span>(T&amp; group);</span><br></pre></td></tr></table></figure>
<p><code>sync</code> 同步组中指定的线程。 <code>T</code> 可以是任何现有的组类型，因为它们都支持同步。 如果组是 <code>grid_group</code> 或 <code>multi_grid_group</code>，则内核必须已使用适当的协作启动 API 启动。</p>
<h3 id="C-6-2-Data-Transfer"><a href="#C-6-2-Data-Transfer" class="headerlink" title="C.6.2. Data Transfer"></a>C.6.2. Data Transfer</h3><h4 id="C-6-2-1-memcpy-async"><a href="#C-6-2-1-memcpy-async" class="headerlink" title="C.6.2.1. memcpy_async"></a>C.6.2.1. memcpy_async</h4><p><code>memcpy_async</code> 是一个组范围的集体 <code>memcpy</code>，它利用硬件加速支持从全局到共享内存的非阻塞内存事务。给定组中命名的一组线程，<code>memcpy_async</code> 将通过单个管道阶段传输指定数量的字节或输入类型的元素。此外，为了在使用 <code>memcpy_async</code> API 时获得最佳性能，共享内存和全局内存都需要 16 字节对齐。需要注意的是，虽然在一般情况下这是一个 <code>memcpy</code>，但只有当源(source)是全局内存而目标是共享内存并且两者都可以通过 16、8 或 4 字节对齐来寻址时，它才是异步的。异步复制的数据只能在调用 <code>wait</code>或 <code>wait_prior</code> 之后读取，这表明相应阶段已完成将数据移动到共享内存。</p>
<p>必须等待所有未完成的请求可能会失去一些灵活性（但会变得简单）。为了有效地重叠数据传输和执行，重要的是能够在等待和操作请求 <code>N</code> 时启动 <code>N+1 memcpy_async</code> 请求。为此，请使用 <code>memcpy_async</code> 并使用基于集体阶段的 <code>wait_prior</code> API 等待它.有关详细信息，请参阅 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#collectives-cg-wait">wait 和 wait_prior</a>。</p>
<p>用法1:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> TyGroup, <span class="keyword">typename</span> TyElem, <span class="keyword">typename</span> TyShape&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">memcpy_async</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">  <span class="type">const</span> TyGroup &amp;group,</span></span></span><br><span class="line"><span class="params"><span class="function">  TyElem *__restrict__ _dst,</span></span></span><br><span class="line"><span class="params"><span class="function">  <span class="type">const</span> TyElem *__restrict__ _src,</span></span></span><br><span class="line"><span class="params"><span class="function">  <span class="type">const</span> TyShape &amp;shape</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>;</span><br></pre></td></tr></table></figure>
<p>执行<code>shape</code>字节的拷贝</p>
<p>用法2:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> TyGroup, <span class="keyword">typename</span> TyElem, <span class="keyword">typename</span> TyDstLayout, <span class="keyword">typename</span> TySrcLayout&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">memcpy_async</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">  <span class="type">const</span> TyGroup &amp;group,</span></span></span><br><span class="line"><span class="params"><span class="function">  TyElem *__restrict__ dst,</span></span></span><br><span class="line"><span class="params"><span class="function">  <span class="type">const</span> TyDstLayout &amp;dstLayout,</span></span></span><br><span class="line"><span class="params"><span class="function">  <span class="type">const</span> TyElem *__restrict__ src,</span></span></span><br><span class="line"><span class="params"><span class="function">  <span class="type">const</span> TySrcLayout &amp;srcLayout</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>;</span><br></pre></td></tr></table></figure>
<p>执行 <code>min(dstLayout, srcLayout)</code> 元素的拷贝。 如果布局的类型为 <code>cuda::aligned_size_t&lt;N&gt;</code>，则两者必须指定相同的对齐方式。</p>
<p>勘误表</p>
<p>CUDA 11.1 中引入的具有 src 和 dst 输入布局的 <code>memcpy_async</code> API 期望布局以元素而不是字节形式提供。 元素类型是从 <code>TyElem</code> 推断出来的，大小为 <code>sizeof(TyElem)</code>。 如果使用 <code>cuda::aligned_size_t&lt;N&gt;</code> 类型作为布局，指定的元素个数乘以 <code>sizeof(TyElem)</code> 必须是 N 的倍数，建议使用 <code>std::byte</code> 或 <code>char</code> 作为元素类型。</p>
<p>如果副本的指定形状或布局是 <code>cuda::aligned_size_t&lt;N&gt;</code> 类型，则将保证至少为 <code>min(16, N)</code>。 在这种情况下，dst 和 src 指针都需要与 N 个字节对齐，并且复制的字节数需要是 N 的倍数。</p>
<p>Codegen 要求：最低计算能力 3.5，异步计算能力 8.0，C++11</p>
<p>需要包含<code>collaborative_groups/memcpy_async.h</code> 头文件。</p>
<p>示例:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/// This example streams elementsPerThreadBlock worth of data from global memory</span></span><br><span class="line"><span class="comment">/// into a limited sized shared memory (elementsInShared) block to operate on.</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups/memcpy_async.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> cg = cooperative_groups;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(<span class="type">int</span>* global_data)</span> </span>&#123;</span><br><span class="line">    cg::thread_block tb = cg::<span class="built_in">this_thread_block</span>();</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> elementsPerThreadBlock = <span class="number">16</span> * <span class="number">1024</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> elementsInShared = <span class="number">128</span>;</span><br><span class="line">    __shared__ <span class="type">int</span> local_smem[elementsInShared];</span><br><span class="line"></span><br><span class="line">    <span class="type">size_t</span> copy_count;</span><br><span class="line">    <span class="type">size_t</span> index = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (index &lt; elementsPerThreadBlock) &#123;</span><br><span class="line">        cg::<span class="built_in">memcpy_async</span>(tb, local_smem, elementsInShared, global_data + index, elementsPerThreadBlock - index);</span><br><span class="line">        copy_count = <span class="built_in">min</span>(elementsInShared, elementsPerThreadBlock - index);</span><br><span class="line">        cg::<span class="built_in">wait</span>(tb);</span><br><span class="line">        <span class="comment">// Work with local_smem</span></span><br><span class="line">        index += copy_count;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="C-6-2-2-wait-and-wait-prior"><a href="#C-6-2-2-wait-and-wait-prior" class="headerlink" title="C.6.2.2. wait and wait_prior"></a>C.6.2.2. wait and wait_prior</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> TyGroup&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">wait</span><span class="params">(TyGroup &amp; group)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="type">unsigned</span> <span class="type">int</span> NumStages, <span class="keyword">typename</span> TyGroup&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">wair_prior</span><span class="params">(TyGroup &amp; group)</span></span>;</span><br></pre></td></tr></table></figure>
<p><code>wait</code> 和 <code>wait_prior</code> 集合同步指定的线程和线程块，直到所有未完成的 <code>memcpy_async</code> 请求（在等待的情况下）或第一个 <code>NumStages</code>（在 wait_prior 的情况下）完成。</p>
<p>Codegen 要求：最低计算能力 3.5，异步计算能力 8.0，C++11</p>
<p>需要包含collaborative_groups/memcpy_async.h 头文件。</p>
<p>示例:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/// This example streams elementsPerThreadBlock worth of data from global memory</span></span><br><span class="line"><span class="comment">/// into a limited sized shared memory (elementsInShared) block to operate on in</span></span><br><span class="line"><span class="comment">/// multiple (two) stages. As stage N is kicked off, we can wait on and operate on stage N-1.</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups/memcpy_async.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> cg = cooperative_groups;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(<span class="type">int</span>* global_data)</span> </span>&#123;</span><br><span class="line">    cg::thread_block tb = cg::<span class="built_in">this_thread_block</span>();</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> elementsPerThreadBlock = <span class="number">16</span> * <span class="number">1024</span> + <span class="number">64</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">size_t</span> elementsInShared = <span class="number">128</span>;</span><br><span class="line">    __align__(<span class="number">16</span>) __shared__ <span class="type">int</span> local_smem[<span class="number">2</span>][elementsInShared];</span><br><span class="line">    <span class="type">int</span> stage = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// First kick off an extra request</span></span><br><span class="line">    <span class="type">size_t</span> copy_count = elementsInShared;</span><br><span class="line">    <span class="type">size_t</span> index = copy_count;</span><br><span class="line">    cg::<span class="built_in">memcpy_async</span>(tb, local_smem[stage], elementsInShared, global_data, elementsPerThreadBlock - index);</span><br><span class="line">    <span class="keyword">while</span> (index &lt; elementsPerThreadBlock) &#123;</span><br><span class="line">        <span class="comment">// Now we kick off the next request...</span></span><br><span class="line">        cg::<span class="built_in">memcpy_async</span>(tb, local_smem[stage ^ <span class="number">1</span>], elementsInShared, global_data + index, elementsPerThreadBlock - index);</span><br><span class="line">        <span class="comment">// ... but we wait on the one before it</span></span><br><span class="line">        cg::<span class="built_in">wait_prior</span>&lt;<span class="number">1</span>&gt;(tb);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Its now available and we can work with local_smem[stage] here</span></span><br><span class="line">        <span class="comment">// (...)</span></span><br><span class="line">        <span class="comment">//</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// Calculate the amount fo data that was actually copied, for the next iteration.</span></span><br><span class="line">        copy_count = <span class="built_in">min</span>(elementsInShared, elementsPerThreadBlock - index);</span><br><span class="line">        index += copy_count;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// A cg::sync(tb) might be needed here depending on whether</span></span><br><span class="line">        <span class="comment">// the work done with local_smem[stage] can release threads to race ahead or not</span></span><br><span class="line">        <span class="comment">// Wrap to the next stage</span></span><br><span class="line">        stage ^= <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    cg::<span class="built_in">wait</span>(tb);</span><br><span class="line">    <span class="comment">// The last local_smem[stage] can be handled here</span></span><br></pre></td></tr></table></figure>
<h2 id="C-6-3-Data-manipulation"><a href="#C-6-3-Data-manipulation" class="headerlink" title="C.6.3. Data manipulation"></a>C.6.3. Data manipulation</h2><h3 id="C-6-3-1-reduce"><a href="#C-6-3-1-reduce" class="headerlink" title="C.6.3.1. reduce"></a>C.6.3.1. reduce</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> TyArg, <span class="keyword">typename</span> TyOp, <span class="keyword">typename</span> TyGroup&gt;</span><br><span class="line"><span class="function"><span class="keyword">auto</span> <span class="title">reduce</span><span class="params">(<span class="type">const</span> TyGroup&amp; group, TyArg&amp;&amp; val, TyOp&amp;&amp; op)</span> -&gt; <span class="title">decltype</span><span class="params">(op(val, val))</span></span>;</span><br></pre></td></tr></table></figure>
<p><code>reduce</code> 对传入的组中指定的每个线程提供的数据执行归约操作。这利用硬件加速（在计算 80 及更高的设备上）进行算术加法、最小或最大操作以及逻辑 AND、OR、或 XOR，以及在老一代硬件上提供软件替代支持(fallback)。只有 4B 类型由硬件加速。</p>
<p><code>group</code>：有效的组类型是 <code>coalesced_group</code> 和 <code>thread_block_tile</code>。</p>
<p><code>val</code>：满足以下要求的任何类型：</p>
<ul>
<li>符合普通可复制的条件，即 <code>is_trivially_copyable&lt;TyArg&gt;::value == true</code></li>
<li><code>sizeof(TyArg) &lt;= 32</code></li>
<li>对给定的函数对象具有合适的算术或比较运算符。</li>
</ul>
<p><code>op</code>：将提供具有整数类型的硬件加速的有效函数对象是 <code>plus()</code>、<code>less()</code>、<code>greater()</code>、<code>bit_and()</code>、<code>bit_xor()</code>、<code>bit_or()</code>。这些必须构造，因此需要 <code>TyVal</code> 模板参数，即 <code>plus&lt;int&gt;()</code>。 <code>Reduce</code> 还支持可以使用 <code>operator()</code> 调用的 <code>lambda</code> 和其他函数对象</p>
<p>Codegen 要求：计算能力 3.5 最低，计算能力 8.0 用于硬件加速，C++11。</p>
<p>需要包含collaborative_groups/reduce.h 头文件。</p>
<p>示例:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups/reduce.h&gt;</span></span></span><br><span class="line"><span class="keyword">namespace</span> cg=cooperative_groups;</span><br><span class="line"></span><br><span class="line"><span class="comment">/// The following example accepts input in *A and outputs a result into *sum</span></span><br><span class="line"><span class="comment">/// It spreads the data within the block, one element per thread</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> blocksz 256</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">block_reduce</span><span class="params">(<span class="type">const</span> <span class="type">int</span> *A, <span class="type">int</span> *sum)</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span> reduction_s[blocksz];</span><br><span class="line"></span><br><span class="line">    cg::thread_block cta = cg::<span class="built_in">this_thread_block</span>();</span><br><span class="line">    cg::thread_block_tile&lt;<span class="number">32</span>&gt; tile = cg::<span class="built_in">tiled_partition</span>&lt;<span class="number">32</span>&gt;(cta);</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = cta.<span class="built_in">thread_rank</span>();</span><br><span class="line">    <span class="type">int</span> beta = A[tid];</span><br><span class="line">    <span class="comment">// reduce across the tile</span></span><br><span class="line">    <span class="comment">// cg::plus&lt;int&gt; allows cg::reduce() to know it can use hardware acceleration for addition</span></span><br><span class="line">    reduction_s[tid] = cg::<span class="built_in">reduce</span>(tile, beta, cg::<span class="built_in">plus</span>&lt;<span class="type">int</span>&gt;());</span><br><span class="line">    <span class="comment">// synchronize the block so all data is ready</span></span><br><span class="line">    cg::<span class="built_in">sync</span>(cta);</span><br><span class="line">    <span class="comment">// single leader accumulates the result</span></span><br><span class="line">    <span class="keyword">if</span> (cta.<span class="built_in">thread_rank</span>() == <span class="number">0</span>) &#123;</span><br><span class="line">        beta = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; blocksz; i += tile.<span class="built_in">num_threads</span>()) &#123;</span><br><span class="line">            beta += reduction_s[i];</span><br><span class="line">        &#125;</span><br><span class="line">        sum[blockIdx.x] = beta;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="C-6-3-2-Reduce-Operators"><a href="#C-6-3-2-Reduce-Operators" class="headerlink" title="C.6.3.2. Reduce Operators"></a>C.6.3.2. Reduce Operators</h4><p>下面是一些可以用<code>reduce</code>完成的基本操作的函数对象的原型</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> cooperative_groups &#123;</span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Ty&gt;</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">cg</span>::plus;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Ty&gt;</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">cg</span>::less;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Ty&gt;</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">cg</span>::greater;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Ty&gt;</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">cg</span>::bit_and;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Ty&gt;</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">cg</span>::bit_xor;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Ty&gt;</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">cg</span>::bit_or;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>Reduce</code> 仅限于在编译时可用于实现的信息。 因此，为了利用 CC 8.0 中引入的内在函数，<code>cg::</code> 命名空间公开了几个镜像硬件的功能对象。 这些对象看起来与 C++ STL 中呈现的对象相似，除了 <code>less/greater</code>。 与 STL 有任何差异的原因在于，这些函数对象旨在实际反映硬件内联函数的操作。</p>
<p>功能说明：</p>
<ul>
<li><code>cg::plus</code>：接受两个值并使用 <code>operator +</code> 返回两者之和。</li>
<li><code>cg::less</code>: 接受两个值并使用 <code>operator &lt;</code>返回较小的值。 这不同之处在于返回较低的值而不是布尔值。</li>
<li><code>cg::greater</code>：接受两个值并使用 <code>operator &lt;</code> 返回较大的值。 这不同之处在于返回更大的值而不是布尔值。</li>
<li><code>cg::bit_and</code>：接受两个值并返回<code>operator &amp;</code>的结果。</li>
<li><code>cg::bit_xor</code>：接受两个值并返回<code>operator ^</code>的结果。</li>
<li><code>cg::bit_or</code>：接受两个值并返回 <code>operator |</code> 的结果。</li>
</ul>
<p>示例:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="comment">// cg::plus&lt;int&gt; is specialized within cg::reduce and calls __reduce_add_sync(...) on CC 8.0+</span></span><br><span class="line">    cg::<span class="built_in">reduce</span>(tile, (<span class="type">int</span>)val, cg::<span class="built_in">plus</span>&lt;<span class="type">int</span>&gt;());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// cg::plus&lt;float&gt; fails to match with an accelerator and instead performs a standard shuffle based reduction</span></span><br><span class="line">    cg::<span class="built_in">reduce</span>(tile, (<span class="type">float</span>)val, cg::<span class="built_in">plus</span>&lt;<span class="type">float</span>&gt;());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// While individual components of a vector are supported, reduce will not use hardware intrinsics for the following</span></span><br><span class="line">    <span class="comment">// It will also be necessary to define a corresponding operator for vector and any custom types that may be used</span></span><br><span class="line">    int4 vec = &#123;...&#125;;</span><br><span class="line">    cg::<span class="built_in">reduce</span>(tile, vec, cg::<span class="built_in">plus</span>&lt;int4&gt;())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Finally lambdas and other function objects cannot be inspected for dispatch</span></span><br><span class="line">    <span class="comment">// and will instead perform shuffle based reductions using the provided function object.</span></span><br><span class="line">    cg::<span class="built_in">reduce</span>(tile, (<span class="type">int</span>)val, [](<span class="type">int</span> l, <span class="type">int</span> r) -&gt; <span class="type">int</span> &#123;<span class="keyword">return</span> l + r;&#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="C-6-3-3-inclusive-scan-and-exclusive-scan"><a href="#C-6-3-3-inclusive-scan-and-exclusive-scan" class="headerlink" title="C.6.3.3. inclusive_scan and exclusive_scan"></a>C.6.3.3. inclusive_scan and exclusive_scan</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> TyGroup, <span class="keyword">typename</span> TyVal, <span class="keyword">typename</span> TyFn&gt;</span><br><span class="line"><span class="function"><span class="keyword">auto</span> <span class="title">inclusive_scan</span><span class="params">(<span class="type">const</span> TyGroup&amp; group, TyVal&amp;&amp; val, TyFn&amp;&amp; op)</span> -&gt; <span class="title">decltype</span><span class="params">(op(val, val))</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> TyGroup, <span class="keyword">typename</span> TyVal&gt;</span><br><span class="line"><span class="function">TyVal <span class="title">inclusive_scan</span><span class="params">(<span class="type">const</span> TyGroup&amp; group, TyVal&amp;&amp; val)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> TyGroup, <span class="keyword">typename</span> TyVal, <span class="keyword">typename</span> TyFn&gt;</span><br><span class="line"><span class="function"><span class="keyword">auto</span> <span class="title">exclusive_scan</span><span class="params">(<span class="type">const</span> TyGroup&amp; group, TyVal&amp;&amp; val, TyFn&amp;&amp; op)</span> -&gt; <span class="title">decltype</span><span class="params">(op(val, val))</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> TyGroup, <span class="keyword">typename</span> TyVal&gt;</span><br><span class="line"><span class="function">TyVal <span class="title">exclusive_scan</span><span class="params">(<span class="type">const</span> TyGroup&amp; group, TyVal&amp;&amp; val)</span></span>;</span><br></pre></td></tr></table></figure>
<p><code>inclusive_scan</code> 和<code>exclusive_scan</code> 对传入组中指定的每个线程提供的数据执行扫描操作。在<code>exclusive_scan</code> 的情况下，每个线程的结果是减少<code>thread_rank</code> 低于该线程的线程的数据。 <code>inclusive_scan</code> 的结果还包括调用线程中的归约数据。</p>
<p><code>group</code>：有效的组类型是 <code>coalesced_group</code> 和 <code>thread_block_tile</code>。</p>
<p><code>val</code>：满足以下要求的任何类型：</p>
<ul>
<li>符合普通可复制的条件，即 <code>is_trivially_copyable&lt;TyArg&gt;::value == true</code></li>
<li><code>sizeof(TyArg) &lt;= 32</code></li>
<li>对给定的函数对象具有合适的算术或比较运算符。</li>
</ul>
<p><code>op</code>：为了方便而定义的函数对象有<code>reduce Operators</code>中描述的<code>plus()</code>、<code>less()</code>、<code>greater()</code>、<code>bit_and()</code>、<code>bit_xor()</code>、<code>bit_or()</code>。这些必须构造，因此需要 <code>TyVal</code> 模板参数，即 <code>plus&lt;int&gt;()</code>。 <code>inclusive_scan</code> 和 <code>exclusive_scan</code> 还支持可以使用 <code>operator()</code> 调用的 <code>lambdas</code> 和其他函数对象</p>
<p>Codegen 要求：计算能力 3.5 最低，C++11。</p>
<p>需要包含collaborative_groups/scan.h 头文件。</p>
<p>示例:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups/scan.h&gt;</span></span></span><br><span class="line"><span class="keyword">namespace</span> cg = cooperative_groups;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> thread_block = cg::<span class="built_in">this_thread_block</span>();</span><br><span class="line">    <span class="keyword">auto</span> tile = cg::<span class="built_in">tiled_partition</span>&lt;<span class="number">8</span>&gt;(thread_block);</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> val = cg::<span class="built_in">inclusive_scan</span>(tile, tile.<span class="built_in">thread_rank</span>());</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%u: %u\n&quot;</span>, tile.<span class="built_in">thread_rank</span>(), val);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*  prints for each group:</span></span><br><span class="line"><span class="comment">    0: 0</span></span><br><span class="line"><span class="comment">    1: 1</span></span><br><span class="line"><span class="comment">    2: 3</span></span><br><span class="line"><span class="comment">    3: 6</span></span><br><span class="line"><span class="comment">    4: 10</span></span><br><span class="line"><span class="comment">    5: 15</span></span><br><span class="line"><span class="comment">    6: 21</span></span><br><span class="line"><span class="comment">    7: 28</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>
<p>使用 Exclusive_scan 进行动态缓冲区空间分配的示例：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cooperative_groups/scan.h&gt;</span></span></span><br><span class="line"><span class="keyword">namespace</span> cg = cooperative_groups;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Buffer partitioning is static to make the example easier to follow,</span></span><br><span class="line"><span class="comment">// but any arbitrary dynamic allocation scheme can be implemented by replacing this function.</span></span><br><span class="line"><span class="function">__device__ <span class="type">int</span> <span class="title">calculate_buffer_space_needed</span><span class="params">(cg::thread_block_tile&lt;<span class="number">32</span>&gt;&amp; tile)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> tile.<span class="built_in">thread_rank</span>() % <span class="number">2</span> + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">int</span> <span class="title">my_thread_data</span><span class="params">(<span class="type">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span> buffer_used;</span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="type">int</span> buffer[];</span><br><span class="line">    <span class="keyword">auto</span> thread_block = cg::<span class="built_in">this_thread_block</span>();</span><br><span class="line">    <span class="keyword">auto</span> tile = cg::<span class="built_in">tiled_partition</span>&lt;<span class="number">32</span>&gt;(thread_block);</span><br><span class="line"></span><br><span class="line">    buffer_used = <span class="number">0</span>;</span><br><span class="line">    thread_block.<span class="built_in">sync</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// each thread calculates buffer size it needs and its offset within the allocation</span></span><br><span class="line">    <span class="type">int</span> buf_needed = <span class="built_in">calculate_buffer_space_needed</span>(tile);</span><br><span class="line">    <span class="type">int</span> buf_offset = cg::<span class="built_in">exclusive_scan</span>(tile, buf_needed);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// last thread in the tile allocates buffer space with an atomic operation</span></span><br><span class="line">    <span class="type">int</span> alloc_offset = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span> (tile.<span class="built_in">thread_rank</span>() == tile.<span class="built_in">num_threads</span>() - <span class="number">1</span>) &#123;</span><br><span class="line">        alloc_offset = <span class="built_in">atomicAdd</span>(&amp;buffer_used, buf_offset + buf_needed);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// that thread shares the allocation start with other threads in the tile</span></span><br><span class="line">    alloc_offset = tile.<span class="built_in">shfl</span>(alloc_offset, tile.<span class="built_in">num_threads</span>() - <span class="number">1</span>);</span><br><span class="line">    buf_offset += alloc_offset;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// each thread fill its part of the buffer with thread specific data</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span> ; i &lt; buf_needed ; ++i) &#123;</span><br><span class="line">        buffer[buf_offset + i] = <span class="built_in">my_thread_data</span>(i);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// buffer is &#123;0, 0, 1, 0, 0, 1 ...&#125;;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="C-7-Grid-Synchronization"><a href="#C-7-Grid-Synchronization" class="headerlink" title="C.7. Grid Synchronization"></a>C.7. Grid Synchronization</h2><p>在引入协作组(Cooperative Groups)之前，CUDA 编程模型只允许在内核完成边界的线程块之间进行同步。内核边界带有隐含的状态失效，以及潜在的性能影响。</p>
<p>例如，在某些用例中，应用程序具有大量小内核，每个内核代表处理pipeline中的一个阶段。当前的 CUDA 编程模型需要这些内核的存在，以确保在一个pipeline阶段上运行的线程块在下一个pipeline阶段上运行的线程块准备好使用数据之前产生数据。在这种情况下，提供全局线程间块同步的能力将允许将应用程序重组为具有持久线程块，当给定阶段完成时，这些线程块能够在设备上同步。</p>
<p>要从内核中跨网格同步，您只需使用 <code>grid.sync()</code> 功能：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grid_group grid = <span class="built_in">this_grid</span>();</span><br><span class="line">grid.<span class="built_in">sync</span>();</span><br></pre></td></tr></table></figure>
<p>并且在启动内核时，有必要使用 <code>cudaLaunchCooperativeKernel</code> CUDA 运行时启动 API 或 CUDA 驱动程序等价物，而不是 &lt;&lt;&lt;…&gt;&gt;&gt; 执行配置语法。</p>
<p>例子：</p>
<p>为了保证线程块在 GPU 上的共同驻留，需要仔细考虑启动的块数。 例如，可以按如下方式启动与 SM 一样多的块：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> device = <span class="number">0</span>;</span><br><span class="line">cudaDeviceProp deviceProp;</span><br><span class="line"><span class="built_in">cudaGetDeviceProperties</span>(&amp;deviceProp, dev);</span><br><span class="line"><span class="comment">// initialize, then launch</span></span><br><span class="line"><span class="built_in">cudaLaunchCooperativeKernel</span>((<span class="type">void</span>*)my_kernel, deviceProp.multiProcessorCount, numThreads, args);</span><br></pre></td></tr></table></figure>
<p>或者，您可以通过使用占用计算器(occupancy calculator)计算每个 SM 可以同时容纳多少块来最大化暴露的并行度，如下所示：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/// This will launch a grid that can maximally fill the GPU, on the default stream with kernel arguments</span></span><br><span class="line"><span class="type">int</span> numBlocksPerSm = <span class="number">0</span>;</span><br><span class="line"> <span class="comment">// Number of threads my_kernel will be launched with</span></span><br><span class="line"><span class="type">int</span> numThreads = <span class="number">128</span>;</span><br><span class="line">cudaDeviceProp deviceProp;</span><br><span class="line"><span class="built_in">cudaGetDeviceProperties</span>(&amp;deviceProp, dev);</span><br><span class="line"><span class="built_in">cudaOccupancyMaxActiveBlocksPerMultiprocessor</span>(&amp;numBlocksPerSm, my_kernel, numThreads, <span class="number">0</span>);</span><br><span class="line"><span class="comment">// launch</span></span><br><span class="line"><span class="type">void</span> *kernelArgs[] = &#123; <span class="comment">/* add kernel args */</span> &#125;;</span><br><span class="line"><span class="function">dim3 <span class="title">dimBlock</span><span class="params">(numThreads, <span class="number">1</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">dimGrid</span><span class="params">(deviceProp.multiProcessorCount*numBlocksPerSm, <span class="number">1</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line"><span class="built_in">cudaLaunchCooperativeKernel</span>((<span class="type">void</span>*)my_kernel, dimGrid, dimBlock, kernelArgs);</span><br></pre></td></tr></table></figure>
<p>最好先通过查询设备属性 <code>cudaDevAttrCooperativeLaunch</code> 来确保设备支持协作启动：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line"><span class="type">int</span> supportsCoopLaunch = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">cudaDeviceGetAttribute</span>(&amp;supportsCoopLaunch, cudaDevAttrCooperativeLaunch, dev);</span><br></pre></td></tr></table></figure>
<p>如果设备 0 支持该属性，则将 <code>supportsCoopLaunch</code> 设置为 1。仅支持计算能力为 6.0 及更高版本的设备。 此外，您需要在以下任何一个上运行：</p>
<ul>
<li>没有 MPS 的 Linux 平台</li>
<li>具有 MPS 和计算能力 7.0 或更高版本的设备上的 Linux 平台</li>
<li>最新的 Windows 平台</li>
</ul>
<h2 id="C-8-Multi-Device-Synchronization"><a href="#C-8-Multi-Device-Synchronization" class="headerlink" title="C.8. Multi-Device Synchronization"></a>C.8. Multi-Device Synchronization</h2><p>为了通过协作组启用跨多个设备的同步，需要使用 <code>cudaLaunchCooperativeKernelMultiDevice</code> CUDA API。这与现有的 CUDA API 有很大不同，它将允许单个主机线程跨多个设备启动内核。除了 <code>cudaLaunchCooperativeKernel</code> 做出的约束和保证之外，这个 API 还具有额外的语义：</p>
<ul>
<li>此 API 将确保启动是原子的，即如果 API 调用成功，则提供的线程块数将在所有指定设备上启动。</li>
<li>通过此 API 启动的功能必须相同。驱动程序在这方面没有进行明确的检查，因为这在很大程度上是不可行的。由应用程序来确保这一点。</li>
<li>提供的 <code>cudaLaunchParams</code> 中没有两个条目可以映射到同一设备。</li>
<li>本次发布所针对的所有设备都必须具有相同的计算能力——主要版本和次要版本。</li>
<li>每个网格的块大小、网格大小和共享内存量在所有设备上必须相同。请注意，这意味着每个设备可以启动的最大块数将受到 SM 数量最少的设备的限制。</li>
<li>拥有正在启动的 CUfunction 的模块中存在的任何用户定义的 <strong>device</strong>、<strong>constant</strong> 或 <strong>managed</strong> 设备全局变量都在每个设备上独立实例化。用户负责适当地初始化此类设备全局变量。</li>
</ul>
<p>弃用通知：cudaLaunchCooperativeKernelMultiDevice 已在 CUDA 11.3 中针对所有设备弃用。在多设备共轭梯度样本中可以找到替代方法的示例。</p>
<p>多设备同步的最佳性能是通过 <code>cuCtxEnablePeerAccess</code> 或 <code>cudaDeviceEnablePeerAccess</code> 为所有参与设备启用对等访问来实现的。</p>
<p>启动参数应使用结构数组（每个设备一个）定义，并使用 <code>cudaLaunchCooperativeKernelMultiDevice</code> 启动</p>
<p>Example:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">cudaDeviceProp deviceProp;</span><br><span class="line"><span class="built_in">cudaGetDeviceCount</span>(&amp;numGpus);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Per device launch parameters</span></span><br><span class="line">cudaLaunchParams *launchParams = (cudaLaunchParams*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(cudaLaunchParams) * numGpus);</span><br><span class="line">cudaStream_t *streams = (cudaStream_t*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(cudaStream_t) * numGpus);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The kernel arguments are copied over during launch</span></span><br><span class="line"><span class="comment">// Its also possible to have individual copies of kernel arguments per device, but</span></span><br><span class="line"><span class="comment">// the signature and name of the function/kernel must be the same.</span></span><br><span class="line"><span class="type">void</span> *kernelArgs[] = &#123; <span class="comment">/* Add kernel arguments */</span> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; numGpus; i++) &#123;</span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(i);</span><br><span class="line">    <span class="comment">// Per device stream, but its also possible to use the default NULL stream of each device</span></span><br><span class="line">    <span class="built_in">cudaStreamCreate</span>(&amp;streams[i]);</span><br><span class="line">    <span class="comment">// Loop over other devices and cudaDeviceEnablePeerAccess to get a faster barrier implementation</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Since all devices must be of the same compute capability and have the same launch configuration</span></span><br><span class="line"><span class="comment">// it is sufficient to query device 0 here</span></span><br><span class="line"><span class="built_in">cudaGetDeviceProperties</span>(&amp;deviceProp[i], <span class="number">0</span>);</span><br><span class="line"><span class="function">dim3 <span class="title">dimBlock</span><span class="params">(numThreads, <span class="number">1</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">dimGrid</span><span class="params">(deviceProp.multiProcessorCount, <span class="number">1</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; numGpus; i++) &#123;</span><br><span class="line">    launchParamsList[i].func = (<span class="type">void</span>*)my_kernel;</span><br><span class="line">    launchParamsList[i].gridDim = dimGrid;</span><br><span class="line">    launchParamsList[i].blockDim = dimBlock;</span><br><span class="line">    launchParamsList[i].sharedMem = <span class="number">0</span>;</span><br><span class="line">    launchParamsList[i].stream = streams[i];</span><br><span class="line">    launchParamsList[i].args = kernelArgs;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">cudaLaunchCooperativeKernelMultiDevice</span>(launchParams, numGpus);</span><br></pre></td></tr></table></figure>
<p>此外，与网格范围的同步一样，生成的设备代码看起来非常相似：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">multi_grid_group multi_grid = <span class="built_in">this_multi_grid</span>();</span><br><span class="line">multi_grid.<span class="built_in">sync</span>();</span><br></pre></td></tr></table></figure>
<p>但是，需要通过将 <code>-rdc=true</code> 传递给 nvcc 来单独编译代码。</p>
<p>最好先通过查询设备属性 <code>cudaDevAttrCooperativeMultiDeviceLaunch</code> 来确保设备支持多设备协作启动：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line"><span class="type">int</span> supportsMdCoopLaunch = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">cudaDeviceGetAttribute</span>(&amp;supportsMdCoopLaunch, cudaDevAttrCooperativeMultiDeviceLaunch, dev);</span><br></pre></td></tr></table></figure>
<p>如果设备 0 支持该属性，则将 supportsMdCoopLaunch 设置为 1。仅支持计算能力为 6.0 及更高版本的设备。 此外，您需要在 Linux 平台（无 MPS）或当前版本的 Windows 上运行，并且设备处于 TCC 模式。</p>
<p>有关更多信息，请参阅 <a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EXECUTION.html#group__CUDART__EXECUTION_1g20f8d75d8786c54cc168c47fde66ee52">cudaLaunchCooperativeKernelMultiDevice</a> API 文档。</p>
<h1 id="附录D-CUDA的动态并行"><a href="#附录D-CUDA的动态并行" class="headerlink" title="附录D-CUDA的动态并行"></a>附录D-CUDA的动态并行</h1><h2 id="D-1-Introduction"><a href="#D-1-Introduction" class="headerlink" title="D.1. Introduction"></a>D.1. Introduction</h2><h3 id="D-1-1-Overview"><a href="#D-1-1-Overview" class="headerlink" title="D.1.1. Overview"></a>D.1.1. Overview</h3><p><strong><em>Dynamic Parallelism</em></strong>是 CUDA 编程模型的扩展，使 CUDA 内核能够直接在 GPU 上创建新工作并与新工作同步。在程序中需要的任何位置动态创建并行性提供了令人兴奋的新功能。</p>
<p>直接从 GPU 创建工作的能力可以减少在主机和设备之间传输执行控制和数据的需要，因为现在可以通过在设备上执行的线程在运行时做出启动配置决策。此外，可以在运行时在内核内内联生成依赖于数据的并行工作，动态利用 GPU 的硬件调度程序和负载平衡器，并根据数据驱动的决策或工作负载进行调整。以前需要修改以消除递归、不规则循环结构或其他不适合平面、单级并行性的构造的算法和编程模式可以更透明地表达。</p>
<p>本文档描述了支持动态并行的 CUDA 的扩展功能，包括为利用这些功能而对 CUDA 编程模型进行必要的修改和添加，以及利用此附加功能的指南和最佳实践。</p>
<p>警告：与父块的子内核显式同步（即在设备代码中使用 <code>cudaDeviceSynchronize()</code>）在 CUDA 11.6 中已弃用，并计划在未来的 CUDA 版本中删除。</p>
<p>只有计算能力为 3.5 或更高的设备支持动态并行。</p>
<h3 id="D-1-2-Glossary"><a href="#D-1-2-Glossary" class="headerlink" title="D.1.2. Glossary"></a>D.1.2. Glossary</h3><p>本指南中使用的术语的定义。</p>
<ul>
<li><code>Grid</code>:网格是线程的集合。网格中的线程执行内核函数并被划分为线程。</li>
<li><code>Thread Block</code>:线程块是在同一多处理器 (SM) 上执行的一组线程。线程块中的线程可以访问共享内存并且可以显式同步。</li>
<li><code>Kernel Function</code>:内核函数是一个隐式并行子程序，它在 CUDA 执行和内存模型下为网格中的每个线程执行。</li>
<li><code>Host</code>:Host 指的是最初调用 CUDA 的执行环境。通常是在系统的 CPU 处理器上运行的线程。</li>
<li><code>Parent</code>:父线程、线程块或网格是已启动新网格、子网格的一种。直到所有启动的子网格也完成后，父节点才被视为完成。</li>
<li><code>Child</code>:子线程、块或网格是由父网格启动的线程、块或网格。子网格必须在父线程、线程块或网格被认为完成之前完成。</li>
<li><code>Thread Block Scope</code>:具有线程块作用域的对象具有单个线程块的生命周期。它们仅在由创建对象的线程块中的线程操作时具有定义的行为，并在创建它们的线程块完成时被销毁。</li>
<li><code>Device Runtime</code>:设备运行时是指可用于使内核函数使用动态并行的运行时系统和 API。</li>
</ul>
<h2 id="D-2-Execution-Environment-and-Memory-Model"><a href="#D-2-Execution-Environment-and-Memory-Model" class="headerlink" title="D.2. Execution Environment and Memory Model"></a>D.2. Execution Environment and Memory Model</h2><h3 id="D-2-1-Execution-Environment"><a href="#D-2-1-Execution-Environment" class="headerlink" title="D.2.1. Execution Environment"></a>D.2.1. Execution Environment</h3><p>CUDA 执行模型基于线程、线程块和网格的原语，内核函数定义了线程块和网格内的各个线程执行的程序。 当调用内核函数时，网格的属性由执行配置描述，该配置在 CUDA 中具有特殊的语法。 CUDA 中对动态并行性的支持扩展了在新网格上配置、启动和同步到设备上运行的线程的能力。</p>
<p>警告：与父块的子内核显式同步（即在设备代码中使用 cudaDeviceSynchronize() 块）在 CUDA 11.6 中已弃用，并计划在未来的 CUDA 版本中删除。</p>
<h4 id="D-2-1-1-Parent-and-Child-Grids"><a href="#D-2-1-1-Parent-and-Child-Grids" class="headerlink" title="D.2.1.1. Parent and Child Grids"></a>D.2.1.1. Parent and Child Grids</h4><p>配置并启动新网格的设备线程属于父网格，调用创建的网格是子网格。</p>
<p>子网格的调用和完成是正确嵌套的，这意味着在其线程创建的所有子网格都完成之前，父网格不会被认为是完整的。 即使调用线程没有在启动的子网格上显式同步，运行时也会保证父子网格之间的隐式同步。</p>
<p>警告：与父块的子内核显式同步（即在设备代码中使用 cudaDeviceSynchronize()）在 CUDA 11.6 中已弃用，并计划在未来的 CUDA 版本中删除。</p>
<p><img src="/img/parent-child-launch-nesting.png" alt="parent-child-launch-nesting.png"></p>
<h4 id="D-2-1-2-Scope-of-CUDA-Primitives"><a href="#D-2-1-2-Scope-of-CUDA-Primitives" class="headerlink" title="D.2.1.2. Scope of CUDA Primitives"></a>D.2.1.2. Scope of CUDA Primitives</h4><p>在主机和设备上，CUDA 运行时都提供了一个 API，用于启动内核、等待启动的工作完成以及通过流和事件跟踪启动之间的依赖关系。 在主机系统上，启动状态和引用流和事件的 CUDA 原语由进程内的所有线程共享； 但是进程独立执行，可能不共享 CUDA 对象。</p>
<p>设备上存在类似的层次结构：启动的内核和 CUDA 对象对线程块中的所有线程都是可见的，但在线程块之间是独立的。 这意味着例如一个流可以由一个线程创建并由同一线程块中的任何其他线程使用，但不能与任何其他线程块中的线程共享。</p>
<h4 id="D-2-1-3-Synchronization"><a href="#D-2-1-3-Synchronization" class="headerlink" title="D.2.1.3. Synchronization"></a>D.2.1.3. Synchronization</h4><p>警告：与父块的子内核显式同步（即在设备代码中使用 cudaDeviceSynchronize()）在 CUDA 11.6 中已弃用，并计划在未来的 CUDA 版本中删除。</p>
<p>来自任何线程的 CUDA 运行时操作，包括内核启动，在线程块中都是可见的。 这意味着父网格中的调用线程可以在由该线程启动的网格、线程块中的其他线程或在同一线程块中创建的流上执行同步。 直到块中所有线程的所有启动都完成后，才认为线程块的执行完成。 如果一个块中的所有线程在所有子启动完成之前退出，将自动触发同步操作。</p>
<h4 id="D-2-1-4-Streams-and-Events"><a href="#D-2-1-4-Streams-and-Events" class="headerlink" title="D.2.1.4. Streams and Events"></a>D.2.1.4. Streams and Events</h4><p>CUDA 流和事件允许控制网格启动之间的依赖关系：启动到同一流中的网格按顺序执行，事件可用于创建流之间的依赖关系。 在设备上创建的流和事件服务于这个完全相同的目的。</p>
<p>在网格中创建的流和事件存在于线程块范围内，但在创建它们的线程块之外使用时具有未定义的行为。 如上所述，线程块启动的所有工作在块退出时都会隐式同步； 启动到流中的工作包含在其中，所有依赖关系都得到了适当的解决。 已在线程块范围之外修改的流上的操作行为未定义。</p>
<p>在主机上创建的流和事件在任何内核中使用时具有未定义的行为，就像在子网格中使用时由父网格创建的流和事件具有未定义的行为一样。</p>
<h4 id="D-2-1-5-Ordering-and-Concurrency"><a href="#D-2-1-5-Ordering-and-Concurrency" class="headerlink" title="D.2.1.5. Ordering and Concurrency"></a>D.2.1.5. Ordering and Concurrency</h4><p>从设备运行时启动内核的顺序遵循 CUDA Stream 排序语义。在一个线程块内，所有内核启动到同一个流中都是按顺序执行的。当同一个线程块中的多个线程启动到同一个流中时，流内的顺序取决于块内的线程调度，这可以通过 <code>__syncthreads()</code> 等同步原语进行控制。</p>
<p>请注意，由于流由线程块内的所有线程共享，因此隐式 NULL 流也被共享。如果线程块中的多个线程启动到隐式流中，则这些启动将按顺序执行。如果需要并发，则应使用显式命名流。</p>
<p>动态并行使并发在程序中更容易表达；但是，设备运行时不会在 CUDA 执行模型中引入新的并发保证。无法保证设备上任意数量的不同线程块之间的并发执行。</p>
<p>缺乏并发保证延伸到父线程块及其子网格。当父线程块启动子网格时，在父线程块到达显式同步点（例如 <code>cudaDeviceSynchronize()</code>）之前，不保证子网格开始执行。</p>
<p>警告：与父块的子内核显式同步（即在设备代码中使用 cudaDeviceSynchronize()）在 CUDA 11.6 中已弃用，并计划在未来的 CUDA 版本中删除。</p>
<p>虽然并发通常很容易实现，但它可能会因设备配置、应用程序工作负载和运行时调度而异。因此，依赖不同线程块之间的任何并发性是不安全的。</p>
<h4 id="D-2-1-6-Device-Management"><a href="#D-2-1-6-Device-Management" class="headerlink" title="D.2.1.6. Device Management"></a>D.2.1.6. Device Management</h4><p>设备运行时不支持多 GPU； 设备运行时只能在其当前执行的设备上运行。 但是，允许查询系统中任何支持 CUDA 的设备的属性。</p>
<h3 id="D-2-2-Memory-Model"><a href="#D-2-2-Memory-Model" class="headerlink" title="D.2.2. Memory Model"></a>D.2.2. Memory Model</h3><p>父网格和子网格共享相同的全局和常量内存存储，但具有不同的本地和共享内存。</p>
<h4 id="D-2-2-1-Coherence-and-Consistency"><a href="#D-2-2-1-Coherence-and-Consistency" class="headerlink" title="D.2.2.1. Coherence and Consistency"></a>D.2.2.1. Coherence and Consistency</h4><h4 id="D-2-2-1-1-Global-Memory"><a href="#D-2-2-1-1-Global-Memory" class="headerlink" title="D.2.2.1.1. Global Memory"></a>D.2.2.1.1. Global Memory</h4><p>父子网格可以连贯地访问全局内存，但子网格和父网格之间的一致性保证很弱。当子网格的内存视图与父线程完全一致时，子网格的执行有两点：当子网格被父线程调用时，以及当子网格线程完成时（由父线程中的同步 API 调用发出信号）。</p>
<p>警告：与父块的子内核显式同步（即在设备代码中使用 cudaDeviceSynchronize()）在 CUDA 11.6 中已弃用，并计划在未来的 CUDA 版本中删除。</p>
<p>在子网格调用之前，父线程中的所有全局内存操作对子网格都是可见的。在父网格完成同步后，子网格的所有内存操作对父网格都是可见的。</p>
<p>在下面的示例中，执行 <code>child_launch</code> 的子网格只能保证看到在子网格启动之前对数据所做的修改。由于父线程 <code>0</code> 正在执行启动，子线程将与父线程 <code>0</code> 看到的内存保持一致。由于第一次<code>__syncthreads()</code> 调用，孩子将看到 <code>data[0]=0, data[1]=1, ..., data[255]=255</code>（没有 <code>__syncthreads()</code> 调用，只有 <code>data[0]</code>将保证被孩子看到）。当子网格返回时，线程 <code>0</code> 保证可以看到其子网格中的线程所做的修改。只有在第二次 <code>__syncthreads()</code> 调用之后，这些修改才可用于父网格的其他线程：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">child_launch</span><span class="params">(<span class="type">int</span> *data)</span> </span>&#123;</span><br><span class="line">   data[threadIdx.x] = data[threadIdx.x]<span class="number">+1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">parent_launch</span><span class="params">(<span class="type">int</span> *data)</span> </span>&#123;</span><br><span class="line">   data[threadIdx.x] = threadIdx.x;</span><br><span class="line"></span><br><span class="line">   __syncthreads();</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>) &#123;</span><br><span class="line">       child_launch&lt;&lt;&lt; <span class="number">1</span>, <span class="number">256</span> &gt;&gt;&gt;(data);</span><br><span class="line">       <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   __syncthreads();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">host_launch</span><span class="params">(<span class="type">int</span> *data)</span> </span>&#123;</span><br><span class="line">    parent_launch&lt;&lt;&lt; <span class="number">1</span>, <span class="number">256</span> &gt;&gt;&gt;(data);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="D-2-2-1-2-Zero-Copy-Memory"><a href="#D-2-2-1-2-Zero-Copy-Memory" class="headerlink" title="D.2.2.1.2. Zero Copy Memory"></a>D.2.2.1.2. Zero Copy Memory</h4><p>零拷贝系统内存与全局内存具有相同的一致性和一致性保证，并遵循上面详述的语义。 内核可能不会分配或释放零拷贝内存，但可能会使用从主机程序传入的指向零拷贝的指针。</p>
<h4 id="D-2-2-1-3-Constant-Memory"><a href="#D-2-2-1-3-Constant-Memory" class="headerlink" title="D.2.2.1.3. Constant Memory"></a>D.2.2.1.3. Constant Memory</h4><p>常量是不可变的，不能从设备修改，即使在父子启动之间也是如此。 也就是说，所有 <code>__constant__</code> 变量的值必须在启动之前从主机设置。 所有子内核都从各自的父内核自动继承常量内存。</p>
<p>从内核线程中获取常量内存对象的地址与所有 CUDA 程序具有相同的语义，并且自然支持将该指针从父级传递给子级或从子级传递给父级。</p>
<h4 id="D-2-2-1-4-Shared-and-Local-Memory"><a href="#D-2-2-1-4-Shared-and-Local-Memory" class="headerlink" title="D.2.2.1.4. Shared and Local Memory"></a>D.2.2.1.4. Shared and Local Memory</h4><p>共享内存和本地内存分别是线程块或线程私有的，并且在父子之间不可见或不连贯。 当这些位置之一中的对象在其所属范围之外被引用时，行为未定义，并且可能导致错误。</p>
<p>如果 NVIDIA 编译器可以检测到指向本地或共享内存的指针作为参数传递给内核启动，它将尝试发出警告。 在运行时，程序员可以使用 <code>__isGlobal()</code> 内部函数来确定指针是否引用全局内存，因此可以安全地传递给子启动。</p>
<p>请注意，对 <code>cudaMemcpy*Async()</code> 或 <code>cudaMemset*Async()</code> 的调用可能会调用设备上的新子内核以保留流语义。 因此，将共享或本地内存指针传递给这些 API 是非法的，并且会返回错误。</p>
<h4 id="D-2-2-1-5-Local-Memory"><a href="#D-2-2-1-5-Local-Memory" class="headerlink" title="D.2.2.1.5. Local Memory"></a>D.2.2.1.5. Local Memory</h4><p>本地内存是执行线程的私有存储，在该线程之外不可见。 启动子内核时将指向本地内存的指针作为启动参数传递是非法的。 从子级取消引用此类本地内存指针的结果将是未定义的。</p>
<p>例如，如果 <code>child_launch</code> 访问 <code>x_array</code>，则以下内容是非法的，具有未定义的行为：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> x_array[<span class="number">10</span>];       <span class="comment">// Creates x_array in parent&#x27;s local memory </span></span><br><span class="line">child_launch&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1</span> &gt;&gt;&gt;(x_array);</span><br></pre></td></tr></table></figure>
<p>程序员有时很难知道编译器何时将变量放入本地内存。 作为一般规则，传递给子内核的所有存储都应该从全局内存堆中显式分配，或者使用 <code>cudaMalloc()</code>、<code>new()</code> 或通过在全局范围内声明<code>__device__</code> 存储。 例如：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Correct - &quot;value&quot; is global storage</span></span><br><span class="line">__device__ <span class="type">int</span> value; </span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">x</span><span class="params">()</span> </span>&#123; </span><br><span class="line">    value = <span class="number">5</span>; </span><br><span class="line">    child&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1</span> &gt;&gt;&gt;(&amp;value); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Invalid - &quot;value&quot; is local storage</span></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">y</span><span class="params">()</span> </span>&#123; </span><br><span class="line">    <span class="type">int</span> value = <span class="number">5</span>; </span><br><span class="line">    child&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1</span> &gt;&gt;&gt;(&amp;value); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="D-2-2-1-6-Texture-Memory"><a href="#D-2-2-1-6-Texture-Memory" class="headerlink" title="D.2.2.1.6. Texture Memory"></a>D.2.2.1.6. Texture Memory</h4><p>对纹理映射的全局内存区域的写入相对于纹理访问是不连贯的。 纹理内存的一致性在子网格的调用和子网格完成时强制执行。 这意味着在子内核启动之前写入内存会反映在子内核的纹理内存访问中。 类似地，子进程对内存的写入将反映在父进程对纹理内存的访问中，但只有在父进程同步子进程完成之后。 父子并发访问可能会导致数据不一致。</p>
<p>警告：与父块的子内核显式同步（即在设备代码中使用 <code>cudaDeviceSynchronize()</code>）在 CUDA 11.6 中已弃用，并计划在未来的 CUDA 版本中删除。</p>
<h2 id="D-3-Programming-Interface"><a href="#D-3-Programming-Interface" class="headerlink" title="D.3. Programming Interface"></a>D.3. Programming Interface</h2><h3 id="D-3-1-CUDA-C-Reference"><a href="#D-3-1-CUDA-C-Reference" class="headerlink" title="D.3.1. CUDA C++ Reference"></a>D.3.1. CUDA C++ Reference</h3><p>内核可以使用标准 CUDA <code>&lt;&lt;&lt; &gt;&gt;&gt;</code> 语法从设备启动：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt; Dg, Db, Ns, S &gt;&gt;&gt;([kernel arguments]);</span><br></pre></td></tr></table></figure>
<ul>
<li><code>Dg</code> 是 <code>dim3</code> 类型，并指定网格(grid)的尺寸和大小</li>
<li><code>Db</code> 是 <code>dim3</code> 类型，指定每个线程块(block)的维度和大小</li>
<li><code>Ns</code> 是 <code>size_t</code> 类型，并指定为每个线程块动态分配的共享内存字节数，用于此调用并添加到静态分配的内存中。 <code>Ns</code> 是一个可选参数，默认为 0。</li>
<li><code>S</code> 是 <code>cudaStream_t</code> 类型，并指定与此调用关联的流。 流必须已在进行调用的同一线程块中分配。<code>S</code> 是一个可选参数，默认为 0。</li>
</ul>
<h4 id="D-3-1-1-1-Launches-are-Asynchronous"><a href="#D-3-1-1-1-Launches-are-Asynchronous" class="headerlink" title="D.3.1.1.1. Launches are Asynchronous"></a>D.3.1.1.1. Launches are Asynchronous</h4><p>与主机端启动相同，所有设备端内核启动相对于启动线程都是异步的。 也就是说，<code>&lt;&lt;&lt;&gt;&gt;&gt;</code> 启动命令将立即返回，启动线程将继续执行，直到它命中一个明确的启动同步点，例如 <code>cudaDeviceSynchronize()</code>。</p>
<p>警告：与父块的子内核显式同步（即在设备代码中使用 cudaDeviceSynchronize()）在 CUDA 11.6 中已弃用，并计划在未来的 CUDA 版本中删除。</p>
<p>网格启动会发布到设备，并将独立于父线程执行。 子网格可以在启动后的任何时间开始执行，但不能保证在启动线程到达显式启动同步点之前开始执行。</p>
<h4 id="D-3-1-1-2-Launch-Environment-Configuration"><a href="#D-3-1-1-2-Launch-Environment-Configuration" class="headerlink" title="D.3.1.1.2. Launch Environment Configuration"></a>D.3.1.1.2. Launch Environment Configuration</h4><p>所有全局设备配置设置（例如，从 <code>cudaDeviceGetCacheConfig()</code>返回的共享内存和 L1 缓存大小，以及从 <code>cudaDeviceGetLimit()</code> 返回的设备限制）都将从父级继承。 同样，堆栈大小等设备限制将保持配置不变。</p>
<p>对于主机启动的内核，从主机设置的每个内核配置将优先于全局设置。 这些配置也将在从设备启动内核时使用。 无法从设备重新配置内核环境。</p>
<h4 id="D-3-1-2-Streams"><a href="#D-3-1-2-Streams" class="headerlink" title="D.3.1.2. Streams"></a>D.3.1.2. Streams</h4><p>设备运行时提供命名和未命名 (<code>NULL</code>) 流。线程块中的任何线程都可以使用命名流，但流句柄不能传递给其他块或子/父内核。换句话说，流应该被视为创建它的块的私有。流句柄不能保证在块之间是唯一的，因此在未分配它的块中使用流句柄将导致未定义的行为。</p>
<p>与主机端启动类似，启动到单独流中的工作可能会同时运行，但不能保证实际的并发性。 CUDA 编程模型不支持依赖子内核之间的并发性的程序，并且将具有未定义的行为。</p>
<p>设备不支持主机端 <code>NULL</code> 流的跨流屏障语义（详见下文）。为了保持与主机运行时的语义兼容性，必须使用 <code>cudaStreamCreateWithFlags()</code> API 创建所有设备流，并传递 <code>cudaStreamNonBlocking</code> 标志。 <code>cudaStreamCreate()</code> 调用是仅限主机运行时的 API，将无法为设备编译。</p>
<p>由于设备运行时不支持 <code>cudaStreamSynchronize()</code> 和 <code>cudaStreamQuery()</code>，因此当应用程序需要知道流启动的子内核已完成时，应使用 <code>cudaDeviceSynchronize()</code>。</p>
<p>警告：与父块的子内核显式同步（即在设备代码中使用 cudaDeviceSynchronize()）在 CUDA 11.6 中已弃用，并计划在未来的 CUDA 版本中删除。</p>
<h4 id="D-3-1-2-1-The-Implicit-NULL-Stream"><a href="#D-3-1-2-1-The-Implicit-NULL-Stream" class="headerlink" title="D.3.1.2.1. The Implicit (NULL) Stream"></a>D.3.1.2.1. The Implicit (NULL) Stream</h4><p>在宿主程序中，未命名（NULL）流与其他流具有额外的屏障同步语义（有关详细信息，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#default-stream">默认流</a>）。 设备运行时提供在块中的所有线程之间共享的单个隐式、未命名流，但由于必须使用 <code>cudaStreamNonBlocking</code> 标志创建所有命名流，启动到 <code>NULL</code> 流中的工作不会插入对任何其他流中未决工作的隐式依赖 （包括其他线程块的 <code>NULL</code> 流）。</p>
<h4 id="D-3-1-3-Events"><a href="#D-3-1-3-Events" class="headerlink" title="D.3.1.3. Events"></a>D.3.1.3. Events</h4><p>仅支持 CUDA 事件的流间同步功能。 这意味着支持 <code>cudaStreamWaitEvent()</code>，但不支持 <code>cudaEventSynchronize()</code>、<code>cudaEventElapsedTime()</code> 和 <code>cudaEventQuery()</code>。 由于不支持 <code>cudaEventElapsedTime()</code>，<code>cudaEvents</code> 必须通过 <code>cudaEventCreateWithFlags()</code> 创建，并传递 <code>cudaEventDisableTiming</code> 标志。</p>
<p>对于所有设备运行时对象，事件对象可以在创建它们的线程块内的所有线程之间共享，但对于该块是本地的，并且可能不会传递给其他内核，或者在同一内核内的块之间。 不保证事件句柄在块之间是唯一的，因此在未创建它的块中使用事件句柄将导致未定义的行为。</p>
<h4 id="D-3-1-4-Synchronization"><a href="#D-3-1-4-Synchronization" class="headerlink" title="D.3.1.4. Synchronization"></a>D.3.1.4. Synchronization</h4><p>警告：与父块的子内核显式同步（即在设备代码中使用 cudaDeviceSynchronize()）在 CUDA 11.6 中已弃用，并计划在未来的 CUDA 版本中删除。</p>
<p><code>cudaDeviceSynchronize()</code> 函数将同步线程块中任何线程启动的所有工作，直到调用 <code>cudaDeviceSynchronize()</code> 为止。 请注意，可以从不同的代码中调用 <code>cudaDeviceSynchronize()</code>（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#block-wide-synchronization">块范围同步</a>）。</p>
<p>如果调用线程旨在与从其他线程调用的子网格同步，则由程序执行足够的额外线程间同步，例如通过调用 <code>__syncthreads()</code>。</p>
<h5 id="D-3-1-4-1-Block-Wide-Synchronization"><a href="#D-3-1-4-1-Block-Wide-Synchronization" class="headerlink" title="D.3.1.4.1. Block Wide Synchronization"></a>D.3.1.4.1. Block Wide Synchronization</h5><p><code>cudaDeviceSynchronize()</code> 函数并不意味着块内同步。 特别是，如果没有通过 <code>__syncthreads()</code> 指令进行显式同步，则调用线程无法对除自身之外的任何线程启动的工作做出任何假设。 例如，如果一个块中的多个线程都在启动工作，并且所有这些工作都需要一次同步（可能是因为基于事件的依赖关系），则由程序来保证在调用之前由所有线程提交这项工作 <code>cudaDeviceSynchronize()</code>。</p>
<p>因为允许实现在从块中的任何线程启动时同步，所以很可能多个线程同时调用 <code>cudaDeviceSynchronize()</code> 将耗尽第一次调用中的所有工作，然后对后面的调用没有影响。</p>
<h4 id="D-3-1-5-Device-Management"><a href="#D-3-1-5-Device-Management" class="headerlink" title="D.3.1.5. Device Management"></a>D.3.1.5. Device Management</h4><p>只有运行内核的设备才能从该内核控制。 这意味着设备运行时不支持诸如<code>cudaSetDevice()</code> 之类的设备 API。 从 GPU 看到的活动设备（从 <code>cudaGetDevice()</code> 返回）将具有与从主机系统看到的相同的设备编号。 <code>cudaDeviceGetAttribute()</code> 调用可能会请求有关另一个设备的信息，因为此 API 允许将设备 ID 指定为调用的参数。 请注意，设备运行时不提供包罗万象的 <code>cudaGetDeviceProperties()</code> API - 必须单独查询属性。</p>
<h4 id="D-3-1-6-Memory-Declarations"><a href="#D-3-1-6-Memory-Declarations" class="headerlink" title="D.3.1.6. Memory Declarations"></a>D.3.1.6. Memory Declarations</h4><h5 id="D-3-1-6-1-Device-and-Constant-Memory"><a href="#D-3-1-6-1-Device-and-Constant-Memory" class="headerlink" title="D.3.1.6.1. Device and Constant Memory"></a>D.3.1.6.1. Device and Constant Memory</h5><p>使用 <code>__device__</code> 或 <code>__constant__</code> 内存空间说明符在文件范围内声明的内存在使用设备运行时行为相同。 所有内核都可以读取或写入设备变量，无论内核最初是由主机还是设备运行时启动的。 等效地，所有内核都将具有与在模块范围内声明的 <code>__constant__</code> 相同的视图。</p>
<h5 id="D-3-1-6-2-Textures-amp-Surfaces"><a href="#D-3-1-6-2-Textures-amp-Surfaces" class="headerlink" title="D.3.1.6.2. Textures &amp; Surfaces"></a>D.3.1.6.2. Textures &amp; Surfaces</h5><p>CUDA 支持动态创建的<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_14">纹理和表面对象</a>，其中纹理引用可以在主机上创建，传递给内核，由该内核使用，然后从主机销毁。 设备运行时不允许从设备代码中创建或销毁纹理或表面对象，但从主机创建的纹理和表面对象可以在设备上自由使用和传递。 不管它们是在哪里创建的，动态创建的纹理对象总是有效的，并且可以从父内核传递给子内核。</p>
<p>注意：设备运行时不支持从设备启动的内核中的遗留模块范围（即费米风格）纹理和表面。 模块范围（遗留）纹理可以从主机创建并在设备代码中用于任何内核，但只能由顶级内核（即从主机启动的内核）使用。</p>
<h5 id="D-3-1-6-3-Shared-Memory-Variable-Declarations"><a href="#D-3-1-6-3-Shared-Memory-Variable-Declarations" class="headerlink" title="D.3.1.6.3. Shared Memory Variable Declarations"></a>D.3.1.6.3. Shared Memory Variable Declarations</h5><p>在 CUDA C++ 中，共享内存可以声明为静态大小的文件范围或函数范围的变量，也可以声明为外部变量，其大小由内核调用者在运行时通过启动配置参数确定。 这两种类型的声明在设备运行时都有效。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">permute</span><span class="params">(<span class="type">int</span> n, <span class="type">int</span> *data)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">extern</span> __shared__ <span class="type">int</span> smem[];</span><br><span class="line">   <span class="keyword">if</span> (n &lt;= <span class="number">1</span>)</span><br><span class="line">       <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">   smem[threadIdx.x] = data[threadIdx.x];</span><br><span class="line">   __syncthreads();</span><br><span class="line"></span><br><span class="line">   <span class="built_in">permute_data</span>(smem, n);</span><br><span class="line">   __syncthreads();</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Write back to GMEM since we can&#x27;t pass SMEM to children.</span></span><br><span class="line">   data[threadIdx.x] = smem[threadIdx.x];</span><br><span class="line">   __syncthreads();</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>) &#123;</span><br><span class="line">       permute&lt;&lt;&lt; <span class="number">1</span>, <span class="number">256</span>, n/<span class="number">2</span>*<span class="built_in">sizeof</span>(<span class="type">int</span>) &gt;&gt;&gt;(n/<span class="number">2</span>, data);</span><br><span class="line">       permute&lt;&lt;&lt; <span class="number">1</span>, <span class="number">256</span>, n/<span class="number">2</span>*<span class="built_in">sizeof</span>(<span class="type">int</span>) &gt;&gt;&gt;(n/<span class="number">2</span>, data+n/<span class="number">2</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">host_launch</span><span class="params">(<span class="type">int</span> *data)</span> </span>&#123;</span><br><span class="line">    permute&lt;&lt;&lt; <span class="number">1</span>, <span class="number">256</span>, <span class="number">256</span>*<span class="built_in">sizeof</span>(<span class="type">int</span>) &gt;&gt;&gt;(<span class="number">256</span>, data);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="D-3-1-6-4-Symbol-Addresses"><a href="#D-3-1-6-4-Symbol-Addresses" class="headerlink" title="D.3.1.6.4. Symbol Addresses"></a>D.3.1.6.4. Symbol Addresses</h5><p>设备端符号（即标记为 <code>__device__</code> 的符号）可以简单地通过 <code>&amp;</code> 运算符从内核中引用，因为所有全局范围的设备变量都在内核的可见地址空间中。 这也适用于 <code>__constant__</code> 符号，尽管在这种情况下指针将引用只读数据。</p>
<p>鉴于可以直接引用设备端符号，那些引用符号的 CUDA 运行时 API（例如 <code>cudaMemcpyToSymbol()</code> 或 <code>cudaGetSymbolAddress()</code>）是多余的，因此设备运行时不支持。 请注意，这意味着常量数据不能在正在运行的内核中更改，即使在子内核启动之前也是如此，因为对 <code>__constant__</code> 空间的引用是只读的。</p>
<h4 id="D-3-1-7-API-Errors-and-Launch-Failures"><a href="#D-3-1-7-API-Errors-and-Launch-Failures" class="headerlink" title="D.3.1.7. API Errors and Launch Failures"></a>D.3.1.7. API Errors and Launch Failures</h4><p>与 CUDA 运行时一样，任何函数都可能返回错误代码。 最后返回的错误代码被记录下来，并且可以通过 <code>cudaGetLastError()</code> 调用来检索。 每个线程都会记录错误，以便每个线程都可以识别它最近生成的错误。 错误代码的类型为 <code>cudaError_t</code>。</p>
<p>与主机端启动类似，设备端启动可能由于多种原因（无效参数等）而失败。 用户必须调用 <code>cudaGetLastError()</code> 来确定启动是否产生错误，但是启动后没有错误并不意味着子内核成功完成。</p>
<p>对于设备端异常，例如，访问无效地址，子网格中的错误将返回给主机，而不是由父调用 <code>cudaDeviceSynchronize()</code> 返回。</p>
<h5 id="D-3-1-7-1-Launch-Setup-APIs"><a href="#D-3-1-7-1-Launch-Setup-APIs" class="headerlink" title="D.3.1.7.1. Launch Setup APIs"></a>D.3.1.7.1. Launch Setup APIs</h5><p>内核启动是通过设备运行时库公开的系统级机制，因此可通过底层 <code>cudaGetParameterBuffer()</code> 和 <code>cudaLaunchDevice()</code> API 直接从 PTX 获得。 允许 CUDA 应用程序自己调用这些 API，其要求与 PTX 相同。 在这两种情况下，用户都负责根据规范以正确的格式正确填充所有必要的数据结构。 这些数据结构保证了向后兼容性。</p>
<p>与主机端启动一样，设备端操作符 <code>&lt;&lt;&lt;&gt;&gt;&gt;</code> 映射到底层内核启动 API。 这样一来，以 PTX 为目标的用户将能够启动加载，并且编译器前端可以将 <code>&lt;&lt;&lt;&gt;&gt;&gt;</code> 转换为这些调用。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Runtime API Launch Functions</th>
<th>Description of Difference From Host Runtime Behaviour (behaviour is identical if no description)</th>
</tr>
</thead>
<tbody>
<tr>
<td>cudaGetParameterBuffer</td>
<td>Generated automatically from &lt;&lt;&lt;&gt;&gt;&gt;. Note different API to host equivalent.</td>
</tr>
<tr>
<td>cudaLaunchDevice</td>
<td>Generated automatically from &lt;&lt;&lt;&gt;&gt;&gt;. Note different API to host equivalent.</td>
</tr>
</tbody>
</table>
</div>
<p>这些启动函数的 API 与 CUDA Runtime API 不同，定义如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">extern</span>   device   cudaError_t <span class="title">cudaGetParameterBuffer</span><span class="params">(<span class="type">void</span> **params)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">extern</span> __device__ cudaError_t <span class="title">cudaLaunchDevice</span><span class="params">(<span class="type">void</span> *kernel,</span></span></span><br><span class="line"><span class="params"><span class="function">                                        <span class="type">void</span> *params, dim3 gridDim,</span></span></span><br><span class="line"><span class="params"><span class="function">                                        dim3 blockDim,</span></span></span><br><span class="line"><span class="params"><span class="function">                                        <span class="type">unsigned</span> <span class="type">int</span> sharedMemSize = <span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                                        cudaStream_t stream = <span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure>
<h4 id="D-3-1-8-API-Reference"><a href="#D-3-1-8-API-Reference" class="headerlink" title="D.3.1.8. API Reference"></a>D.3.1.8. API Reference</h4><p> 此处详细介绍了设备运行时支持的 CUDA 运行时 API 部分。 主机和设备运行时 API 具有相同的语法； 语义是相同的，除非另有说明。 下表提供了与主机可用版本相关的 API 概览。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Runtime API Functions</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td>cudaDeviceSynchronize</td>
<td>Synchronizes on work launched from thread’s own block only.</td>
</tr>
<tr>
<td>Warning: Note that calling this API from device code is deprecated in CUDA 11.6, and is slated for removal in a future CUDA release.</td>
<td></td>
</tr>
<tr>
<td>cudaDeviceGetCacheConfig</td>
<td></td>
</tr>
<tr>
<td>cudaDeviceGetLimit</td>
<td></td>
</tr>
<tr>
<td>cudaGetLastError</td>
<td>Last error is per-thread state, not per-block state</td>
</tr>
<tr>
<td>cudaPeekAtLastError</td>
<td></td>
</tr>
<tr>
<td>cudaGetErrorString</td>
<td></td>
</tr>
<tr>
<td>cudaGetDeviceCount</td>
<td></td>
</tr>
<tr>
<td>cudaDeviceGetAttribute</td>
<td>Will return attributes for any device</td>
</tr>
<tr>
<td>cudaGetDevice</td>
<td>Always returns current device ID as would be seen from host</td>
</tr>
<tr>
<td>cudaStreamCreateWithFlags</td>
<td>Must pass <code>cudaStreamNonBlocking</code> flag</td>
</tr>
<tr>
<td>cudaStreamDestroy</td>
<td></td>
</tr>
<tr>
<td>cudaStreamWaitEvent</td>
<td></td>
</tr>
<tr>
<td>cudaEventCreateWithFlags</td>
<td>Must pass <code>cudaEventDisableTiming</code> flag</td>
</tr>
<tr>
<td>cudaEventRecord</td>
<td></td>
</tr>
<tr>
<td>cudaEventDestroy</td>
<td></td>
</tr>
<tr>
<td>cudaFuncGetAttributes</td>
<td></td>
</tr>
<tr>
<td>udaMemsetAsync</td>
<td></td>
</tr>
<tr>
<td>cudaMemset2DAsync</td>
<td></td>
</tr>
<tr>
<td>cudaMemset3DAsync</td>
<td></td>
</tr>
<tr>
<td>cudaRuntimeGetVersion</td>
<td></td>
</tr>
<tr>
<td>cudaMalloc</td>
<td>May not call cudaFree on the device on a pointer created on the host, and vice-versa</td>
</tr>
<tr>
<td>cudaFree</td>
<td></td>
</tr>
<tr>
<td>cudaOccupancyMaxActiveBlocksPerMultiprocessor</td>
<td></td>
</tr>
<tr>
<td>cudaOccupancyMaxPotentialBlockSize</td>
<td></td>
</tr>
<tr>
<td>cudaOccupancyMaxPotentialBlockSizeVariableSMem</td>
<td></td>
</tr>
<tr>
<td>cudaMemcpyAsync</td>
<td>Notes about all memcpy/memset functions: 1.Only async memcpy/set functions are supported 2.Only device-to-device memcpy is permitted 3.May not pass in local or shared memory pointers</td>
</tr>
<tr>
<td>cudaMemcpy2DAsync</td>
<td>Notes about all memcpy/memset functions: 1.Only async memcpy/set functions are supported 2.Only device-to-device memcpy is permitted 3.May not pass in local or shared memory pointers</td>
</tr>
<tr>
<td>cudaMemcpy3DAsync</td>
<td>Notes about all memcpy/memset functions: 1.Only async memcpy/set functions are supported 2.Only device-to-device memcpy is permitted 3.May not pass in local or shared memory pointers</td>
</tr>
</tbody>
</table>
</div>
<h3 id="D-3-2-Device-side-Launch-from-PTX"><a href="#D-3-2-Device-side-Launch-from-PTX" class="headerlink" title="D.3.2. Device-side Launch from PTX"></a>D.3.2. Device-side Launch from PTX</h3><p>本部分适用于以并行线程执行 (PTX) 为目标并计划在其语言中支持动态并行的编程语言和编译器实现者。 它提供了与在 PTX 级别支持内核启动相关的底层详细信息。</p>
<h4 id="D-3-2-1-Kernel-Launch-APIs"><a href="#D-3-2-1-Kernel-Launch-APIs" class="headerlink" title="D.3.2.1. Kernel Launch APIs"></a>D.3.2.1. Kernel Launch APIs</h4><p>可以使用可从 PTX 访问的以下两个 API 来实现设备端内核启动：<code>cudaLaunchDevice()</code> 和 <code>cudaGetParameterBuffer()</code>。 <code>cudaLaunchDevice()</code> 使用通过调用 <code>cudaGetParameterBuffer()</code> 获得的参数缓冲区启动指定的内核，并将参数填充到启动的内核。 参数缓冲区可以为 <code>NULL</code>，即，如果启动的内核不带任何参数，则无需调用 <code>cudaGetParameterBuffer()</code>。</p>
<h5 id="D-3-2-1-1-cudaLaunchDevice"><a href="#D-3-2-1-1-cudaLaunchDevice" class="headerlink" title="D.3.2.1.1. cudaLaunchDevice"></a>D.3.2.1.1. cudaLaunchDevice</h5><p>在 PTX 级别，<code>cudaLaunchDevice()</code> 需要在使用前以如下所示的两种形式之一声明。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// PTX-level Declaration of cudaLaunchDevice() when .address_size is 64</span></span><br><span class="line">.<span class="keyword">extern</span> .<span class="built_in">func</span>(.param .b32 func_retval0) <span class="built_in">cudaLaunchDevice</span> </span><br><span class="line">( </span><br><span class="line">  .param .b64 func, </span><br><span class="line">  .param .b64 parameterBuffer, </span><br><span class="line">  .param .align <span class="number">4</span> .b8 gridDimension[<span class="number">12</span>], </span><br><span class="line">  .param .align <span class="number">4</span> .b8 blockDimension[<span class="number">12</span>], </span><br><span class="line">  .param .b32 sharedMemSize, </span><br><span class="line">  .param .b64 stream </span><br><span class="line">) </span><br><span class="line">;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// PTX-level Declaration of cudaLaunchDevice() when .address_size is 32</span></span><br><span class="line">.<span class="keyword">extern</span> .<span class="built_in">func</span>(.param .b32 func_retval0) <span class="built_in">cudaLaunchDevice</span></span><br><span class="line">(</span><br><span class="line">  .param .b32 func,</span><br><span class="line">  .param .b32 parameterBuffer,</span><br><span class="line">  .param .align <span class="number">4</span> .b8 gridDimension[<span class="number">12</span>],</span><br><span class="line">  .param .align <span class="number">4</span> .b8 blockDimension[<span class="number">12</span>],</span><br><span class="line">  .param .b32 sharedMemSize,</span><br><span class="line">  .param .b32 stream</span><br><span class="line">)</span><br><span class="line">;</span><br></pre></td></tr></table></figure>
<p>下面的 CUDA 级声明映射到上述 PTX 级声明之一，可在系统头文件 <code>cuda_device_runtime_api.h</code> 中找到。 该函数在 <code>cudadevrt</code> 系统库中定义，必须与程序链接才能使用设备端内核启动功能。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// CUDA-level declaration of cudaLaunchDevice()</span></span><br><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span> <span class="function">__device__ </span></span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaLaunchDevice</span><span class="params">(<span class="type">void</span> *func, <span class="type">void</span> *parameterBuffer, </span></span></span><br><span class="line"><span class="params"><span class="function">                             dim3 gridDimension, dim3 blockDimension, </span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">unsigned</span> <span class="type">int</span> sharedMemSize, </span></span></span><br><span class="line"><span class="params"><span class="function">                             cudaStream_t stream)</span></span>;</span><br></pre></td></tr></table></figure>
<p>第一个参数是指向要启动的内核的指针，第二个参数是保存已启动内核的实际参数的参数缓冲区。 参数缓冲区的布局在下面的<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#parameter-buffer-layout">参数缓冲区布局</a>中进行了说明。 其他参数指定启动配置，即网格维度、块维度、共享内存大小以及启动关联的流（启动配置的详细说明请参见<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration">执行配置</a>)。</p>
<h5 id="D-3-2-1-2-cudaGetParameterBuffer"><a href="#D-3-2-1-2-cudaGetParameterBuffer" class="headerlink" title="D.3.2.1.2. cudaGetParameterBuffer"></a>D.3.2.1.2. cudaGetParameterBuffer</h5><p><code>cudaGetParameterBuffer()</code> 需要在使用前在 PTX 级别声明。 PTX 级声明必须采用以下两种形式之一，具体取决于地址大小：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// PTX-level Declaration of cudaGetParameterBuffer() when .address_size is 64</span></span><br><span class="line"><span class="comment">// When .address_size is 64</span></span><br><span class="line">.<span class="keyword">extern</span> .<span class="built_in">func</span>(.param .b64 func_retval0) <span class="built_in">cudaGetParameterBuffer</span></span><br><span class="line">(</span><br><span class="line">  .param .b64 alignment,</span><br><span class="line">  .param .b64 size</span><br><span class="line">)</span><br><span class="line">;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// PTX-level Declaration of cudaGetParameterBuffer() when .address_size is 32</span></span><br><span class="line">.<span class="keyword">extern</span> .<span class="built_in">func</span>(.param .b32 func_retval0) <span class="built_in">cudaGetParameterBuffer</span></span><br><span class="line">(</span><br><span class="line">  .param .b32 alignment,</span><br><span class="line">  .param .b32 size</span><br><span class="line">)</span><br><span class="line">;</span><br></pre></td></tr></table></figure>
<p><code>cudaGetParameterBuffer()</code> 的以下 CUDA 级声明映射到上述 PTX 级声明：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// CUDA-level Declaration of cudaGetParameterBuffer()</span></span><br><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span> <span class="function">__device__</span></span><br><span class="line"><span class="function"><span class="type">void</span> *<span class="title">cudaGetParameterBuffer</span><span class="params">(<span class="type">size_t</span> alignment, <span class="type">size_t</span> size)</span></span>;</span><br></pre></td></tr></table></figure>
<p>第一个参数指定参数缓冲区的对齐要求，第二个参数以字节为单位的大小要求。 在当前实现中，<code>cudaGetParameterBuffer()</code> 返回的参数缓冲区始终保证为 64 字节对齐，忽略对齐要求参数。 但是，建议将正确的对齐要求值（即要放置在参数缓冲区中的任何参数的最大对齐）传递给 <code>cudaGetParameterBuffer()</code> 以确保将来的可移植性。</p>
<h4 id="D-3-2-2-Parameter-Buffer-Layout"><a href="#D-3-2-2-Parameter-Buffer-Layout" class="headerlink" title="D.3.2.2. Parameter Buffer Layout"></a>D.3.2.2. Parameter Buffer Layout</h4><p>禁止参数缓冲区中的参数重新排序，并且要求放置在参数缓冲区中的每个单独的参数对齐。 也就是说，每个参数必须放在参数缓冲区中的第 n 个字节，其中 n 是参数大小的最小倍数，它大于前一个参数占用的最后一个字节的偏移量。 参数缓冲区的最大大小为 4KB。</p>
<p>有关 CUDA 编译器生成的 PTX 代码的更详细说明，请参阅 PTX-3.5 规范。</p>
<h3 id="D-3-3-Toolkit-Support-for-Dynamic-Parallelism"><a href="#D-3-3-Toolkit-Support-for-Dynamic-Parallelism" class="headerlink" title="D.3.3. Toolkit Support for Dynamic Parallelism"></a>D.3.3. Toolkit Support for Dynamic Parallelism</h3><h4 id="D-3-3-1-Including-Device-Runtime-API-in-CUDA-Code"><a href="#D-3-3-1-Including-Device-Runtime-API-in-CUDA-Code" class="headerlink" title="D.3.3.1. Including Device Runtime API in CUDA Code"></a>D.3.3.1. Including Device Runtime API in CUDA Code</h4><p>与主机端运行时 API 类似，CUDA 设备运行时 API 的原型会在程序编译期间自动包含在内。 无需明确包含 <code>cuda_device_runtime_api.h</code>。</p>
<h4 id="D-3-3-2-Compiling-and-Linking"><a href="#D-3-3-2-Compiling-and-Linking" class="headerlink" title="D.3.3.2. Compiling and Linking"></a>D.3.3.2. Compiling and Linking</h4><p>当使用带有 nvcc 的动态并行编译和链接 CUDA 程序时，程序将自动链接到静态设备运行时库 <code>libcudadevrt</code>。</p>
<p>设备运行时作为静态库（Windows 上的 <code>cudadevrt.lib</code>，Linux 下的 <code>libcudadevrt.a</code>）提供，必须链接使用设备运行时的 GPU 应用程序。设备库的链接可以通过 <code>nvcc</code> 或 <code>nvlink</code> 完成。下面显示了两个简单的示例。</p>
<p>如果可以从命令行指定所有必需的源文件，则可以在一个步骤中编译和链接设备运行时程序：</p>
<p><code>$ nvcc -arch=sm_35 -rdc=true hello_world.cu -o hello -lcudadevrt</code></p>
<p>也可以先将 CUDA .cu 源文件编译为目标文件，然后在两个阶段的过程中将它们链接在一起：</p>
<p><code>$ nvcc -arch=sm_35 -dc hello_world.cu -o hello_world.o</code></p>
<p><code>$ nvcc -arch=sm_35 -rdc=true hello_world.o -o hello -lcudadevrt</code></p>
<p>有关详细信息，请参阅 <strong><em>The CUDA Driver Compiler NVCC</em></strong>的使用单独编译部分。</p>
<h2 id="D-4-Programming-Guidelines"><a href="#D-4-Programming-Guidelines" class="headerlink" title="D.4. Programming Guidelines"></a>D.4. Programming Guidelines</h2><h3 id="D-4-1-Basics"><a href="#D-4-1-Basics" class="headerlink" title="D.4.1. Basics"></a>D.4.1. Basics</h3><p>设备运行时是主机运行时的功能子集。 API 级别的设备管理、内核启动、设备 memcpy、流管理和事件管理从设备运行时公开。</p>
<p>已经有 CUDA 经验的人应该熟悉设备运行时的编程。 设备运行时语法和语义与主机 API 基本相同，但本文档前面详细介绍了任何例外情况。</p>
<p>警告：与父块的子内核显式同步（即在设备代码中使用 cudaDeviceSynchronize()）在 CUDA 11.6 中已弃用，并计划在未来的 CUDA 版本中删除。</p>
<p>以下示例显示了一个包含动态并行性的简单 Hello World 程序：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span> </span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">childKernel</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Hello &quot;</span>); </span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">parentKernel</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="comment">// launch child </span></span><br><span class="line">    childKernel&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(); </span><br><span class="line">    <span class="keyword">if</span> (cudaSuccess != <span class="built_in">cudaGetLastError</span>()) &#123; </span><br><span class="line">        <span class="keyword">return</span>; </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// wait for child to complete </span></span><br><span class="line">    <span class="keyword">if</span> (cudaSuccess != <span class="built_in">cudaDeviceSynchronize</span>()) &#123; </span><br><span class="line">        <span class="keyword">return</span>; </span><br><span class="line">    &#125; </span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;World!\n&quot;</span>); </span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="comment">// launch parent </span></span><br><span class="line">    parentKernel&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(); </span><br><span class="line">    <span class="keyword">if</span> (cudaSuccess != <span class="built_in">cudaGetLastError</span>()) &#123; </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>; </span><br><span class="line">    &#125; </span><br><span class="line"></span><br><span class="line">    <span class="comment">// wait for parent to complete </span></span><br><span class="line">    <span class="keyword">if</span> (cudaSuccess != <span class="built_in">cudaDeviceSynchronize</span>()) &#123; </span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span>; </span><br><span class="line">    &#125; </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该程序可以从命令行一步构建，如下所示：</p>
<p><code>$ nvcc -arch=sm_35 -rdc=true hello_world.cu -o hello -lcudadevrt</code></p>
<h3 id="D-4-2-Performance"><a href="#D-4-2-Performance" class="headerlink" title="D.4.2. Performance"></a>D.4.2. Performance</h3><h4 id="D-4-2-1-Synchronization"><a href="#D-4-2-1-Synchronization" class="headerlink" title="D.4.2.1. Synchronization"></a>D.4.2.1. Synchronization</h4><p>警告：与父块的子内核显式同步（即在设备代码中使用 cudaDeviceSynchronize()）在 CUDA 11.6 中已弃用，并计划在未来的 CUDA 版本中删除。</p>
<p>一个线程的同步可能会影响同一线程块中其他线程的性能，即使这些其他线程自己不调用 <code>cudaDeviceSynchronize()</code> 也是如此。 这种影响将取决于底层实现。 通常，与显式调用 <code>cudaDeviceSynchronize()</code> 相比，在线程块结束时完成子内核的隐式同步更有效。 因此，如果需要在线程块结束之前与子内核同步，建议仅调用 <code>cudaDeviceSynchronize()</code>。</p>
<h4 id="D-4-2-2-Dynamic-parallelism-enabled-Kernel-Overhead"><a href="#D-4-2-2-Dynamic-parallelism-enabled-Kernel-Overhead" class="headerlink" title="D.4.2.2. Dynamic-parallelism-enabled Kernel Overhead"></a>D.4.2.2. Dynamic-parallelism-enabled Kernel Overhead</h4><p>在控制动态启动时处于活动状态的系统软件可能会对当时正在运行的任何内核施加开销，无论它是否调用自己的内核启动。 这种开销来自设备运行时的执行跟踪和管理软件，并且可能导致性能下降，例如，与从主机端相比，从设备进行库调用时。 通常，链接到设备运行时库的应用程序会产生这种开销。</p>
<h3 id="D-4-3-Implementation-Restrictions-and-Limitations"><a href="#D-4-3-Implementation-Restrictions-and-Limitations" class="headerlink" title="D.4.3. Implementation Restrictions and Limitations"></a>D.4.3. Implementation Restrictions and Limitations</h3><p>动态并行保证本文档中描述的所有语义，但是，某些硬件和软件资源依赖于实现，并限制了使用设备运行时的程序的规模、性能和其他属性。</p>
<h4 id="D-4-3-1-Runtime"><a href="#D-4-3-1-Runtime" class="headerlink" title="D.4.3.1. Runtime"></a>D.4.3.1. Runtime</h4><h5 id="D-4-3-1-1-Memory-Footprint"><a href="#D-4-3-1-1-Memory-Footprint" class="headerlink" title="D.4.3.1.1. Memory Footprint"></a>D.4.3.1.1. Memory Footprint</h5><p>设备运行时系统软件为各种管理目的预留内存，特别是用于在同步期间保存父网格状态的一个预留，以及用于跟踪未决网格启动的第二个预留。 配置控制可用于减少这些预留的大小，以换取某些启动限制。 有关详细信息，请参阅下面的<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#configuration-options">配置选项</a>。</p>
<p>大多数保留内存被分配为父内核状态的后备存储，用于在子启动时进行同步。 保守地说，该内存必须支持为设备上可能的最大活动线程数存储状态。 这意味着可调用 <code>cudaDeviceSynchronize()</code> 的每个父代可能需要多达 <code>860MB</code> 的设备内存，具体取决于设备配置，即使它没有全部消耗，也将无法供程序使用。</p>
<h5 id="D-4-3-1-2-Nesting-and-Synchronization-Depth"><a href="#D-4-3-1-2-Nesting-and-Synchronization-Depth" class="headerlink" title="D.4.3.1.2. Nesting and Synchronization Depth"></a>D.4.3.1.2. Nesting and Synchronization Depth</h5><p>使用设备运行时，一个内核可能会启动另一个内核，而该内核可能会启动另一个内核，以此类推。每个从属启动都被认为是一个新的嵌套层级，层级总数就是程序的嵌套深度。同步深度定义为程序在子启动时显式同步的最深级别。通常这比程序的嵌套深度小一，但如果程序不需要在所有级别调用 <code>cudaDeviceSynchronize()</code> ，则同步深度可能与嵌套深度有很大不同。</p>
<p>警告：与父块的子内核显式同步（即在设备代码中使用 cudaDeviceSynchronize()）在 CUDA 11.6 中已弃用，并计划在未来的 CUDA 版本中删除。</p>
<p>总体最大嵌套深度限制为 24，但实际上，真正的限制将是系统为每个新级别所需的内存量（请参阅上面的<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-footprint">内存占用量</a>）。任何会导致内核处于比最大值更深的级别的启动都将失败。请注意，这也可能适用于 <code>cudaMemcpyAsync()</code>，它本身可能会生成内核启动。有关详细信息，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#configuration-options">配置选项</a>。</p>
<p>默认情况下，为两级同步保留足够的存储空间。这个最大同步深度（以及因此保留的存储）可以通过调用 <code>cudaDeviceSetLimit()</code> 并指定 <code>cudaLimitDevRuntimeSyncDepth</code> 来控制。必须在主机启动顶层内核之前配置要支持的层数，以保证嵌套程序的成功执行。在大于指定最大同步深度的深度调用 <code>cudaDeviceSynchronize()</code> 将返回错误。</p>
<p>在父内核从不调用 <code>cudaDeviceSynchronize()</code> 的情况下，如果系统检测到不需要为父状态保留空间，则允许进行优化。在这种情况下，由于永远不会发生显式父/子同步，因此程序所需的内存占用量将远小于保守的最大值。这样的程序可以指定较浅的最大同步深度，以避免过度分配后备存储。</p>
<h5 id="D-4-3-1-3-Pending-Kernel-Launches"><a href="#D-4-3-1-3-Pending-Kernel-Launches" class="headerlink" title="D.4.3.1.3. Pending Kernel Launches"></a>D.4.3.1.3. Pending Kernel Launches</h5><p>启动内核时，会跟踪所有关联的配置和参数数据，直到内核完成。 此数据存储在系统管理的启动池中。</p>
<p>启动池分为固定大小的池和性能较低的虚拟化池。 设备运行时系统软件将首先尝试跟踪固定大小池中的启动数据。 当固定大小的池已满时，虚拟化池将用于跟踪新的启动。</p>
<p>固定大小启动池的大小可通过从主机调用 <code>cudaDeviceSetLimit()</code>并指定 <code>cudaLimitDevRuntimePendingLaunchCount</code> 来配置。</p>
<h5 id="D-4-3-1-4-Configuration-Options"><a href="#D-4-3-1-4-Configuration-Options" class="headerlink" title="D.4.3.1.4. Configuration Options"></a>D.4.3.1.4. Configuration Options</h5><p>设备运行时系统软件的资源分配通过主机程序的 <code>cudaDeviceSetLimit()</code> API 进行控制。 限制必须在任何内核启动之前设置，并且在 GPU 正在运行程序时不得更改。</p>
<p>警告：与父块的子内核显式同步（即在设备代码中使用 cudaDeviceSynchronize()）在 CUDA 11.6 中已弃用，并计划在未来的 CUDA 版本中删除。</p>
<p>可以设置以下命名限制：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Limit</th>
<th>Behavior</th>
</tr>
</thead>
<tbody>
<tr>
<td>cudaLimitDevRuntimeSyncDepth</td>
<td>Sets the maximum depth at which cudaDeviceSynchronize() may be called. Launches may be performed deeper than this, but explicit synchronization deeper than this limit will return the cudaErrorLaunchMaxDepthExceeded. The default maximum sync depth is 2.</td>
</tr>
<tr>
<td>cudaLimitDevRuntimePendingLaunchCount</td>
<td>Controls the amount of memory set aside for buffering kernel launches which have not yet begun to execute, due either to unresolved dependencies or lack of execution resources. When the buffer is full, the device runtime system software will attempt to track new pending launches in a lower performance virtualized buffer. If the virtualized buffer is also full, i.e. when all available heap space is consumed, launches will not occur, and the thread’s last error will be set to cudaErrorLaunchPendingCountExceeded. The default pending launch count is 2048 launches.</td>
</tr>
<tr>
<td>cudaLimitStackSize</td>
<td>Controls the stack size in bytes of each GPU thread. The CUDA driver automatically increases the per-thread stack size for each kernel launch as needed. This size isn’t reset back to the original value after each launch. To set the per-thread stack size to a different value, cudaDeviceSetLimit() can be called to set this limit. The stack will be immediately resized, and if necessary, the device will block until all preceding requested tasks are complete. cudaDeviceGetLimit() can be called to get the current per-thread stack size.</td>
</tr>
</tbody>
</table>
</div>
<h5 id="D-4-3-1-5-Memory-Allocation-and-Lifetime"><a href="#D-4-3-1-5-Memory-Allocation-and-Lifetime" class="headerlink" title="D.4.3.1.5. Memory Allocation and Lifetime"></a>D.4.3.1.5. Memory Allocation and Lifetime</h5><p><code>cudaMalloc()</code> 和 <code>cudaFree()</code> 在主机和设备环境之间具有不同的语义。 当从主机调用时，<code>cudaMalloc()</code> 从未使用的设备内存中分配一个新区域。 当从设备运行时调用时，这些函数映射到设备端的 <code>malloc()</code> 和 <code>free()</code>。 这意味着在设备环境中，总可分配内存限制为设备 <code>malloc()</code> 堆大小，它可能小于可用的未使用设备内存。 此外，在设备上由 <code>cudaMalloc()</code> 分配的指针上从主机程序调用 <code>cudaFree()</code> 是错误的，反之亦然。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>cudaMalloc() on Host</th>
<th>cudaMalloc() on Device</th>
</tr>
</thead>
<tbody>
<tr>
<td>cudaFree() on Host</td>
<td>Supported</td>
<td>Not Supported</td>
</tr>
<tr>
<td>cudaFree() on Device</td>
<td>Not Supported</td>
<td>Supported</td>
</tr>
<tr>
<td>Allocation limit</td>
<td>Free device memory</td>
<td>cudaLimitMallocHeapSize</td>
</tr>
</tbody>
</table>
</div>
<h5 id="D-4-3-1-6-SM-Id-and-Warp-Id"><a href="#D-4-3-1-6-SM-Id-and-Warp-Id" class="headerlink" title="D.4.3.1.6. SM Id and Warp Id"></a>D.4.3.1.6. SM Id and Warp Id</h5><p>请注意，在 PTX 中，<code>%smid</code> 和 <code>%warpid</code> 被定义为 <code>volatile</code> 值。 设备运行时可以将线程块重新调度到不同的 SM 上，以便更有效地管理资源。 因此，依赖 <code>%smid</code> 或 <code>%warpid</code> 在线程或线程块的生命周期内保持不变是不安全的。</p>
<h5 id="D-4-3-1-7-ECC-Errors"><a href="#D-4-3-1-7-ECC-Errors" class="headerlink" title="D.4.3.1.7. ECC Errors"></a>D.4.3.1.7. ECC Errors</h5><p>CUDA 内核中的代码没有可用的 ECC 错误通知。 整个启动树完成后，主机端会报告 ECC 错误。 在嵌套程序执行期间出现的任何 ECC 错误都将生成异常或继续执行（取决于错误和配置）。</p>
<h1 id="附录E虚拟内存管理"><a href="#附录E虚拟内存管理" class="headerlink" title="附录E虚拟内存管理"></a>附录E虚拟内存管理</h1><h2 id="E-1-Introduction"><a href="#E-1-Introduction" class="headerlink" title="E.1. Introduction"></a>E.1. Introduction</h2><p><a href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__VA.html">虚拟内存管理 API</a> 为应用程序提供了一种直接管理统一虚拟地址空间的方法，该空间由 CUDA 提供，用于将物理内存映射到 GPU 可访问的虚拟地址。在 CUDA 10.2 中引入的这些 API 还提供了一种与其他进程和图形 API（如 OpenGL 和 Vulkan）进行互操作的新方法，并提供了用户可以调整以适应其应用程序的更新内存属性。</p>
<p>从历史上看，CUDA 编程模型中的内存分配调用（例如 <code>cudaMalloc</code>）返回了一个指向 GPU 内存的内存地址。这样获得的地址可以与任何 CUDA API 一起使用，也可以在设备内核中使用。但是，分配的内存无法根据用户的内存需求调整大小。为了增加分配的大小，用户必须显式分配更大的缓冲区，从初始分配中复制数据，释放它，然后继续跟踪新分配的地址。这通常会导致应用程序的性能降低和峰值内存利用率更高。本质上，用户有一个类似 <code>malloc</code> 的接口来分配 GPU 内存，但没有相应的 <code>realloc</code> 来补充它。虚拟内存管理 API 将地址和内存的概念解耦，并允许应用程序分别处理它们。 API 允许应用程序在他们认为合适的时候从虚拟地址范围映射和取消映射内存。</p>
<p>在通过 <code>cudaEnablePeerAccess</code> 启用对等设备访问内存分配的情况下，所有过去和未来的用户分配都映射到目标对等设备。这导致用户无意中支付了将所有 <code>cudaMalloc</code> 分配映射到对等设备的运行时成本。然而，在大多数情况下，应用程序通过仅与另一个设备共享少量分配进行通信，并且并非所有分配都需要映射到所有设备。使用虚拟内存管理，应用程序可以专门选择某些分配可从目标设备访问。</p>
<p>CUDA 虚拟内存管理 API 向用户提供细粒度控制，以管理应用程序中的 GPU 内存。它提供的 API 允许用户：</p>
<ul>
<li>将分配在不同设备上的内存放入一个连续的 VA 范围内。</li>
<li>使用平台特定机制执行内存共享的进程间通信。</li>
<li>在支持它们的设备上选择更新的内存类型。</li>
</ul>
<p>为了分配内存，虚拟内存管理编程模型公开了以下功能：</p>
<ul>
<li>分配物理内存。</li>
<li>保留 VA 范围。</li>
<li>将分配的内存映射到 VA 范围。</li>
<li>控制映射范围的访问权限。</li>
</ul>
<p>请注意，本节中描述的 API 套件需要支持 UVA 的系统。</p>
<h2 id="E-2-Query-for-support"><a href="#E-2-Query-for-support" class="headerlink" title="E.2. Query for support"></a>E.2. Query for support</h2><p>在尝试使用虚拟内存管理 API 之前，应用程序必须确保他们希望使用的设备支持 CUDA 虚拟内存管理。 以下代码示例显示了查询虚拟内存管理支持：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> deviceSupportsVmm;</span><br><span class="line">CUresult result = <span class="built_in">cuDeviceGetAttribute</span>(&amp;deviceSupportsVmm, CU_DEVICE_ATTRIBUTE_VIRTUAL_MEMORY_MANAGEMENT_SUPPORTED, device);</span><br><span class="line"><span class="keyword">if</span> (deviceSupportsVmm != <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// `device` supports Virtual Memory Management </span></span><br><span class="line">&#125;</span><br><span class="line">   </span><br></pre></td></tr></table></figure>
<h2 id="E-3-Allocating-Physical-Memory"><a href="#E-3-Allocating-Physical-Memory" class="headerlink" title="E.3. Allocating Physical Memory"></a>E.3. Allocating Physical Memory</h2><p>通过虚拟内存管理 API 进行内存分配的第一步是创建一个物理内存块，为分配提供支持。 为了分配物理内存，应用程序必须使用 <code>cuMemCreate</code> API。 此函数创建的分配没有任何设备或主机映射。 函数参数 <code>CUmemGenericAllocationHandle</code> 描述了要分配的内存的属性，例如分配的位置、分配是否要共享给另一个进程（或其他图形 API），或者要分配的内存的物理属性。 用户必须确保请求分配的大小必须与适当的粒度对齐。 可以使用 <code>cuMemGetAllocationGranularity</code> 查询有关分配粒度要求的信息。 以下代码片段显示了使用 <code>cuMemCreate</code> 分配物理内存：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">CUmemGenericAllocationHandle <span class="title">allocatePhysicalMemory</span><span class="params">(<span class="type">int</span> device, <span class="type">size_t</span> size)</span> </span>&#123;</span><br><span class="line">    CUmemAllocationProp prop = &#123;&#125;;</span><br><span class="line">    prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;</span><br><span class="line">    prop.location.type = CU_MEM_LOCATION_TYPE_DEVICE;</span><br><span class="line">    prop.location.id = device;</span><br><span class="line">    <span class="built_in">cuMemGetAllocationGranularity</span>(&amp;granularity, &amp;prop, CU_MEM_ALLOC_GRANULARITY_MINIMUM);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Ensure size matches granularity requirements for the allocation</span></span><br><span class="line">    <span class="type">size_t</span> padded_size = <span class="built_in">ROUND_UP</span>(size, granularity);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate physical memory</span></span><br><span class="line">    CUmemGenericAllocationHandle allocHandle;</span><br><span class="line">    <span class="built_in">cuMemCreate</span>(&amp;allocHandle, padded_size, &amp;prop, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> allocHandle;</span><br><span class="line">&#125;</span><br><span class="line">   </span><br></pre></td></tr></table></figure>
<p>由 <code>cuMemCreate</code> 分配的内存由它返回的 <code>CUmemGenericAllocationHandle</code> 引用。 这与 <code>cudaMalloc</code>风格的分配不同，后者返回一个指向 GPU 内存的指针，该指针可由在设备上执行的 CUDA 内核直接访问。 除了使用 <code>cuMemGetAllocationPropertiesFromHandle</code> 查询属性之外，分配的内存不能用于任何操作。 为了使此内存可访问，应用程序必须将此内存映射到由 <code>cuMemAddressReserve</code> 保留的 VA 范围，并为其提供适当的访问权限。 应用程序必须使用 <code>cuMemRelease</code> API 释放分配的内存。</p>
<h3 id="E-3-1-Shareable-Memory-Allocations"><a href="#E-3-1-Shareable-Memory-Allocations" class="headerlink" title="E.3.1. Shareable Memory Allocations"></a>E.3.1. Shareable Memory Allocations</h3><p>使用 <code>cuMemCreate</code> 用户现在可以在分配时向 CUDA 指示他们已指定特定分配用于进程间通信或图形互操作目的。应用程序可以通过将 <code>CUmemAllocationProp::requestedHandleTypes</code> 设置为平台特定字段来完成此操作。在 Windows 上，当 <code>CUmemAllocationProp::requestedHandleTypes</code> 设置为 <code>CU_MEM_HANDLE_TYPE_WIN32</code> 时，应用程序还必须在 <code>CUmemAllocationProp::win32HandleMetaData</code> 中指定 <code>LPSECURITYATTRIBUTES</code> 属性。该安全属性定义了可以将导出的分配转移到其他进程的范围。</p>
<p>CUDA 虚拟内存管理 API 函数不支持传统的进程间通信函数及其内存。相反，它们公开了一种利用操作系统特定句柄的进程间通信的新机制。应用程序可以使用 <code>cuMemExportToShareableHandle</code> 获取与分配相对应的这些操作系统特定句柄。这样获得的句柄可以通过使用通常的 OS 本地机制进行传输，以进行进程间通信。接收进程应使用 <code>cuMemImportFromShareableHandle</code> 导入分配。</p>
<p>用户必须确保在尝试导出使用 <code>cuMemCreate</code> 分配的内存之前查询是否支持请求的句柄类型。以下代码片段说明了以特定平台方式查询句柄类型支持。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> deviceSupportsIpcHandle;</span><br><span class="line"><span class="meta">#<span class="keyword">if</span> defined(__linux__)</span></span><br><span class="line">    <span class="built_in">cuDeviceGetAttribute</span>(&amp;deviceSupportsIpcHandle, CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED, device));</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    <span class="built_in">cuDeviceGetAttribute</span>(&amp;deviceSupportsIpcHandle, CU_DEVICE_ATTRIBUTE_HANDLE_TYPE_WIN32_HANDLE_SUPPORTED, device));</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br></pre></td></tr></table></figure>
<p>用户应适当设置 <code>CUmemAllocationProp::requestedHandleTypes</code>，如下所示：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">if</span> defined(__linux__)</span></span><br><span class="line">    prop.requestedHandleTypes = CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR;</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    prop.requestedHandleTypes = CU_MEM_HANDLE_TYPE_WIN32;</span><br><span class="line">    prop.win32HandleMetaData = <span class="comment">// Windows specific LPSECURITYATTRIBUTES attribute.</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/memMapIPCDrv">memMapIpcDrv</a> 示例可用作将 IPC 与虚拟内存管理分配一起使用的示例。</p>
<h3 id="E-3-2-Memory-Type"><a href="#E-3-2-Memory-Type" class="headerlink" title="E.3.2. Memory Type"></a>E.3.2. Memory Type</h3><p>在 CUDA 10.2 之前，应用程序没有用户控制的方式来分配某些设备可能支持的任何特殊类型的内存。 使用 <code>cuMemCreate</code> 应用程序还可以使用 <code>CUmemAllocationProp::allocFlags</code> 指定内存类型要求，以选择任何特定的内存功能。 应用程序还必须确保分配设备支持请求的内存类型。</p>
<h4 id="E-3-2-1-Compressible-Memory"><a href="#E-3-2-1-Compressible-Memory" class="headerlink" title="E.3.2.1. Compressible Memory"></a>E.3.2.1. Compressible Memory</h4><p>可压缩内存可用于加速对具有非结构化稀疏性和其他可压缩数据模式的数据的访问。 压缩可以节省 <code>DRAM</code> 带宽、L2 读取带宽和 L2 容量，具体取决于正在操作的数据。 想要在支持计算数据压缩的设备上分配可压缩内存的应用程序可以通过将 <code>CUmemAllocationProp::allocFlags::compressionType</code> 设置为 <code>CU_MEM_ALLOCATION_COMP_GENERIC</code> 来实现。 用户必须通过 <code>CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED</code> 查询设备是否支持计算数据压缩。 以下代码片段说明了查询可压缩内存支持 <code>cuDeviceGetAttribute</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> compressionSupported = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">cuDeviceGetAttribute</span>(&amp;compressionSupported, CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED, device);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在支持计算数据压缩的设备上，用户需要在分配时选择加入，如下所示：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">prop.allocFlags.compressionType = CU_MEM_ALLOCATION_COMP_GENERIC;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>由于硬件资源有限等各种原因，分配的内存可能没有压缩属性，用户需要使用<code>cuMemGetAllocationPropertiesFromHandle</code>查询回分配内存的属性并检查压缩属性。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CUmemAllocationPropPrivate allocationProp = &#123;&#125;;</span><br><span class="line"><span class="built_in">cuMemGetAllocationPropertiesFromHandle</span>(&amp;allocationProp, allocationHandle);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (allocationProp.allocFlags.compressionType == CU_MEM_ALLOCATION_COMP_GENERIC)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Obtained compressible memory allocation</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="E-4-Reserving-a-Virtual-Address-Range"><a href="#E-4-Reserving-a-Virtual-Address-Range" class="headerlink" title="E.4. Reserving a Virtual Address Range"></a>E.4. Reserving a Virtual Address Range</h2><p>由于使用虚拟内存管理，地址和内存的概念是不同的，因此应用程序必须划出一个地址范围，以容纳由 <code>cuMemCreate</code> 进行的内存分配。保留的地址范围必须至少与用户计划放入其中的所有物理内存分配大小的总和一样大。</p>
<p>应用程序可以通过将适当的参数传递给 <code>cuMemAddressReserve</code> 来保留虚拟地址范围。获得的地址范围不会有任何与之关联的设备或主机物理内存。保留的虚拟地址范围可以映射到属于系统中任何设备的内存块，从而为应用程序提供由属于不同设备的内存支持和映射的连续 VA 范围。应用程序应使用 <code>cuMemAddressFree</code> 将虚拟地址范围返回给 CUDA。用户必须确保在调用 <code>cuMemAddressFree</code> 之前未映射整个 VA 范围。这些函数在概念上类似于 <code>mmap/munmap</code>（在 Linux 上）或 <code>VirtualAlloc/VirtualFree</code>（在 Windows 上）函数。以下代码片段说明了该函数的用法：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CUdeviceptr ptr;</span><br><span class="line"><span class="comment">// `ptr` holds the returned start of virtual address range reserved.</span></span><br><span class="line">CUresult result = <span class="built_in">cuMemAddressReserve</span>(&amp;ptr, size, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>); <span class="comment">// alignment = 0 for default alignment</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="E-5-Virtual-Aliasing-Support"><a href="#E-5-Virtual-Aliasing-Support" class="headerlink" title="E.5. Virtual Aliasing Support"></a>E.5. Virtual Aliasing Support</h2><p>虚拟内存管理 API 提供了一种创建多个虚拟内存映射或“代理”到相同分配的方法，该方法使用对具有不同虚拟地址的 <code>cuMemMap</code> 的多次调用，即所谓的虚拟别名。 除非在 <code>PTX ISA</code> 中另有说明，否则写入分配的一个代理被认为与同一内存的任何其他代理不一致和不连贯，直到写入设备操作（网格启动、memcpy、memset 等）完成。 在写入设备操作之前出现在 GPU 上但在写入设备操作完成后读取的网格也被认为具有不一致和不连贯的代理。</p>
<p>例如，下面的代码片段被认为是未定义的，假设设备指针 A 和 B 是相同内存分配的虚拟别名：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">char</span> *A, <span class="type">char</span> *B)</span> </span>&#123;</span><br><span class="line">  *A = <span class="number">0x1</span>;</span><br><span class="line">  <span class="built_in">printf</span>(“%d\n”, *B);    <span class="comment">// Undefined behavior!  *B can take on either</span></span><br><span class="line"><span class="comment">// the previous value or some value in-between.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以下是定义的行为，假设这两个内核是单调排序的（通过流或事件）。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">foo1</span><span class="params">(<span class="type">char</span> *A)</span> </span>&#123;</span><br><span class="line">  *A = <span class="number">0x1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">foo2</span><span class="params">(<span class="type">char</span> *B)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(“%d\n”, *B);    <span class="comment">// *B == *A == 0x1 assuming foo2 waits for foo1</span></span><br><span class="line"><span class="comment">// to complete before launching</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaMemcpyAsync</span>(B, input, size, stream1);    <span class="comment">// Aliases are allowed at</span></span><br><span class="line"><span class="comment">// operation boundaries</span></span><br><span class="line">foo1&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,stream1&gt;&gt;&gt;(A);                  <span class="comment">// allowing foo1 to access A.</span></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(event, stream1);</span><br><span class="line"><span class="built_in">cudaStreamWaitEvent</span>(stream2, event);</span><br><span class="line">foo2&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,stream2&gt;&gt;&gt;(B);</span><br><span class="line"><span class="built_in">cudaStreamWaitEvent</span>(stream3, event);</span><br><span class="line"><span class="built_in">cudaMemcpyAsync</span>(output, B, size, stream3);  <span class="comment">// Both launches of foo2 and</span></span><br><span class="line">                                            <span class="comment">// cudaMemcpy (which both</span></span><br><span class="line">                                            <span class="comment">// read) wait for foo1 (which writes)</span></span><br><span class="line">                                            <span class="comment">// to complete before proceeding</span></span><br></pre></td></tr></table></figure>
<h2 id="E-6-Mapping-Memory"><a href="#E-6-Mapping-Memory" class="headerlink" title="E.6. Mapping Memory"></a>E.6. Mapping Memory</h2><p>前两节分配的物理内存和挖出的虚拟地址空间代表了虚拟内存管理 API 引入的内存和地址区别。为了使分配的内存可用，用户必须首先将内存放在地址空间中。从 <code>cuMemAddressReserve</code> 获取的地址范围和从 <code>cuMemCreate</code> 或 <code>cuMemImportFromShareableHandle</code> 获取的物理分配必须通过 <code>cuMemMap</code> 相互关联。</p>
<p>用户可以关联来自多个设备的分配以驻留在连续的虚拟地址范围内，只要他们已经划分出足够的地址空间。为了解耦物理分配和地址范围，用户必须通过  <code>cuMemUnmap</code> 取消映射的地址。用户可以根据需要多次将内存映射和取消映射到同一地址范围，只要他们确保不会尝试在已映射的 VA 范围保留上创建映射。以下代码片段说明了该函数的用法：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CUdeviceptr ptr;</span><br><span class="line"><span class="comment">// `ptr`: address in the address range previously reserved by cuMemAddressReserve.</span></span><br><span class="line"><span class="comment">// `allocHandle`: CUmemGenericAllocationHandle obtained by a previous call to cuMemCreate. </span></span><br><span class="line">CUresult result = <span class="built_in">cuMemMap</span>(ptr, size, <span class="number">0</span>, allocHandle, <span class="number">0</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="E-7-Control-Access-Rights"><a href="#E-7-Control-Access-Rights" class="headerlink" title="E.7. Control Access Rights"></a>E.7. Control Access Rights</h2><p>虚拟内存管理 API 使应用程序能够通过访问控制机制显式保护其 VA 范围。 使用 <code>cuMemMap</code> 将分配映射到地址范围的区域不会使地址可访问，并且如果被 CUDA 内核访问会导致程序崩溃。 用户必须使用 <code>cuMemSetAccess</code> 函数专门选择访问控制，该函数允许或限制特定设备对映射地址范围的访问。 以下代码片段说明了该函数的用法：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">setAccessOnDevice</span><span class="params">(<span class="type">int</span> device, CUdeviceptr ptr, <span class="type">size_t</span> size)</span> </span>&#123;</span><br><span class="line">    CUmemAccessDesc accessDesc = &#123;&#125;;</span><br><span class="line">    accessDesc.location.type = CU_MEM_LOCATION_TYPE_DEVICE;</span><br><span class="line">    accessDesc.location.id = device;</span><br><span class="line">    accessDesc.flags = CU_MEM_ACCESS_FLAGS_PROT_READWRITE;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Make the address accessible</span></span><br><span class="line">    <span class="built_in">cuMemSetAccess</span>(ptr, size, &amp;accessDesc, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>使用虚拟内存管理公开的访问控制机制允许用户明确他们希望与系统上的其他对等设备共享哪些分配。 如前所述，<code>cudaEnablePeerAccess</code> 强制将所有先前和将来的 <code>cudaMalloc</code> 分配映射到目标对等设备。 这在许多情况下很方便，因为用户不必担心跟踪每个分配到系统中每个设备的映射状态。 但是对于关心其应用程序性能的用户来说，这种方法具有性能影响。 通过分配粒度的访问控制，虚拟内存管理公开了一种机制，可以以最小的开销进行对等映射。</p>
<p><a href="https://github.com/NVIDIA/cuda-samples/tree/master/Samples/vectorAddMMAP">vectorAddMMAP 示例</a>可用作使用虚拟内存管理 API 的示例。</p>
<h1 id="附录F-流序内存分配"><a href="#附录F-流序内存分配" class="headerlink" title="附录F 流序内存分配"></a>附录F 流序内存分配</h1><h2 id="F-1-Introduction"><a href="#F-1-Introduction" class="headerlink" title="F.1. Introduction"></a>F.1. Introduction</h2><p>使用 <code>cudaMalloc</code> 和 <code>cudaFree</code> 管理内存分配会导致 GPU 在所有正在执行的 CUDA 流之间进行同步。 Stream Order Memory Allocator 使应用程序能够通过启动到 CUDA 流中的其他工作（例如内核启动和异步拷贝）来对内存分配和释放进行排序。这通过利用流排序语义来重用内存分配来改进应用程序内存使用。分配器还允许应用程序控制分配器的内存缓存行为。当设置了适当的释放阈值时，缓存行为允许分配器在应用程序表明它愿意接受更大的内存占用时避免对操作系统进行昂贵的调用。分配器还支持在进程之间轻松安全地共享分配。</p>
<p>对于许多应用程序，Stream Ordered Memory Allocator 减少了对自定义内存管理抽象的需求，并使为需要它的应用程序创建高性能自定义内存管理变得更加容易。对于已经具有自定义内存分配器的应用程序和库，采用 Stream Ordered Memory Allocator 可以使多个库共享由驱动程序管理的公共内存池，从而减少过多的内存消耗。此外，驱动程序可以根据其对分配器和其他流管理 API 的感知执行优化。最后，Nsight Compute 和 Next-Gen CUDA 调试器知道分配器是其 CUDA 11.3 工具包支持的一部分。</p>
<h2 id="F-2-Query-for-Support"><a href="#F-2-Query-for-Support" class="headerlink" title="F.2. Query for Support"></a>F.2. Query for Support</h2><p>用户可以通过使用设备属性 <code>cudaDevAttrMemoryPoolsSupported</code> 调用 <code>cudaDeviceGetAttribute()</code> 来确定设备是否支持流序内存分配器。</p>
<p>从 CUDA 11.3 开始，可以使用 <code>cudaDevAttrMemoryPoolSupportedHandleTypes</code> 设备属性查询 IPC 内存池支持。 以前的驱动程序将返回 <code>cudaErrorInvalidValue</code>，因为这些驱动程序不知道属性枚举。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> driverVersion = <span class="number">0</span>;</span><br><span class="line"><span class="type">int</span> deviceSupportsMemoryPools = <span class="number">0</span>;</span><br><span class="line"><span class="type">int</span> poolSupportedHandleTypes = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">cudaDriverGetVersion</span>(&amp;driverVersion);</span><br><span class="line"><span class="keyword">if</span> (driverVersion &gt;= <span class="number">11020</span>) &#123;</span><br><span class="line">    <span class="built_in">cudaDeviceGetAttribute</span>(&amp;deviceSupportsMemoryPools,</span><br><span class="line">                           cudaDevAttrMemoryPoolsSupported, device);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (deviceSupportsMemoryPools != <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// `device` supports the Stream Ordered Memory Allocator</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (driverVersion &gt;= <span class="number">11030</span>) &#123;</span><br><span class="line">    <span class="built_in">cudaDeviceGetAttribute</span>(&amp;poolSupportedHandleTypes,</span><br><span class="line">              cudaDevAttrMemoryPoolSupportedHandleTypes, device);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (poolSupportedHandleTypes &amp; cudaMemHandleTypePosixFileDescriptor) &#123;</span><br><span class="line">   <span class="comment">// Pools on the specified device can be created with posix file descriptor-based IPC</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在查询之前执行驱动程序版本检查可避免在尚未定义属性的驱动程序上遇到 <code>cudaErrorInvalidValue</code> 错误。 可以使用 <code>cudaGetLastError</code> 来清除错误而不是避免它。</p>
<h2 id="F-3-API-Fundamentals-cudaMallocAsync-and-cudaFreeAsync"><a href="#F-3-API-Fundamentals-cudaMallocAsync-and-cudaFreeAsync" class="headerlink" title="F.3. API Fundamentals (cudaMallocAsync and cudaFreeAsync)"></a>F.3. API Fundamentals (cudaMallocAsync and cudaFreeAsync)</h2><p>API <code>cudaMallocAsync</code> 和 <code>cudaFreeAsync</code> 构成了分配器的核心。 <code>cudaMallocAsync</code> <code>返回分配，cudaFreeAsync</code> 释放分配。 两个 API 都接受流参数来定义分配何时变为可用和停止可用。 <code>cudaMallocAsync</code> 返回的指针值是同步确定的，可用于构建未来的工作。 重要的是要注意 <code>cudaMallocAsync</code> 在确定分配的位置时会忽略当前设备/上下文。 相反，<code>cudaMallocAsync</code> 根据指定的内存池或提供的流来确定常驻设备。 最简单的使用模式是分配、使用和释放内存到同一个流中。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> *ptr;</span><br><span class="line"><span class="type">size_t</span> size = <span class="number">512</span>;</span><br><span class="line"><span class="built_in">cudaMallocAsync</span>(&amp;ptr, size, cudaStreamPerThread);</span><br><span class="line"><span class="comment">// do work using the allocation</span></span><br><span class="line">kernel&lt;&lt;&lt;..., cudaStreamPerThread&gt;&gt;&gt;(ptr, ...);</span><br><span class="line"><span class="comment">// An asynchronous free can be specified without synchronizing the CPU and GPU</span></span><br><span class="line"><span class="built_in">cudaFreeAsync</span>(ptr, cudaStreamPerThread);</span><br></pre></td></tr></table></figure>
<p>用户可以使用 <code>cudaFreeAsync()</code> 释放使用 <code>cudaMalloc()</code> 分配的内存。 在自由操作开始之前，用户必须对访问完成做出同样的保证。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaMalloc</span>(&amp;ptr, size);</span><br><span class="line">kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;(ptr, ...);</span><br><span class="line"><span class="built_in">cudaFreeAsync</span>(ptr, stream);</span><br></pre></td></tr></table></figure>
<p>用户可以使用 <code>cudaFree()</code> 释放使用 <code>cudaMallocAsync</code> 分配的内存。 通过 <code>cudaFree()</code> API 释放此类分配时，驱动程序假定对分配的所有访问都已完成，并且不执行进一步的同步。 用户可以使用 <code>cudaStreamQuery / cudaStreamSynchronize / cudaEventQuery / cudaEventSynchronize / cudaDeviceSynchronize</code> 来保证适当的异步工作完成并且GPU不会尝试访问分配。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaMallocAsync</span>(&amp;ptr, size,stream);</span><br><span class="line">kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;(ptr, ...);</span><br><span class="line"><span class="comment">// synchronize is needed to avoid prematurely freeing the memory</span></span><br><span class="line"><span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line"><span class="built_in">cudaFree</span>(ptr);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>F.4. Memory Pools and the cudaMemPool_t</p>
<p>内存池封装了虚拟地址和物理内存资源，根据内存池的属性和属性进行分配和管理。内存池的主要方面是它所管理的内存的种类和位置。</p>
<p>所有对 <code>cudaMallocAsync</code> 的调用都使用内存池的资源。在没有指定内存池的情况下，<code>cudaMallocAsync</code>API 使用提供的流设备的当前内存池。设备的当前内存池可以使用 <code>cudaDeviceSetMempool</code> 设置并使用 <code>cudaDeviceGetMempool</code> 查询。默认情况下（在没有 <code>cudaDeviceSetMempool</code> 调用的情况下），当前内存池是设备的默认内存池。 <code>cudaMallocFromPoolAsync</code> 的 API <code>cudaMallocFromPoolAsync</code> 和 c++ 重载允许用户指定要用于分配的池，而无需将其设置为当前池。 API <code>cudaDeviceGetDefaultMempool</code> 和 <code>cudaMemPoolCreate</code> 为用户提供内存池的句柄。</p>
<p>注意：设备的内存池当前将是该设备的本地。因此，在不指定内存池的情况下进行分配将始终产生流设备本地的分配。</p>
<p>注意：<code>cudaMemPoolSetAttribute</code> 和 <code>cudaMemPoolGetAttribute</code> 控制内存池的属性。</p>
<h2 id="F-5-Default-Impicit-Pools"><a href="#F-5-Default-Impicit-Pools" class="headerlink" title="F.5. Default/Impicit Pools"></a>F.5. Default/Impicit Pools</h2><p>可以使用 <code>cudaDeviceGetDefaultMempool</code> API 检索设备的默认内存池。 来自设备默认内存池的分配是位于该设备上的不可迁移设备分配。 这些分配将始终可以从该设备访问。 默认内存池的可访问性可以通过 <code>cudaMemPoolSetAccess</code>进行修改，并通过 <code>cudaMemPoolGetAccess</code> 进行查询。 由于不需要显式创建默认池，因此有时将它们称为隐式池。 设备默认内存池不支持IPC。</p>
<h2 id="F-6-Explicit-Pools"><a href="#F-6-Explicit-Pools" class="headerlink" title="F.6. Explicit Pools"></a>F.6. Explicit Pools</h2><p>API <code>cudaMemPoolCreate</code> 创建一个显式池。 目前内存池只能分配设备分配。 分配将驻留的设备必须在属性结构中指定。 显式池的主要用例是 IPC 功能。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create a pool similar to the implicit pool on device 0</span></span><br><span class="line"><span class="type">int</span> device = <span class="number">0</span>;</span><br><span class="line">cudaMemPoolProps poolProps = &#123; &#125;;</span><br><span class="line">poolProps.allocType = cudaMemAllocationTypePinned;</span><br><span class="line">poolProps.location.id = device;</span><br><span class="line">poolProps.location.type = cudaMemLocationTypeDevice;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaMemPoolCreate</span>(&amp;memPool, &amp;poolProps));</span><br></pre></td></tr></table></figure>
<h2 id="F-7-Physical-Page-Caching-Behavior"><a href="#F-7-Physical-Page-Caching-Behavior" class="headerlink" title="F.7. Physical Page Caching Behavior"></a>F.7. Physical Page Caching Behavior</h2><p>默认情况下，分配器尝试最小化池拥有的物理内存。 为了尽量减少分配和释放物理内存的操作系统调用，应用程序必须为每个池配置内存占用。 应用程序可以使用释放阈值属性 (<code>cudaMemPoolAttrReleaseThreshold</code>) 执行此操作。</p>
<p>释放阈值是池在尝试将内存释放回操作系统之前应保留的内存量（以字节为单位）。 当内存池持有超过释放阈值字节的内存时，分配器将尝试在下一次调用流、事件或设备同步时将内存释放回操作系统。 将释放阈值设置为 <code>UINT64_MAX</code> 将防止驱动程序在每次同步后尝试收缩池。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Cuuint64_t setVal = UINT64_MAX;</span><br><span class="line"><span class="built_in">cudaMemPoolSetAttribute</span>(memPool, cudaMemPoolAttrReleaseThreshold, &amp;setVal);</span><br></pre></td></tr></table></figure>
<p>将 <code>cudaMemPoolAttrReleaseThreshold</code> 设置得足够高以有效禁用内存池收缩的应用程序可能希望显式收缩内存池的内存占用。 <code>cudaMemPoolTrimTo</code> 允许此类应用程序这样做。 在修剪内存池的占用空间时，<code>minBytesToKeep</code> 参数允许应用程序保留它预期在后续执行阶段需要的内存量。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Cuuint64_t setVal = UINT64_MAX;</span><br><span class="line"><span class="built_in">cudaMemPoolSetAttribute</span>(memPool, cudaMemPoolAttrReleaseThreshold, &amp;setVal);</span><br><span class="line"></span><br><span class="line"><span class="comment">// application phase needing a lot of memory from the stream ordered allocator</span></span><br><span class="line"><span class="keyword">for</span> (i=<span class="number">0</span>; i&lt;<span class="number">10</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">for</span> (j=<span class="number">0</span>; j&lt;<span class="number">10</span>; j++) &#123;</span><br><span class="line">        <span class="built_in">cudaMallocAsync</span>(&amp;ptrs[j],size[j], stream);</span><br><span class="line">    &#125;</span><br><span class="line">    kernel&lt;&lt;&lt;...,stream&gt;&gt;&gt;(ptrs,...);</span><br><span class="line">    <span class="keyword">for</span> (j=<span class="number">0</span>; j&lt;<span class="number">10</span>; j++) &#123;</span><br><span class="line">        <span class="built_in">cudaFreeAsync</span>(ptrs[j], stream);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Process does not need as much memory for the next phase.</span></span><br><span class="line"><span class="comment">// Synchronize so that the trim operation will know that the allocations are no </span></span><br><span class="line"><span class="comment">// longer in use.</span></span><br><span class="line"><span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line"><span class="built_in">cudaMemPoolTrimTo</span>(mempool, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Some other process/allocation mechanism can now use the physical memory </span></span><br><span class="line"><span class="comment">// released by the trimming operation.</span></span><br></pre></td></tr></table></figure>
<h2 id="F-8-Resource-Usage-Statistics"><a href="#F-8-Resource-Usage-Statistics" class="headerlink" title="F.8. Resource Usage Statistics"></a>F.8. Resource Usage Statistics</h2><p>在 CUDA 11.3 中，添加了池属性 <code>cudaMemPoolAttrReservedMemCurrent、cudaMemPoolAttrReservedMemHigh、cudaMemPoolAttrUsedMemCurrent 和 cudaMemPoolAttrUsedMemHigh</code> 来查询池的内存使用情况。</p>
<p>查询池的 <code>cudaMemPoolAttrReservedMemCurrent</code> 属性会报告该池当前消耗的总物理 GPU 内存。 查询池的 <code>cudaMemPoolAttrUsedMemCurrent</code> 会返回从池中分配且不可重用的所有内存的总大小。</p>
<p><code>cudaMemPoolAttr*MemHigh</code> 属性是记录自上次重置以来各个 <code>cudaMemPoolAttr*MemCurrent</code> 属性达到的最大值的水印。 可以使用 <code>cudaMemPoolSetAttribute</code> API 将它们重置为当前值。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sample helper functions for getting the usage statistics in bulk</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">usageStatistics</span> &#123;</span><br><span class="line">    <span class="type">cuuint64_t</span> reserved;</span><br><span class="line">    <span class="type">cuuint64_t</span> reservedHigh;</span><br><span class="line">    <span class="type">cuuint64_t</span> used;</span><br><span class="line">    <span class="type">cuuint64_t</span> usedHigh;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">getUsageStatistics</span><span class="params">(cudaMemoryPool_t memPool, <span class="keyword">struct</span> usageStatistics *statistics)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">cudaMemPoolGetAttribute</span>(memPool, cudaMemPoolAttrReservedMemCurrent, statistics-&gt;reserved);</span><br><span class="line">    <span class="built_in">cudaMemPoolGetAttribute</span>(memPool, cudaMemPoolAttrReservedMemHigh, statistics-&gt;reservedHigh);</span><br><span class="line">    <span class="built_in">cudaMemPoolGetAttribute</span>(memPool, cudaMemPoolAttrUsedMemCurrent, statistics-&gt;used);</span><br><span class="line">    <span class="built_in">cudaMemPoolGetAttribute</span>(memPool, cudaMemPoolAttrUsedMemHigh, statistics-&gt;usedHigh);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// resetting the watermarks will make them take on the current value.</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">resetStatistics</span><span class="params">(cudaMemoryPool_t memPool)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">cuuint64_t</span> value = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaMemPoolSetAttribute</span>(memPool, cudaMemPoolAttrReservedMemHigh, &amp;value);</span><br><span class="line">    <span class="built_in">cudaMemPoolSetAttribute</span>(memPool, cudaMemPoolAttrUsedMemHigh, &amp;value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="F-9-Memory-Reuse-Policies"><a href="#F-9-Memory-Reuse-Policies" class="headerlink" title="F.9. Memory Reuse Policies"></a>F.9. Memory Reuse Policies</h2><p>为了服务分配请求，驱动程序在尝试从操作系统分配更多内存之前尝试重用之前通过 <code>cudaFreeAsync()</code> 释放的内存。 例如，流中释放的内存可以立即重新用于同一流中的后续分配请求。 类似地，当一个流与 CPU 同步时，之前在该流中释放的内存可以重新用于任何流中的分配。</p>
<p>流序分配器有一些可控的分配策略。 池属性 <code>cudaMemPoolReuseFollowEventDependencies、cudaMemPoolReuseAllowOpportunistic 和 cudaMemPoolReuseAllowInternalDependencies</code> 控制这些策略。 升级到更新的 CUDA 驱动程序可能会更改、增强、增加或重新排序重用策略。</p>
<h3 id="F-9-1-cudaMemPoolReuseFollowEventDependencies"><a href="#F-9-1-cudaMemPoolReuseFollowEventDependencies" class="headerlink" title="F.9.1. cudaMemPoolReuseFollowEventDependencies"></a>F.9.1. cudaMemPoolReuseFollowEventDependencies</h3><p>在分配更多物理 GPU 内存之前，分配器会检查由 CUDA 事件建立的依赖信息，并尝试从另一个流中释放的内存中进行分配。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaMallocAsync</span>(&amp;ptr, size, originalStream);</span><br><span class="line">kernel&lt;&lt;&lt;..., originalStream&gt;&gt;&gt;(ptr, ...);</span><br><span class="line"><span class="built_in">cudaFreeAsync</span>(ptr, originalStream);</span><br><span class="line"><span class="built_in">cudaEventRecord</span>(event,originalStream);</span><br><span class="line"></span><br><span class="line"><span class="comment">// waiting on the event that captures the free in another stream </span></span><br><span class="line"><span class="comment">// allows the allocator to reuse the memory to satisfy </span></span><br><span class="line"><span class="comment">// a new allocation request in the other stream when</span></span><br><span class="line"><span class="comment">// cudaMemPoolReuseFollowEventDependencies is enabled.</span></span><br><span class="line"><span class="built_in">cudaStreamWaitEvent</span>(otherStream, event);</span><br><span class="line"><span class="built_in">cudaMallocAsync</span>(&amp;ptr2, size, otherStream);</span><br></pre></td></tr></table></figure>
<h3 id="F-9-2-cudaMemPoolReuseAllowOpportunistic"><a href="#F-9-2-cudaMemPoolReuseAllowOpportunistic" class="headerlink" title="F.9.2. cudaMemPoolReuseAllowOpportunistic"></a>F.9.2. cudaMemPoolReuseAllowOpportunistic</h3><p>根据 <code>cudaMemPoolReuseAllowOpportunistic</code> 策略，分配器检查释放的分配以查看是否满足释放的流序语义（即流已通过释放指示的执行点）。 禁用此功能后，分配器仍将重用在流与 cpu 同步时可用的内存。 禁用此策略不会阻止 <code>cudaMemPoolReuseFollowEventDependencies</code> 应用。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaMallocAsync</span>(&amp;ptr, size, originalStream);</span><br><span class="line">kernel&lt;&lt;&lt;..., originalStream&gt;&gt;&gt;(ptr, ...);</span><br><span class="line"><span class="built_in">cudaFreeAsync</span>(ptr, originalStream);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// after some time, the kernel finishes running</span></span><br><span class="line"><span class="built_in">wait</span>(<span class="number">10</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// When cudaMemPoolReuseAllowOpportunistic is enabled this allocation request</span></span><br><span class="line"><span class="comment">// can be fulfilled with the prior allocation based on the progress of originalStream.</span></span><br><span class="line"><span class="built_in">cudaMallocAsync</span>(&amp;ptr2, size, otherStream);</span><br></pre></td></tr></table></figure>
<p>F.9.3. cudaMemPoolReuseAllowInternalDependencies</p>
<p>如果无法从操作系统分配和映射更多物理内存，驱动程序将寻找其可用性取决于另一个流的待处理进度的内存。 如果找到这样的内存，驱动程序会将所需的依赖项插入分配流并重用内存。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaMallocAsync</span>(&amp;ptr, size, originalStream);</span><br><span class="line">kernel&lt;&lt;&lt;..., originalStream&gt;&gt;&gt;(ptr, ...);</span><br><span class="line"><span class="built_in">cudaFreeAsync</span>(ptr, originalStream);</span><br><span class="line"></span><br><span class="line"><span class="comment">// When cudaMemPoolReuseAllowInternalDependencies is enabled</span></span><br><span class="line"><span class="comment">// and the driver fails to allocate more physical memory, the driver may</span></span><br><span class="line"><span class="comment">// effectively perform a cudaStreamWaitEvent in the allocating stream</span></span><br><span class="line"><span class="comment">// to make sure that future work in ‘otherStream’ happens after the work</span></span><br><span class="line"><span class="comment">// in the original stream that would be allowed to access the original allocation. </span></span><br><span class="line"><span class="built_in">cudaMallocAsync</span>(&amp;ptr2, size, otherStream);</span><br></pre></td></tr></table></figure>
<p>F.9.4. Disabling Reuse Policies</p>
<p>虽然可控重用策略提高了内存重用，但用户可能希望禁用它们。 允许机会重用（即 <code>cudaMemPoolReuseAllowOpportunistic</code>）基于 CPU 和 GPU 执行的交错引入了运行到运行分配模式的差异。 当用户宁愿在分配失败时显式同步事件或流时，内部依赖插入（即 <code>cudaMemPoolReuseAllowInternalDependencies</code>）可以以意想不到的和潜在的非确定性方式序列化工作。</p>
<h2 id="F-10-Device-Accessibility-for-Multi-GPU-Support"><a href="#F-10-Device-Accessibility-for-Multi-GPU-Support" class="headerlink" title="F.10. Device Accessibility for Multi-GPU Support"></a>F.10. Device Accessibility for Multi-GPU Support</h2><p>就像通过虚拟内存管理 API 控制的分配可访问性一样，内存池分配可访问性不遵循 <code>cudaDeviceEnablePeerAccess</code> 或 <code>cuCtxEnablePeerAccess</code>。相反，API <code>cudaMemPoolSetAccess</code> 修改了哪些设备可以访问池中的分配。默认情况下，可以从分配所在的设备访问分配。无法撤销此访问权限。要启用其他设备的访问，访问设备必须与内存池的设备对等；检查 <code>cudaDeviceCanAccessPeer</code>。如果未检查对等功能，则设置访问可能会失败并显示 <code>cudaErrorInvalidDevice</code>。如果没有从池中进行分配，即使设备不具备对等能力，<code>cudaMemPoolSetAccess</code> 调用也可能成功；在这种情况下，池中的下一次分配将失败。</p>
<p>值得注意的是，<code>cudaMemPoolSetAccess</code> 会影响内存池中的所有分配，而不仅仅是未来的分配。此外，<code>cudaMemPoolGetAccess</code> 报告的可访问性适用于池中的所有分配，而不仅仅是未来的分配。建议不要频繁更改给定 GPU 的池的可访问性设置；一旦池可以从给定的 GPU 访问，它应该在池的整个生命周期内都可以从该 GPU 访问。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// snippet showing usage of cudaMemPoolSetAccess:</span></span><br><span class="line"><span class="function">cudaError_t <span class="title">setAccessOnDevice</span><span class="params">(cudaMemPool_t memPool, <span class="type">int</span> residentDevice,</span></span></span><br><span class="line"><span class="params"><span class="function">              <span class="type">int</span> accessingDevice)</span> </span>&#123;</span><br><span class="line">    cudaMemAccessDesc accessDesc = &#123;&#125;;</span><br><span class="line">    accessDesc.location.type = cudaMemLocationTypeDevice;</span><br><span class="line">    accessDesc.location.id = accessingDevice;</span><br><span class="line">    accessDesc.flags = cudaMemAccessFlagsProtReadWrite;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> canAccess = <span class="number">0</span>;</span><br><span class="line">    cudaError_t error = <span class="built_in">cudaDeviceCanAccessPeer</span>(&amp;canAccess, accessingDevice,</span><br><span class="line">              residentDevice);</span><br><span class="line">    <span class="keyword">if</span> (error != cudaSuccess) &#123;</span><br><span class="line">        <span class="keyword">return</span> error;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (canAccess == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> cudaErrorPeerAccessUnsupported;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Make the address accessible</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">cudaMemPoolSetAccess</span>(memPool, &amp;accessDesc, <span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="F-11-IPC-Memory-Pools"><a href="#F-11-IPC-Memory-Pools" class="headerlink" title="F.11. IPC Memory Pools"></a>F.11. IPC Memory Pools</h2><p>支持 IPC 的内存池允许在进程之间轻松、高效和安全地共享 GPU 内存。 CUDA 的 IPC 内存池提供与 CUDA 的虚拟内存管理 API 相同的安全优势。</p>
<p>在具有内存池的进程之间共享内存有两个阶段。 进程首先需要共享对池的访问权限，然后共享来自该池的特定分配。 第一阶段建立并实施安全性。 第二阶段协调每个进程中使用的虚拟地址以及映射何时需要在导入过程中有效。</p>
<h3 id="F-11-1-Creating-and-Sharing-IPC-Memory-Pools"><a href="#F-11-1-Creating-and-Sharing-IPC-Memory-Pools" class="headerlink" title="F.11.1. Creating and Sharing IPC Memory Pools"></a>F.11.1. Creating and Sharing IPC Memory Pools</h3><p>共享对池的访问涉及检索池的 OS 本机句柄（使用 <code>cudaMemPoolExportToShareableHandle()</code> API），使用通常的 OS 本机 IPC 机制将句柄转移到导入进程，并创建导入的内存池（使用 <code>cudaMemPoolImportFromShareableHandle()</code> API）。 要使 <code>cudaMemPoolExportToShareableHandle</code> 成功，必须使用池属性结构中指定的请求句柄类型创建内存池。 请参考示例以了解在进程之间传输操作系统本机句柄的适当 IPC 机制。 该过程的其余部分可以在以下代码片段中找到。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in exporting process</span></span><br><span class="line"><span class="comment">// create an exportable IPC capable pool on device 0</span></span><br><span class="line">cudaMemPoolProps poolProps = &#123; &#125;;</span><br><span class="line">poolProps.allocType = cudaMemAllocationTypePinned;</span><br><span class="line">poolProps.location.id = <span class="number">0</span>;</span><br><span class="line">poolProps.location.type = cudaMemLocationTypeDevice;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Setting handleTypes to a non zero value will make the pool exportable (IPC capable)</span></span><br><span class="line">poolProps.handleTypes = CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaMemPoolCreate</span>(&amp;memPool, &amp;poolProps));</span><br><span class="line"></span><br><span class="line"><span class="comment">// FD based handles are integer types</span></span><br><span class="line"><span class="type">int</span> fdHandle = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Retrieve an OS native handle to the pool.</span></span><br><span class="line"><span class="comment">// Note that a pointer to the handle memory is passed in here.</span></span><br><span class="line"><span class="built_in">cudaMemPoolExportToShareableHandle</span>(&amp;fdHandle,</span><br><span class="line">             memPool,</span><br><span class="line">             CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR,</span><br><span class="line">             <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The handle must be sent to the importing process with the appropriate</span></span><br><span class="line"><span class="comment">// OS specific APIs.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in importing process</span></span><br><span class="line"> <span class="type">int</span> fdHandle;</span><br><span class="line"><span class="comment">// The handle needs to be retrieved from the exporting process with the</span></span><br><span class="line"><span class="comment">// appropriate OS specific APIs.</span></span><br><span class="line"><span class="comment">// Create an imported pool from the shareable handle.</span></span><br><span class="line"><span class="comment">// Note that the handle is passed by value here. </span></span><br><span class="line"><span class="built_in">cudaMemPoolImportFromShareableHandle</span>(&amp;importedMemPool,</span><br><span class="line">          (<span class="type">void</span>*)fdHandle,</span><br><span class="line">          CU_MEM_HANDLE_TYPE_POSIX_FILE_DESCRIPTOR,</span><br><span class="line">          <span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<h3 id="F-11-2-Set-Access-in-the-Importing-Process"><a href="#F-11-2-Set-Access-in-the-Importing-Process" class="headerlink" title="F.11.2. Set Access in the Importing Process"></a>F.11.2. Set Access in the Importing Process</h3><p>导入的内存池最初只能从其常驻设备访问。 导入的内存池不继承导出进程设置的任何可访问性。 导入过程需要启用从它计划访问内存的任何 GPU 的访问（使用 <code>cudaMemPoolSetAccess</code>）。</p>
<p>如果导入的内存池在导入过程中属于不可见的设备，则用户必须使用 <code>cudaMemPoolSetAccess</code> API 来启用从将使用分配的 GPU 的访问。</p>
<h3 id="F-11-3-Creating-and-Sharing-Allocations-from-an-Exported-Pool"><a href="#F-11-3-Creating-and-Sharing-Allocations-from-an-Exported-Pool" class="headerlink" title="F.11.3. Creating and Sharing Allocations from an Exported Pool"></a>F.11.3. Creating and Sharing Allocations from an Exported Pool</h3><p>共享池后，在导出进程中使用 <code>cudaMallocAsync()</code>从池中进行的分配可以与已导入池的其他进程共享。由于池的安全策略是在池级别建立和验证的，操作系统不需要额外的簿记来为特定的池分配提供安全性；换句话说，导入池分配所需的不透明 <code>cudaMemPoolPtrExportData</code> 可以使用任何机制发送到导入进程。</p>
<p>虽然分配可以在不以任何方式与分配流同步的情况下导出甚至导入，但在访问分配时，导入过程必须遵循与导出过程相同的规则。即，对分配的访问必须发生在分配流中分配操作的流排序之后。以下两个代码片段显示 <code>cudaMemPoolExportPointer()</code> 和 <code>cudaMemPoolImportPointer()</code> 与 IPC 事件共享分配，用于保证在分配准备好之前在导入过程中不会访问分配。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// preparing an allocation in the exporting process</span></span><br><span class="line">cudaMemPoolPtrExportData exportData;</span><br><span class="line">cudaEvent_t readyIpcEvent;</span><br><span class="line">cudaIpcEventHandle_t readyIpcEventHandle;</span><br><span class="line"></span><br><span class="line"><span class="comment">// IPC event for coordinating between processes</span></span><br><span class="line"><span class="comment">// cudaEventInterprocess flag makes the event an IPC event</span></span><br><span class="line"><span class="comment">// cudaEventDisableTiming  is set for performance reasons</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaEventCreate</span>(</span><br><span class="line">        &amp;readyIpcEvent, cudaEventDisableTiming | cudaEventInterprocess)</span><br><span class="line"></span><br><span class="line"><span class="comment">// allocate from the exporting mem pool</span></span><br><span class="line"><span class="built_in">cudaMallocAsync</span>(&amp;ptr, size,exportMemPool, stream);</span><br><span class="line"></span><br><span class="line"><span class="comment">// event for sharing when the allocation is ready.</span></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(readyIpcEvent, stream);</span><br><span class="line"><span class="built_in">cudaMemPoolExportPointer</span>(&amp;exportData, ptr);</span><br><span class="line"><span class="built_in">cudaIpcGetEventHandle</span>(&amp;readyIpcEventHandle, readyIpcEvent);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Share IPC event and pointer export data with the importing process using</span></span><br><span class="line"><span class="comment">//  any mechanism. Here we copy the data into shared memory</span></span><br><span class="line">shmem-&gt;ptrData = exportData;</span><br><span class="line">shmem-&gt;readyIpcEventHandle = readyIpcEventHandle;</span><br><span class="line"><span class="comment">// signal consumers data is ready</span></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Importing an allocation</span></span><br><span class="line">cudaMemPoolPtrExportData *importData = &amp;shmem-&gt;prtData;</span><br><span class="line">cudaEvent_t readyIpcEvent;</span><br><span class="line">cudaIpcEventHandle_t *readyIpcEventHandle = &amp;shmem-&gt;readyIpcEventHandle;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Need to retrieve the IPC event handle and the export data from the</span></span><br><span class="line"><span class="comment">// exporting process using any mechanism.  Here we are using shmem and just</span></span><br><span class="line"><span class="comment">// need synchronization to make sure the shared memory is filled in.</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaIpcOpenEventHandle</span>(&amp;readyIpcEvent, readyIpcEventHandle);</span><br><span class="line"></span><br><span class="line"><span class="comment">// import the allocation. The operation does not block on the allocation being ready.</span></span><br><span class="line"><span class="built_in">cudaMemPoolImportPointer</span>(&amp;ptr, importedMemPool, importData);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Wait for the prior stream operations in the allocating stream to complete before</span></span><br><span class="line"><span class="comment">// using the allocation in the importing process.</span></span><br><span class="line"><span class="built_in">cudaStreamWaitEvent</span>(stream, readyIpcEvent);</span><br><span class="line">kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;(ptr, ...);</span><br></pre></td></tr></table></figure>
<p>释放分配时，需要先在导入过程中释放分配，然后在导出过程中释放分配。 以下代码片段演示了使用 CUDA IPC 事件在两个进程中的 <code>cudaFreeAsync</code> 操作之间提供所需的同步。 导入过程中对分配的访问显然受到导入过程侧的自由操作的限制。 值得注意的是，<code>cudaFree</code> 可用于释放两个进程中的分配，并且可以使用其他流同步 API 代替 CUDA IPC 事件。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The free must happen in importing process before the exporting process</span></span><br><span class="line">kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;(ptr, ...); </span><br><span class="line"></span><br><span class="line"><span class="comment">// Last access in importing process</span></span><br><span class="line"><span class="built_in">cudaFreeAsync</span>(ptr, stream); </span><br><span class="line"></span><br><span class="line"><span class="comment">// Access not allowed in the importing process after the free</span></span><br><span class="line"><span class="built_in">cudaIpcEventRecord</span>(finishedIpcEvent, stream);</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Exporting process</span></span><br><span class="line"><span class="comment">// The exporting process needs to coordinate its free with the stream order </span></span><br><span class="line"><span class="comment">// of the importing process’s free.</span></span><br><span class="line"><span class="built_in">cudaStreamWaitEvent</span>(stream, finishedIpcEvent);</span><br><span class="line">kernel&lt;&lt;&lt;..., stream&gt;&gt;&gt;(ptrInExportingProcess, ...); </span><br><span class="line"></span><br><span class="line"><span class="comment">// The free in the importing process doesn’t stop the exporting process </span></span><br><span class="line"><span class="comment">// from using the allocation.</span></span><br><span class="line"><span class="built_in">cudFreeAsync</span>(ptrInExportingProcess,stream);</span><br></pre></td></tr></table></figure>
<h3 id="F-11-4-IPC-Export-Pool-Limitations"><a href="#F-11-4-IPC-Export-Pool-Limitations" class="headerlink" title="F.11.4. IPC Export Pool Limitations"></a>F.11.4. IPC Export Pool Limitations</h3><p>IPC 池目前不支持将物理块释放回操作系统。 因此，<code>cudaMemPoolTrimTo</code> API 充当空操作，并且 <code>cudaMemPoolAttrReleaseThreshold</code> 被有效地忽略。 此行为由驱动程序控制，而不是运行时控制，并且可能会在未来的驱动程序更新中发生变化。</p>
<h3 id="F-11-5-IPC-Import-Pool-Limitations"><a href="#F-11-5-IPC-Import-Pool-Limitations" class="headerlink" title="F.11.5. IPC Import Pool Limitations"></a>F.11.5. IPC Import Pool Limitations</h3><p>不允许从导入池中分配； 具体来说，导入池不能设置为当前，也不能在 <code>cudaMallocFromPoolAsync</code> API 中使用。 因此，分配重用策略属性对这些池没有意义。</p>
<p>IPC 池目前不支持将物理块释放回操作系统。 因此，<code>cudaMemPoolTrimTo</code> API 充当空操作，并且 <code>cudaMemPoolAttrReleaseThreshold</code> 被有效地忽略。</p>
<p>资源使用统计属性查询仅反映导入进程的分配和相关的物理内存。</p>
<h2 id="F-12-Synchronization-API-Actions"><a href="#F-12-Synchronization-API-Actions" class="headerlink" title="F.12. Synchronization API Actions"></a>F.12. Synchronization API Actions</h2><p>作为 CUDA 驱动程序一部分的分配器带来的优化之一是与同步 API 的集成。 当用户请求 CUDA 驱动程序同步时，驱动程序等待异步工作完成。 在返回之前，驱动程序将确定什么释放了保证完成的同步。 无论指定的流或禁用的分配策略如何，这些分配都可用于分配。 驱动程序还在这里检查 <code>cudaMemPoolAttrReleaseThreshold</code> 并释放它可以释放的任何多余的物理内存。</p>
<h2 id="F-13-Addendums"><a href="#F-13-Addendums" class="headerlink" title="F.13. Addendums"></a>F.13. Addendums</h2><h3 id="F-13-1-cudaMemcpyAsync-Current-Context-Device-Sensitivity"><a href="#F-13-1-cudaMemcpyAsync-Current-Context-Device-Sensitivity" class="headerlink" title="F.13.1. cudaMemcpyAsync Current Context/Device Sensitivity"></a>F.13.1. cudaMemcpyAsync Current Context/Device Sensitivity</h3><p>在当前的 CUDA 驱动程序中，任何涉及来自 <code>cudaMallocAsync</code> 的内存的异步 <code>memcpy</code> 都应该使用指定流的上下文作为调用线程的当前上下文来完成。 这对于 <code>cudaMemcpyPeerAsync</code> 不是必需的，因为引用了 API 中指定的设备主上下文而不是当前上下文。</p>
<h3 id="F-13-2-cuPointerGetAttribute-Query"><a href="#F-13-2-cuPointerGetAttribute-Query" class="headerlink" title="F.13.2. cuPointerGetAttribute Query"></a>F.13.2. cuPointerGetAttribute Query</h3><p>在对分配调用 <code>cudaFreeAsync</code> 后在分配上调用 <code>cuPointerGetAttribute</code> 会导致未定义的行为。 具体来说，分配是否仍然可以从给定的流中访问并不重要：行为仍然是未定义的。</p>
<h3 id="F-13-3-cuGraphAddMemsetNode"><a href="#F-13-3-cuGraphAddMemsetNode" class="headerlink" title="F.13.3. cuGraphAddMemsetNode"></a>F.13.3. cuGraphAddMemsetNode</h3><p><code>cuGraphAddMemsetNode</code> 不适用于通过流排序分配器分配的内存。 但是，分配的 <code>memset</code> 可以被流捕获。</p>
<h3 id="F-13-4-Pointer-Attributes"><a href="#F-13-4-Pointer-Attributes" class="headerlink" title="F.13.4. Pointer Attributes"></a>F.13.4. Pointer Attributes</h3><p><code>cuPointerGetAttributes</code> 查询适用于流有序分配。 由于流排序分配与上下文无关，因此查询 <code>CU_POINTER_ATTRIBUTE_CONTEXT</code> 将成功，但在 <code>*data</code> 中返回 <code>NULL</code>。 属性 <code>CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL</code> 可用于确定分配的位置：这在选择使用 <code>cudaMemcpyPeerAsync</code> 制作 <code>p2h2p</code> 拷贝的上下文时很有用。 <code>CU_POINTER_ATTRIBUTE_MEMPOOL_HANDLE</code> 属性是在 CUDA 11.3 中添加的，可用于调试和在执行 IPC 之前确认分配来自哪个池。</p>
<h1 id="附录G-图内存节点"><a href="#附录G-图内存节点" class="headerlink" title="附录G 图内存节点"></a>附录G 图内存节点</h1><h2 id="G-1-Introduction"><a href="#G-1-Introduction" class="headerlink" title="G.1. Introduction"></a>G.1. Introduction</h2><p>图内存节点允许图创建和拥有内存分配功能。图内存节点具有 GPU 有序生命周期语义，它指示何时允许在设备上访问内存。这些 GPU 有序生命周期语义支持驱动程序管理的内存重用，并与流序分配 API <code>cudaMallocAsync</code> 和 <code>cudaFreeAsync</code> 相匹配，这可能在创建图形时被捕获。</p>
<p>图分配在图的生命周期内具有固定的地址，包括重复的实例化和启动。这允许图中的其他操作直接引用内存，而无需更新图，即使 CUDA 更改了后备物理内存也是如此。在一个图中，其图有序生命周期不重叠的分配可以使用相同的底层物理内存。</p>
<p>CUDA 可以重用相同的物理内存进行跨多个图的分配，根据 GPU 有序生命周期语义对虚拟地址映射进行别名化。例如，当不同的图被启动到同一个流中时，CUDA 可以虚拟地为相同的物理内存取别名，以满足具有单图生命周期的分配的需求。</p>
<h2 id="G-2-Support-and-Compatibility"><a href="#G-2-Support-and-Compatibility" class="headerlink" title="G.2. Support and Compatibility"></a>G.2. Support and Compatibility</h2><p>图内存节点需要支持 11.4 的 CUDA 驱动程序并支持 GPU 上的流序分配器。 以下代码段显示了如何检查给定设备上的支持。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> driverVersion = <span class="number">0</span>;</span><br><span class="line"><span class="type">int</span> deviceSupportsMemoryPools = <span class="number">0</span>;</span><br><span class="line"><span class="type">int</span> deviceSupportsMemoryNodes = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">cudaDriverGetVersion</span>(&amp;driverVersion);</span><br><span class="line"><span class="keyword">if</span> (driverVersion &gt;= <span class="number">11020</span>) &#123; <span class="comment">// avoid invalid value error in cudaDeviceGetAttribute</span></span><br><span class="line">    <span class="built_in">cudaDeviceGetAttribute</span>(&amp;deviceSupportsMemoryPools, cudaDevAttrMemoryPoolsSupported, device);</span><br><span class="line">&#125;</span><br><span class="line">deviceSupportsMemoryNodes = (driverVersion &gt;= <span class="number">11040</span>) &amp;&amp; (deviceSupportsMemoryPools != <span class="number">0</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在驱动程序版本检查中执行属性查询可避免 11.0 和 11.1 驱动程序上的无效值返回代码。 请注意，计算清理程序在检测到 CUDA 返回错误代码时会发出警告，并且在读取属性之前进行版本检查将避免这种情况。 图形内存节点仅在驱动程序版本 11.4 和更高版本上受支持。</p>
<h2 id="G-3-API-Fundamentals"><a href="#G-3-API-Fundamentals" class="headerlink" title="G.3. API Fundamentals"></a>G.3. API Fundamentals</h2><p>图内存节点是表示内存分配或空闲操作的图节点。 简而言之，分配内存的节点称为分配节点。 同样，释放内存的节点称为空闲节点。 分配节点创建的分配称为图分配。 CUDA 在节点创建时为图分配分配虚拟地址。 虽然这些虚拟地址在分配节点的生命周期内是固定的，但分配内容在释放操作之后不会持久，并且可能被引用不同分配的访问覆盖。</p>
<p>每次图运行时，图分配都被视为重新创建。 图分配的生命周期与节点的生命周期不同，从 GPU 执行到达分配图节点时开始，并在发生以下情况之一时结束：</p>
<ul>
<li>GPU 执行到达释放图节点</li>
<li>GPU 执行到达释放 <code>cudaFreeAsync()</code> 流调用</li>
<li>立即释放对 <code>cudaFree()</code> 的调用</li>
</ul>
<p>注意：图销毁不会自动释放任何实时图分配的内存，即使它结束了分配节点的生命周期。 随后必须在另一个图中或使用 <code>cudaFreeAsync()/cudaFree()</code> 释放分配。</p>
<p>就像其他<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#graph-structure">图节点</a>一样，图内存节点在图中按依赖边排序。 程序必须保证访问图内存的操作：</p>
<ul>
<li>在分配节点之后排序。</li>
<li>在释放内存的操作之前排序</li>
</ul>
<p>图分配生命周期根据 GPU 执行开始和结束（与 API 调用相反）。 GPU 排序是工作在 GPU 上运行的顺序，而不是工作队列或描述的顺序。 因此，图分配被认为是“GPU 有序”。</p>
<h3 id="G-3-1-Graph-Node-APIs"><a href="#G-3-1-Graph-Node-APIs" class="headerlink" title="G.3.1. Graph Node APIs"></a>G.3.1. Graph Node APIs</h3><p>可以使用内存节点创建 API、<code>cudaGraphAddMemAllocNode</code> 和 <code>cudaGraphAddMemFreeNode</code> 显式创建图形内存节点。 <code>cudaGraphAddMemAllocNode</code> 分配的地址在传递的 <code>CUDA_MEM_ALLOC_NODE_PARAMS</code> 结构的 <code>dptr</code> 字段中返回给用户。 在分配图中使用图分配的所有操作必须在分配节点之后排序。 类似地，任何空闲节点都必须在图中所有分配的使用之后进行排序。 <code>cudaGraphAddMemFreeNode</code> 创建空闲节点。</p>
<p>在下图中，有一个带有分配和空闲节点的示例图。 内核节点 <code>a</code>、<code>b</code> 和 <code>c</code> 在分配节点之后和空闲节点之前排序，以便内核可以访问分配。 内核节点 <code>e</code> 没有排在 <code>alloc</code> 节点之后，因此无法安全地访问内存。 内核节点 <code>d</code> 没有排在空闲节点之前，因此它不能安全地访问内存。</p>
<p><img src="/img/kernel-nodes.png" alt=""></p>
<p>以下代码片段建立了该图中的图：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create the graph - it starts out empty</span></span><br><span class="line"><span class="built_in">cudaGraphCreate</span>(&amp;graph, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// parameters for a basic allocation</span></span><br><span class="line">cudaMemAllocNodeParams params = &#123;&#125;;</span><br><span class="line">params.poolProps.allocType = cudaMemAllocationTypePinned;</span><br><span class="line">params.poolProps.location.type = cudaMemLocationTypeDevice;</span><br><span class="line"><span class="comment">// specify device 0 as the resident device</span></span><br><span class="line">params.poolProps.location.id = <span class="number">0</span>;</span><br><span class="line">params.bytesize = size;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaGraphAddMemAllocNode</span>(&amp;allocNode, graph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;params);</span><br><span class="line">nodeParams-&gt;kernelParams[<span class="number">0</span>] = params.dptr;</span><br><span class="line"><span class="built_in">cudaGraphAddKernelNode</span>(&amp;a, graph, &amp;allocNode, <span class="number">1</span>, &amp;nodeParams);</span><br><span class="line"><span class="built_in">cudaGraphAddKernelNode</span>(&amp;b, graph, &amp;a, <span class="number">1</span>, &amp;nodeParams);</span><br><span class="line"><span class="built_in">cudaGraphAddKernelNode</span>(&amp;c, graph, &amp;a, <span class="number">1</span>, &amp;nodeParams);</span><br><span class="line">cudaGraphNode_t dependencies[<span class="number">2</span>];</span><br><span class="line"><span class="comment">// kernel nodes b and c are using the graph allocation, so the freeing node must depend on them.  Since the dependency of node b on node a establishes an indirect dependency, the free node does not need to explicitly depend on node a.</span></span><br><span class="line">dependencies[<span class="number">0</span>] = b;</span><br><span class="line">dependencies[<span class="number">1</span>] = c;</span><br><span class="line"><span class="built_in">cudaGraphAddMemFreeNode</span>(&amp;freeNode, graph, dependencies, <span class="number">2</span>, params.dptr);</span><br><span class="line"><span class="comment">// free node does not depend on kernel node d, so it must not access the freed graph allocation.</span></span><br><span class="line"><span class="built_in">cudaGraphAddKernelNode</span>(&amp;d, graph, &amp;c, <span class="number">1</span>, &amp;nodeParams);</span><br><span class="line"></span><br><span class="line"><span class="comment">// node e does not depend on the allocation node, so it must not access the allocation.  This would be true even if the freeNode depended on kernel node e.</span></span><br><span class="line"><span class="built_in">cudaGraphAddKernelNode</span>(&amp;e, graph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;nodeParams);</span><br></pre></td></tr></table></figure>
<h3 id="G-3-2-Stream-Capture"><a href="#G-3-2-Stream-Capture" class="headerlink" title="G.3.2. Stream Capture"></a>G.3.2. Stream Capture</h3><p>可以通过捕获相应的流序分配和免费调用 <code>cudaMallocAsync</code> 和 <code>cudaFreeAsync</code> 来创建图形内存节点。 在这种情况下，捕获的分配 API 返回的虚拟地址可以被图中的其他操作使用。 由于流序的依赖关系将被捕获到图中，流序分配 API 的排序要求保证了图内存节点将根据捕获的流操作正确排序（对于正确编写的流代码）。</p>
<p>忽略内核节点 <code>d</code> 和 <code>e</code>，为清楚起见，以下代码片段显示了如何使用流捕获来创建上图中的图形：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaMallocAsync</span>(&amp;dptr, size, stream1);</span><br><span class="line">kernel_A&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(dptr, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Fork into stream2</span></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(event1, stream1);</span><br><span class="line"><span class="built_in">cudaStreamWaitEvent</span>(stream2, event1);</span><br><span class="line"></span><br><span class="line">kernel_B&lt;&lt;&lt; ..., stream1 &gt;&gt;&gt;(dptr, ...);</span><br><span class="line"><span class="comment">// event dependencies translated into graph dependencies, so the kernel node created by the capture of kernel C will depend on the allocation node created by capturing the cudaMallocAsync call. </span></span><br><span class="line">kernel_C&lt;&lt;&lt; ..., stream2 &gt;&gt;&gt;(dptr, ...);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Join stream2 back to origin stream (stream1)</span></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(event2, stream2);</span><br><span class="line"><span class="built_in">cudaStreamWaitEvent</span>(stream1, event2);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Free depends on all work accessing the memory.</span></span><br><span class="line"><span class="built_in">cudaFreeAsync</span>(dptr, stream1);</span><br><span class="line"></span><br><span class="line"><span class="comment">// End capture in the origin stream</span></span><br><span class="line"><span class="built_in">cudaStreamEndCapture</span>(stream1, &amp;graph);</span><br></pre></td></tr></table></figure>
<h3 id="G-3-3-Accessing-and-Freeing-Graph-Memory-Outside-of-the-Allocating-Graph"><a href="#G-3-3-Accessing-and-Freeing-Graph-Memory-Outside-of-the-Allocating-Graph" class="headerlink" title="G.3.3. Accessing and Freeing Graph Memory Outside of the Allocating Graph"></a>G.3.3. Accessing and Freeing Graph Memory Outside of the Allocating Graph</h3><p>图分配不必由分配图释放。当图不释放分配时，该分配会在图执行之后持续存在，并且可以通过后续 CUDA 操作访问。这些分配可以在另一个图中访问或直接通过流操作访问，只要访问操作在分配之后通过 CUDA 事件和其他流排序机制进行排序。随后可以通过定期调用 <code>cudaFree、cudaFreeAsync</code> 或通过启动具有相应空闲节点的另一个图，或随后启动分配图（如果它是使用 <code>cudaGraphInstantiateFlagAutoFreeOnLaunch</code> 标志实例化）来释放分配。在内存被释放后访问内存是非法的 - 必须在所有使用图依赖、CUDA 事件和其他流排序机制访问内存的操作之后对释放操作进行排序。</p>
<p>注意:因为图分配可能彼此共享底层物理内存，所以必须考虑与一致性和一致性相关的虚拟混叠支持规则。简单地说，空闲操作必须在完整的设备操作(例如，计算内核/ memcpy)完成后排序。具体来说，带外同步——例如，作为访问图形内存的计算内核的一部分，通过内存进行信号交换——不足以提供对图形内存的写操作和该图形内存的自由操作之间的排序保证。</p>
<p>以下代码片段演示了在分配图之外访问图分配，并通过以下方式正确建立顺序：使用单个流，使用流之间的事件，以及使用嵌入到分配和释放图中的事件。</p>
<p>使用单个流建立的排序：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> *dptr;</span><br><span class="line"><span class="built_in">cudaGraphAddMemAllocNode</span>(&amp;allocNode, allocGraph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;params);</span><br><span class="line">dptr = params.dptr;</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaGraphInstantiate</span>(&amp;allocGraphExec, allocGraph, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaGraphLaunch</span>(allocGraphExec, stream);</span><br><span class="line">kernel&lt;&lt;&lt; …, stream &gt;&gt;&gt;(dptr, …);</span><br><span class="line"><span class="built_in">cudaFreeAsync</span>(dptr, stream);</span><br></pre></td></tr></table></figure>
<p>通过记录和等待 CUDA 事件建立的排序：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> *dptr;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Contents of allocating graph</span></span><br><span class="line"><span class="built_in">cudaGraphAddMemAllocNode</span>(&amp;allocNode, allocGraph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;params);</span><br><span class="line">dptr = params.dptr;</span><br><span class="line"></span><br><span class="line"><span class="comment">// contents of consuming/freeing graph</span></span><br><span class="line">nodeParams-&gt;kernelParams[<span class="number">0</span>] = params.dptr;</span><br><span class="line"><span class="built_in">cudaGraphAddKernelNode</span>(&amp;a, graph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;nodeParams);</span><br><span class="line"><span class="built_in">cudaGraphAddMemFreeNode</span>(&amp;freeNode, freeGraph, &amp;a, <span class="number">1</span>, dptr);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaGraphInstantiate</span>(&amp;allocGraphExec, allocGraph, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">0</span>);</span><br><span class="line"><span class="built_in">cudaGraphInstantiate</span>(&amp;freeGraphExec, freeGraph, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaGraphLaunch</span>(allocGraphExec, allocStream);</span><br><span class="line"></span><br><span class="line"><span class="comment">// establish the dependency of stream2 on the allocation node</span></span><br><span class="line"><span class="comment">// note: the dependency could also have been established with a stream synchronize operation</span></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(allocEvent, allocStream)</span><br><span class="line"><span class="built_in">cudaStreamWaitEvent</span>(stream2, allocEvent);</span><br><span class="line"></span><br><span class="line">kernel&lt;&lt;&lt; …, stream2 &gt;&gt;&gt; (dptr, …);</span><br><span class="line"></span><br><span class="line"><span class="comment">// establish the dependency between the stream 3 and the allocation use</span></span><br><span class="line"><span class="built_in">cudaStreamRecordEvent</span>(streamUseDoneEvent, stream2);</span><br><span class="line"><span class="built_in">cudaStreamWaitEvent</span>(stream3, streamUseDoneEvent);</span><br><span class="line"></span><br><span class="line"><span class="comment">// it is now safe to launch the freeing graph, which may also access the memory</span></span><br><span class="line"><span class="built_in">cudaGraphLaunch</span>(freeGraphExec, stream3);</span><br></pre></td></tr></table></figure>
<p>使用图外部事件节点建立的排序：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> *dptr;</span><br><span class="line">cudaEvent_t allocEvent; <span class="comment">// event indicating when the allocation will be ready for use.</span></span><br><span class="line">cudaEvent_t streamUseDoneEvent; <span class="comment">// event indicating when the stream operations are done with the allocation.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Contents of allocating graph with event record node</span></span><br><span class="line"><span class="built_in">cudaGraphAddMemAllocNode</span>(&amp;allocNode, allocGraph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;params);</span><br><span class="line">dptr = params.dptr;</span><br><span class="line"><span class="comment">// note: this event record node depends on the alloc node</span></span><br><span class="line"><span class="built_in">cudaGraphAddEventRecordNode</span>(&amp;recordNode, allocGraph, &amp;allocNode, <span class="number">1</span>, allocEvent);</span><br><span class="line"><span class="built_in">cudaGraphInstantiate</span>(&amp;allocGraphExec, allocGraph, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// contents of consuming/freeing graph with event wait nodes</span></span><br><span class="line"><span class="built_in">cudaGraphAddEventWaitNode</span>(&amp;streamUseDoneEventNode, waitAndFreeGraph, <span class="literal">NULL</span>, <span class="number">0</span>, streamUseDoneEvent);</span><br><span class="line"><span class="built_in">cudaGraphAddEventWaitNode</span>(&amp;allocReadyEventNode, waitAndFreeGraph, <span class="literal">NULL</span>, <span class="number">0</span>, allocEvent);</span><br><span class="line">nodeParams-&gt;kernelParams[<span class="number">0</span>] = params.dptr;</span><br><span class="line"></span><br><span class="line"><span class="comment">// The allocReadyEventNode provides ordering with the alloc node for use in a consuming graph.</span></span><br><span class="line"><span class="built_in">cudaGraphAddKernelNode</span>(&amp;kernelNode, waitAndFreeGraph, &amp;allocReadyEventNode, <span class="number">1</span>, &amp;nodeParams);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The free node has to be ordered after both external and internal users.</span></span><br><span class="line"><span class="comment">// Thus the node must depend on both the kernelNode and the </span></span><br><span class="line"><span class="comment">// streamUseDoneEventNode.</span></span><br><span class="line">dependencies[<span class="number">0</span>] = kernelNode;</span><br><span class="line">dependencies[<span class="number">1</span>] = streamUseDoneEventNode;</span><br><span class="line"><span class="built_in">cudaGraphAddMemFreeNode</span>(&amp;freeNode, waitAndFreeGraph, &amp;dependencies, <span class="number">2</span>, dptr);</span><br><span class="line"><span class="built_in">cudaGraphInstantiate</span>(&amp;waitAndFreeGraphExec, waitAndFreeGraph, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaGraphLaunch</span>(allocGraphExec, allocStream);</span><br><span class="line"></span><br><span class="line"><span class="comment">// establish the dependency of stream2 on the event node satisfies the ordering requirement</span></span><br><span class="line"><span class="built_in">cudaStreamWaitEvent</span>(stream2, allocEvent);</span><br><span class="line">kernel&lt;&lt;&lt; …, stream2 &gt;&gt;&gt; (dptr, …);</span><br><span class="line"><span class="built_in">cudaStreamRecordEvent</span>(streamUseDoneEvent, stream2);</span><br><span class="line"></span><br><span class="line"><span class="comment">// the event wait node in the waitAndFreeGraphExec establishes the dependency on the “readyForFreeEvent” that is needed to prevent the kernel running in stream two from accessing the allocation after the free node in execution order.</span></span><br><span class="line"><span class="built_in">cudaGraphLaunch</span>(waitAndFreeGraphExec, stream3);</span><br></pre></td></tr></table></figure>
<h3 id="G-3-4-cudaGraphInstantiateFlagAutoFreeOnLaunch"><a href="#G-3-4-cudaGraphInstantiateFlagAutoFreeOnLaunch" class="headerlink" title="G.3.4. cudaGraphInstantiateFlagAutoFreeOnLaunch"></a>G.3.4. cudaGraphInstantiateFlagAutoFreeOnLaunch</h3><p>在正常情况下，如果图有未释放的内存分配，CUDA 将阻止重新启动图，因为同一地址的多个分配会泄漏内存。使用 <code>cudaGraphInstantiateFlagAutoFreeOnLaunch</code> 标志实例化图允许图在其仍有未释放的分配时重新启动。在这种情况下，启动会自动插入一个异步释放的未释放分配。</p>
<p>启动时自动对于单生产者多消费者算法很有用。在每次迭代中，生产者图创建多个分配，并且根据运行时条件，一组不同的消费者访问这些分配。这种类型的变量执行序列意味着消费者无法释放分配，因为后续消费者可能需要访问。启动时自动释放意味着启动循环不需要跟踪生产者的分配 - 相反，该信息与生产者的创建和销毁逻辑保持隔离。通常，启动时自动释放简化了算法，否则该算法需要在每次重新启动之前释放图所拥有的所有分配。</p>
<p>注意： <code>cudaGraphInstantiateFlagAutoFreeOnLaunch</code> 标志不会改变图销毁的行为。应用程序必须显式释放未释放的内存以避免内存泄漏，即使对于使用标志实例化的图也是如此。</p>
<p>以下代码展示了使用 <code>cudaGraphInstantiateFlagAutoFreeOnLaunch</code> 来简化单生产者/多消费者算法：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create producer graph which allocates memory and populates it with data</span></span><br><span class="line"><span class="built_in">cudaStreamBeginCapture</span>(cudaStreamPerThread, cudaStreamCaptureModeGlobal);</span><br><span class="line"><span class="built_in">cudaMallocAsync</span>(&amp;data1, blocks * threads, cudaStreamPerThread);</span><br><span class="line"><span class="built_in">cudaMallocAsync</span>(&amp;data2, blocks * threads, cudaStreamPerThread);</span><br><span class="line">produce&lt;&lt;&lt;blocks, threads, <span class="number">0</span>, cudaStreamPerThread&gt;&gt;&gt;(data1, data2);</span><br><span class="line">...</span><br><span class="line"><span class="built_in">cudaStreamEndCapture</span>(cudaStreamPerThread, &amp;graph);</span><br><span class="line"><span class="built_in">cudaGraphInstantiateWithFlags</span>(&amp;producer,</span><br><span class="line">                              graph,</span><br><span class="line">                              cudaGraphInstantiateFlagAutoFreeOnLaunch);</span><br><span class="line"><span class="built_in">cudaGraphDestroy</span>(graph);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create first consumer graph by capturing an asynchronous library call</span></span><br><span class="line"><span class="built_in">cudaStreamBeginCapture</span>(cudaStreamPerThread, cudaStreamCaptureModeGlobal);</span><br><span class="line"><span class="built_in">consumerFromLibrary</span>(data1, cudaStreamPerThread);</span><br><span class="line"><span class="built_in">cudaStreamEndCapture</span>(cudaStreamPerThread, &amp;graph);</span><br><span class="line"><span class="built_in">cudaGraphInstantiateWithFlags</span>(&amp;consumer1, graph, <span class="number">0</span>); <span class="comment">//regular instantiation</span></span><br><span class="line"><span class="built_in">cudaGraphDestroy</span>(graph);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create second consumer graph</span></span><br><span class="line"><span class="built_in">cudaStreamBeginCapture</span>(cudaStreamPerThread, cudaStreamCaptureModeGlobal);</span><br><span class="line">consume2&lt;&lt;&lt;blocks, threads, <span class="number">0</span>, cudaStreamPerThread&gt;&gt;&gt;(data2);</span><br><span class="line">...</span><br><span class="line"><span class="built_in">cudaStreamEndCapture</span>(cudaStreamPerThread, &amp;graph);</span><br><span class="line"><span class="built_in">cudaGraphInstantiateWithFlags</span>(&amp;consumer2, graph, <span class="number">0</span>);</span><br><span class="line"><span class="built_in">cudaGraphDestroy</span>(graph);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Launch in a loop</span></span><br><span class="line"><span class="type">bool</span> launchConsumer2 = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">do</span> &#123;</span><br><span class="line">    <span class="built_in">cudaGraphLaunch</span>(producer, myStream);</span><br><span class="line">    <span class="built_in">cudaGraphLaunch</span>(consumer1, myStream);</span><br><span class="line">    <span class="keyword">if</span> (launchConsumer2) &#123;</span><br><span class="line">        <span class="built_in">cudaGraphLaunch</span>(consumer2, myStream);</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">while</span> (<span class="built_in">determineAction</span>(&amp;launchConsumer2));</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaFreeAsync</span>(data1, myStream);</span><br><span class="line"><span class="built_in">cudaFreeAsync</span>(data2, myStream);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaGraphExecDestroy</span>(producer);</span><br><span class="line"><span class="built_in">cudaGraphExecDestroy</span>(consumer1);</span><br><span class="line"><span class="built_in">cudaGraphExecDestroy</span>(consumer2);</span><br></pre></td></tr></table></figure>
<h2 id="G-4-Optimized-Memory-Reuse"><a href="#G-4-Optimized-Memory-Reuse" class="headerlink" title="G.4. Optimized Memory Reuse"></a>G.4. Optimized Memory Reuse</h2><p>CUDA 以两种方式重用内存：</p>
<ul>
<li>图中的虚拟和物理内存重用基于虚拟地址分配，就像在流序分配器中一样。</li>
<li>图之间的物理内存重用是通过虚拟别名完成的：不同的图可以将相同的物理内存映射到它们唯一的虚拟地址。</li>
</ul>
<h3 id="G-4-1-Address-Reuse-within-a-Graph"><a href="#G-4-1-Address-Reuse-within-a-Graph" class="headerlink" title="G.4.1. Address Reuse within a Graph"></a>G.4.1. Address Reuse within a Graph</h3><p>CUDA 可以通过将相同的虚拟地址范围分配给生命周期不重叠的不同分配来重用图中的内存。 由于可以重用虚拟地址，因此不能保证指向具有不相交生命周期的不同分配的指针是唯一的。</p>
<p>下图显示了添加一个新的分配节点 (2)，它可以重用依赖节点 (1) 释放的地址。</p>
<p><img src="/img/new-alloc-node.png" alt=""></p>
<p>下图显示了添加新的 <code>alloc</code> 节点（3）。 新的分配节点不依赖于空闲节点 (2)，因此不能重用来自关联分配节点 (2) 的地址。 如果分配节点 (2) 使用由空闲节点 (1) 释放的地址，则新分配节点 3 将需要一个新地址。</p>
<p><img src="/img/adding-new-alloc-nodes.png" alt=""></p>
<h3 id="G-4-2-Physical-Memory-Management-and-Sharing"><a href="#G-4-2-Physical-Memory-Management-and-Sharing" class="headerlink" title="G.4.2. Physical Memory Management and Sharing"></a>G.4.2. Physical Memory Management and Sharing</h3><p>CUDA 负责在按 GPU 顺序到达分配节点之前将物理内存映射到虚拟地址。作为内存占用和映射开销的优化，如果多个图不会同时运行，它们可能会使用相同的物理内存进行不同的分配，但是如果它们同时绑定到多个执行图，则物理页面不能被重用，或未释放的图形分配。</p>
<p>CUDA 可以在图形实例化、启动或执行期间随时更新物理内存映射。 CUDA 还可以在未来的图启动之间引入同步，以防止实时图分配引用相同的物理内存。对于任何 <code>allocate-free-allocate</code> 模式，如果程序在分配的生命周期之外访问指针，错误的访问可能会默默地读取或写入另一个分配拥有的实时数据（即使分配的虚拟地址是唯一的）。使用计算清理工具可以捕获此错误。</p>
<p>下图显示了在同一流中按顺序启动的图形。在此示例中，每个图都会释放它分配的所有内存。由于同一流中的图永远不会同时运行，CUDA 可以而且应该使用相同的物理内存来满足所有分配。</p>
<p><img src="/img/sequentially-launched-graphs.png" alt=""></p>
<h2 id="G-5-Performance-Considerations"><a href="#G-5-Performance-Considerations" class="headerlink" title="G.5. Performance Considerations"></a>G.5. Performance Considerations</h2><p>当多个图启动到同一个流中时，CUDA 会尝试为它们分配相同的物理内存，因为这些图的执行不能重叠。 在启动之间保留图形的物理映射作为优化以避免重新映射的成本。 如果稍后启动其中一个图，使其执行可能与其他图重叠（例如，如果它启动到不同的流中），则 CUDA 必须执行一些重新映射，因为并发图需要不同的内存以避免数据损坏 .</p>
<p>一般来说，CUDA中图内存的重新映射很可能是由这些操作引起的</p>
<ul>
<li>更改启动图形的流</li>
<li>图内存池上的修剪操作，显式释放未使用的内存（在<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#graph-memory-nodes-physical-memory-footprint">物理内存占用</a>中讨论）</li>
<li>当另一个图的未释放分配映射到同一内存时重新启动一个图将导致在重新启动之前重新映射内存  </li>
</ul>
<p>重新映射必须按执行顺序发生，但在该图的任何先前执行完成之后（否则可能会取消映射仍在使用的内存）。 由于这种排序依赖性，以及映射操作是操作系统调用，映射操作可能相对昂贵。 应用程序可以通过将包含分配内存节点的图一致地启动到同一流中来避免这种成本。</p>
<h3 id="G-5-1-First-Launch-cudaGraphUpload"><a href="#G-5-1-First-Launch-cudaGraphUpload" class="headerlink" title="G.5.1. First Launch / cudaGraphUpload"></a>G.5.1. First Launch / cudaGraphUpload</h3><p>在图实例化期间无法分配或映射物理内存，因为图将在其中执行的流是未知的。 映射是在图形启动期间完成的。 调用 <code>cudaGraphUpload</code> 可以通过立即执行该图的所有映射并将该图与上传流相关联，将分配成本与启动分开。 如果图随后启动到同一流中，它将启动而无需任何额外的重新映射。</p>
<p>使用不同的流进行图上传和图启动的行为类似于切换流，可能会导致重新映射操作。 此外，允许无关的内存池管理从空闲流中提取内存，这可能会抵消上传的影响。</p>
<h2 id="G-6-Physical-Memory-Footprint"><a href="#G-6-Physical-Memory-Footprint" class="headerlink" title="G.6. Physical Memory Footprint"></a>G.6. Physical Memory Footprint</h2><p>异步分配的池管理行为意味着销毁包含内存节点的图（即使它们的分配是空闲的）不会立即将物理内存返回给操作系统以供其他进程使用。要显式将内存释放回操作系统，应用程序应使用 <code>cudaDeviceGraphMemTrim</code> API。</p>
<p><code>cudaDeviceGraphMemTrim</code> 将取消映射并释放由图形内存节点保留的未主动使用的任何物理内存。尚未释放的分配和计划或运行的图被认为正在积极使用物理内存，不会受到影响。使用修剪 API 将使物理内存可用于其他分配 API 和其他应用程序或进程，但会导致 CUDA 在下次启动修剪图时重新分配和重新映射内存。请注意，<code>cudaDeviceGraphMemTrim</code> 在与 <code>cudaMemPoolTrimTo()</code> 不同的池上运行。图形内存池不会暴露给流序内存分配器。 CUDA 允许应用程序通过 <code>cudaDeviceGetGraphMemAttribute</code> API 查询其图形内存占用量。查询属性 <code>cudaGraphMemAttrReservedMemCurrent</code> 返回驱动程序为当前进程中的图形分配保留的物理内存量。查询 <code>cudaGraphMemAttrUsedMemCurrent</code> 返回至少一个图当前映射的物理内存量。这些属性中的任何一个都可用于跟踪 CUDA 何时为分配图而获取新的物理内存。这两个属性对于检查共享机制节省了多少内存都很有用。</p>
<h2 id="G-7-Peer-Access"><a href="#G-7-Peer-Access" class="headerlink" title="G.7. Peer Access"></a>G.7. Peer Access</h2><p>图分配可以配置为从多个 GPU 访问，在这种情况下，CUDA 将根据需要将分配映射到对等 GPU。 CUDA 允许需要不同映射的图分配重用相同的虚拟地址。 发生这种情况时，地址范围将映射到不同分配所需的所有 GPU。 这意味着分配有时可能允许比其创建期间请求的更多对等访问； 然而，依赖这些额外的映射仍然是一个错误。</p>
<h3 id="G-7-1-Peer-Access-with-Graph-Node-APIs"><a href="#G-7-1-Peer-Access-with-Graph-Node-APIs" class="headerlink" title="G.7.1. Peer Access with Graph Node APIs"></a>G.7.1. Peer Access with Graph Node APIs</h3><p><code>cudaGraphAddMemAllocNode</code> API 接受节点参数结构的 <code>accessDescs</code> 数组字段中的映射请求。 <code>poolProps.location</code> 嵌入式结构指定分配的常驻设备。 假设需要来自分配 GPU 的访问，因此应用程序不需要在 <code>accessDescs</code> 数组中为常驻设备指定条目。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cudaMemAllocNodeParams params = &#123;&#125;;</span><br><span class="line">params.poolProps.allocType = cudaMemAllocationTypePinned;</span><br><span class="line">params.poolProps.location.type = cudaMemLocationTypeDevice;</span><br><span class="line"><span class="comment">// specify device 1 as the resident device</span></span><br><span class="line">params.poolProps.location.id = <span class="number">1</span>;</span><br><span class="line">params.bytesize = size;</span><br><span class="line"></span><br><span class="line"><span class="comment">// allocate an allocation resident on device 1 accessible from device 1</span></span><br><span class="line"><span class="built_in">cudaGraphAddMemAllocNode</span>(&amp;allocNode, graph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;params);</span><br><span class="line"></span><br><span class="line">accessDescs[<span class="number">2</span>];</span><br><span class="line"><span class="comment">// boilerplate for the access descs (only ReadWrite and Device access supported by the add node api)</span></span><br><span class="line">accessDescs[<span class="number">0</span>].flags = cudaMemAccessFlagsProtReadWrite;</span><br><span class="line">accessDescs[<span class="number">0</span>].location.type = cudaMemLocationTypeDevice;</span><br><span class="line">accessDescs[<span class="number">1</span>].flags = cudaMemAccessFlagsProtReadWrite;</span><br><span class="line">accessDescs[<span class="number">1</span>].location.type = cudaMemLocationTypeDevice;</span><br><span class="line"></span><br><span class="line"><span class="comment">// access being requested for device 0 &amp; 2.  Device 1 access requirement left implicit.</span></span><br><span class="line">accessDescs[<span class="number">0</span>].location.id = <span class="number">0</span>;</span><br><span class="line">accessDescs[<span class="number">1</span>].location.id = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// access request array has 2 entries.</span></span><br><span class="line">params.accessDescCount = <span class="number">2</span>;</span><br><span class="line">params.accessDescs = accessDescs;</span><br><span class="line"></span><br><span class="line"><span class="comment">// allocate an allocation resident on device 1 accessible from devices 0, 1 and 2. (0 &amp; 2 from the descriptors, 1 from it being the resident device).</span></span><br><span class="line"><span class="built_in">cudaGraphAddMemAllocNode</span>(&amp;allocNode, graph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;params);</span><br></pre></td></tr></table></figure>
<h3 id="G-7-2-Peer-Access-with-Stream-Capture"><a href="#G-7-2-Peer-Access-with-Stream-Capture" class="headerlink" title="G.7.2. Peer Access with Stream Capture"></a>G.7.2. Peer Access with Stream Capture</h3><p>对于流捕获，分配节点在捕获时记录分配池的对等可访问性。 在捕获 <code>cudaMallocFromPoolAsync</code> 调用后更改分配池的对等可访问性不会影响图将为分配进行的映射。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// boilerplate for the access descs (only ReadWrite and Device access supported by the add node api)</span></span><br><span class="line">accessDesc.flags = cudaMemAccessFlagsProtReadWrite;</span><br><span class="line">accessDesc.location.type = cudaMemLocationTypeDevice;</span><br><span class="line">accessDesc.location.id = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// let memPool be resident and accessible on device 0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaStreamBeginCapture</span>(stream);</span><br><span class="line"><span class="built_in">cudaMallocAsync</span>(&amp;dptr1, size, memPool, stream);</span><br><span class="line"><span class="built_in">cudaStreamEndCapture</span>(stream, &amp;graph1);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaMemPoolSetAccess</span>(memPool, &amp;accessDesc, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cudaStreamBeginCapture</span>(stream);</span><br><span class="line"><span class="built_in">cudaMallocAsync</span>(&amp;dptr2, size, memPool, stream);</span><br><span class="line"><span class="built_in">cudaStreamEndCapture</span>(stream, &amp;graph2);</span><br><span class="line"></span><br><span class="line"><span class="comment">//The graph node allocating dptr1 would only have the device 0 accessibility even though memPool now has device 1 accessibility.</span></span><br><span class="line"><span class="comment">//The graph node allocating dptr2 will have device 0 and device 1 accessibility, since that was the pool accessibility at the time of the cudaMallocAsync call.</span></span><br></pre></td></tr></table></figure>
<h1 id="附录H-数学方法"><a href="#附录H-数学方法" class="headerlink" title="附录H 数学方法"></a>附录H 数学方法</h1><p>参考手册列出了设备代码中支持的 C/C++ 标准库数学函数的所有函数及其描述，以及所有内部函数（仅在设备代码中支持）。</p>
<p>本附录在适用时提供了其中一些功能的准确性信息。它使用 ULP 进行量化。有关最后位置单元 (ULP: Unit in the Last Place, 上面是直译的,这里可以理解为最小精度单元) 定义的更多信息，请参阅 Jean-Michel Muller’s paper On the definition of ulp(x), RR-5504, LIP RR-2005-09, INRIA, LIP. 2005, pp.16 at <a href="https://hal.inria.fr/inria-00070503/document">https://hal.inria.fr/inria-00070503/document</a></p>
<p>设备代码中支持的数学函数不设置全局 <code>errno</code> 变量，也不报告任何浮点异常来指示错误；因此，如果需要错误诊断机制，用户应该对函数的输入和输出实施额外的筛选。用户负责指针参数的有效性。用户不得将未初始化的参数传递给数学函数，因为这可能导致未定义的行为：函数在用户程序中内联，因此受到编译器优化的影响。</p>
<h2 id="H-1-Standard-Functions"><a href="#H-1-Standard-Functions" class="headerlink" title="H.1. Standard Functions"></a>H.1. Standard Functions</h2><p>本节中的函数可用于主机和设备代码。</p>
<p>本节指定每个函数在设备上执行时的错误范围，以及在主机不提供函数的情况下在主机上执行时的错误范围。</p>
<p>错误界限是从广泛但并非详尽的测试中生成的，因此它们不是保证界限。</p>
<h3 id="Single-Precision-Floating-Point-Functions"><a href="#Single-Precision-Floating-Point-Functions" class="headerlink" title="Single-Precision Floating-Point Functions"></a>Single-Precision Floating-Point Functions</h3><p>加法和乘法符合 IEEE 标准，因此最大误差为 0.5 ulp。</p>
<p>将单精度浮点操作数舍入为整数的推荐方法是 <code>rintf()</code>，而不是 <code>roundf()</code>。 原因是 <code>roundf()</code> 映射到设备上的 4 条指令序列，而 <code>rintf()</code> 映射到单个指令。 <code>truncf()</code>、<code>ceilf()</code> 和 <code>floorf()</code> 也都映射到一条指令。</p>
<div class="tablenoborder"><a name="standard-functions__single-precision-stdlib" shape="rect">
                                 <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="standard-functions__single-precision-stdlib" class="table" frame="border" border="1" rules="all">
                                 <caption><span class="tablecap">Table 7. Single-Precision Mathematical Standard Library Functions with
                                       Maximum ULP Error</span>. <span class="desc tabledesc">The maximum error is stated as the absolute value of the
                                       difference in ulps between a correctly rounded single-precision
                                       result and the result returned by the CUDA library function.</span></caption>
                                 <thead class="thead" align="left">
                                    <tr class="row">
                                       <th class="entry" valign="top" width="40%" id="d117e25637" rowspan="1" colspan="1">Function</th>
                                       <th class="entry" valign="top" width="60%" id="d117e25640" rowspan="1" colspan="1">Maximum ulp error</th>
                                    </tr>
                                 </thead>
                                 <tbody class="tbody">
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">x+y</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">
                                          <p class="p">0 (IEEE-754 round-to-nearest-even)</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">x*y</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">
                                          <p class="p">0 (IEEE-754 round-to-nearest-even)</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">x/y</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">
                                          <p class="p">0 for compute capability ≥
                                             2 when compiled with <samp class="ph codeph">-prec-div=true</samp></p>
                                          <p class="p">2 (full range), otherwise</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">1/x</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">
                                          <p class="p">0 for compute capability ≥
                                             2 when compiled with <samp class="ph codeph">-prec-div=true</samp></p>
                                          <p class="p">1 (full range), otherwise</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1">
                                          <p class="p"><samp class="ph codeph">rsqrtf(x)</samp></p>
                                          <p class="p"><samp class="ph codeph">1/sqrtf(x)</samp></p>
                                       </td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">
                                          <p class="p">2 (full range)</p>
                                          <p class="p">Applies to <samp class="ph codeph">1/sqrtf(x)</samp> only when it is
                                             converted to <samp class="ph codeph">rsqrtf(x)</samp> by the compiler.
                                          </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">sqrtf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">
                                          <p class="p">0 when compiled with <samp class="ph codeph">-prec-sqrt=true</samp></p>
                                          <p class="p">Otherwise 1 for compute capability ≥
                                             5.2
                                          </p>
                                          <p class="p">and 3 for older architectures</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">cbrtf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">rcbrtf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">hypotf(x,y)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1"> 3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">rhypotf(x,y)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">norm3df(x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1"> 3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">rnorm3df(x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">norm4df(x,y,z,t)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1"> 3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">rnorm4df(x,y,z,t)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">normf(dim,arr)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1"> An error bound can't be provided because a fast algorithm is used with accuracy loss due to round-off </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">rnormf(dim,arr)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1"> An error bound can't be provided because a fast algorithm is used with accuracy loss due to round-off </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">expf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">exp2f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">exp10f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">expm1f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">logf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">log2f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">log10f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">log1pf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">sinf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">cosf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">tanf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">4 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">sincosf(x,sptr,cptr)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">sinpif(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">cospif(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">sincospif(x,sptr,cptr)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">asinf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">4 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">acosf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">atanf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">atan2f(y,x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">sinhf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">coshf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">tanhf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">asinhf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">acoshf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">4 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">atanhf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">3 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">powf(x,y)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">9 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">erff(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">erfcf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">4 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">erfinvf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">erfcinvf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">4 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">erfcxf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">4 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">normcdff(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">5 (full range)</td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">normcdfinvf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">5 (full range)</td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">lgammaf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">6 (outside interval -10.001 ... -2.264; larger
                                          inside)
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">tgammaf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">11 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">fmaf(x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">frexpf(x,exp)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">ldexpf(x,exp)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">scalbnf(x,n)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">scalblnf(x,l)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">logbf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">ilogbf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">j0f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">
                                          <p class="p">9 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 2.2 x
                                             10<sup class="ph sup">-6</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">j1f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">
                                          <p class="p">9 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 2.2 x
                                             10<sup class="ph sup">-6</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">jnf(n,x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">
                                          For n = 128, the maximum absolute error is 2.2 x 10<sup class="ph sup">-6</sup></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">y0f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">
                                          <p class="p">9 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 2.2 x
                                             10<sup class="ph sup">-6</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">y1f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">
                                          <p class="p">9 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 2.2 x
                                             10<sup class="ph sup">-6</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">ynf(n,x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">
                                          <p class="p">ceil(2 + 2.5n) for |x| &lt; n</p>
                                          <p class="p">otherwise, the maximum absolute error is 2.2 x
                                             10<sup class="ph sup">-6</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">cyl_bessel_i0f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">6 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">cyl_bessel_i1f(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">6 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">fmodf(x,y)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">remainderf(x,y)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">remquof(x,y,iptr)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">modff(x,iptr)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">fdimf(x,y)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">truncf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">roundf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">rintf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">nearbyintf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">ceilf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">floorf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">lrintf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">lroundf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">llrintf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="40%" headers="d117e25637" rowspan="1" colspan="1"><samp class="ph codeph">llroundf(x)</samp></td>
                                       <td class="entry" valign="top" width="60%" headers="d117e25640" rowspan="1" colspan="1">0 (full range) </td>
                                    </tr>
                                 </tbody>
                              </table>
                           </div>



<h3 id="Double-Precision-Floating-Point-Functions"><a href="#Double-Precision-Floating-Point-Functions" class="headerlink" title="Double-Precision Floating-Point Functions"></a>Double-Precision Floating-Point Functions</h3><p>将双精度浮点操作数舍入为整数的推荐方法是 <code>rint()</code>，而不是 <code>round()</code>。 原因是 <code>round()</code> 映射到设备上的 5 条指令序列，而 <code>rint()</code> 映射到单个指令。 <code>trunc()、ceil() 和 floor()</code> 也都映射到一条指令。</p>
<table cellpadding="4" cellspacing="0" summary="" class="table" frame="border" border="1" rules="all">
                                 <caption><span class="tablecap"></span>. <span class="desc tabledesc"></span></caption>
                                 <thead class="thead" align="left">
                                    <tr class="row">
                                       <th class="entry" valign="top" width="51.690821256038646%" id="d117e26777" rowspan="1" colspan="1">Function</th>
                                       <th class="entry" valign="top" width="48.30917874396135%" id="d117e26780" rowspan="1" colspan="1">Maximum ulp error</th>
                                    </tr>
                                 </thead>
                                 <tbody class="tbody">
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">x+y</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1">
                                          <p class="p"> 0 (IEEE-754 round-to-nearest-even) </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">x*y</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1">
                                          <p class="p"> 0 (IEEE-754 round-to-nearest-even) </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">x/y</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1">
                                          <p class="p"> 0 (IEEE-754 round-to-nearest-even) </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">1/x</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1">
                                          <p class="p">0 (IEEE-754 round-to-nearest-even)</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">sqrt(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (IEEE-754 round-to-nearest-even) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">rsqrt(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1">
                                          <p class="p"> 1 (full range) </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">cbrt(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">rcbrt(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">hypot(x,y)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">rhypot(x,y)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">norm3d(x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">rnorm3d(x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">norm4d(x,y,z,t)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">rnorm4d(x,y,z,t)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">norm(dim,arr)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> An error bound can't be provided because a fast algorithm is used with accuracy loss due to round-off </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">rnorm(dim,arr)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> An error bound can't be provided because a fast algorithm is used with accuracy loss due to round-off </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">exp(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">exp2(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">exp10(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">expm1(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">log(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">log2(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">log10(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">log1p(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">sin(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">cos(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">tan(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">sincos(x,sptr,cptr)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">sinpi(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">cospi(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">sincospi(x,sptr,cptr)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range)</td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">asin(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">acos(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">atan(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">atan2(y,x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">sinh(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">cosh(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">tanh(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 1 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">asinh(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">acosh(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">atanh(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">pow(x,y)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">erf(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 2 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">erfc(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 5 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">erfinv(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 5 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">erfcinv(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 6 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">erfcx(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 4 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">normcdf(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 5 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">normcdfinv(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 8 (full range)</td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">lgamma(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 4 (outside interval -11.0001 ... -2.2637; larger
                                          inside)
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">tgamma(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 8 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">fma(x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (IEEE-754 round-to-nearest-even) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">frexp(x,exp)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">ldexp(x,exp)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">scalbn(x,n)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">scalbln(x,l)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">logb(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">ilogb(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">j0(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1">
                                          <p class="p"> 7 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 5 x
                                             10<sup class="ph sup">-12</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">j1(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1">
                                          <p class="p"> 7 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 5 x
                                             10<sup class="ph sup">-12</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">jn(n,x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1">
                                          For n = 128, the maximum absolute error is 5 x 10<sup class="ph sup">-12</sup></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">y0(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1">
                                          <p class="p"> 7 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 5 x
                                             10<sup class="ph sup">-12</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">y1(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1">
                                          <p class="p"> 7 for |x| &lt; 8</p>
                                          <p class="p">otherwise, the maximum absolute error is 5 x
                                             10<sup class="ph sup">-12</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">yn(n,x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1">
                                          <p class="p">For |x| &gt; 1.5n, the maximum absolute error is 5 x
                                             10<sup class="ph sup">-12</sup></p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">cyl_bessel_i0(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 6 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">cyl_bessel_i1(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 6 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">fmod(x,y)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">remainder(x,y)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">remquo(x,y,iptr)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">modf(x,iptr)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">fdim(x,y)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">trunc(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">round(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">rint(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">nearbyint(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">ceil(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">floor(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">lrint(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">lround(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">llrint(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="51.690821256038646%" headers="d117e26777" rowspan="1" colspan="1"><samp class="ph codeph">llround(x)</samp></td>
                                       <td class="entry" valign="top" width="48.30917874396135%" headers="d117e26780" rowspan="1" colspan="1"> 0 (full range) </td>
                                    </tr>
                                 </tbody>
                              </table>



<h2 id="H-2-Intrinsic-Functions"><a href="#H-2-Intrinsic-Functions" class="headerlink" title="H.2. Intrinsic Functions"></a>H.2. Intrinsic Functions</h2><p>本节中的函数只能在设备代码中使用。</p>
<p>在这些函数中，有一些标准函数的精度较低但速度更快的版本。它们具有相同的名称，前缀为 <strong>（例如 </strong>sinf(x)）。 它们更快，因为它们映射到更少的本机指令。 编译器有一个选项 (-use_fast_math)，它强制下表 中的每个函数编译为其内在对应项。 除了降低受影响函数的准确性外，还可能导致特殊情况处理的一些差异。 一种更健壮的方法是通过调用内联函数来选择性地替换数学函数调用，仅在性能增益值得考虑的情况下以及可以容忍更改的属性（例如降低的准确性和不同的特殊情况处理）的情况下。</p>
<div class="tablenoborder"><a name="intrinsic-functions__functions-affected-use-fast-math" shape="rect">
                              <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="intrinsic-functions__functions-affected-use-fast-math" class="table" frame="border" border="1" rules="all">
                              <caption><span class="tablecap">Table 9. Functions Affected by -use_fast_math</span></caption>
                              <thead class="thead" align="left">
                                 <tr class="row">
                                    <th class="entry" valign="top" width="50%" id="d117e27876" rowspan="1" colspan="1">Operator/Function</th>
                                    <th class="entry" valign="top" width="50%" id="d117e27879" rowspan="1" colspan="1">Device Function</th>
                                 </tr>
                              </thead>
                              <tbody class="tbody">
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d117e27876" rowspan="1" colspan="1"><samp class="ph codeph">x/y</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d117e27879" rowspan="1" colspan="1">
                                       <p class="p"><samp class="ph codeph">__fdividef(x,y)</samp></p>
                                    </td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d117e27876" rowspan="1" colspan="1"><samp class="ph codeph">sinf(x)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d117e27879" rowspan="1" colspan="1">
                                       <p class="p"><samp class="ph codeph">__sinf(x)</samp></p>
                                    </td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d117e27876" rowspan="1" colspan="1"><samp class="ph codeph">cosf(x)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d117e27879" rowspan="1" colspan="1">
                                       <p class="p"><samp class="ph codeph">__cosf(x)</samp></p>
                                    </td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d117e27876" rowspan="1" colspan="1"><samp class="ph codeph">tanf(x) </samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d117e27879" rowspan="1" colspan="1"><samp class="ph codeph">__tanf(x)</samp></td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d117e27876" rowspan="1" colspan="1"><samp class="ph codeph">sincosf(x,sptr,cptr)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d117e27879" rowspan="1" colspan="1"><samp class="ph codeph">__sincosf(x,sptr,cptr)</samp></td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d117e27876" rowspan="1" colspan="1"><samp class="ph codeph">logf(x)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d117e27879" rowspan="1" colspan="1">
                                       <p class="p"><samp class="ph codeph">__logf(x)</samp></p>
                                    </td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d117e27876" rowspan="1" colspan="1"><samp class="ph codeph">log2f(x)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d117e27879" rowspan="1" colspan="1"><samp class="ph codeph">__log2f(x)</samp></td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d117e27876" rowspan="1" colspan="1"><samp class="ph codeph">log10f(x)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d117e27879" rowspan="1" colspan="1"><samp class="ph codeph">__log10f(x)</samp></td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d117e27876" rowspan="1" colspan="1"><samp class="ph codeph">expf(x)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d117e27879" rowspan="1" colspan="1"><samp class="ph codeph">__expf(x)</samp></td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d117e27876" rowspan="1" colspan="1"><samp class="ph codeph">exp10f(x)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d117e27879" rowspan="1" colspan="1"><samp class="ph codeph">__exp10f(x)</samp></td>
                                 </tr>
                                 <tr class="row">
                                    <td class="entry" valign="top" width="50%" headers="d117e27876" rowspan="1" colspan="1"><samp class="ph codeph">powf(x,y)</samp></td>
                                    <td class="entry" valign="top" width="50%" headers="d117e27879" rowspan="1" colspan="1"><samp class="ph codeph">__powf(x,y)</samp></td>
                                 </tr>
                              </tbody>
                           </table>
                        </div>



<h3 id="Single-Precision-Floating-Point-Functions-1"><a href="#Single-Precision-Floating-Point-Functions-1" class="headerlink" title="Single-Precision Floating-Point Functions"></a>Single-Precision Floating-Point Functions</h3><p><code>__fadd_[rn,rz,ru,rd]()</code> 和 <code>__fmul_[rn,rz,ru,rd]()</code> 映射到编译器从不合并到 <code>FMAD</code> 中的加法和乘法运算。相比之下，由“*”和“+”运算符生成的加法和乘法将经常组合到 FMAD 中。</p>
<p>以 <code>_rn</code> 为后缀的函数使用舍入到最接近的偶数舍入模式运行。</p>
<p>以 <code>_rz</code> 为后缀的函数使用向零舍入模式进行舍入操作。</p>
<p>以 <code>_ru</code> 为后缀的函数使用向上舍入（到正无穷大）舍入模式运行。</p>
<p>以 <code>_rd</code> 为后缀的函数使用向下舍入（到负无穷大）舍入模式进行操作。</p>
<p>浮点除法的准确性取决于代码是使用 <code>-prec-div=false</code> 还是 <code>-prec-div=true</code> 编译的。使用<code>-prec-div=false</code>编译代码时，正则除法/运算符和<code>__fdividef(x,y)</code>精度相同，但对于2<sup class="ph sup">126</sup> &lt; <samp class="ph codeph">|y|</samp> &lt;2<sup class="ph sup">128</sup>，<code>__fdividef(x,y)</code> 提供的结果为零，而 / 运算符提供的正确结果在下表 中规定的精度范围内。此外，对于 2<sup class="ph sup">126</sup> &lt; <samp class="ph codeph">|y|</samp> &lt;2<sup class="ph sup">128</sup>，如果 x 为无穷大，则 <code>__fdividef(x,y)</code>提供 NaN（作为无穷大乘以零的结果），而 / 运算符返回无穷大。另一方面，当使用 <code>-prec-div=true</code> 或根本没有任何 <code>-prec-div</code> 选项编译代码时， / 运算符符合 IEEE 标准，因为它的默认值为 true。</p>
<div class="tablenoborder"><a name="intrinsic-functions__single-precision-floating-point-intrinsic-functions-supported-by-cuda-runtime-library" shape="rect">
                                 <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="intrinsic-functions__single-precision-floating-point-intrinsic-functions-supported-by-cuda-runtime-library" class="table" frame="border" border="1" rules="all">
                                 <caption><span class="tablecap"></span></caption>
                                 <thead class="thead" align="left">
                                    <tr class="row">
                                       <th class="entry" valign="top" width="50%" id="d117e28195" rowspan="1" colspan="1">Function</th>
                                       <th class="entry" valign="top" width="50%" id="d117e28198" rowspan="1" colspan="1">Error bounds</th>
                                    </tr>
                                 </thead>
                                 <tbody class="tbody">
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__fadd_[rn,rz,ru,rd](x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">
                                          <p class="p">  IEEE-compliant. </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__fsub_[rn,rz,ru,rd](x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">
                                          <p class="p">  IEEE-compliant. </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__fmul_[rn,rz,ru,rd](x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">
                                          <p class="p">  IEEE-compliant. </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__fmaf_[rn,rz,ru,rd](x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">
                                          <p class="p">  IEEE-compliant. </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__frcp_[rn,rz,ru,rd](x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">  IEEE-compliant. </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__fsqrt_[rn,rz,ru,rd](x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">  IEEE-compliant. </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__frsqrt_rn(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1"> IEEE-compliant. </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__fdiv_[rn,rz,ru,rd](x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">
                                          <p class="p"> IEEE-compliant. </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__fdividef(x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">For <samp class="ph codeph">|y|</samp> in [2<sup class="ph sup">-126</sup>,
                                          2<sup class="ph sup">126</sup>], the maximum ulp error is 2.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__expf(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">The maximum ulp error is <samp class="ph codeph">2 + floor(abs(1.16 *
                                             x))</samp>.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__exp10f(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">The maximum ulp error is <samp class="ph codeph">2+ floor(abs(2.95 *
                                             x))</samp>.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__logf(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">For <samp class="ph codeph">x</samp> in [0.5, 2], the maximum absolute
                                          error is 2<sup class="ph sup">-21.41</sup>, otherwise, the maximum ulp error
                                          is 3.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__log2f(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">For <samp class="ph codeph">x</samp> in [0.5, 2], the maximum absolute
                                          error is 2<sup class="ph sup">-22</sup>, otherwise, the maximum ulp error is
                                          2.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__log10f(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">For <samp class="ph codeph">x</samp> in [0.5, 2], the maximum absolute
                                          error is 2<sup class="ph sup">-24</sup>, otherwise, the maximum ulp error is
                                          3.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__sinf(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">For <samp class="ph codeph">x</samp> in [-π,π], the maximum absolute
                                          error is 2<sup class="ph sup">-21.41</sup>, and larger otherwise.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__cosf(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">For <samp class="ph codeph">x</samp> in [-π,π], the maximum absolute
                                          error is 2<sup class="ph sup">-21.19</sup>, and larger otherwise.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__sincosf(x,sptr,cptr)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">Same as <samp class="ph codeph">__sinf(x)</samp> and
                                          <samp class="ph codeph">__cosf(x)</samp>.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__tanf(x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">Derived from its implementation as <samp class="ph codeph">__sinf(x) *
                                             (1/__cosf(x))</samp>.
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28195" rowspan="1" colspan="1"><samp class="ph codeph">__powf(x, y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28198" rowspan="1" colspan="1">Derived from its implementation as <samp class="ph codeph">exp2f(y *
                                             __log2f(x))</samp>.
                                       </td>
                                    </tr>
                                 </tbody>
                              </table>
                           </div>



<h3 id="Double-Precision-Floating-Point-Functions-1"><a href="#Double-Precision-Floating-Point-Functions-1" class="headerlink" title="Double-Precision Floating-Point Functions"></a>Double-Precision Floating-Point Functions</h3><p><code>__dadd_rn()</code> 和 <code>__dmul_rn()</code> 映射到编译器从不合并到 FMAD 中的加法和乘法运算。 相比之下，由“*”和“+”运算符生成的加法和乘法将经常组合到 FMAD 中。</p>
<table cellpadding="4" cellspacing="0" summary="" class="table" frame="border" border="1" rules="all">
                                 <caption><span class="tablecap">Table 11. Double-Precision Floating-Point Intrinsic Functions</span>. <span class="desc tabledesc">(Supported by the CUDA Runtime Library with Respective Error Bounds)</span></caption>
                                 <thead class="thead" align="left">
                                    <tr class="row">
                                       <th class="entry" valign="top" width="50%" id="d117e28543" rowspan="1" colspan="1">Function</th>
                                       <th class="entry" valign="top" width="50%" id="d117e28546" rowspan="1" colspan="1">Error bounds</th>
                                    </tr>
                                 </thead>
                                 <tbody class="tbody">
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28543" rowspan="1" colspan="1"><samp class="ph codeph">__dadd_[rn,rz,ru,rd](x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28546" rowspan="1" colspan="1">
                                          <p class="p">IEEE-compliant.</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28543" rowspan="1" colspan="1"><samp class="ph codeph">__dsub_[rn,rz,ru,rd](x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28546" rowspan="1" colspan="1">
                                          <p class="p">IEEE-compliant.</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28543" rowspan="1" colspan="1"><samp class="ph codeph">__dmul_[rn,rz,ru,rd](x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28546" rowspan="1" colspan="1">
                                          <p class="p">IEEE-compliant.</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28543" rowspan="1" colspan="1"><samp class="ph codeph">__fma_[rn,rz,ru,rd](x,y,z)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28546" rowspan="1" colspan="1">
                                          <p class="p">IEEE-compliant.</p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28543" rowspan="1" colspan="1"><samp class="ph codeph">__ddiv_[rn,rz,ru,rd](x,y)(x,y)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28546" rowspan="1" colspan="1">
                                          <p class="p">IEEE-compliant.</p>
                                          <p class="p">
                                             Requires compute capability <u class="ph u">&gt;</u> 2.
                                          </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28543" rowspan="1" colspan="1"><samp class="ph codeph">__drcp_[rn,rz,ru,rd](x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28546" rowspan="1" colspan="1">
                                          <p class="p"> IEEE-compliant.</p>
                                          <p class="p">
                                             Requires compute capability <u class="ph u">&gt;</u> 2.
                                          </p>
                                       </td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e28543" rowspan="1" colspan="1"><samp class="ph codeph">__dsqrt_[rn,rz,ru,rd](x)</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e28546" rowspan="1" colspan="1">
                                          <p class="p"> IEEE-compliant.</p>
                                          <p class="p">
                                             Requires compute capability <u class="ph u">&gt;</u> 2.  
                                          </p>
                                       </td>
                                    </tr>
                                 </tbody>
                              </table>



<h1 id="附录I-C-语言支持"><a href="#附录I-C-语言支持" class="headerlink" title="附录I C++ 语言支持"></a>附录I C++ 语言支持</h1><p>如使用 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compilation-with-nvcc">NVCC 编译中所述</a>，使用 nvcc 编译的 CUDA 源文件可以包含主机代码和设备代码的混合。 CUDA 前端编译器旨在模拟主机编译器对 C++ 输入代码的行为。 输入源代码根据 C++ ISO/IEC 14882:2003、C++ ISO/IEC 14882:2011、C++ ISO/IEC 14882:2014 或 C++ ISO/IEC 14882:2017 规范进行处理，CUDA 前端编译器旨在模拟 任何主机编译器与 ISO 规范的差异。 此外，支持的语言使用本<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_15">文档</a> 中描述的特定于 CUDA 的结构进行了扩展，并受到下面描述的限制。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cpp11-language-features">C++11 语言特性</a>、<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cpp14-language-features">C++14</a> 语言特性和 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cpp17-language-features">C++17</a> 语言特性分别为 C++11、C++14 和 C++17 特性提供支持矩阵。 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#restrictions">限制</a>列出了语言限制。 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#polymorphic-function-wrappers">多态函数包装器</a>和<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#extended-lambda">扩展 Lambda</a> 描述了其他特性。 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#code-samples">代码示例</a>提供代码示例。</p>
<h2 id="I-1-C-11-Language-Features"><a href="#I-1-C-11-Language-Features" class="headerlink" title="I.1. C++11 Language Features"></a>I.1. C++11 Language Features</h2><p>下表列出了已被 C++11 标准接受的新语言功能。 “Proposal”列提供了描述该功能的 ISO C++ 委员会提案的链接，而“Available in nvcc (device code)”列表示包含此功能实现的第一个 nvcc 版本（如果已实现） ) 用于设备代码。</p>
<div class="tablenoborder"><a name="cpp11-language-features__cpp11-language-features-support-matrix" shape="rect">
                              <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="cpp11-language-features__cpp11-language-features-support-matrix" class="table" frame="border" border="1" rules="all">
                              <caption><span class="tablecap">Table 12. C++11 Language Features</span></caption>
                              <thead class="thead" align="left">
                                 <tr class="row" valign="middle">
                                    <th class="entry" align="left" valign="middle" width="71.42857142857143%" id="d117e28759" rowspan="1" colspan="1">Language Feature</th>
                                    <th class="entry" align="center" valign="middle" width="14.285714285714285%" id="d117e28762" rowspan="1" colspan="1">C++11 Proposal</th>
                                    <th class="entry" align="center" valign="middle" width="14.285714285714285%" id="d117e28765" rowspan="1" colspan="1">Available in nvcc (device code)</th>
                                 </tr>
                              </thead>
                              <tbody class="tbody">
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Rvalue references</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n2118.html" target="_blank" shape="rect">N2118</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">
                                       &nbsp;&nbsp;&nbsp;&nbsp;Rvalue references for <samp class="ph codeph">*this</samp></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2439.htm" target="_blank" shape="rect">N2439</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Initialization of class objects by rvalues</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2004/n1610.html" target="_blank" shape="rect">N1610</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Non-static data member initializers</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2008/n2756.htm" target="_blank" shape="rect">N2756</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Variadic templates</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2242.pdf" target="_blank" shape="rect">N2242</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">&nbsp;&nbsp;&nbsp;&nbsp;Extending variadic template template parameters</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2555.pdf" target="_blank" shape="rect">N2555</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Initializer lists</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2672.htm" target="_blank" shape="rect">N2672</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Static assertions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2004/n1720.html" target="_blank" shape="rect">N1720</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1"><samp class="ph codeph">auto</samp>-typed variables 
                                    </td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n1984.pdf" target="_blank" shape="rect">N1984</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">
                                       &nbsp;&nbsp;&nbsp;&nbsp;Multi-declarator <samp class="ph codeph">auto</samp></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2004/n1737.pdf" target="_blank" shape="rect">N1737</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">&nbsp;&nbsp;&nbsp;&nbsp;Removal of auto as a storage-class specifier</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2546.htm" target="_blank" shape="rect">N2546</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">&nbsp;&nbsp;&nbsp;&nbsp;New function declarator syntax</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2541.htm" target="_blank" shape="rect">N2541</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Lambda expressions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2009/n2927.pdf" target="_blank" shape="rect">N2927</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Declared type of an expression</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2343.pdf" target="_blank" shape="rect">N2343</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">&nbsp;&nbsp;&nbsp;&nbsp;Incomplete return types</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2011/n3276.pdf" target="_blank" shape="rect">N3276</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Right angle brackets</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2005/n1757.html" target="_blank" shape="rect">N1757</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Default template arguments for function templates</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/cwg_defects.html#226" target="_blank" shape="rect">DR226</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Solving the SFINAE problem for expressions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2634.html" target="_blank" shape="rect">DR339</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Alias templates</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2258.pdf" target="_blank" shape="rect">N2258</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Extern templates</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n1987.htm" target="_blank" shape="rect">N1987</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Null pointer constant</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2431.pdf" target="_blank" shape="rect">N2431</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Strongly-typed enums</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2347.pdf" target="_blank" shape="rect">N2347</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Forward declarations for enums</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2764.pdf" target="_blank" shape="rect">N2764</a><br clear="none"></br><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/cwg_defects.html#1206" target="_blank" shape="rect">DR1206</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Standardized attribute syntax</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2761.pdf" target="_blank" shape="rect">N2761</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Generalized constant expressions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2235.pdf" target="_blank" shape="rect">N2235</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Alignment support</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2341.pdf" target="_blank" shape="rect">N2341</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Conditionally-support behavior</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2004/n1627.pdf" target="_blank" shape="rect">N1627</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Changing undefined behavior into diagnosable errors</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2004/n1727.pdf" target="_blank" shape="rect">N1727</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Delegating constructors</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n1986.pdf" target="_blank" shape="rect">N1986</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Inheriting constructors</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2540.htm" target="_blank" shape="rect">N2540</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Explicit conversion operators</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2437.pdf" target="_blank" shape="rect">N2437</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">New character types</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2249.html" target="_blank" shape="rect">N2249</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Unicode string literals</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2442.htm" target="_blank" shape="rect">N2442</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Raw string literals</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2442.htm" target="_blank" shape="rect">N2442</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Universal character names in literals</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2170.html" target="_blank" shape="rect">N2170</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">User-defined literals</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2765.pdf" target="_blank" shape="rect">N2765</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Standard Layout Types</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2342.htm" target="_blank" shape="rect">N2342</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Defaulted functions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2346.htm" target="_blank" shape="rect">N2346</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Deleted functions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2346.htm" target="_blank" shape="rect">N2346</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Extended friend declarations</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2005/n1791.pdf" target="_blank" shape="rect">N1791</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">
                                       Extending <samp class="ph codeph">sizeof</samp></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2253.html" target="_blank" shape="rect">N2253</a><br clear="none"></br><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/cwg_defects.html#850" target="_blank" shape="rect">DR850</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Inline namespaces</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2535.htm" target="_blank" shape="rect">N2535</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Unrestricted unions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2544.pdf" target="_blank" shape="rect">N2544</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Local and unnamed types as template arguments</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2657.htm" target="_blank" shape="rect">N2657</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Range-based for</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2009/n2930.html" target="_blank" shape="rect">N2930</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Explicit virtual overrides</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/JTC1/SC22/WG21/docs/papers/2009/n2928.htm" target="_blank" shape="rect">N2928</a><br clear="none"></br><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2010/n3206.htm" target="_blank" shape="rect">N3206</a><br clear="none"></br><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2011/n3272.htm" target="_blank" shape="rect">N3272</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Minimal support for garbage collection and reachability-based leak detection</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2670.htm" target="_blank" shape="rect">N2670</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">
                                       N/A (see <a class="xref" href="index.html#restrictions" shape="rect">Restrictions</a>)
                                    </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Allowing move constructors to throw [noexcept]</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2010/n3050.html" target="_blank" shape="rect">N3050</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Defining move special member functions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2010/n3053.html" target="_blank" shape="rect">N3053</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d117e28759 d117e28762 d117e28765" rowspan="1"><strong class="ph b">Concurrency</strong></td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Sequence points</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2239.html" target="_blank" shape="rect">N2239</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Atomic operations</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2427.html" target="_blank" shape="rect">N2427</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Strong Compare and Exchange</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2748.html" target="_blank" shape="rect">N2748</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Bidirectional Fences</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2752.htm" target="_blank" shape="rect">N2752</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Memory model</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2429.htm" target="_blank" shape="rect">N2429</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Data-dependency ordering: atomics and memory model</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2664.htm" target="_blank" shape="rect">N2664</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Propagating exceptions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2179.html" target="_blank" shape="rect">N2179</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Allow atomics use in signal handlers</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2547.htm" target="_blank" shape="rect">N2547</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Thread-local storage</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2659.htm" target="_blank" shape="rect">N2659</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Dynamic initialization and destruction with concurrency</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2008/n2660.htm" target="_blank" shape="rect">N2660</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d117e28759 d117e28762 d117e28765" rowspan="1"><strong class="ph b">C99 Features in C++11</strong></td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1"><samp class="ph codeph">__func__</samp> predefined identifier
                                    </td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2007/n2340.htm" target="_blank" shape="rect">N2340</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">C99 preprocessor</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2004/n1653.htm" target="_blank" shape="rect">N1653</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1"><samp class="ph codeph">long long</samp></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2005/n1811.pdf" target="_blank" shape="rect">N1811</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">7.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e28759" rowspan="1" colspan="1">Extended integral types</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28762" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2006/n1988.pdf" target="_blank" shape="rect">N1988</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e28765" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                              </tbody>
                           </table>
                        </div>



<h2 id="I-2-C-14-Language-Features"><a href="#I-2-C-14-Language-Features" class="headerlink" title="I.2. C++14 Language Features"></a>I.2. C++14 Language Features</h2><p>下表列出了已被 C++14 标准接受的新语言功能。</p>
<div class="tablenoborder"><a name="cpp14-language-features__cpp14-language-features-support-matrix" shape="rect">
                              <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="cpp14-language-features__cpp14-language-features-support-matrix" class="table" frame="border" border="1" rules="all">
                              <caption><span class="tablecap">Table 13. C++14 Language Features</span></caption>
                              <thead class="thead" align="left">
                                 <tr class="row" valign="middle">
                                    <th class="entry" align="left" valign="middle" width="71.42857142857143%" id="d117e29817" rowspan="1" colspan="1">Language Feature</th>
                                    <th class="entry" align="center" valign="middle" width="14.285714285714285%" id="d117e29820" rowspan="1" colspan="1">C++14 Proposal</th>
                                    <th class="entry" align="center" valign="middle" width="14.285714285714285%" id="d117e29823" rowspan="1" colspan="1">Available in nvcc (device code)</th>
                                 </tr>
                              </thead>
                              <tbody class="tbody">
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e29817" rowspan="1" colspan="1">Tweak to certain C++ contextual conversions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29820" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3323.pdf" target="_blank" shape="rect">N3323</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29823" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e29817" rowspan="1" colspan="1">Binary literals</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29820" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3472.pdf" target="_blank" shape="rect">N3472</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29823" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e29817" rowspan="1" colspan="1">Functions with deduced return type</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29820" rowspan="1" colspan="1"><a class="xref" href="https://isocpp.org/files/papers/N3638.html" target="_blank" shape="rect">N3638</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29823" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e29817" rowspan="1" colspan="1">Generalized lambda capture (init-capture)</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29820" rowspan="1" colspan="1"><a class="xref" href="https://isocpp.org/files/papers/N3648.html" target="_blank" shape="rect">N3648</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29823" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e29817" rowspan="1" colspan="1">Generic (polymorphic) lambda expressions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29820" rowspan="1" colspan="1"><a class="xref" href="https://isocpp.org/files/papers/N3649.html" target="_blank" shape="rect">N3649</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29823" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e29817" rowspan="1" colspan="1">Variable templates</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29820" rowspan="1" colspan="1"><a class="xref" href="https://isocpp.org/files/papers/N3651.pdf" target="_blank" shape="rect">N3651</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29823" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e29817" rowspan="1" colspan="1">Relaxing requirements on constexpr functions</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29820" rowspan="1" colspan="1"><a class="xref" href="https://isocpp.org/files/papers/N3652.html" target="_blank" shape="rect">N3652</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29823" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e29817" rowspan="1" colspan="1">Member initializers and aggregates</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29820" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3653.html" target="_blank" shape="rect">N3653</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29823" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e29817" rowspan="1" colspan="1">Clarifying memory allocation</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29820" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3664.html" target="_blank" shape="rect">N3664</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29823" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e29817" rowspan="1" colspan="1">Sized deallocation</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29820" rowspan="1" colspan="1"><a class="xref" href="https://isocpp.org/files/papers/n3778.html" target="_blank" shape="rect">N3778</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29823" rowspan="1" colspan="1">&nbsp;</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e29817" rowspan="1" colspan="1"><samp class="ph codeph">[[deprecated]]</samp> attribute
                                    </td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29820" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3760.html" target="_blank" shape="rect">N3760</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29823" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="71.42857142857143%" headers="d117e29817" rowspan="1" colspan="1">Single-quotation-mark as a digit separator</td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29820" rowspan="1" colspan="1"><a class="xref" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3781.pdf" target="_blank" shape="rect">N3781</a></td>
                                    <td class="entry" align="center" valign="middle" width="14.285714285714285%" headers="d117e29823" rowspan="1" colspan="1">9.0</td>
                                 </tr>
                              </tbody>
                           </table>
                        </div>


<h2 id="I-3-C-17-Language-Features"><a href="#I-3-C-17-Language-Features" class="headerlink" title="I.3. C++17 Language Features"></a>I.3. C++17 Language Features</h2><p>nvcc 版本 11.0 及更高版本支持所有 C++17 语言功能，但受<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cpp17">此处</a>描述的限制的约束。</p>
<h2 id="I-4-Restrictions"><a href="#I-4-Restrictions" class="headerlink" title="I.4. Restrictions"></a>I.4. Restrictions</h2><h3 id="I-4-1-Host-Compiler-Extensions"><a href="#I-4-1-Host-Compiler-Extensions" class="headerlink" title="I.4.1. Host Compiler Extensions"></a>I.4.1. Host Compiler Extensions</h3><p>设备代码不支持主机编译器特定的语言扩展。</p>
<p><code>_Complex</code>类型仅在主机代码中受支持。</p>
<p>当与支持它的主机编译器一起编译时，设备代码中支持 <code>__int128</code> 类型。</p>
<p><code>__float128</code> 类型仅在 64 位 x86 Linux 平台上的主机代码中受支持。 <code>__float128</code> 类型的常量表达式可以由编译器以较低精度的浮点表示形式处理。</p>
<h3 id="I-4-2-Preprocessor-Symbols"><a href="#I-4-2-Preprocessor-Symbols" class="headerlink" title="I.4.2. Preprocessor Symbols"></a>I.4.2. Preprocessor Symbols</h3><h4 id="I-4-2-1-CUDA-ARCH"><a href="#I-4-2-1-CUDA-ARCH" class="headerlink" title="I.4.2.1. CUDA_ARCH"></a>I.4.2.1. <strong>CUDA_ARCH</strong></h4><ol>
<li><p>以下实体的类型签名不应取决于是否定义了 <code>__CUDA_ARCH__</code>，或者取决于 <code>__CUDA_ARCH__</code> 的特定值：</p>
<ul>
<li><code>__global__</code> 函数和函数模板</li>
<li><code>__device__</code> 和 <code>__constant__</code> 变量</li>
<li><p>纹理和表面</p>
<p>例子：</p>
</li>
</ul>
</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">if</span> !defined(__CUDA_ARCH__)</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">int</span> mytype;</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">double</span> mytype;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">__device__ mytype xxx;         <span class="comment">// error: xxx&#x27;s type depends on __CUDA_ARCH__</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">foo</span><span class="params">(mytype in, <span class="comment">// error: foo&#x27;s type depends on __CUDA_ARCH__</span></span></span></span><br><span class="line"><span class="params"><span class="function">                    mytype *ptr)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  *ptr = in;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol>
<li><p>如果 <code>__global__</code> 函数模板被实例化并从主机启动，则无论是否定义了 <code>__CUDA_ARCH__</code> 以及无论 <code>__CUDA_ARCH__</code> 的值如何，都必须使用相同的模板参数实例化该函数模板。</p>
<p> 例子：</p>
</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">int</span> result;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kern</span><span class="params">(T in)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  result = in;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__host__ __device__ <span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="meta">#<span class="keyword">if</span> !defined(__CUDA_ARCH__)</span></span><br><span class="line">  kern&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(<span class="number">1</span>);      <span class="comment">// error: &quot;kern&lt;int&gt;&quot; instantiation only</span></span><br><span class="line">                         <span class="comment">// when __CUDA_ARCH__ is undefined!</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">foo</span>();</span><br><span class="line">  <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol>
<li><p>在单独编译模式下，是否存在具有外部链接的函数或变量的定义不应取决于是否定义了 <code>__CUDA_ARCH__</code> 或 <code>__CUDA_ARCH__16</code> 的特定值。</p>
<p> 例子：</p>
</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">if</span> !defined(__CUDA_ARCH__)</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123; &#125;                  <span class="comment">// error: The definition of foo()</span></span><br><span class="line">                                    <span class="comment">// is only present when __CUDA_ARCH__</span></span><br><span class="line">                                    <span class="comment">// is undefined</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br></pre></td></tr></table></figure>
<ol>
<li><p>在单独的编译中， <code>__CUDA_ARCH__</code> 不得在头文件中使用，这样不同的对象可能包含不同的行为。 或者，必须保证所有对象都将针对相同的 compute_arch 进行编译。 如果在头文件中定义了弱函数或模板函数，并且其行为取决于 <code>__CUDA_ARCH__</code>，那么如果为不同的计算架构编译对象，则对象中该函数的实例可能会发生冲突。</p>
<p> 例如，如果 a.h 包含：</p>
</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">__device__ T* <span class="title">getptr</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="meta">#<span class="keyword">if</span> __CUDA_ARCH__ == 200</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">NULL</span>; <span class="comment">/* no address */</span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">  __shared__ T arr[<span class="number">256</span>];</span><br><span class="line">  <span class="keyword">return</span> arr;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后，如果 a.cu 和 b.cu 都包含 a.h 并为同一类型实例化 <code>getptr</code>，并且 b.cu 需要一个非 NULL 地址，则编译：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nvcc –arch=compute_20 –dc a.cu</span><br><span class="line">nvcc –arch=compute_30 –dc b.cu</span><br><span class="line">nvcc –arch=sm_30 a.o b.o</span><br></pre></td></tr></table></figure>
<p>在链接时只使用一个版本的 <code>getptr</code>，因此行为将取决于选择哪个版本。 为避免这种情况，必须为相同的计算架构编译 a.cu 和 b.cu，或者 <code>__CUDA_ARCH__</code> 不应在共享头函数中使用。</p>
<p>编译器不保证将为上述不受支持的 <code>__CUDA_ARCH__</code> 使用生成诊断。</p>
<h3 id="I-4-3-Qualifiers"><a href="#I-4-3-Qualifiers" class="headerlink" title="I.4.3. Qualifiers"></a>I.4.3. Qualifiers</h3><h4 id="I-4-3-1-Device-Memory-Space-Specifiers"><a href="#I-4-3-1-Device-Memory-Space-Specifiers" class="headerlink" title="I.4.3.1. Device Memory Space Specifiers"></a>I.4.3.1. Device Memory Space Specifiers</h4><p><code>__device__</code>、<code>__shared__</code>、<code>__managed__</code> 和 <code>__constant__</code> 内存空间说明符不允许用于：</p>
<ul>
<li>类、结构和联合数据成员，</li>
<li>形式参数，</li>
<li>在主机上执行的函数中的非外部变量声明。</li>
</ul>
<p><code>__device__</code>、<code>__constant__</code> 和 <code>__managed__</code> 内存空间说明符不允许用于在设备上执行的函数中既不是外部也不是静态的变量声明。</p>
<p><code>__device__</code>、<code>__constant__</code>、<code>__managed__</code> 或 <code>__shared__</code> 变量定义不能具有包含非空构造函数或非空析构函数的类的类型。一个类类型的构造函数在翻译单元中的某个点被认为是空的，如果它是一个普通的构造函数或者它满足以下所有条件：</p>
<ul>
<li>构造函数已定义。</li>
<li>构造函数没有参数，初始化列表是空的，函数体是一个空的复合语句。</li>
<li>它的类没有虚函数，没有虚基类，也没有非静态数据成员初始化器。</li>
<li>其类的所有基类的默认构造函数都可以认为是空的。</li>
<li>对于其类的所有属于类类型（或其数组）的非静态数据成员，默认构造函数可以被认为是空的。</li>
</ul>
<p>一个类的析构函数在翻译单元中的某个点被认为是空的，如果它是一个普通的析构函数或者它满足以下所有条件：</p>
<ul>
<li>已定义析构函数。</li>
<li>析构函数体是一个空的复合语句。</li>
<li>它的类没有虚函数，也没有虚基类。</li>
<li>其类的所有基类的析构函数都可以认为是空的。</li>
<li>对于其类的所有属于类类型（或其数组）的非静态数据成员，析构函数可以被认为是空的。</li>
</ul>
<p>在整个程序编译模式下编译时（有关此模式的说明，请参见 nvcc 用户手册），<code>__device__</code>、<code>__shared__</code>、<code>__managed__</code> 和 <code>__constant__</code> 变量不能使用 extern 关键字定义为外部变量。 唯一的例外是动态分配的 <code>__shared__</code> 变量，如 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared"><code>__shared__</code></a> 中所述。</p>
<p>在单独编译模式下编译时（有关此模式的说明，请参阅 nvcc 用户手册），可以使用 extern 关键字将 <code>__device__</code>、<code>__shared__</code>、<code>__managed__</code> 和 <code>__constant__</code> 变量定义为外部变量。 当 nvlink 找不到外部变量的定义时（除非它是动态分配的 <code>__shared__</code> 变量），它会产生错误。</p>
<h4 id="I-4-3-2-managed-Memory-Space-Specifier"><a href="#I-4-3-2-managed-Memory-Space-Specifier" class="headerlink" title="I.4.3.2. __managed__ Memory Space Specifier"></a>I.4.3.2. <code>__managed__</code> Memory Space Specifier</h4><p>用 <code>__managed__</code> 内存空间说明符标记的变量（“managed—托管”变量）具有以下限制：</p>
<ul>
<li>托管变量的地址不是常量表达式。</li>
<li>托管变量不应具有 const 限定类型。</li>
<li>托管变量不应具有引用类型。</li>
<li>当 CUDA 运行时可能不处于有效状态时，不应使用托管变量的地址或值，包括以下情况：<ul>
<li>在具有静态或线程本地存储持续时间的对象的静态/动态初始化或销毁中。</li>
<li>在调用 exit() 之后执行的代码中（例如，一个标有 gcc 的“<code>__attribute__</code>((destructor))”的函数）。</li>
<li>在 CUDA 运行时可能未初始化时执行的代码中（例如，标有 gcc 的“<code>__attribute__</code>((constructor))”的函数）。</li>
</ul>
</li>
<li>托管变量不能用作 <code>decltype()</code> 表达式的未加括号的 id 表达式参数。</li>
<li>托管变量具有与为动态分配的托管内存指定的相同的连贯性和一致性行为。</li>
<li>当包含托管变量的 CUDA 程序在具有多个 GPU 的执行平台上运行时，变量仅分配一次，而不是每个 GPU。</li>
<li>在主机上执行的函数中不允许使用没有外部链接的托管变量声明。</li>
<li>在设备上执行的函数中不允许使用没有外部或静态链接的托管变量声明。</li>
</ul>
<p>以下是托管变量的合法和非法使用示例</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">__device__ __managed__ <span class="type">int</span> xxx = <span class="number">10</span>;         <span class="comment">// OK</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> *ptr = &amp;xxx;                             <span class="comment">// error: use of managed variable </span></span><br><span class="line">                                             <span class="comment">// (xxx) in static initialization</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123;</span><br><span class="line">  <span class="type">int</span> field;</span><br><span class="line">  <span class="built_in">S1_t</span>(<span class="type">void</span>) : <span class="built_in">field</span>(xxx) &#123; &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S2_t</span> &#123;</span><br><span class="line">  ~<span class="built_in">S2_t</span>(<span class="type">void</span>) &#123; xxx = <span class="number">10</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">S1_t temp1;                                 <span class="comment">// error: use of managed variable </span></span><br><span class="line">                                            <span class="comment">// (xxx) in dynamic initialization</span></span><br><span class="line"></span><br><span class="line">S2_t temp2;                                 <span class="comment">// error: use of managed variable</span></span><br><span class="line">                                            <span class="comment">// (xxx) in the destructor of </span></span><br><span class="line">                                            <span class="comment">// object with static storage </span></span><br><span class="line">                                            <span class="comment">// duration</span></span><br><span class="line"></span><br><span class="line">__device__ __managed__ <span class="type">const</span> <span class="type">int</span> yyy = <span class="number">10</span>;  <span class="comment">// error: const qualified type</span></span><br><span class="line"></span><br><span class="line">__device__ __managed__ <span class="type">int</span> &amp;zzz = xxx;      <span class="comment">// error: reference type</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="type">int</span> *addr&gt; <span class="keyword">struct</span> <span class="title class_">S3_t</span> &#123; &#125;;</span><br><span class="line">S3_t&lt;&amp;xxx&gt; temp;                            <span class="comment">// error: address of managed </span></span><br><span class="line">                                            <span class="comment">// variable(xxx) not a </span></span><br><span class="line">                                            <span class="comment">// constant expression</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kern</span><span class="params">(<span class="type">int</span> *ptr)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">assert</span>(ptr == &amp;xxx);                      <span class="comment">// OK</span></span><br><span class="line">  xxx = <span class="number">20</span>;                                 <span class="comment">// OK</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">void</span>)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int</span> *ptr = &amp;xxx;                          <span class="comment">// OK</span></span><br><span class="line">  kern&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(ptr);</span><br><span class="line">  <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">  xxx++;                                    <span class="comment">// OK</span></span><br><span class="line">  <span class="keyword">decltype</span>(xxx) qqq;                        <span class="comment">// error: managed variable(xxx) used</span></span><br><span class="line">                                            <span class="comment">// as unparenthized argument to</span></span><br><span class="line">                                            <span class="comment">// decltype</span></span><br><span class="line">                                            </span><br><span class="line">  <span class="keyword">decltype</span>((xxx)) zzz = yyy;                <span class="comment">// OK</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="I-4-3-3-Volatile-Qualifier"><a href="#I-4-3-3-Volatile-Qualifier" class="headerlink" title="I.4.3.3. Volatile Qualifier"></a>I.4.3.3. Volatile Qualifier</h4><p>编译器可以自由优化对全局或共享内存的读取和写入（例如，通过将全局读取缓存到寄存器或 L1 缓存中），只要它尊重内存围栏函数（<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-fence-functions">Memory Fence Functions</a>）的内存排序语义和内存可见性语义 同步函数（<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#synchronization-functions">Synchronization Functions</a>）。</p>
<p>可以使用 <code>volatile</code> 关键字禁用这些优化：如果将位于全局或共享内存中的变量声明为 <code>volatile</code>，编译器假定它的值可以随时被另一个线程更改或使用，因此对该变量的任何引用都会编译为 实际的内存读取或写入指令。</p>
<h3 id="I-4-4-Pointers"><a href="#I-4-4-Pointers" class="headerlink" title="I.4.4. Pointers"></a>I.4.4. Pointers</h3><p>取消引用指向在主机上执行的代码中的全局或共享内存的指针，或在设备上执行的代码中指向主机内存的指针会导致未定义的行为，最常见的是segmentation fault和应用程序终止。</p>
<p>获取 <code>__device__</code>、<code>__shared__</code> 或 <code>__constant__</code> 变量的地址获得的地址只能在设备代码中使用。 设备内存中描述的通过 <code>cudaGetSymbolAddress()</code> 获得的 <code>__device__</code> 或 <code>__constant__</code> 变量的地址只能在主机代码中使用。</p>
<h3 id="I-4-5-Operators"><a href="#I-4-5-Operators" class="headerlink" title="I.4.5. Operators"></a>I.4.5. Operators</h3><h4 id="I-4-5-1-Assignment-Operator"><a href="#I-4-5-1-Assignment-Operator" class="headerlink" title="I.4.5.1. Assignment Operator"></a>I.4.5.1. Assignment Operator</h4><p><code>__constant__</code> 变量只能通过运行时函数（设备内存）从主机代码分配； 它们不能从设备代码中分配。</p>
<p><code>__shared__</code> 变量不能将初始化作为其声明的一部分。</p>
<p>不允许为内置变量中定义的任何内置变量赋值。</p>
<h4 id="I-4-5-2-Address-Operator"><a href="#I-4-5-2-Address-Operator" class="headerlink" title="I.4.5.2. Address Operator"></a>I.4.5.2. Address Operator</h4><p>不允许使用内置变量中定义的任何内置变量的地址。</p>
<h3 id="I-4-6-Run-Time-Type-Information-RTTI"><a href="#I-4-6-Run-Time-Type-Information-RTTI" class="headerlink" title="I.4.6. Run Time Type Information (RTTI)"></a>I.4.6. Run Time Type Information (RTTI)</h3><p>主机代码支持以下与 RTTI 相关的功能，但设备代码不支持。</p>
<ul>
<li><code>typeid</code> operator</li>
<li><code>std::type_info</code></li>
<li><code>dynamic_cast</code> operator</li>
</ul>
<h3 id="I-4-7-Exception-Handling"><a href="#I-4-7-Exception-Handling" class="headerlink" title="I.4.7. Exception Handling"></a>I.4.7. Exception Handling</h3><p>异常处理仅在主机代码中受支持，但在设备代码中不支持。</p>
<p><code>__global__</code> 函数不支持异常规范。</p>
<h3 id="I-4-8-Standard-Library"><a href="#I-4-8-Standard-Library" class="headerlink" title="I.4.8. Standard Library"></a>I.4.8. Standard Library</h3><p>除非另有说明，标准库仅在主机代码中受支持，而不在设备代码中受支持。</p>
<h3 id="I-4-9-Functions"><a href="#I-4-9-Functions" class="headerlink" title="I.4.9. Functions"></a>I.4.9. Functions</h3><h4 id="I-4-9-1-External-Linkage"><a href="#I-4-9-1-External-Linkage" class="headerlink" title="I.4.9.1. External Linkage"></a>I.4.9.1. External Linkage</h4><p>仅当函数在与设备代码相同的编译单元中定义时，才允许在某些设备代码中调用使用 <code>extern</code> 限定符声明的函数，即单个文件或通过可重定位设备代码和 nvlink 链接在一起的多个文件。</p>
<h4 id="I-4-9-2-Implicitly-declared-and-explicitly-defaulted-functions"><a href="#I-4-9-2-Implicitly-declared-and-explicitly-defaulted-functions" class="headerlink" title="I.4.9.2. Implicitly-declared and explicitly-defaulted functions"></a>I.4.9.2. Implicitly-declared and explicitly-defaulted functions</h4><p>令 F 表示一个在其第一个声明中隐式声明或显式默认的函数 或 F 的执行空间说明符 (<code>__host__</code>, <code>__device__</code>) 是调用它的所有函数的执行空间说明符的并集（请注意， <code>__global__</code> 调用者将被视为 <code>__device__</code> 调用者进行此分析）。 例如：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Base</span> &#123;</span><br><span class="line">  <span class="type">int</span> x;</span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">  <span class="function">__host__ __device__ <span class="title">Base</span><span class="params">(<span class="type">void</span>)</span> : x(<span class="number">10</span>) &#123;</span>&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Derived</span> : <span class="keyword">public</span> Base &#123;</span><br><span class="line">  <span class="type">int</span> y;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Other</span>: <span class="keyword">public</span> Base &#123;</span><br><span class="line">  <span class="type">int</span> z;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  Derived D1;</span><br><span class="line">  Other D2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__host__ <span class="type">void</span> <span class="title">bar</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  Other D3;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里，隐式声明的构造函数“<code>Derived::Derived</code>”将被视为 <code>__device__</code> 函数，因为它仅从 <code>__device__</code> 函数“foo”调用。 隐式声明的构造函数 “<code>Other::Other</code>“ 将被视为 <code>__host__ __device__</code> 函数，因为它是从 <code>__device__</code> 函数 “foo” 和 <code>__host__</code> 函数 “bar” 调用的。<br>此外，如果 F 是一个虚拟析构函数，则被 F 覆盖的每个虚拟析构函数 D 的执行空间都被添加到 F 的执行空间集合中，如果 D 不是隐式定义的，或者是显式默认的声明而不是它的声明 第一次声明。</p>
<p>例如：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Base1</span> &#123; <span class="keyword">virtual</span> __host__ __device__ ~<span class="built_in">Base1</span>() &#123; &#125; &#125;;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Derived1</span> : Base1 &#123; &#125;; <span class="comment">// implicitly-declared virtual destructor</span></span><br><span class="line">                             <span class="comment">// ~Derived1 has __host__ __device__ </span></span><br><span class="line">                             <span class="comment">// execution space specifiers</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Base2</span> &#123; <span class="keyword">virtual</span> __device__ ~<span class="built_in">Base2</span>(); &#125;;</span><br><span class="line">__device__ Base2::~<span class="built_in">Base2</span>() = <span class="keyword">default</span>;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Derived2</span> : Base2 &#123; &#125;; <span class="comment">// implicitly-declared virtual destructor</span></span><br><span class="line">                             <span class="comment">// ~Derived2 has __device__ execution </span></span><br><span class="line">                             <span class="comment">// space specifiers </span></span><br></pre></td></tr></table></figure>
<h4 id="I-4-9-3-Function-Parameters"><a href="#I-4-9-3-Function-Parameters" class="headerlink" title="I.4.9.3. Function Parameters"></a>I.4.9.3. Function Parameters</h4><p><code>__global__</code> 函数参数通过常量内存传递给设备，并且限制为 4 KB。</p>
<p><code>__global__</code> 函数不能有可变数量的参数。</p>
<p><code>__global__</code> 函数参数不能通过引用传递。</p>
<p>在单独编译模式下，如果 <code>__device__</code> 或 <code>__global__</code> 函数在特定翻译单元中被 <code>ODR</code> 使用，则该函数的参数和返回类型在该翻译单元中必须是完整的。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//first.cu:</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S</span>;</span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">foo</span><span class="params">(S)</span></span>; <span class="comment">// error: type &#x27;S&#x27; is incomplete</span></span><br><span class="line">__device__ <span class="keyword">auto</span> *ptr = foo;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//second.cu:</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S</span> &#123; <span class="type">int</span> x; &#125;;</span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">foo</span><span class="params">(S)</span> </span>&#123; &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//compiler invocation</span></span><br><span class="line">$nvcc -std=c+<span class="number">+14</span> -rdc=<span class="literal">true</span> first.cu second.cu -o first</span><br><span class="line">nvlink error   : Prototype doesn<span class="string">&#x27;t match for &#x27;</span>_Z3foo1S<span class="string">&#x27; in &#x27;</span>/tmp/tmpxft_00005c8c_00000000<span class="number">-18</span>_second.o<span class="string">&#x27;, first defined in &#x27;</span>/tmp/tmpxft_00005c8c_00000000<span class="number">-18</span>_second.o&#x27;</span><br><span class="line">nvlink fatal   : merge_elf failed</span><br></pre></td></tr></table></figure>
<h5 id="I-4-9-3-1-global-Function-Argument-Processing"><a href="#I-4-9-3-1-global-Function-Argument-Processing" class="headerlink" title="I.4.9.3.1. global Function Argument Processing"></a>I.4.9.3.1. <strong>global</strong> Function Argument Processing</h5><p>当从设备代码启动 <code>__global__</code> 函数时，每个参数都必须是可简单复制和可简单销毁的。</p>
<p>当从主机代码启动 <code>__global__</code> 函数时，每个参数类型都可以是不可复制或不可销毁的，但对此类类型的处理不遵循标准 C++ 模型，如下所述。 用户代码必须确保此工作流程不会影响程序的正确性。 工作流在两个方面与标准 C++ 不同：</p>
<pre><code>  1. Memcpy instead of copy constructor invocation;  
     从主机代码降低 `__global__` 函数启动时，编译器会生成存根函数，这些函数按值复制参数一次或多次，然后最终使用 `memcpy` 将参数复制到设备上的 `__global__` 函数的参数内存中。 即使参数是不可复制的，也会发生这种情况，因此可能会破坏复制构造函数具有副作用的程序。  
        例子：
</code></pre><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cassert&gt;</span></span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S</span> &#123;</span><br><span class="line"> <span class="type">int</span> x;</span><br><span class="line"> <span class="type">int</span> *ptr;</span><br><span class="line"> <span class="function">__host__ __device__ <span class="title">S</span><span class="params">()</span> </span>&#123; &#125;</span><br><span class="line"> <span class="function">__host__ __device__ <span class="title">S</span><span class="params">(<span class="type">const</span> S &amp;)</span> </span>&#123; ptr = &amp;x; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">foo</span><span class="params">(S in)</span> </span>&#123;</span><br><span class="line"> <span class="comment">// this assert may fail, because the compiler</span></span><br><span class="line"> <span class="comment">// generated code will memcpy the contents of &quot;in&quot;</span></span><br><span class="line"> <span class="comment">// from host to kernel parameter memory, so the</span></span><br><span class="line"> <span class="comment">// &quot;in.ptr&quot; is not initialized to &quot;&amp;in.x&quot; because</span></span><br><span class="line"> <span class="comment">// the copy constructor is skipped.</span></span><br><span class="line"> <span class="built_in">assert</span>(in.ptr == &amp;in.x);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  S tmp;</span><br><span class="line">  foo&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(tmp);</span><br><span class="line">  <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cassert&gt;</span></span></span><br><span class="line"></span><br><span class="line">__managed__ <span class="type">int</span> counter;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S1</span> &#123;</span><br><span class="line"><span class="built_in">S1</span>() &#123; &#125;</span><br><span class="line"><span class="built_in">S1</span>(<span class="type">const</span> S1 &amp;) &#123; ++counter; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">foo</span><span class="params">(S1)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* this assertion may fail, because</span></span><br><span class="line"><span class="comment">   the compiler generates stub</span></span><br><span class="line"><span class="comment">   functions on the host for a kernel</span></span><br><span class="line"><span class="comment">   launch, and they may copy the</span></span><br><span class="line"><span class="comment">   argument by value more than once.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="built_in">assert</span>(counter == <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">S1 V;</span><br><span class="line">foo&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(V);</span><br><span class="line"><span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<pre><code>  2. Destructor may be invoked before the __global__ function has finished;          
     内核启动与主机执行是异步的。 因此，如果 `__global__` 函数参数具有非平凡的析构函数，则析构函数甚至可以在 `__global__` 函数完成执行之前在宿主代码中执行。 这可能会破坏析构函数具有副作用的程序。
        示例:
</code></pre><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">S</span> &#123;</span><br><span class="line"> <span class="type">int</span> *ptr;</span><br><span class="line"> <span class="built_in">S</span>() : <span class="built_in">ptr</span>(<span class="literal">nullptr</span>) &#123; &#125;</span><br><span class="line"> <span class="built_in">S</span>(<span class="type">const</span> S &amp;) &#123; <span class="built_in">cudaMallocManaged</span>(&amp;ptr, <span class="built_in">sizeof</span>(<span class="type">int</span>)); &#125;</span><br><span class="line"> ~<span class="built_in">S</span>() &#123; <span class="built_in">cudaFree</span>(ptr); &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">foo</span><span class="params">(S in)</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">  <span class="comment">//error: This store may write to memory that has already been</span></span><br><span class="line">  <span class="comment">//       freed (see below).</span></span><br><span class="line">  *(in.ptr) = <span class="number">4</span>;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> S V;</span><br><span class="line"> </span><br><span class="line"> <span class="comment">/* The object &#x27;V&#x27; is first copied by value to a compiler-generated</span></span><br><span class="line"><span class="comment">  * stub function that does the kernel launch, and the stub function</span></span><br><span class="line"><span class="comment">  * bitwise copies the contents of the argument to kernel parameter</span></span><br><span class="line"><span class="comment">  * memory.</span></span><br><span class="line"><span class="comment">  * However, GPU kernel execution is asynchronous with host</span></span><br><span class="line"><span class="comment">  * execution. </span></span><br><span class="line"><span class="comment">  * As a result, S::~S() will execute when the stub function   returns, releasing allocated memory, even though the kernel may not have finished execution.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> foo&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(V);</span><br><span class="line"> <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="I-4-9-4-Static-Variables-within-Function"><a href="#I-4-9-4-Static-Variables-within-Function" class="headerlink" title="I.4.9.4. Static Variables within Function"></a>I.4.9.4. Static Variables within Function</h4><p>在函数 F 的直接或嵌套块范围内，静态变量 V 的声明中允许使用可变内存空间说明符，其中：</p>
<ul>
<li>F 是一个 <code>__global__</code> 或 <code>__device__</code>-only 函数。</li>
<li>F 是一个 <code>__host__ __device__</code> 函数，<code>__CUDA_ARCH__</code> 定义为 17。</li>
</ul>
<p>如果 V 的声明中没有显式的内存空间说明符，则在设备编译期间假定隐式 <code>__device__</code> 说明符。</p>
<p>V 具有与在命名空间范围内声明的具有相同内存空间说明符的变量相同的初始化限制，例如 <code>__device__</code> 变量不能有“非空”构造函数（请参阅设备内存空间说明符）。</p>
<p>函数范围静态变量的合法和非法使用示例如下所示。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123;</span><br><span class="line">  <span class="type">int</span> x;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S2_t</span> &#123;</span><br><span class="line">  <span class="type">int</span> x;</span><br><span class="line">  <span class="function">__device__ <span class="title">S2_t</span><span class="params">(<span class="type">void</span>)</span> </span>&#123; x = <span class="number">10</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S3_t</span> &#123;</span><br><span class="line">  <span class="type">int</span> x;</span><br><span class="line">  <span class="function">__device__ <span class="title">S3_t</span><span class="params">(<span class="type">int</span> p)</span> : x(p) &#123;</span> &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">f1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="type">static</span> <span class="type">int</span> i1;              <span class="comment">// OK, implicit __device__ memory space specifier</span></span><br><span class="line">  <span class="type">static</span> <span class="type">int</span> i2 = <span class="number">11</span>;         <span class="comment">// OK, implicit __device__ memory space specifier</span></span><br><span class="line">  <span class="type">static</span> __managed__ <span class="type">int</span> m1;  <span class="comment">// OK</span></span><br><span class="line">  <span class="type">static</span> __device__ <span class="type">int</span> d1;   <span class="comment">// OK</span></span><br><span class="line">  <span class="type">static</span> __constant__ <span class="type">int</span> c1; <span class="comment">// OK</span></span><br><span class="line">  </span><br><span class="line">  <span class="type">static</span> S1_t i3;             <span class="comment">// OK, implicit __device__ memory space specifier</span></span><br><span class="line">  <span class="type">static</span> S1_t i4 = &#123;<span class="number">22</span>&#125;;      <span class="comment">// OK, implicit __device__ memory space specifier</span></span><br><span class="line"></span><br><span class="line">  <span class="type">static</span> __shared__ <span class="type">int</span> i5;   <span class="comment">// OK</span></span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> x = <span class="number">33</span>;</span><br><span class="line">  <span class="type">static</span> <span class="type">int</span> i6 = x;          <span class="comment">// error: dynamic initialization is not allowed</span></span><br><span class="line">  <span class="type">static</span> S1_t i7 = &#123;x&#125;;       <span class="comment">// error: dynamic initialization is not allowed</span></span><br><span class="line"></span><br><span class="line">  <span class="type">static</span> S2_t i8;             <span class="comment">// error: dynamic initialization is not allowed</span></span><br><span class="line">  <span class="function"><span class="type">static</span> S3_t <span class="title">i9</span><span class="params">(<span class="number">44</span>)</span></span>;         <span class="comment">// error: dynamic initialization is not allowed</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__host__ __device__ <span class="type">void</span> <span class="title">f2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="type">static</span> <span class="type">int</span> i1;              <span class="comment">// OK, implicit __device__ memory space specifier</span></span><br><span class="line">                              <span class="comment">// during device compilation.</span></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> __CUDA_ARCH__</span></span><br><span class="line">  <span class="type">static</span> __device__ <span class="type">int</span> d1;   <span class="comment">// OK, declaration is only visible during device</span></span><br><span class="line">                              <span class="comment">// compilation  (__CUDA_ARCH__ is defined)</span></span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">  <span class="type">static</span> <span class="type">int</span> d0;              <span class="comment">// OK, declaration is only visible during host</span></span><br><span class="line">                              <span class="comment">// compilation (__CUDA_ARCH__ is not defined)</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span>  </span></span><br><span class="line"></span><br><span class="line">  <span class="type">static</span> __device__ <span class="type">int</span> d2;   <span class="comment">// error: __device__ variable inside</span></span><br><span class="line">                              <span class="comment">// a host function during host compilation</span></span><br><span class="line">                              <span class="comment">// i.e. when __CUDA_ARCH__ is not defined</span></span><br><span class="line"></span><br><span class="line">  <span class="type">static</span> __shared__ <span class="type">int</span> i2;  <span class="comment">// error: __shared__ variable inside</span></span><br><span class="line">                             <span class="comment">// a host function during host compilation</span></span><br><span class="line">                             <span class="comment">// i.e. when __CUDA_ARCH__ is not defined</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="I-4-9-5-Function-Pointers"><a href="#I-4-9-5-Function-Pointers" class="headerlink" title="I.4.9.5. Function Pointers"></a>I.4.9.5. Function Pointers</h4><p>在主机代码中获取的 <code>__global__</code> 函数的地址不能在设备代码中使用（例如，启动内核）。 同样，在设备代码中获取的 <code>__global__</code> 函数的地址不能在主机代码中使用。</p>
<p>不允许在主机代码中获取 <code>__device__</code> 函数的地址。</p>
<h4 id="I-4-9-6-Function-Recursion"><a href="#I-4-9-6-Function-Recursion" class="headerlink" title="I.4.9.6. Function Recursion"></a>I.4.9.6. Function Recursion</h4><p><code>__global__</code> 函数不支持递归。</p>
<h4 id="I-4-9-7-Friend-Functions"><a href="#I-4-9-7-Friend-Functions" class="headerlink" title="I.4.9.7. Friend Functions"></a>I.4.9.7. Friend Functions</h4><p><code>__global__</code> 函数或函数模板不能在友元声明中定义。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123;</span><br><span class="line">  <span class="function"><span class="keyword">friend</span> __global__ </span></span><br><span class="line"><span class="function">  <span class="type">void</span> <span class="title">foo1</span><span class="params">(<span class="type">void</span>)</span></span>;  <span class="comment">// OK: not a definition</span></span><br><span class="line">  <span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">  <span class="keyword">friend</span> __global__ </span></span><br><span class="line"><span class="function">  <span class="type">void</span> <span class="title">foo2</span><span class="params">(<span class="type">void</span>)</span></span>; <span class="comment">// OK: not a definition</span></span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">friend</span> __global__ </span></span><br><span class="line"><span class="function">  <span class="type">void</span> <span class="title">foo3</span><span class="params">(<span class="type">void</span>)</span> </span>&#123; &#125; <span class="comment">// error: definition in friend declaration</span></span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">  <span class="keyword">friend</span> __global__ </span></span><br><span class="line"><span class="function">  <span class="type">void</span> <span class="title">foo4</span><span class="params">(<span class="type">void</span>)</span> </span>&#123; &#125; <span class="comment">// error: definition in friend declaration</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h4 id="I-4-9-8-Operator-Function"><a href="#I-4-9-8-Operator-Function" class="headerlink" title="I.4.9.8. Operator Function"></a>I.4.9.8. Operator Function</h4><p>运算符函数不能是 <code>__global__</code> 函数。</p>
<h3 id="I-4-10-Classes"><a href="#I-4-10-Classes" class="headerlink" title="I.4.10. Classes"></a>I.4.10. Classes</h3><p>I.4.10.1. 数据成员<br>不支持静态数据成员，除了那些也是 const 限定的（请参阅 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#const-variables">Const 限定变量</a>）。</p>
<h4 id="I-4-10-2-函数成员"><a href="#I-4-10-2-函数成员" class="headerlink" title="I.4.10.2. 函数成员"></a>I.4.10.2. 函数成员</h4><p>静态成员函数不能是 <code>__global__</code> 函数。</p>
<h4 id="I-4-10-3-虚函数"><a href="#I-4-10-3-虚函数" class="headerlink" title="I.4.10.3. 虚函数"></a>I.4.10.3. 虚函数</h4><p>当派生类中的函数覆盖基类中的虚函数时，被覆盖函数和覆盖函数上的执行空间说明符（即 <code>__host__、__device__</code>）必须匹配。</p>
<p>不允许将具有虚函数的类的对象作为参数传递给 <code>__global__</code> 函数。</p>
<p>如果在主机代码中创建对象，则在设备代码中为该对象调用虚函数具有未定义的行为。</p>
<p>如果在设备代码中创建了一个对象，则在主机代码中为该对象调用虚函数具有未定义的行为。</p>
<p>使用 Microsoft 主机编译器时，请参阅特定于 Windows 的其他限制。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">S1</span> &#123; <span class="function"><span class="keyword">virtual</span> __host__ __device__ <span class="type">void</span> <span class="title">foo</span><span class="params">()</span> </span>&#123; &#125; &#125;;</span><br><span class="line"></span><br><span class="line">__managed__ S1 *ptr1, *ptr2;</span><br><span class="line"></span><br><span class="line">__managed__ __align__(<span class="number">16</span>) <span class="type">char</span> buf1[<span class="number">128</span>];</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kern</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  ptr1-&gt;<span class="built_in">foo</span>();     <span class="comment">// error: virtual function call on a object</span></span><br><span class="line">                   <span class="comment">//        created in host code.</span></span><br><span class="line">  ptr2 = <span class="built_in">new</span>(buf1) <span class="built_in">S1</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="type">void</span> *buf;</span><br><span class="line">  <span class="built_in">cudaMallocManaged</span>(&amp;buf, <span class="built_in">sizeof</span>(S1), cudaMemAttachGlobal);</span><br><span class="line">  ptr1 = <span class="built_in">new</span> (buf) <span class="built_in">S1</span>();</span><br><span class="line">  kern&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">  <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">  ptr2-&gt;<span class="built_in">foo</span>();  <span class="comment">// error: virtual function call on an object</span></span><br><span class="line">                <span class="comment">//        created in device code.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="I-4-10-4-Virtual-Base-Classes"><a href="#I-4-10-4-Virtual-Base-Classes" class="headerlink" title="I.4.10.4.  Virtual Base Classes"></a>I.4.10.4.  Virtual Base Classes</h4><p>不允许将派生自虚拟基类的类的对象作为参数传递给 <code>__global__</code> 函数。</p>
<p>使用 Microsoft 主机编译器时，请参阅特定于 Windows 的其他限制。</p>
<h4 id="I-4-10-5-Anonymous-Unions"><a href="#I-4-10-5-Anonymous-Unions" class="headerlink" title="I.4.10.5. Anonymous Unions"></a>I.4.10.5. Anonymous Unions</h4><p>命名空间范围匿名联合的成员变量不能在 <code>__global__</code> 或 <code>__device__</code> 函数中引用。</p>
<h4 id="I-4-10-6-特定于-Windows-的"><a href="#I-4-10-6-特定于-Windows-的" class="headerlink" title="I.4.10.6. 特定于 Windows 的"></a>I.4.10.6. 特定于 Windows 的</h4><p>CUDA 编译器遵循 IA64 ABI 进行类布局，而 Microsoft 主机编译器则不遵循。 令 T 表示指向成员类型的指针，或满足以下任一条件的类类型：</p>
<ul>
<li>T has virtual functions.</li>
<li>T has a virtual base class.</li>
<li>T has multiple inheritance with more than one direct or indirect empty base class.</li>
<li>All direct and indirect base classes B of T are empty and the type of the first field F of T uses B in its definition, such that B is laid out at offset 0 in the definition of F.</li>
</ul>
<p>让 <code>C</code> 表示 <code>T</code> 或以 <code>T</code> 作为字段类型或基类类型的类类型。 CUDA 编译器计算类布局和大小的方式可能不同于 <code>C</code> 类型的 Microsoft 主机编译器。<br>只要类型 <code>C</code> 专门用于主机或设备代码，程序就应该可以正常工作。</p>
<p>在主机和设备代码之间传递 <code>C</code> 类型的对象具有未定义的行为，例如，作为 <code>__global__</code> 函数的参数或通过 <code>cudaMemcpy*()</code> 调用。</p>
<p>如果在主机代码中创建对象，则访问 <code>C</code> 类型的对象或设备代码中的任何子对象，或调用设备代码中的成员函数具有未定义的行为。</p>
<p>如果对象是在设备代码中创建的，则访问 <code>C</code>类型的对象或主机代码中的任何子对象，或调用主机代码中的成员函数具有未定义的行为。</p>
<h3 id="I-4-11-Templates"><a href="#I-4-11-Templates" class="headerlink" title="I.4.11. Templates"></a>I.4.11. Templates</h3><p>类型或模板不能在 <code>__global__</code> 函数模板实例化或 <code>__device__</code>/<code>__constant__</code> 变量实例化的类型、非类型或模板模板参数中使用，如果：</p>
<ul>
<li>类型或模板在 <code>__host__</code> 或 <code>__host__ __device__</code> 中定义。</li>
<li>类型或模板是具有私有或受保护访问的类成员，其父类未在 <code>__device__</code> 或 <code>__global__</code> 函数中定义。</li>
<li>该类型未命名。<br>  *该类型由上述任何类型复合而成。<br>  例子：</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">myKernel</span><span class="params">(<span class="type">void</span>)</span> </span>&#123; &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">myClass</span> &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">inner_t</span> &#123; &#125;; </span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">static</span> <span class="type">void</span> <span class="title">launch</span><span class="params">(<span class="type">void</span>)</span> </span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">       <span class="comment">// error: inner_t is used in template argument</span></span><br><span class="line">       <span class="comment">// but it is private</span></span><br><span class="line">       myKernel&lt;<span class="type">inner_t</span>&gt;&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// C++14 only</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; __device__ T d1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T1, <span class="keyword">typename</span> T2&gt; __device__ T1 d2;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">fn</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123; &#125;;</span><br><span class="line">  <span class="comment">// error (C++14 only): S1_t is local to the function fn</span></span><br><span class="line">  d1&lt;S1_t&gt; = &#123;&#125;;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> lam1 = [] &#123; &#125;;</span><br><span class="line">  <span class="comment">// error (C++14 only): a closure type cannot be used for</span></span><br><span class="line">  <span class="comment">// instantiating a variable template</span></span><br><span class="line">  d2&lt;<span class="type">int</span>, <span class="keyword">decltype</span>(lam1)&gt; = <span class="number">10</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="I-4-12-Trigraphs-and-Digraphs"><a href="#I-4-12-Trigraphs-and-Digraphs" class="headerlink" title="I.4.12. Trigraphs and Digraphs"></a>I.4.12. Trigraphs and Digraphs</h3><p>任何平台都不支持三元组。 Windows 不支持有向图。</p>
<h3 id="I-4-13-Const-qualified-variables"><a href="#I-4-13-Const-qualified-variables" class="headerlink" title="I.4.13. Const-qualified variables"></a>I.4.13. Const-qualified variables</h3><p>让“V”表示名称空间范围变量或具有 <code>const</code> 限定类型且没有执行空间注释的类静态成员变量（例如，<code>__device__</code>、<code>__constant__</code>、<code>__shared__</code>）。 V 被认为是主机代码变量。</p>
<p>V 的值可以直接在设备代码中使用，如果</p>
<ul>
<li>V 在使用点之前已经用常量表达式初始化，</li>
<li>V 的类型不是 volatile 限定的，并且</li>
<li>它具有以下类型之一：<ul>
<li>内置浮点类型，除非将 Microsoft 编译器用作主机编译器，</li>
<li>内置整型。</li>
</ul>
</li>
</ul>
<p>设备源代码不能包含对 V 的引用或获取 V 的地址。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int</span> xxx = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123;  <span class="type">static</span> <span class="type">const</span> <span class="type">int</span> yyy = <span class="number">20</span>; &#125;;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">extern</span> <span class="type">const</span> <span class="type">int</span> zzz;</span><br><span class="line"><span class="type">const</span> <span class="type">float</span> www = <span class="number">5.0</span>;</span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> local1[xxx];          <span class="comment">// OK</span></span><br><span class="line">  <span class="type">int</span> local2[S1_t::yyy];    <span class="comment">// OK</span></span><br><span class="line">      </span><br><span class="line">  <span class="type">int</span> val1 = xxx;           <span class="comment">// OK</span></span><br><span class="line">    					</span><br><span class="line">  <span class="type">int</span> val2 = S1_t::yyy;     <span class="comment">// OK</span></span><br><span class="line">    					</span><br><span class="line">  <span class="type">int</span> val3 = zzz;           <span class="comment">// error: zzz not initialized with constant </span></span><br><span class="line">                            <span class="comment">// expression at the point of use.</span></span><br><span class="line">  </span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> &amp;val3 = xxx;    <span class="comment">// error: reference to host variable  </span></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> *val4 = &amp;xxx;   <span class="comment">// error: address of host variable</span></span><br><span class="line">  <span class="type">const</span> <span class="type">float</span> val5 = www;   <span class="comment">// OK except when the Microsoft compiler is used as</span></span><br><span class="line">                            <span class="comment">// the host compiler.</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> zzz = <span class="number">20</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="I-4-14-Long-Double"><a href="#I-4-14-Long-Double" class="headerlink" title="I.4.14. Long Double"></a>I.4.14. Long Double</h3><p>设备代码不支持使用 long double 类型。</p>
<h3 id="I-4-15-Deprecation-Annotation"><a href="#I-4-15-Deprecation-Annotation" class="headerlink" title="I.4.15. Deprecation Annotation"></a>I.4.15. Deprecation Annotation</h3><p><code>nvcc</code> 支持在使用 <code>gcc、clang、xlC、icc</code> 或 <code>pgcc</code> 主机编译器时使用 <code>deprecated</code> 属性，以及在使用 <code>cl.exe</code> 主机编译器时使用 <code>deprecated declspec</code>。当启用 C++14 时，它还支持 <code>[[deprecated]]</code> 标准属性。当定义 <code>__CUDA_ARCH__</code> 时（即在设备编译阶段），CUDA 前端编译器将为从 <code>__device__</code>、<code>__global__</code> 或 <code>__host__ __device__</code> 函数的主体内对已弃用实体的引用生成弃用诊断。对不推荐使用的实体的其他引用将由主机编译器处理，例如，来自 <code>__host__</code> 函数中的引用。</p>
<p>CUDA 前端编译器不支持各种主机编译器支持的<code>#pragma</code> gcc 诊断或<code>#pragma</code> 警告机制。因此，CUDA 前端编译器生成的弃用诊断不受这些 <code>pragma</code> 的影响，但主机编译器生成的诊断会受到影响。要抑制设备代码的警告，用户可以使用 NVIDIA 特定的 pragma <code>#pragma nv_diag_suppress</code>。 nvcc 标志 <code>-Wno-deprecated-declarations</code> 可用于禁止所有弃用警告，标志 <code>-Werror=deprecated-declarations</code> 可用于将弃用警告转换为错误。</p>
<h3 id="I-4-16-Noreturn-Annotation"><a href="#I-4-16-Noreturn-Annotation" class="headerlink" title="I.4.16. Noreturn Annotation"></a>I.4.16. Noreturn Annotation</h3><p><code>nvcc</code> 支持在使用 <code>gcc、clang、xlC、icc 或 pgcc</code> 主机编译器时使用 <code>noreturn</code> 属性，并在使用 <code>cl.exe</code> 主机编译器时使用 <code>noreturn</code> <code>declspec</code>。 当启用 C++11 时，它还支持 <code>[[noreturn]]</code> 标准属性。</p>
<p><code>属性/declspec</code> 可以在主机和设备代码中使用。</p>
<h3 id="I-4-17-likely-unlikely-Standard-Attributes"><a href="#I-4-17-likely-unlikely-Standard-Attributes" class="headerlink" title="I.4.17. [[likely]] / [[unlikely]] Standard Attributes"></a>I.4.17. [[likely]] / [[unlikely]] Standard Attributes</h3><p>所有支持 C++ 标准属性语法的配置都接受这些属性。 这些属性可用于向设备编译器优化器提示与不包含该语句的任何替代路径相比，该语句是否更有可能被执行。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__device__ <span class="type">int</span> <span class="title">foo</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> (i &lt; <span class="number">10</span>) [[likely]] &#123; <span class="comment">// the &#x27;if&#x27; block will likely be entered</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">4</span>; </span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">if</span> (i &lt; <span class="number">20</span>) [[unlikely]] &#123; <span class="comment">// the &#x27;if&#x27; block will not likely be entered</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如果在 <code>__CUDA_ARCH__</code> 未定义时在主机代码中使用这些属性，则它们将出现在主机编译器解析的代码中，如果不支持这些属性，则可能会生成警告。 例如，<code>clang11</code> 主机编译器将生成<code>“unknown attribute”</code>警告。</p>
<h1 id="I-4-18-const-and-pure-GNU-Attributes"><a href="#I-4-18-const-and-pure-GNU-Attributes" class="headerlink" title="I.4.18. const and pure GNU Attributes"></a>I.4.18. const and pure GNU Attributes</h1><p>当使用也支持这些属性的语言和主机编译器时，主机和设备功能都支持这些属性，例如 使用 <code>g++</code> 主机编译器。</p>
<p>对于使用 <code>pure</code> 属性注释的设备函数，设备代码优化器假定该函数不会更改调用者函数（例如内存）可见的任何可变状态。</p>
<p>对于使用 <code>const</code> 属性注释的设备函数，设备代码优化器假定该函数不会访问或更改调用者函数可见的任何可变状态（例如内存）。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">__attribute__((<span class="type">const</span>)) <span class="function">__device__ <span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> in)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">int</span> <span class="title">doit</span><span class="params">(<span class="type">int</span> in)</span> </span>&#123;</span><br><span class="line"><span class="type">int</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//because &#x27;get&#x27; is marked with &#x27;const&#x27; attribute</span></span><br><span class="line"><span class="comment">//device code optimizer can recognize that the</span></span><br><span class="line"><span class="comment">//second call to get() can be commoned out.</span></span><br><span class="line">sum = <span class="built_in">get</span>(in);</span><br><span class="line">sum += <span class="built_in">get</span>(in);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="I-4-19-Intel-Host-Compiler-Specific"><a href="#I-4-19-Intel-Host-Compiler-Specific" class="headerlink" title="I.4.19. Intel Host Compiler Specific"></a>I.4.19. Intel Host Compiler Specific</h3><p>CUDA 前端编译器解析器无法识别英特尔编译器（例如 icc）支持的某些内在函数。 因此，当使用 Intel 编译器作为主机编译器时，<code>nvcc</code> 将在预处理期间启用宏 <code>__INTEL_COMPILER_USE_INTRINSIC_PROTOTYPES</code>。 此宏允许在相关头文件中显式声明英特尔编译器内部函数，从而允许 <code>nvcc</code> 支持在主机代码中使用<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_21">此类函数</a>。</p>
<h3 id="I-4-20-C-11-Features"><a href="#I-4-20-C-11-Features" class="headerlink" title="I.4.20. C++11 Features"></a>I.4.20. C++11 Features</h3><p><code>nvcc</code> 也支持主机编译器默认启用的 C++11 功能，但须遵守本文档中描述的限制。 此外，使用 <code>-std=c++11</code> 标志调用 <code>nvcc</code> 会打开所有 C++11 功能，还会使用相应的 C++11 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_21">选项</a>调用主机预处理器、编译器和链接器。</p>
<h4 id="I-4-20-1-Lambda-Expressions"><a href="#I-4-20-1-Lambda-Expressions" class="headerlink" title="I.4.20.1. Lambda Expressions"></a>I.4.20.1. Lambda Expressions</h4><p>与 lambda 表达式关联的闭包类的所有<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_22">成员函数</a>的执行空间说明符由编译器派生如下。 如 C++11 标准中所述，编译器在包含 lambda 表达式的最小块范围、类范围或命名空间范围内创建闭包类型。 计算封闭闭包类型的最内层函数作用域，并将相应函数的执行空间说明符分配给闭包类成员函数。 如果没有封闭函数范围，则执行空间说明符为 <code>__host__</code>。</p>
<p>lambda 表达式和计算的执行空间说明符的示例如下所示（在注释中）。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> globalVar = [] &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;; <span class="comment">// __host__ </span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">f1</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> l1 = [] &#123; <span class="keyword">return</span> <span class="number">1</span>; &#125;;      <span class="comment">// __host__</span></span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">f2</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> l2 = [] &#123; <span class="keyword">return</span> <span class="number">2</span>; &#125;;      <span class="comment">// __device__</span></span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="function">__host__ __device__ <span class="type">void</span> <span class="title">f3</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> l3 = [] &#123; <span class="keyword">return</span> <span class="number">3</span>; &#125;;      <span class="comment">// __host__ __device__</span></span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">f4</span><span class="params">(<span class="type">int</span> (*fp)() = [] &#123; <span class="keyword">return</span> <span class="number">4</span>; &#125; <span class="comment">/* __host__ */</span>)</span> </span>&#123;</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">f5</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> l5 = [] &#123; <span class="keyword">return</span> <span class="number">5</span>; &#125;;      <span class="comment">// __device__</span></span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">f6</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123;</span><br><span class="line">    <span class="function"><span class="type">static</span> <span class="type">void</span> <span class="title">helper</span><span class="params">(<span class="type">int</span> (*fp)() = [] &#123;<span class="keyword">return</span> <span class="number">6</span>; &#125; <span class="comment">/* __device__ */</span>)</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>lambda 表达式的闭包类型不能用于 <code>__global__</code> 函数模板实例化的类型或非类型参数，除非 lambda 在 <code>__device__</code> 或 <code>__global__</code> 函数中定义。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">foo</span><span class="params">(T in)</span> </span>&#123; &#125;;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123; &#125;;</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> temp1 = [] &#123; &#125;;</span><br><span class="line">      </span><br><span class="line">  foo&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(temp1);                    <span class="comment">// error: lambda closure type used in</span></span><br><span class="line">                                          <span class="comment">// template type argument</span></span><br><span class="line">  foo&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;( <span class="built_in">S1_t</span>&lt;<span class="keyword">decltype</span>(temp1)&gt;()); <span class="comment">// error: lambda closure type used in </span></span><br><span class="line">                                          <span class="comment">// template type argument</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="I-4-20-2-std-initializer-list"><a href="#I-4-20-2-std-initializer-list" class="headerlink" title="I.4.20.2. std::initializer_list"></a>I.4.20.2. std::initializer_list</h4><p>默认情况下，CUDA 编译器将隐式认为 <code>std::initializer_list</code> 的成员函数具有 <code>__host__ __device__</code> 执行空间说明符，因此可以直接从设备代码调用它们。 nvcc 标志 <code>--no-host-device-initializer-list</code> 将禁用此行为； <code>std::initializer_list</code> 的成员函数将被视为 <code>__host__</code> 函数，并且不能直接从设备代码调用。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;initializer_list&gt;</span></span></span><br><span class="line">    </span><br><span class="line"><span class="function">__device__ <span class="type">int</span> <span class="title">foo</span><span class="params">(std::initializer_list&lt;<span class="type">int</span>&gt; in)</span></span>;</span><br><span class="line">    </span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">bar</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="built_in">foo</span>(&#123;<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>&#125;);   <span class="comment">// (a) initializer list containing only </span></span><br><span class="line">                    <span class="comment">// constant expressions.</span></span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> i = <span class="number">4</span>;</span><br><span class="line">    <span class="built_in">foo</span>(&#123;i,<span class="number">5</span>,<span class="number">6</span>&#125;);   <span class="comment">// (b) initializer list with at least one </span></span><br><span class="line">                    <span class="comment">// non-constant element.</span></span><br><span class="line">                    <span class="comment">// This form may have better performance than (a). </span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="I-4-20-3-Rvalue-references"><a href="#I-4-20-3-Rvalue-references" class="headerlink" title="I.4.20.3. Rvalue references"></a>I.4.20.3. Rvalue references</h4><p>默认情况下，CUDA 编译器将隐式认为 <code>std::move</code> 和 <code>std::forward</code> 函数模板具有 <code>__host__ __device__</code> 执行空间说明符，因此可以直接从设备代码调用它们。 nvcc 标志 <code>--no-host-device-move-forward</code> 将禁用此行为； <code>std::move</code> 和 <code>std::forward</code> 将被视为 <code>__host__</code> 函数，不能直接从设备代码调用。</p>
<h4 id="I-4-20-4-Constexpr-functions-and-function-templates"><a href="#I-4-20-4-Constexpr-functions-and-function-templates" class="headerlink" title="I.4.20.4. Constexpr functions and function templates"></a>I.4.20.4. Constexpr functions and function templates</h4><p>默认情况下，不能从执行空间不兼容的函数中调用 <code>constexpr</code> <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_23">函数</a>. 实验性 nvcc 标志 <code>--expt-relaxed-constexpr</code> 消除了<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_24">此限制</a>. 当指定此标志时，主机代码可以调用 <code>__device__ constexpr</code> 函数和设备 代码可以调用 <code>__host__ constexpr</code> 函数。 当指定了 <code>--expt-relaxed-constexpr</code> 时，nvcc 将定义宏 <code>__CUDACC_RELAXED_CONSTEXPR__</code>。 请注意，即使相应的模板用关键字 <code>constexpr</code> 标记（C++11 标准节 <code>[dcl.constexpr.p6]</code>），函数模板实例化也可能不是 <code>constexpr</code> 函数。</p>
<h4 id="I-4-20-5-Constexpr-variables"><a href="#I-4-20-5-Constexpr-variables" class="headerlink" title="I.4.20.5. Constexpr variables"></a>I.4.20.5. Constexpr variables</h4><p>让“<code>V</code>”表示命名空间范围变量或已标记为 <code>constexpr</code> 且没有执行空间注释的类静态成员变量（例如，<code>__device__</code>、<code>__constant__</code>、<code>__shared__</code>）。 V 被认为是主机代码变量。</p>
<p>如果 V 是除 <code>long double</code> 以外的<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_25">标量类型</a> 并且该类型不是 <code>volatile</code> 限定的，则 V 的值可以直接在设备代码中使用。 此外，如果 V 是非标量类型，则 V 的标量元素可以在 <code>constexpr __device__</code> 或 <code>__host__ __device__</code> 函数中使用，如果对函数的调用是<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_26">常量表达式</a>. 设备源代码不能包含对 V 的引用 或取 V 的地址。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> xxx = <span class="number">10</span>;</span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> yyy = xxx + <span class="number">4</span>;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123; <span class="type">static</span> <span class="keyword">constexpr</span> <span class="type">int</span> qqq = <span class="number">100</span>; &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">int</span> host_arr[] = &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;;</span><br><span class="line"><span class="function"><span class="keyword">constexpr</span> __device__ <span class="type">int</span> <span class="title">get</span><span class="params">(<span class="type">int</span> idx)</span> </span>&#123; <span class="keyword">return</span> host_arr[idx]; &#125; </span><br><span class="line">  </span><br><span class="line"><span class="function">__device__ <span class="type">int</span> <span class="title">foo</span><span class="params">(<span class="type">int</span> idx)</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> v1 = xxx + yyy + S1_t::qqq;  <span class="comment">// OK</span></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> &amp;v2 = xxx;             <span class="comment">// error: reference to host constexpr </span></span><br><span class="line">                                   <span class="comment">// variable</span></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> *v3 = &amp;xxx;            <span class="comment">// error: address of host constexpr </span></span><br><span class="line">                                   <span class="comment">// variable</span></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> &amp;v4 = S1_t::qqq;       <span class="comment">// error: reference to host constexpr </span></span><br><span class="line">                                   <span class="comment">// variable</span></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> *v5 = &amp;S1_t::qqq;      <span class="comment">// error: address of host constexpr </span></span><br><span class="line">                                   <span class="comment">// variable</span></span><br><span class="line">                                   </span><br><span class="line">  v1 += <span class="built_in">get</span>(<span class="number">2</span>);                    <span class="comment">// OK: &#x27;get(2)&#x27; is a constant </span></span><br><span class="line">                                   <span class="comment">// expression.</span></span><br><span class="line">  v1 += <span class="built_in">get</span>(idx);                  <span class="comment">// error: &#x27;get(idx)&#x27; is not a constant </span></span><br><span class="line">                                   <span class="comment">// expression</span></span><br><span class="line">  v1 += host_arr[<span class="number">2</span>];               <span class="comment">// error: &#x27;host_arr&#x27; does not have </span></span><br><span class="line">                                   <span class="comment">// scalar type.</span></span><br><span class="line">  <span class="keyword">return</span> v1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="I-4-20-6-Inline-namespaces"><a href="#I-4-20-6-Inline-namespaces" class="headerlink" title="I.4.20.6. Inline namespaces"></a>I.4.20.6. Inline namespaces</h4><p>对于输入的CUDA翻译单元，CUDA编译器可以调用主机编译器来编译翻译单元内的主机代码。 在传递给主机编译器的代码中，如果输入的 CUDA 翻译单元包含以下任何实体的定义，CUDA 编译器将注入额外的编译器生成的代码：</p>
<ul>
<li><code>__global__</code> 函数或函数模板实例化</li>
<li><code>__device__</code>, <code>__constant__</code></li>
<li>具有表面或纹理类型的变量</li>
</ul>
<p>编译器生成的代码包含对已定义实体的引用。 如果实体是在内联命名空间中定义的，而另一个具有相同名称和类型签名的实体在封闭命名空间中定义，则主机编译器可能会认为此引用不明确，主机编译将失败。<br>可以通过对内联命名空间中定义的此类实体使用唯一名称来避免此限制。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">int</span> Gvar;</span><br><span class="line"><span class="keyword">inline</span> <span class="keyword">namespace</span> N1 &#123;</span><br><span class="line">  __device__ <span class="type">int</span> Gvar;  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// &lt;-- CUDA compiler inserts a reference to &quot;Gvar&quot; at this point in the </span></span><br><span class="line"><span class="comment">// translation unit. This reference will be considered ambiguous by the </span></span><br><span class="line"><span class="comment">// host compiler and compilation will fail.</span></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="keyword">namespace</span> N1 &#123;</span><br><span class="line">  <span class="keyword">namespace</span> N2 &#123;</span><br><span class="line">    __device__ <span class="type">int</span> Gvar;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> N2 &#123;</span><br><span class="line">  __device__ <span class="type">int</span> Gvar;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// &lt;-- CUDA compiler inserts reference to &quot;::N2::Gvar&quot; at this point in </span></span><br><span class="line"><span class="comment">// the translation unit. This reference will be considered ambiguous by </span></span><br><span class="line"><span class="comment">// the host compiler and compilation will fail.</span></span><br></pre></td></tr></table></figure>
<h5 id="I-4-20-6-1-Inline-unnamed-namespaces"><a href="#I-4-20-6-1-Inline-unnamed-namespaces" class="headerlink" title="I.4.20.6.1. Inline unnamed namespaces"></a>I.4.20.6.1. Inline unnamed namespaces</h5><p>不能在内联未命名命名空间内的命名空间范围内声明以下实体：</p>
<ul>
<li><code>__managed__</code>、<code>__device__</code>、<code>__shared__</code> 和 <code>__constant__</code> 变量</li>
<li><code>__global__</code> 函数和函数模板</li>
<li>具有表面或纹理类型的变量</li>
</ul>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="keyword">namespace</span> &#123;</span><br><span class="line">  <span class="keyword">namespace</span> N2 &#123;</span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">    <span class="function">__global__ <span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span></span>;            <span class="comment">// error</span></span><br><span class="line">    </span><br><span class="line">    <span class="function">__global__ <span class="type">void</span> <span class="title">bar</span><span class="params">(<span class="type">void</span>)</span> </span>&#123; &#125;         <span class="comment">// error</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">template</span> &lt;&gt;</span><br><span class="line">    __global__ <span class="type">void</span> <span class="built_in">foo</span>&lt;<span class="type">int</span>&gt;(<span class="type">void</span>) &#123; &#125;    <span class="comment">// error</span></span><br><span class="line">      </span><br><span class="line">    __device__ <span class="type">int</span> x1b;                   <span class="comment">// error</span></span><br><span class="line">    __constant__ <span class="type">int</span> x2b;                 <span class="comment">// error</span></span><br><span class="line">    __shared__ <span class="type">int</span> x3b;                   <span class="comment">// error </span></span><br><span class="line">	</span><br><span class="line">    texture&lt;<span class="type">int</span>&gt; q2;                      <span class="comment">// error</span></span><br><span class="line">    surface&lt;<span class="type">int</span>&gt; s2;                      <span class="comment">// error</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h4 id="I-4-20-7-thread-local"><a href="#I-4-20-7-thread-local" class="headerlink" title="I.4.20.7. thread_local"></a>I.4.20.7. thread_local</h4><p>设备代码中不允许使用 <code>thread_local</code> 存储说明符。</p>
<h4 id="I-4-20-8-global-functions-and-function-templates"><a href="#I-4-20-8-global-functions-and-function-templates" class="headerlink" title="I.4.20.8. global functions and function templates"></a>I.4.20.8. <strong>global</strong> functions and function templates</h4><p>如果在 <code>__global__</code> 函数模板实例化的模板参数中使用与 lambda 表达式关联的闭包类型，则 lambda 表达式必须在 <code>__device__</code> 或 <code>__global__</code> 函数的直接或嵌套块范围内定义，或者必须是扩展 lambda。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(T in)</span> </span>&#123; &#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">foo_device</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">// All kernel instantiations in this function</span></span><br><span class="line">  <span class="comment">// are valid, since the lambdas are defined inside</span></span><br><span class="line">  <span class="comment">// a __device__ function.</span></span><br><span class="line">  </span><br><span class="line">  kernel&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;( [] __device__ &#123; &#125; );</span><br><span class="line">  kernel&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;( [] __host__ __device__ &#123; &#125; );</span><br><span class="line">  kernel&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;( []  &#123; &#125; );</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> lam1 = [] &#123; &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> lam2 = [] __host__ __device__ &#123; &#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo_host</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="comment">// OK: instantiated with closure type of an extended __device__ lambda</span></span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;( [] __device__ &#123; &#125; );</span><br><span class="line">   </span><br><span class="line">   <span class="comment">// OK: instantiated with closure type of an extended __host__ __device__ </span></span><br><span class="line">   <span class="comment">// lambda</span></span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;( [] __host__ __device__ &#123; &#125; );</span><br><span class="line"> </span><br><span class="line">   <span class="comment">// error: unsupported: instantiated with closure type of a lambda</span></span><br><span class="line">   <span class="comment">// that is not an extended lambda</span></span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;( []  &#123; &#125; );</span><br><span class="line">   </span><br><span class="line">   <span class="comment">// error: unsupported: instantiated with closure type of a lambda</span></span><br><span class="line">   <span class="comment">// that is not an extended lambda</span></span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;( lam1);</span><br><span class="line">   </span><br><span class="line">   <span class="comment">// error: unsupported: instantiated with closure type of a lambda</span></span><br><span class="line">   <span class="comment">// that is not an extended lambda</span></span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;( lam2);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>__global__</code> 函数或函数模板不能声明为 <code>constexpr</code>。</p>
<p><code>__global__</code> 函数或函数模板不能有 <code>std::initializer_list</code> 或 <code>va_list</code> 类型的参数。</p>
<p><code>__global__</code> 函数不能有右值引用类型的参数。</p>
<p>可变参数 <code>__global__</code> 函数模板具有以下限制：</p>
<ul>
<li>只允许一个包参数。</li>
<li>pack 参数必须在模板参数列表中最后列出。</li>
</ul>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ok</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">template</span> &lt;<span class="keyword">typename</span>...&gt; <span class="keyword">class</span> <span class="title class_">Wrapper</span>, <span class="keyword">typename</span>... Pack&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">foo1</span><span class="params">(Wrapper&lt;Pack...&gt;)</span></span>;</span><br><span class="line">    </span><br><span class="line"><span class="comment">// error: pack parameter is not last in parameter list</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span>... Pack, <span class="keyword">template</span> &lt;<span class="keyword">typename</span>...&gt; <span class="keyword">class</span> <span class="title class_">Wrapper</span>&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">foo2</span><span class="params">(Wrapper&lt;Pack...&gt;)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// error: multiple parameter packs</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span>... Pack1, <span class="type">int</span>...Pack2, <span class="keyword">template</span>&lt;<span class="keyword">typename</span>...&gt; <span class="keyword">class</span> <span class="title class_">Wrapper1</span>, </span><br><span class="line">          <span class="keyword">template</span>&lt;<span class="type">int</span>...&gt; <span class="keyword">class</span> <span class="title class_">Wrapper2</span>&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">foo3</span><span class="params">(Wrapper1&lt;Pack<span class="number">1.</span>..&gt;, Wrapper2&lt;Pack<span class="number">2.</span>..&gt;)</span></span>;</span><br></pre></td></tr></table></figure>
<h4 id="I-4-20-9-managed-and-shared-variables"><a href="#I-4-20-9-managed-and-shared-variables" class="headerlink" title="I.4.20.9. managed and shared variables"></a>I.4.20.9. <strong>managed</strong> and <strong>shared</strong> variables</h4><p><code>__managed__</code> 和 <code>__shared__</code> 变量不能用关键字 <code>constexpr</code> 标记。</p>
<h4 id="I-4-20-10-Defaulted-functions"><a href="#I-4-20-10-Defaulted-functions" class="headerlink" title="I.4.20.10. Defaulted functions"></a>I.4.20.10. Defaulted functions</h4><p>CUDA 编译器会忽略在第一个声明中显式默认的函数上的执行空间说明符。 相反，CUDA 编译器将推断执行空间说明符，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compiler-generated-functions">隐式声明和显式默认函数</a>中所述。</p>
<p>如果函数是显式默认的，则不会忽略执行空间说明符，但不会在其第一次声明时忽略。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">S1</span> &#123;</span><br><span class="line">  <span class="comment">// warning: __host__ annotation is ignored on a function that </span></span><br><span class="line">  <span class="comment">//          is explicitly-defaulted on its first declaration</span></span><br><span class="line">  <span class="function">__host__ <span class="title">S1</span><span class="params">()</span> </span>= <span class="keyword">default</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">foo1</span><span class="params">()</span> </span>&#123; </span><br><span class="line">  <span class="comment">//note: __device__ execution space is derived for S1::S1 </span></span><br><span class="line">  <span class="comment">//       based on implicit call from within __device__ function </span></span><br><span class="line">  <span class="comment">//       foo1</span></span><br><span class="line">  S1 s1;    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S2</span> &#123;</span><br><span class="line">  <span class="function">__host__ <span class="title">S2</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//note: S2::S2 is not defaulted on its first declaration, and </span></span><br><span class="line"><span class="comment">//      its execution space is fixed to __host__  based on its </span></span><br><span class="line"><span class="comment">//      first declaration.</span></span><br><span class="line">S2::<span class="built_in">S2</span>() = <span class="keyword">default</span>;  </span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">foo2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="comment">// error: call from __device__ function &#x27;foo2&#x27; to </span></span><br><span class="line">   <span class="comment">//        __host__ function &#x27;S2::S2&#x27;</span></span><br><span class="line">   S2 s2;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="I-4-21-C-14-Features"><a href="#I-4-21-C-14-Features" class="headerlink" title="I.4.21. C++14 Features"></a>I.4.21. C++14 Features</h3><p>nvcc 也支持主机编译器默认启用的 C++14 功能。 传递 nvcc <code>-std=c++14</code> 标志打开所有 C++14 功能，并使用相应的 C++14 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_27">选项</a> 调用主机预处理器、编译器和链接器。本节描述了对受支持的 C++ 14 的限制特点。</p>
<h4 id="I-4-21-1-Functions-with-deduced-return-type"><a href="#I-4-21-1-Functions-with-deduced-return-type" class="headerlink" title="I.4.21.1. Functions with deduced return type"></a>I.4.21.1. Functions with deduced return type</h4><p><code>__global__</code> 函数不能有推导的返回类型。</p>
<p>如果 <code>__device__</code> 函数推导出返回类型，CUDA 前端编译器将在调用主机编译器之前将函数声明更改为具有 void 返回类型。 这可能会导致在主机代码中自省 <code>__device__</code> 函数的推导返回类型时出现问题。 因此，CUDA 编译器将发出编译时错误，用于在设备函数体之外引用此类推导的返回类型，除非在 <code>__CUDA_ARCH__</code> 未定义时引用不存在。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__device__ <span class="keyword">auto</span> <span class="title">fn1</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="keyword">decltype</span>(<span class="keyword">auto</span>) <span class="title">fn2</span><span class="params">(<span class="type">int</span> x)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">device_fn1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// OK</span></span><br><span class="line">  <span class="built_in">int</span> (*p1)(<span class="type">int</span>) = fn1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// error: referenced outside device function bodies</span></span><br><span class="line"><span class="keyword">decltype</span>(<span class="built_in">fn1</span>(<span class="number">10</span>)) g1;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">host_fn1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// error: referenced outside device function bodies</span></span><br><span class="line">  <span class="built_in">int</span> (*p1)(<span class="type">int</span>) = fn1;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">S_local_t</span> &#123;</span><br><span class="line">    <span class="comment">// error: referenced outside device function bodies</span></span><br><span class="line">    <span class="keyword">decltype</span>(<span class="built_in">fn2</span>(<span class="number">10</span>)) m1;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">S_local_t</span>() : <span class="built_in">m1</span>(<span class="number">10</span>) &#123; &#125;</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// error: referenced outside device function bodies</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T = <span class="keyword">decltype</span>(fn2)&gt;</span><br><span class="line"><span class="type">void</span> <span class="built_in">host_fn2</span>() &#123; &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt; <span class="keyword">struct</span> S1_t &#123; &#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// error: referenced outside device function bodies</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S1_derived_t</span> : S1_t&lt;<span class="keyword">decltype</span>(fn1)&gt; &#123; &#125;;</span><br></pre></td></tr></table></figure>
<h4 id="I-4-21-2-Variable-templates"><a href="#I-4-21-2-Variable-templates" class="headerlink" title="I.4.21.2. Variable templates"></a>I.4.21.2. Variable templates</h4><p>使用 Microsoft 主机编译器时，<code>__device__/__constant__</code> 变量模板不能具有 <code>const</code> 限定类型。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// error: a __device__ variable template cannot</span></span><br><span class="line"><span class="comment">// have a const qualified type on Windows</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ <span class="type">const</span> T <span class="title">d1</span><span class="params">(<span class="number">2</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> *<span class="type">const</span> x = <span class="literal">nullptr</span>;</span><br><span class="line"><span class="comment">// error: a __device__ variable template cannot</span></span><br><span class="line"><span class="comment">// have a const qualified type on Windows</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ T *<span class="type">const</span> <span class="title">d2</span><span class="params">(x)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// OK</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">__device__ <span class="type">const</span> T *d3;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">fn</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> t1 = d1&lt;<span class="type">int</span>&gt;;</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> *<span class="type">const</span> t2 = d2&lt;<span class="type">int</span>&gt;;</span><br><span class="line"></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> *t3 = d3&lt;<span class="type">int</span>&gt;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="I-4-22-C-17-Features"><a href="#I-4-22-C-17-Features" class="headerlink" title="I.4.22. C++17 Features"></a>I.4.22. C++17 Features</h3><p>nvcc 也支持主机编译器默认启用的 C++17 功能。 传递 nvcc <code>-std=c++17</code> 标志会打开所有 C++17 功能，并使用相应的 C++17 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_28">选项</a>调用主机预处理器、编译器和链接器。本节描述对支持的 C++ 17的限制特点。</p>
<h4 id="I-4-22-1-Inline-Variable"><a href="#I-4-22-1-Inline-Variable" class="headerlink" title="I.4.22.1. Inline Variable"></a>I.4.22.1. Inline Variable</h4><p>如果代码在整个程序编译模式下使用 nvcc 编译，则使用 <code>__device__</code> 或 <code>__constant__</code> 或 <code>__managed__</code> 内存空间说明符声明的命名空间范围内联变量必须具有内部链接。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> __device__ <span class="type">int</span> xxx; <span class="comment">//error when compiled with nvcc in</span></span><br><span class="line">                                    <span class="comment">//whole program compilation mode.</span></span><br><span class="line">                                    <span class="comment">//ok when compiled with nvcc in</span></span><br><span class="line">                                    <span class="comment">//separate compilation mode.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">inline</span> __shared__ <span class="type">int</span> yyy0; <span class="comment">// ok.</span></span><br><span class="line"></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span> __device__ <span class="type">int</span> yyy; <span class="comment">// ok: internal linkage</span></span><br><span class="line"><span class="keyword">namespace</span> &#123;</span><br><span class="line"><span class="keyword">inline</span> __device__ <span class="type">int</span> zzz; <span class="comment">// ok: internal linkage</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>使用 g++ 主机编译器时，使用 <code>__managed__</code> 内存空间说明符声明的内联变量可能对调试器不可见。</p>
<h4 id="I-4-22-2-Structured-Binding"><a href="#I-4-22-2-Structured-Binding" class="headerlink" title="I.4.22.2. Structured Binding"></a>I.4.22.2. Structured Binding</h4><p>不能使用可变内存空间说明符声明结构化绑定。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">S</span> &#123; <span class="type">int</span> x; <span class="type">int</span> y; &#125;;</span><br><span class="line">__device__ <span class="keyword">auto</span> [a1, b1] = S&#123;<span class="number">4</span>,<span class="number">5</span>&#125;; <span class="comment">// error</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="I-5-Polymorphic-Function-Wrappers"><a href="#I-5-Polymorphic-Function-Wrappers" class="headerlink" title="I.5. Polymorphic Function Wrappers"></a>I.5. Polymorphic Function Wrappers</h2><p>在 <code>nvfunctional</code> 头文件中提供了一个多态函数包装类模板 <code>nvstd::function</code>。 此类模板的实例可用于存储、复制和调用任何可调用目标，例如 lambda 表达式。 <code>nvstd::function</code> 可以在主机和设备代码中使用。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;nvfunctional&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">int</span> <span class="title">foo_d</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="number">1</span>; &#125;</span><br><span class="line"><span class="function">__host__ __device__ <span class="type">int</span> <span class="title">foo_hd</span> <span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="number">2</span>; &#125;</span><br><span class="line"><span class="function">__host__ <span class="type">int</span> <span class="title">foo_h</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="number">3</span>; &#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(<span class="type">int</span> *result)</span> </span>&#123;</span><br><span class="line">  nvstd::function&lt;<span class="type">int</span>()&gt; fn1 = foo_d;  </span><br><span class="line">  nvstd::function&lt;<span class="type">int</span>()&gt; fn2 = foo_hd;</span><br><span class="line">  nvstd::function&lt;<span class="type">int</span>()&gt; fn3 =  []() &#123; <span class="keyword">return</span> <span class="number">10</span>; &#125;;</span><br><span class="line"></span><br><span class="line">  *result = <span class="built_in">fn1</span>() + <span class="built_in">fn2</span>() + <span class="built_in">fn3</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__host__ __device__ <span class="type">void</span> <span class="title">hostdevice_func</span><span class="params">(<span class="type">int</span> *result)</span> </span>&#123;</span><br><span class="line">  nvstd::function&lt;<span class="type">int</span>()&gt; fn1 = foo_hd;  </span><br><span class="line">  nvstd::function&lt;<span class="type">int</span>()&gt; fn2 =  []() &#123; <span class="keyword">return</span> <span class="number">10</span>; &#125;;</span><br><span class="line"></span><br><span class="line">  *result = <span class="built_in">fn1</span>() + <span class="built_in">fn2</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__host__ <span class="type">void</span> <span class="title">host_func</span><span class="params">(<span class="type">int</span> *result)</span> </span>&#123;</span><br><span class="line">  nvstd::function&lt;<span class="type">int</span>()&gt; fn1 = foo_h;  </span><br><span class="line">  nvstd::function&lt;<span class="type">int</span>()&gt; fn2 = foo_hd;  </span><br><span class="line">  nvstd::function&lt;<span class="type">int</span>()&gt; fn3 =  []() &#123; <span class="keyword">return</span> <span class="number">10</span>; &#125;;</span><br><span class="line"></span><br><span class="line">  *result = <span class="built_in">fn1</span>() + <span class="built_in">fn2</span>() + <span class="built_in">fn3</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>主机代码中的 <code>nvstd::function</code> 实例不能用 <code>__device__</code> 函数的地址或 <code>operator()</code> 为 <code>__device__</code> 函数的函子初始化。 设备代码中的 <code>nvstd::function</code> 实例不能用 <code>__host__</code> 函数的地址或 <code>operator()</code> 为 <code>__host__</code> 函数的仿函数初始化。</p>
<p><code>nvstd::function</code> 实例不能在运行时从主机代码传递到设备代码（反之亦然）。 如果 <code>__global__</code> 函数是从主机代码启动的，则 <code>nvstd::function</code> 不能用于 <code>__global__</code> 函数的参数类型。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;nvfunctional&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">int</span> <span class="title">foo_d</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="number">1</span>; &#125;</span><br><span class="line"><span class="function">__host__ <span class="type">int</span> <span class="title">foo_h</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="number">3</span>; &#125;</span><br><span class="line"><span class="keyword">auto</span> lam_h = [] &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">k</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// error: initialized with address of __host__ function </span></span><br><span class="line">  nvstd::function&lt;<span class="type">int</span>()&gt; fn1 = foo_h;  </span><br><span class="line"></span><br><span class="line">  <span class="comment">// error: initialized with address of functor with</span></span><br><span class="line">  <span class="comment">// __host__ operator() function </span></span><br><span class="line">  nvstd::function&lt;<span class="type">int</span>()&gt; fn2 = lam_h;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kern</span><span class="params">(nvstd::function&lt;<span class="type">int</span>()&gt; f1)</span> </span>&#123; &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// error: initialized with address of __device__ function </span></span><br><span class="line">  nvstd::function&lt;<span class="type">int</span>()&gt; fn1 = foo_d;  </span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> lam_d = [=] __device__ &#123; <span class="keyword">return</span> <span class="number">1</span>; &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// error: initialized with address of functor with</span></span><br><span class="line">  <span class="comment">// __device__ operator() function </span></span><br><span class="line">  nvstd::function&lt;<span class="type">int</span>()&gt; fn2 = lam_d;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// error: passing nvstd::function from host to device</span></span><br><span class="line">  kern&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(fn2);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><code>nvstd::function</code> 在 <code>nvfunctional</code> 头文件中定义如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> nvstd &#123;</span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">_RetType</span>, <span class="keyword">class</span> ..._ArgTypes&gt;</span><br><span class="line">  <span class="keyword">class</span> <span class="title class_">function</span>&lt;_RetType(_ArgTypes...)&gt; </span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">      <span class="comment">// constructors</span></span><br><span class="line">      <span class="function">__device__ __host__  <span class="title">function</span><span class="params">()</span> <span class="keyword">noexcept</span></span>;</span><br><span class="line">      <span class="function">__device__ __host__  <span class="title">function</span><span class="params">(<span class="type">nullptr_t</span>)</span> <span class="keyword">noexcept</span></span>;</span><br><span class="line">      <span class="function">__device__ __host__  <span class="title">function</span><span class="params">(<span class="type">const</span> function &amp;)</span></span>;</span><br><span class="line">      <span class="function">__device__ __host__  <span class="title">function</span><span class="params">(function &amp;&amp;)</span></span>;</span><br><span class="line"></span><br><span class="line">      <span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> _F&gt;</span></span><br><span class="line"><span class="function">      __device__ __host__  <span class="title">function</span><span class="params">(_F)</span></span>;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// destructor</span></span><br><span class="line">      __device__ __host__  ~<span class="built_in">function</span>();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// assignment operators</span></span><br><span class="line">      __device__ __host__  function&amp; <span class="keyword">operator</span>=(<span class="type">const</span> function&amp;);</span><br><span class="line">      __device__ __host__  function&amp; <span class="keyword">operator</span>=(function&amp;&amp;);</span><br><span class="line">      __device__ __host__  function&amp; <span class="keyword">operator</span>=(<span class="type">nullptr_t</span>);</span><br><span class="line">      __device__ __host__  function&amp; <span class="keyword">operator</span>=(_F&amp;&amp;);</span><br><span class="line"></span><br><span class="line">      <span class="comment">// swap</span></span><br><span class="line">      <span class="function">__device__ __host__  <span class="type">void</span> <span class="title">swap</span><span class="params">(function&amp;)</span> <span class="keyword">noexcept</span></span>;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// function capacity</span></span><br><span class="line">      <span class="function">__device__ __host__  <span class="keyword">explicit</span> <span class="keyword">operator</span> <span class="title">bool</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span>;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// function invocation</span></span><br><span class="line">      <span class="function">__device__ _RetType <span class="title">operator</span><span class="params">()</span><span class="params">(_ArgTypes...)</span> <span class="type">const</span></span>;</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// null pointer comparisons</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">_R</span>, <span class="keyword">class</span>... _ArgTypes&gt;</span><br><span class="line">  __device__ __host__</span><br><span class="line">  <span class="type">bool</span> <span class="keyword">operator</span>==(<span class="type">const</span> function&lt;_R(_ArgTypes...)&gt;&amp;, <span class="type">nullptr_t</span>) <span class="keyword">noexcept</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">_R</span>, <span class="keyword">class</span>... _ArgTypes&gt;</span><br><span class="line">  __device__ __host__</span><br><span class="line">  <span class="type">bool</span> <span class="keyword">operator</span>==(<span class="type">nullptr_t</span>, <span class="type">const</span> function&lt;_R(_ArgTypes...)&gt;&amp;) <span class="keyword">noexcept</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">_R</span>, <span class="keyword">class</span>... _ArgTypes&gt;</span><br><span class="line">  __device__ __host__</span><br><span class="line">  <span class="type">bool</span> <span class="keyword">operator</span>!=(<span class="type">const</span> function&lt;_R(_ArgTypes...)&gt;&amp;, <span class="type">nullptr_t</span>) <span class="keyword">noexcept</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">_R</span>, <span class="keyword">class</span>... _ArgTypes&gt;</span><br><span class="line">  __device__ __host__</span><br><span class="line">  <span class="type">bool</span> <span class="keyword">operator</span>!=(<span class="type">nullptr_t</span>, <span class="type">const</span> function&lt;_R(_ArgTypes...)&gt;&amp;) <span class="keyword">noexcept</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// specialized algorithms</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">_R</span>, <span class="keyword">class</span>... _ArgTypes&gt;</span><br><span class="line">  <span class="function">__device__ __host__</span></span><br><span class="line"><span class="function">  <span class="type">void</span> <span class="title">swap</span><span class="params">(function&lt;_R(_ArgTypes...)&gt;&amp;, function&lt;_R(_ArgTypes...)&gt;&amp;)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="I-6-Extended-Lambdas"><a href="#I-6-Extended-Lambdas" class="headerlink" title="I.6. Extended Lambdas"></a>I.6. Extended Lambdas</h2><p>nvcc 标志 ‘<code>--extended-lambda</code>‘ 允许在 lambda <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_29">表达式</a>中显式执行空间注释。执行空间注释应该出现在 ‘<code>lambda-introducer</code>‘ 之后和可选的 ‘<code>lambda-declarator</code>‘ 之前。当指定了“<code>--extended-lambda</code>”标志时，nvcc 将定义宏 <code>__CUDACC_EXTENDED_LAMBDA__</code>。</p>
<p>‘<code>extended __device__ lambda</code>‘ 是一个用 ‘<code>__device__</code>‘ 显式注释的 lambda 表达式，并在 <code>__host__</code> 或 <code>__host__ __device__</code> 函数的直接或嵌套块范围内定义。</p>
<p>‘<code>extended __host__ __device__ lambda</code>‘ 是一个用 ‘<code>__host__</code>‘ 和 ‘<code>__device__</code>‘ 显式注释的 lambda 表达式，并在 <code>__host__</code> 或 <code>__host__ __device__</code> 函数的直接或嵌套块范围内定义。</p>
<p>“<code>extended lambda</code>”表示扩展的 <code>__device__</code> lambda 或扩展的 <code>__host__ __device__</code> lambda。扩展的 lambda 可用于 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cpp11-global"><code>__global__</code> 函数模板实例化</a>的类型参数。</p>
<p>如果未明确指定执行空间注释，则它们是根据包含与 lambda 关联的闭包类的范围计算的，如 C++11 支持部分所述。执行空间注释应用于与 lambda 关联的闭包类的所有方法。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo_host</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// not an extended lambda: no explicit execution space annotations</span></span><br><span class="line">  <span class="keyword">auto</span> lam1 = [] &#123; &#125;;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// extended __device__ lambda</span></span><br><span class="line">  <span class="keyword">auto</span> lam2 = [] __device__ &#123; &#125;;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// extended __host__ __device__ lambda</span></span><br><span class="line">  <span class="keyword">auto</span> lam3 = [] __host__ __device__ &#123; &#125;;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// not an extended lambda: explicitly annotated with only &#x27;__host__&#x27;</span></span><br><span class="line">  <span class="keyword">auto</span> lam4 = [] __host__ &#123; &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__host__ __device__ <span class="type">void</span> <span class="title">foo_host_device</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// not an extended lambda: no explicit execution space annotations</span></span><br><span class="line">  <span class="keyword">auto</span> lam1 = [] &#123; &#125;;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// extended __device__ lambda</span></span><br><span class="line">  <span class="keyword">auto</span> lam2 = [] __device__ &#123; &#125;;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// extended __host__ __device__ lambda</span></span><br><span class="line">  <span class="keyword">auto</span> lam3 = [] __host__ __device__ &#123; &#125;;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// not an extended lambda: explicitly annotated with only &#x27;__host__&#x27;</span></span><br><span class="line">  <span class="keyword">auto</span> lam4 = [] __host__ &#123; &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">foo_device</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// none of the lambdas within this function are extended lambdas, </span></span><br><span class="line">  <span class="comment">// because the enclosing function is not a __host__ or __host__ __device__</span></span><br><span class="line">  <span class="comment">// function.</span></span><br><span class="line">  <span class="keyword">auto</span> lam1 = [] &#123; &#125;;</span><br><span class="line">  <span class="keyword">auto</span> lam2 = [] __device__ &#123; &#125;;</span><br><span class="line">  <span class="keyword">auto</span> lam3 = [] __host__ __device__ &#123; &#125;;</span><br><span class="line">  <span class="keyword">auto</span> lam4 = [] __host__ &#123; &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// lam1 and lam2 are not extended lambdas because they are not defined</span></span><br><span class="line"><span class="comment">// within a __host__ or __host__ __device__ function.</span></span><br><span class="line"><span class="keyword">auto</span> lam1 = [] &#123; &#125;;</span><br><span class="line"><span class="keyword">auto</span> lam2 = [] __host__ __device__ &#123; &#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="I-6-1-Extended-Lambda-Type-Traits"><a href="#I-6-1-Extended-Lambda-Type-Traits" class="headerlink" title="I.6.1. Extended Lambda Type Traits"></a>I.6.1. Extended Lambda Type Traits</h3><p>编译器提供类型特征来在编译时检测扩展 lambda 的闭包类型：</p>
<p><code>__nv_is_extended_device_lambda_closure_type(type)</code>：如果 ‘type’ 是为扩展的 <code>__device__</code> lambda 创建的闭包类，则 <code>trait</code> 为真，否则为假。</p>
<p><code>__nv_is_extended_host_device_lambda_closure_type(type)</code>：如果 ‘type’ 是为扩展的 <code>__host__ __device__</code> lambda 创建的闭包类，则 <code>trait</code> 为真，否则为假。</p>
<p>这些特征可以在所有编译模式中使用，<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_30">无论是启用 lambda 还是扩展 lambda</a>。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> IS_D_LAMBDA(X) __nv_is_extended_device_lambda_closure_type(X)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> IS_HD_LAMBDA(X) __nv_is_extended_host_device_lambda_closure_type(X)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> lam0 = [] __host__ __device__ &#123; &#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> lam1 = [] &#123; &#125;; </span><br><span class="line">  <span class="keyword">auto</span> lam2 = [] __device__ &#123; &#125;;</span><br><span class="line">  <span class="keyword">auto</span> lam3 = [] __host__ __device__ &#123; &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// lam0 is not an extended lambda (since defined outside function scope)</span></span><br><span class="line">  <span class="built_in">static_assert</span>(!<span class="built_in">IS_D_LAMBDA</span>(<span class="keyword">decltype</span>(lam0)), <span class="string">&quot;&quot;</span>);</span><br><span class="line">  <span class="built_in">static_assert</span>(!<span class="built_in">IS_HD_LAMBDA</span>(<span class="keyword">decltype</span>(lam0)), <span class="string">&quot;&quot;</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// lam1 is not an extended lambda (since no execution space annotations)</span></span><br><span class="line">  <span class="built_in">static_assert</span>(!<span class="built_in">IS_D_LAMBDA</span>(<span class="keyword">decltype</span>(lam1)), <span class="string">&quot;&quot;</span>);</span><br><span class="line">  <span class="built_in">static_assert</span>(!<span class="built_in">IS_HD_LAMBDA</span>(<span class="keyword">decltype</span>(lam1)), <span class="string">&quot;&quot;</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// lam2 is an extended __device__ lambda</span></span><br><span class="line">  <span class="built_in">static_assert</span>(<span class="built_in">IS_D_LAMBDA</span>(<span class="keyword">decltype</span>(lam2)), <span class="string">&quot;&quot;</span>);</span><br><span class="line">  <span class="built_in">static_assert</span>(!<span class="built_in">IS_HD_LAMBDA</span>(<span class="keyword">decltype</span>(lam2)), <span class="string">&quot;&quot;</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// lam3 is an extended __host__ __device__ lambda</span></span><br><span class="line">  <span class="built_in">static_assert</span>(!<span class="built_in">IS_D_LAMBDA</span>(<span class="keyword">decltype</span>(lam3)), <span class="string">&quot;&quot;</span>);</span><br><span class="line">  <span class="built_in">static_assert</span>(<span class="built_in">IS_HD_LAMBDA</span>(<span class="keyword">decltype</span>(lam3)), <span class="string">&quot;&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="I-6-2-Extended-Lambda-Restrictions"><a href="#I-6-2-Extended-Lambda-Restrictions" class="headerlink" title="I.6.2. Extended Lambda Restrictions"></a>I.6.2. Extended Lambda Restrictions</h3><p>在调用主机编译器之前，CUDA 编译器将用命名空间范围内定义的占位符类型的实例替换扩展的 lambda 表达式。占位符类型的模板参数需要获取包含原始扩展 lambda 表达式的函数的地址。这是正确执行任何模板参数涉及扩展 lambda 的闭包类型的 <code>__global__</code> 函数模板所必需的。封闭函数计算如下。</p>
<p>根据定义，扩展 lambda 存在于 <code>__host__</code> 或 <code>__host__ __device__</code> 函数的直接或嵌套块范围内。如果此函数不是 lambda 表达式的 <code>operator()</code>，则将其视为扩展 lambda 的封闭函数。否则，扩展 lambda 定义在一个或多个封闭 lambda 表达式的 <code>operator()</code> 的直接或嵌套块范围内。如果最外层的这种 lambda 表达式定义在函数 F 的直接或嵌套块范围内，则 F 是计算的封闭函数，否则封闭函数不存在。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// enclosing function for lam1 is &quot;foo&quot;</span></span><br><span class="line">  <span class="keyword">auto</span> lam1 = [] __device__ &#123; &#125;;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">auto</span> lam2 = [] &#123;</span><br><span class="line">     <span class="keyword">auto</span> lam3 = [] &#123;</span><br><span class="line">        <span class="comment">// enclosing function for lam4 is &quot;foo&quot;</span></span><br><span class="line">        <span class="keyword">auto</span> lam4 = [] __host__ __device__ &#123; &#125;;</span><br><span class="line">     &#125;;</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> lam6 = [] &#123;</span><br><span class="line">  <span class="comment">// enclosing function for lam7 does not exist</span></span><br><span class="line">  <span class="keyword">auto</span> lam7 = [] __host__ __device__ &#123; &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以下是对扩展 lambda 的限制：</p>
<p>扩展 lambda 不能在另一个扩展 lambda 表达式中定义。<br>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> lam1 = [] __host__ __device__  &#123;</span><br><span class="line">    <span class="comment">// error: extended lambda defined within another extended lambda</span></span><br><span class="line">    <span class="keyword">auto</span> lam2 = [] __host__ __device__ &#123; &#125;;</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>不能在通用 lambda 表达式中定义扩展 lambda。<br>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> lam1 = [] (<span class="keyword">auto</span>) &#123;</span><br><span class="line">    <span class="comment">// error: extended lambda defined within a generic lambda</span></span><br><span class="line">    <span class="keyword">auto</span> lam2 = [] __host__ __device__ &#123; &#125;;</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如果扩展 lambda 定义在一个或多个嵌套 lambda 表达式的直接或嵌套块范围内，则最外层的此类 lambda 表达式必须定义在函数的直接或嵌套块范围内。<br>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> lam1 = []  &#123;</span><br><span class="line">  <span class="comment">// error: outer enclosing lambda is not defined within a</span></span><br><span class="line">  <span class="comment">// non-lambda-operator() function. </span></span><br><span class="line">  <span class="keyword">auto</span> lam2 = [] __host__ __device__ &#123; &#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>必须命名扩展 lambda 的封闭函数，并且可以获取其地址。 如果封闭函数是类成员，则必须满足以下条件：</p>
<ul>
<li>包含成员函数的所有类都必须有一个名称。</li>
<li>成员函数在其父类中不得具有私有或受保护的访问权限。</li>
<li>所有封闭类在其各自的父类中不得具有私有或受保护的访问权限。</li>
</ul>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// OK</span></span><br><span class="line">  <span class="keyword">auto</span> lam1 = [] __device__ &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;;</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">// OK</span></span><br><span class="line">    <span class="keyword">auto</span> lam2 = [] __device__ &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;;</span><br><span class="line">    <span class="comment">// OK</span></span><br><span class="line">    <span class="keyword">auto</span> lam3 = [] __device__ __host__ &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123;</span><br><span class="line">  <span class="built_in">S1_t</span>(<span class="type">void</span>) &#123;</span><br><span class="line">    <span class="comment">// Error: cannot take address of enclosing function</span></span><br><span class="line">    <span class="keyword">auto</span> lam4 = [] __device__ &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;; </span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">C0_t</span> &#123;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123; </span><br><span class="line">    <span class="comment">// Error: enclosing function has private access in parent class</span></span><br><span class="line">    <span class="keyword">auto</span> temp1 = [] __device__ &#123; <span class="keyword">return</span> <span class="number">10</span>; &#125;;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">S2_t</span> &#123;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// Error: enclosing class S2_t has private access in its </span></span><br><span class="line">      <span class="comment">// parent class</span></span><br><span class="line">      <span class="keyword">auto</span> temp1 = [] __device__ &#123; <span class="keyword">return</span> <span class="number">10</span>; &#125;;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>必须可以在定义扩展 lambda 的位置明确地获取封闭例程的地址。 这在某些情况下可能不可行，例如 当类 <code>typedef</code> 隐藏同名的模板类型参数时。<br>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span>&gt; <span class="keyword">struct</span> <span class="title class_">A</span> &#123;</span><br><span class="line">  <span class="keyword">typedef</span> <span class="type">void</span> Bar;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">test</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;&gt; <span class="keyword">struct</span> <span class="title class_">A</span>&lt;<span class="type">void</span>&gt; &#123; &#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Bar&gt;</span><br><span class="line"><span class="type">void</span> A&lt;Bar&gt;::<span class="built_in">test</span>() &#123;</span><br><span class="line">  <span class="comment">/* In code sent to host compiler, nvcc will inject an</span></span><br><span class="line"><span class="comment">     address expression here, of the form: </span></span><br><span class="line"><span class="comment">     (void (A&lt; Bar&gt; ::*)(void))(&amp;A::test))</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment">     However, the class typedef &#x27;Bar&#x27; (to void) shadows the</span></span><br><span class="line"><span class="comment">     template argument &#x27;Bar&#x27;, causing the address </span></span><br><span class="line"><span class="comment">     expression in A&lt;int&gt;::test to actually refer to:</span></span><br><span class="line"><span class="comment">     (void (A&lt; void&gt; ::*)(void))(&amp;A::test))</span></span><br><span class="line"><span class="comment">    </span></span><br><span class="line"><span class="comment">     ..which doesn&#x27;t take the address of the enclosing</span></span><br><span class="line"><span class="comment">     routine &#x27;A&lt;int&gt;::test&#x27; correctly.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">  <span class="keyword">auto</span> lam1 = [] __host__ __device__ &#123; <span class="keyword">return</span> <span class="number">4</span>; &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  A&lt;<span class="type">int</span>&gt; xxx;</span><br><span class="line">  xxx.<span class="built_in">test</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>不能在函数本地的类中定义扩展 lambda。<br>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// Error: bar is member of a class that is local to a function.</span></span><br><span class="line">      <span class="keyword">auto</span> lam4 = [] __host__ __device__ &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;; </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>扩展 lambda 的封闭函数不能推导出返回类型。<br>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">auto</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Error: the return type of foo is deduced.</span></span><br><span class="line">  <span class="keyword">auto</span> lam1 = [] __host__ __device__ &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;; </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><code>__host__ __device__</code> 扩展 lambda 不能是通用 lambda。<br>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Error: __host__ __device__ extended lambdas cannot be</span></span><br><span class="line">  <span class="comment">// generic lambdas.</span></span><br><span class="line">  <span class="keyword">auto</span> lam1 = [] __host__ __device__ (<span class="keyword">auto</span> i) &#123; <span class="keyword">return</span> i; &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Error: __host__ __device__ extended lambdas cannot be</span></span><br><span class="line">  <span class="comment">// generic lambdas.</span></span><br><span class="line">  <span class="keyword">auto</span> lam2 = [] __host__ __device__ (<span class="keyword">auto</span> ...i) &#123;</span><br><span class="line">               <span class="keyword">return</span> <span class="keyword">sizeof</span>...(i);</span><br><span class="line">              &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如果封闭函数是函数模板或成员函数模板的实例化，或函数是类模板的成员，则模板必须满足以下约束：</p>
<ul>
<li>模板最多只能有一个可变参数，并且必须在模板参数列表中最后列出。</li>
<li>模板参数必须命名。</li>
<li>模板实例化参数类型不能涉及函数本地的类型（扩展 lambda 的闭包类型除外），或者是私有或受保护的类成员。</li>
</ul>
<p>例子</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kern</span><span class="params">(T in)</span> </span>&#123; <span class="built_in">in</span>(); &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span>... T&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">foo</span> &#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt; <span class="keyword">template</span> &lt;<span class="keyword">typename</span>...&gt; <span class="keyword">class</span> <span class="title class_">T</span>, <span class="keyword">typename</span>... P1, </span><br><span class="line">          <span class="keyword">typename</span>... P2&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar1</span><span class="params">(<span class="type">const</span> T&lt;P<span class="number">1.</span>..&gt;, <span class="type">const</span> T&lt;P<span class="number">2.</span>..&gt;)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Error: enclosing function has multiple parameter packs</span></span><br><span class="line">  <span class="keyword">auto</span> lam1 =  [] __device__ &#123; <span class="keyword">return</span> <span class="number">10</span>; &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt; <span class="keyword">template</span> &lt;<span class="keyword">typename</span>...&gt; <span class="keyword">class</span> <span class="title class_">T</span>, <span class="keyword">typename</span>... P1, </span><br><span class="line">          <span class="keyword">typename</span> T2&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar2</span><span class="params">(<span class="type">const</span> T&lt;P<span class="number">1.</span>..&gt;, T2)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Error: for enclosing function, the</span></span><br><span class="line">  <span class="comment">// parameter pack is not last in the template parameter list.</span></span><br><span class="line">  <span class="keyword">auto</span> lam1 =  [] __device__ &#123; <span class="keyword">return</span> <span class="number">10</span>; &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, T&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar3</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Error: for enclosing function, the second template</span></span><br><span class="line">  <span class="comment">// parameter is not named.</span></span><br><span class="line">  <span class="keyword">auto</span> lam1 =  [] __device__ &#123; <span class="keyword">return</span> <span class="number">10</span>; &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  foo&lt;<span class="type">char</span>, <span class="type">int</span>, <span class="type">float</span>&gt; f1;</span><br><span class="line">  foo&lt;<span class="type">char</span>, <span class="type">int</span>&gt; f2;</span><br><span class="line">  <span class="built_in">bar1</span>(f1, f2);</span><br><span class="line">  <span class="built_in">bar2</span>(f1, <span class="number">10</span>);</span><br><span class="line">  <span class="built_in">bar3</span>&lt;<span class="type">int</span>, <span class="number">10</span>&gt;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kern</span><span class="params">(T in)</span> </span>&#123; <span class="built_in">in</span>(); &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar4</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> lam1 =  [] __device__ &#123; <span class="keyword">return</span> <span class="number">10</span>; &#125;;</span><br><span class="line">  kern&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(lam1);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">C1_t</span> &#123; <span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123; &#125;; <span class="function"><span class="keyword">friend</span> <span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">void</span>)</span></span>; &#125;;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123; &#125;;</span><br><span class="line">  <span class="comment">// Error: enclosing function for device lambda in bar4</span></span><br><span class="line">  <span class="comment">// is instantiated with a type local to main.</span></span><br><span class="line">  <span class="built_in">bar4</span>&lt;S1_t&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Error: enclosing function for device lambda in bar4</span></span><br><span class="line">  <span class="comment">// is instantiated with a type that is a private member</span></span><br><span class="line">  <span class="comment">// of a class.</span></span><br><span class="line">  <span class="built_in">bar4</span>&lt;C1_t::S1_t&gt;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对于 Visual Studio 主机编译器，封闭函数必须具有外部链接。存在限制是因为此主机编译器不支持使用非外部链接函数的地址作为模板参数，而 CUDA 编译器转换需要它来支持扩展的 lambda。</p>
<p>对于 Visual Studio 主机编译器，不应在“if-constexpr”块的主体内定义扩展 lambda。</p>
<p>扩展的 lambda 对捕获的变量有以下限制：</p>
<ul>
<li>在发送到宿主编译器的代码中，变量可以通过值传递给一系列辅助函数，然后用于直接初始化用于表示<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_31">扩展 lambda</a>的闭包类型的类型的字段。</li>
<li>变量只能按值捕获。</li>
<li>如果数组维数大于 7，则无法捕获数组类型的变量。</li>
<li>对于数组类型的变量，在发送到宿主编译器的代码中，首先对闭包类型的数组字段进行默认初始化，然后将数组字段的每个元素从捕获的数组变量的相应元素中复制分配。因此，数组元素类型在宿主代码中必须是默认可构造和可复制分配的。</li>
<li>无法捕获作为可变参数包元素的函数参数。</li>
<li>捕获的变量的类型不能涉及函数本地的类型（扩展 lambda 的闭包类型除外），或者是私有或受保护的类成员。</li>
<li>对于 <code>__host__ __device__</code> 扩展 lambda，在 lambda 表达式的 <code>operator()</code> 的返回或参数类型中使用的类型不能涉及函数本地的类型（扩展 lambda 的闭包类型除外），或者是私有或受保护的类成员.</li>
<li><code>__host__ __device__</code> 扩展 lambdas 不支持初始化捕获。 <code>__device__</code> 扩展 lambda 支持初始化捕获，除非初始化捕获是数组类型或 <code>std::initializer_list</code> 类型。</li>
<li>扩展 lambda 的函数调用运算符不是 constexpr。扩展 lambda 的闭包类型不是文字类型。 constexpr 说明符不能用于扩展 lambda 的声明。</li>
<li>一个变量不能被隐式地捕获在一个词法嵌套在扩展 lambda 内的 if-constexpr 块中，除非它已经在 if-constexpr 块之外早先被隐式捕获或出现在扩展 lambda 的显式捕获列表中（参见下面的示例）。</li>
</ul>
<p>例子</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// OK: an init-capture is allowed for an</span></span><br><span class="line">  <span class="comment">// extended __device__ lambda.</span></span><br><span class="line">  <span class="keyword">auto</span> lam1 = [x = <span class="number">1</span>] __device__ () &#123; <span class="keyword">return</span> x; &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Error: an init-capture is not allowed for</span></span><br><span class="line">  <span class="comment">// an extended __host__ __device__ lambda.</span></span><br><span class="line">  <span class="keyword">auto</span> lam2 = [x = <span class="number">1</span>] __host__ __device__ () &#123; <span class="keyword">return</span> x; &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> a = <span class="number">1</span>;</span><br><span class="line">  <span class="comment">// Error: an extended __device__ lambda cannot capture</span></span><br><span class="line">  <span class="comment">// variables by reference.</span></span><br><span class="line">  <span class="keyword">auto</span> lam3 = [&amp;a] __device__ () &#123; <span class="keyword">return</span> a; &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Error: by-reference capture is not allowed</span></span><br><span class="line">  <span class="comment">// for an extended __device__ lambda.</span></span><br><span class="line">  <span class="keyword">auto</span> lam4 = [&amp;x = a] __device__ () &#123; <span class="keyword">return</span> x; &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123; &#125;;</span><br><span class="line">  S1_t s1;</span><br><span class="line">  <span class="comment">// Error: a type local to a function cannot be used in the type</span></span><br><span class="line">  <span class="comment">// of a captured variable.</span></span><br><span class="line">  <span class="keyword">auto</span> lam6 = [s1] __device__ () &#123; &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Error: an init-capture cannot be of type std::initializer_list.</span></span><br><span class="line">  <span class="keyword">auto</span> lam7 = [x = &#123;<span class="number">11</span>&#125;] __device__ () &#123; &#125;;</span><br><span class="line"></span><br><span class="line">  std::initializer_list&lt;<span class="type">int</span>&gt; b = &#123;<span class="number">11</span>,<span class="number">22</span>,<span class="number">33</span>&#125;;</span><br><span class="line">  <span class="comment">// Error: an init-capture cannot be of type std::initializer_list.</span></span><br><span class="line">  <span class="keyword">auto</span> lam8 = [x = b] __device__ () &#123; &#125;; </span><br><span class="line"> </span><br><span class="line">  <span class="comment">// Error scenario (lam9) and supported scenarios (lam10, lam11)</span></span><br><span class="line">  <span class="comment">// for capture within &#x27;if-constexpr&#x27; block </span></span><br><span class="line">  <span class="type">int</span> yyy = <span class="number">4</span>;</span><br><span class="line">  <span class="keyword">auto</span> lam9 = [=] __device__ &#123;  </span><br><span class="line">    <span class="type">int</span> result = <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span><span class="params">(<span class="literal">false</span>)</span> </span>&#123;</span><br><span class="line">      <span class="comment">//Error: An extended __device__ lambda cannot first-capture </span></span><br><span class="line">      <span class="comment">//      &#x27;yyy&#x27; in constexpr-if context</span></span><br><span class="line">      result += yyy;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> lam10 = [yyy] __device__ &#123;  </span><br><span class="line">    <span class="type">int</span> result = <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span><span class="params">(<span class="literal">false</span>)</span> </span>&#123;</span><br><span class="line">      <span class="comment">//OK: &#x27;yyy&#x27; already listed in explicit capture list for the extended lambda</span></span><br><span class="line">      result += yyy;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> lam11 = [=] __device__ &#123;  </span><br><span class="line">    <span class="type">int</span> result = yyy;</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span><span class="params">(<span class="literal">false</span>)</span> </span>&#123;</span><br><span class="line">      <span class="comment">//OK: &#x27;yyy&#x27; already implicit captured outside the &#x27;if-constexpr&#x27; block</span></span><br><span class="line">      result += yyy;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>解析函数时，CUDA 编译器为该函数中的每个扩展 lambda 分配一个计数器值。 此计数器值用于传递给主机编译器的替代命名类型。 因此，是否在函数中定义扩展 lambda 不应取决于 <code>__CUDA_ARCH__</code> 的特定值，或 <code>__CUDA_ARCH__</code> 未定义。</p>
<p>例子</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(T in)</span> </span>&#123; <span class="built_in">in</span>(); &#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__host__ __device__ <span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Error: the number and relative declaration</span></span><br><span class="line">  <span class="comment">// order of extended lambdas depends on</span></span><br><span class="line">  <span class="comment">// __CUDA_ARCH__</span></span><br><span class="line"><span class="meta">#<span class="keyword">if</span> defined(__CUDA_ARCH__)</span></span><br><span class="line">  <span class="keyword">auto</span> lam1 = [] __device__ &#123; <span class="keyword">return</span> <span class="number">0</span>; &#125;;</span><br><span class="line">  <span class="keyword">auto</span> lam1b = [] __host___ __device__ &#123; <span class="keyword">return</span> <span class="number">10</span>; &#125;;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">  <span class="keyword">auto</span> lam2 = [] __device__ &#123; <span class="keyword">return</span> <span class="number">4</span>; &#125;;</span><br><span class="line">  kernel&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(lam2);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如上所述，CUDA 编译器将主机函数中定义的 <code>__device__</code> 扩展 lambda 替换为命名空间范围中定义的占位符类型。 此占位符类型未定义与原始 lambda 声明等效的 operator() 函数。 因此，尝试确定 operator() 函数的返回类型或参数类型可能在宿主代码中无法正常工作，因为宿主编译器处理的代码在语义上与 CUDA 编译器处理的输入代码不同。 但是，可以在设备代码中内省 operator() 函数的返回类型或参数类型。 请注意，此限制不适用于 <code>__host__ __device__</code> 扩展 lambda。</p>
<p>例子</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;type_traits&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> lam1 = [] __device__ &#123; <span class="keyword">return</span> <span class="number">10</span>; &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Error: attempt to extract the return type</span></span><br><span class="line">  <span class="comment">// of a __device__ lambda in host code</span></span><br><span class="line">  std::result_of&lt;<span class="keyword">decltype</span>(lam1)()&gt;::type xx1 = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> lam2 = [] __host__ __device__  &#123; <span class="keyword">return</span> <span class="number">10</span>; &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// OK : lam2 represents a __host__ __device__ extended lambda</span></span><br><span class="line">  std::result_of&lt;<span class="keyword">decltype</span>(lam2)()&gt;::type xx2 = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果由扩展 lambda 表示的仿函数对象从主机传递到设备代码（例如，作为 <code>__global__</code> 函数的参数），则 lambda 表达式主体中捕获变量的任何表达式都必须保持不变，无论 <code>__CUDA_ARCH__</code> 是否 定义宏，以及宏是否具有特定值。 出现这个限制是因为 lambda 的闭包类布局取决于编译器处理 lambda 表达式时遇到捕获的变量的顺序； 如果闭包类布局在设备和主机编译中不同，则程序可能执行不正确。</p>
<p>例子</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">int</span> result;</span><br><span class="line">   </span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(T in)</span> </span>&#123; result = <span class="built_in">in</span>(); &#125;</span><br><span class="line">   </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> x1 = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">auto</span> lam1 = [=] __host__ __device__ &#123; </span><br><span class="line">    <span class="comment">// Error: &quot;x1&quot; is only captured when __CUDA_ARCH__ is defined.</span></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> __CUDA_ARCH__</span></span><br><span class="line">    <span class="keyword">return</span> x1 + <span class="number">1</span>;</span><br><span class="line"><span class="meta">#<span class="keyword">else</span>	</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">10</span>; </span><br><span class="line"><span class="meta">#<span class="keyword">endif</span>       </span></span><br><span class="line">  &#125;;</span><br><span class="line">  kernel&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(lam1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如前所述，CUDA 编译器将扩展的 <code>__device__</code> lambda 表达式替换为发送到主机编译器的代码中的占位符类型的实例。 此占位符类型未在主机代码中定义指向函数的转换运算符，但在设备代码中提供了转换运算符。 请注意，此限制不适用于 <code>__host__ __device__</code> 扩展 lambda。</p>
<p>例子</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kern</span><span class="params">(T in)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">int</span> (*fp)(<span class="type">double</span>) = in;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// OK: conversion in device code is supported</span></span><br><span class="line">  <span class="built_in">fp</span>(<span class="number">0</span>);</span><br><span class="line">  <span class="keyword">auto</span> lam1 = [](<span class="type">double</span>) &#123; <span class="keyword">return</span> <span class="number">1</span>; &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// OK: conversion in device code is supported</span></span><br><span class="line">  fp = lam1;</span><br><span class="line">  <span class="built_in">fp</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> lam_d = [] __device__ (<span class="type">double</span>) &#123; <span class="keyword">return</span> <span class="number">1</span>; &#125;;</span><br><span class="line">  <span class="keyword">auto</span> lam_hd = [] __host__ __device__ (<span class="type">double</span>) &#123; <span class="keyword">return</span> <span class="number">1</span>; &#125;;</span><br><span class="line">  kern&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(lam_d);</span><br><span class="line">  kern&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(lam_hd);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// OK : conversion for __host__ __device__ lambda is supported</span></span><br><span class="line">  <span class="comment">// in host code</span></span><br><span class="line">  <span class="built_in">int</span> (*fp)(<span class="type">double</span>) = lam_hd;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// Error: conversion for __device__ lambda is not supported in</span></span><br><span class="line">  <span class="comment">// host code.</span></span><br><span class="line">  <span class="built_in">int</span> (*fp2)(<span class="type">double</span>) = lam_d;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如前所述，CUDA 编译器将扩展的 <code>__device__</code> 或 <code>__host__ __device__</code> lambda 表达式替换为发送到主机编译器的代码中的占位符类型的实例。 此占位符类型可以定义 C++ 特殊成员函数（例如构造函数、析构函数）。 因此，在 CUDA 前端编译器与主机编译器中，一些标准 C++ 类型特征可能会为扩展 lambda 的闭包类型返回不同的结果。 以下类型特征受到影响：<code>std::is_trivially_copyable、std::is_trivially_constructible、std::is_trivially_copy_constructible、std::is_trivially_move_constructible、std::is_trivially_destructible</code>。</p>
<p>必须注意这些类型特征的结果不用于 <code>__global__</code> 函数模板实例化或 <code>__device__</code> / <code>__constant__</code> / <code>__managed__</code> 变量模板实例化。</p>
<p>例子</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="type">bool</span> b&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> __global__ <span class="title">foo</span><span class="params">()</span> </span>&#123; <span class="built_in">printf</span>(<span class="string">&quot;hi&quot;</span>); &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">dolaunch</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ERROR: this kernel launch may fail, because CUDA frontend compiler</span></span><br><span class="line"><span class="comment">// and host compiler may disagree on the result of</span></span><br><span class="line"><span class="comment">// std::is_trivially_copyable() trait on the closure type of the </span></span><br><span class="line"><span class="comment">// extended lambda</span></span><br><span class="line">foo&lt;std::is_trivially_copyable&lt;T&gt;::value&gt;&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line"><span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="type">int</span> x = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">auto</span> lam1 = [=] __host__ __device__ () &#123; <span class="keyword">return</span> x; &#125;;</span><br><span class="line"><span class="built_in">dolaunch</span>&lt;<span class="keyword">decltype</span>(lam1)&gt;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>CUDA 编译器将为 1-12 中描述的部分情况生成编译器诊断； 不会为案例 13-17 生成诊断，但主机编译器可能无法编译生成的代码。</p>
<h3 id="I-6-3-Notes-on-host-device-lambdas"><a href="#I-6-3-Notes-on-host-device-lambdas" class="headerlink" title="I.6.3. Notes on host device lambdas"></a>I.6.3. Notes on <strong>host</strong> <strong>device</strong> lambdas</h3><p>与 <code>__device__</code> lambdas 不同，<code>__host__ __device__</code> lambdas 可以从主机代码中调用。如前所述，CUDA 编译器将主机代码中定义的扩展 lambda 表达式替换为命名占位符类型的实例。扩展的 <code>__host__ __device__</code> lambda 的占位符类型通过间接函数<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#fntarg_32">调用</a>原始 lambda 的 operator()。</p>
<p>间接函数调用的存在可能会导致主机编译器对扩展的 <code>__host__ __device__</code> lambda 的优化程度低于仅隐式或显式 <code>__host__</code> 的 lambda。在后一种情况下，宿主编译器可以轻松地将 lambda 的主体内联到调用上下文中。但是在扩展 <code>__host__ __device__</code> lambda 的情况下，主机编译器会遇到间接函数调用，并且可能无法轻松内联原始 <code>__host__ __device__</code> lambda 主体。</p>
<h3 id="I-6-4-this-Capture-By-Value"><a href="#I-6-4-this-Capture-By-Value" class="headerlink" title="I.6.4. *this Capture By Value"></a>I.6.4. *this Capture By Value</h3><p>当在非静态类成员函数中定义 lambda，并且 lambda 的主体引用类成员变量时，C++11/C++14 规则要求类的 this 指针按值捕获，而不是引用的成员变量。如果 lambda 是在主机函数中定义的扩展 <code>__device__</code> 或 <code>__host__ __device__</code> lambda，并且 lambda 在 GPU 上执行，如果 this 指针指向主机内存，则在 GPU 上访问引用的成员变量将导致运行时错误。</p>
<p>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">foo</span><span class="params">(T in)</span> </span>&#123; <span class="built_in">printf</span>(<span class="string">&quot;\n value = %d&quot;</span>, <span class="built_in">in</span>()); &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123; </span><br><span class="line">  <span class="type">int</span> xxx;</span><br><span class="line">  <span class="function">__host__ __device__ <span class="title">S1_t</span><span class="params">(<span class="type">void</span>)</span> : xxx(<span class="number">10</span>) &#123;</span> &#125;;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">doit</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">auto</span> lam1 = [=] __device__ &#123; </span><br><span class="line">       <span class="comment">// reference to &quot;xxx&quot; causes </span></span><br><span class="line">       <span class="comment">// the &#x27;this&#x27; pointer (S1_t*) to be captured by value</span></span><br><span class="line">       <span class="keyword">return</span> xxx + <span class="number">1</span>; </span><br><span class="line">      </span><br><span class="line">    &#125;;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Kernel launch fails at run time because &#x27;this-&gt;xxx&#x27;</span></span><br><span class="line">    <span class="comment">// is not accessible from the GPU</span></span><br><span class="line">    foo&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(lam1);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  S1_t s1;</span><br><span class="line">  s<span class="number">1.</span><span class="built_in">doit</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>C++17 通过添加新的“<em>this”捕获模式解决了这个问题。 在这种模式下，编译器复制由“</em>this”表示的对象，而不是按值捕获指针 this。 此处更详细地描述了“*this”捕获模式：<a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0018r3.html。">http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0018r3.html。</a></p>
<p>当使用 <code>--extended-lambda</code> nvcc 标志时，CUDA 编译器支持 <code>__device__</code> 和 <code>__global__</code> 函数中定义的 lambdas 以及主机代码中定义的扩展 <code>__device__</code> lambdas 的“*this”捕获模式。</p>
<p>这是修改为使用“*this”捕获模式的上述示例：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">foo</span><span class="params">(T in)</span> </span>&#123; <span class="built_in">printf</span>(<span class="string">&quot;\n value = %d&quot;</span>, <span class="built_in">in</span>()); &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123; </span><br><span class="line">  <span class="type">int</span> xxx;</span><br><span class="line">  <span class="function">__host__ __device__ <span class="title">S1_t</span><span class="params">(<span class="type">void</span>)</span> : xxx(<span class="number">10</span>) &#123;</span> &#125;;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">doit</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// note the &quot;*this&quot; capture specification</span></span><br><span class="line">    <span class="keyword">auto</span> lam1 = [=, *<span class="keyword">this</span>] __device__ &#123; </span><br><span class="line">      </span><br><span class="line">       <span class="comment">// reference to &quot;xxx&quot; causes </span></span><br><span class="line">       <span class="comment">// the object denoted by &#x27;*this&#x27; to be captured by</span></span><br><span class="line">       <span class="comment">// value, and the GPU code will access copy_of_star_this-&gt;xxx</span></span><br><span class="line">       <span class="keyword">return</span> xxx + <span class="number">1</span>; </span><br><span class="line">      </span><br><span class="line">    &#125;;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Kernel launch succeeds</span></span><br><span class="line">    foo&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;(lam1);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">  S1_t s1;</span><br><span class="line">  s<span class="number">1.</span><span class="built_in">doit</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>主机代码中定义的未注释 lambda 或扩展的 <code>__host__</code> <code>__device__</code> lambda 不允许使用“*this”捕获模式。 支持和不支持的用法示例：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123; </span><br><span class="line">  <span class="type">int</span> xxx;</span><br><span class="line">  <span class="function">__host__ __device__ <span class="title">S1_t</span><span class="params">(<span class="type">void</span>)</span> : xxx(<span class="number">10</span>) &#123;</span> &#125;;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">host_func</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// OK: use in an extended __device__ lambda</span></span><br><span class="line">    <span class="keyword">auto</span> lam1 = [=, *<span class="keyword">this</span>] __device__ &#123; <span class="keyword">return</span> xxx; &#125;;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Error: use in an extended __host__ __device__ lambda</span></span><br><span class="line">    <span class="keyword">auto</span> lam2 = [=, *<span class="keyword">this</span>] __host__ __device__ &#123; <span class="keyword">return</span> xxx; &#125;;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Error: use in an unannotated lambda in host function</span></span><br><span class="line">    <span class="keyword">auto</span> lam3 = [=, *<span class="keyword">this</span>]  &#123; <span class="keyword">return</span> xxx; &#125;;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="function">__device__ <span class="type">void</span> <span class="title">device_func</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// OK: use in a lambda defined in a __device__ function</span></span><br><span class="line">    <span class="keyword">auto</span> lam1 = [=, *<span class="keyword">this</span>] __device__ &#123; <span class="keyword">return</span> xxx; &#125;;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// OK: use in a lambda defined in a __device__ function</span></span><br><span class="line">    <span class="keyword">auto</span> lam2 = [=, *<span class="keyword">this</span>] __host__ __device__ &#123; <span class="keyword">return</span> xxx; &#125;;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// OK: use in a lambda defined in a __device__ function</span></span><br><span class="line">    <span class="keyword">auto</span> lam3 = [=, *<span class="keyword">this</span>]  &#123; <span class="keyword">return</span> xxx; &#125;;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">   <span class="function">__host__ __device__ <span class="type">void</span> <span class="title">host_device_func</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// OK: use in an extended __device__ lambda</span></span><br><span class="line">    <span class="keyword">auto</span> lam1 = [=, *<span class="keyword">this</span>] __device__ &#123; <span class="keyword">return</span> xxx; &#125;;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Error: use in an extended __host__ __device__ lambda</span></span><br><span class="line">    <span class="keyword">auto</span> lam2 = [=, *<span class="keyword">this</span>] __host__ __device__ &#123; <span class="keyword">return</span> xxx; &#125;;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Error: use in an unannotated lambda in a __host__ __device__ function</span></span><br><span class="line">    <span class="keyword">auto</span> lam3 = [=, *<span class="keyword">this</span>]  &#123; <span class="keyword">return</span> xxx; &#125;;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h3 id="I-6-5-Additional-Notes"><a href="#I-6-5-Additional-Notes" class="headerlink" title="I.6.5. Additional Notes"></a>I.6.5. Additional Notes</h3><p><code>ADL Lookup</code>：如前所述，CUDA 编译器将在调用宿主编译器之前将扩展的 lambda 表达式替换为占位符类型的实例。 占位符类型的一个模板参数使用包含原始 lambda 表达式的函数的地址。 对于参数类型涉及扩展 lambda 表达式的闭包类型的任何主机函数调用，这可能会导致其他命名空间参与参数相关查找 (ADL)。 这可能会导致主机编译器选择不正确的函数。<br>例子：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> N1 &#123;</span><br><span class="line">  <span class="keyword">struct</span> <span class="title class_">S1_t</span> &#123; &#125;;</span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;  <span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(T)</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">namespace</span> N2 &#123;</span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="function"><span class="type">int</span> <span class="title">foo</span><span class="params">(T)</span></span>;</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;  <span class="function"><span class="type">void</span> <span class="title">doit</span><span class="params">(T in)</span> </span>&#123;     <span class="built_in">foo</span>(in);  &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">bar</span><span class="params">(N1::S1_t in)</span> </span>&#123;</span><br><span class="line">  <span class="comment">/* extended __device__ lambda. In the code sent to the host compiler, this </span></span><br><span class="line"><span class="comment">     is replaced with the placeholder type instantiation expression</span></span><br><span class="line"><span class="comment">     &#x27; __nv_dl_wrapper_t&lt; __nv_dl_tag&lt;void (*)(N1::S1_t in),(&amp;bar),1&gt; &gt; &#123; &#125;&#x27;</span></span><br><span class="line"><span class="comment">   </span></span><br><span class="line"><span class="comment">     As a result, the namespace &#x27;N1&#x27; participates in ADL lookup of the </span></span><br><span class="line"><span class="comment">     call to &quot;foo&quot; in the body of N2::doit, causing ambiguity.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line">  <span class="keyword">auto</span> lam1 = [=] __device__ &#123; &#125;;</span><br><span class="line">  N2::<span class="built_in">doit</span>(lam1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上面的示例中，CUDA 编译器将扩展 lambda 替换为涉及 N1 命名空间的占位符类型。 结果，命名空间 N1 参与了对 <code>N2::doit</code> 主体中的 <code>foo(in)</code> 的 ADL 查找，并且主机编译失败，因为找到了多个重载候选 <code>N1::foo</code> 和 <code>N2::foo</code>。</p>
<h2 id="I-7-Code-Samples"><a href="#I-7-Code-Samples" class="headerlink" title="I.7. Code Samples"></a>I.7. Code Samples</h2><h3 id="I-7-1-Data-Aggregation-Class"><a href="#I-7-1-Data-Aggregation-Class" class="headerlink" title="I.7.1. Data Aggregation Class"></a>I.7.1. Data Aggregation Class</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PixelRGBA</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">__device__ <span class="title">PixelRGBA</span><span class="params">()</span>: r_(<span class="number">0</span>), g_(<span class="number">0</span>), b_(<span class="number">0</span>), a_(<span class="number">0</span>) &#123;</span> &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="function">__device__ <span class="title">PixelRGBA</span><span class="params">(<span class="type">unsigned</span> <span class="type">char</span> r, <span class="type">unsigned</span> <span class="type">char</span> g,</span></span></span><br><span class="line"><span class="params"><span class="function">                         <span class="type">unsigned</span> <span class="type">char</span> b, <span class="type">unsigned</span> <span class="type">char</span> a = <span class="number">255</span>)</span>:</span></span><br><span class="line"><span class="function">                         r_(r), g_(g), b_(b), a_(a) &#123;</span> &#125;</span><br><span class="line">    </span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> r_, g_, b_, a_;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">friend</span> PixelRGBA <span class="keyword">operator</span>+(<span class="type">const</span> PixelRGBA&amp;, <span class="type">const</span> PixelRGBA&amp;);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">__device__ </span><br><span class="line">PixelRGBA <span class="keyword">operator</span>+(<span class="type">const</span> PixelRGBA&amp; p1, <span class="type">const</span> PixelRGBA&amp; p2)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">PixelRGBA</span>(p<span class="number">1.</span>r_ + p<span class="number">2.</span>r_, p<span class="number">1.</span>g_ + p<span class="number">2.</span>g_, </span><br><span class="line">                     p<span class="number">1.</span>b_ + p<span class="number">2.</span>b_, p<span class="number">1.</span>a_ + p<span class="number">2.</span>a_);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">func</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    PixelRGBA p1, p2;</span><br><span class="line">    <span class="comment">// ...      // Initialization of p1 and p2 here</span></span><br><span class="line">    PixelRGBA p3 = p1 + p2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="I-7-2-Derived-Class"><a href="#I-7-2-Derived-Class" class="headerlink" title="I.7.2. Derived Class"></a>I.7.2. Derived Class</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__device__ <span class="type">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="type">size_t</span> bytes, MemoryPool&amp; p)</span></span>;</span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="keyword">operator</span> <span class="title">delete</span><span class="params">(<span class="type">void</span>*, MemoryPool&amp; p)</span></span>;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Shape</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">__device__ <span class="title">Shape</span><span class="params">(<span class="type">void</span>)</span> </span>&#123; &#125;</span><br><span class="line">    <span class="function">__device__ <span class="type">void</span> <span class="title">putThis</span><span class="params">(PrintBuffer *p)</span> <span class="type">const</span></span>;</span><br><span class="line">    <span class="function">__device__ <span class="keyword">virtual</span> <span class="type">void</span> <span class="title">Draw</span><span class="params">(PrintBuffer *p)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">         p-&gt;<span class="built_in">put</span>(<span class="string">&quot;Shapeless&quot;</span>); </span><br><span class="line">    &#125;</span><br><span class="line">    __device__ <span class="keyword">virtual</span> ~<span class="built_in">Shape</span>() &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Point</span> : <span class="keyword">public</span> Shape &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">__device__ <span class="title">Point</span><span class="params">()</span> : x(<span class="number">0</span>), y(<span class="number">0</span>) &#123;</span>&#125;</span><br><span class="line">    <span class="function">__device__ <span class="title">Point</span><span class="params">(<span class="type">int</span> ix, <span class="type">int</span> iy)</span> : x(ix), y(iy) &#123;</span> &#125;</span><br><span class="line">    <span class="function">__device__ <span class="type">void</span> <span class="title">PutCoord</span><span class="params">(PrintBuffer *p)</span> <span class="type">const</span></span>;</span><br><span class="line">    <span class="function">__device__ <span class="type">void</span> <span class="title">Draw</span><span class="params">(PrintBuffer *p)</span> <span class="type">const</span></span>;</span><br><span class="line">    __device__ ~<span class="built_in">Point</span>() &#123;&#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">int</span> x, y;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function">__device__ Shape* <span class="title">GetPointObj</span><span class="params">(MemoryPool&amp; pool)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Shape* shape = <span class="built_in">new</span>(pool) <span class="built_in">Point</span>(<span class="built_in">rand</span>(<span class="number">-20</span>,<span class="number">10</span>), <span class="built_in">rand</span>(<span class="number">-100</span>,<span class="number">-20</span>));</span><br><span class="line">    <span class="keyword">return</span> shape;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="I-7-3-Class-Template"><a href="#I-7-3-Class-Template" class="headerlink" title="I.7.3. Class Template"></a>I.7.3. Class Template</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">T</span>&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">myValues</span> &#123;</span><br><span class="line">    T values[MAX_VALUES];</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">__device__ <span class="title">myValues</span><span class="params">(T clear)</span> </span>&#123; ... &#125;</span><br><span class="line">    <span class="function">__device__ <span class="type">void</span> <span class="title">setValue</span><span class="params">(<span class="type">int</span> Idx, T value)</span> </span>&#123; ... &#125;</span><br><span class="line">    <span class="function">__device__ <span class="type">void</span> <span class="title">putToMemory</span><span class="params">(T* valueLocation)</span> </span>&#123; ... &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">T</span>&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> __global__ <span class="title">useValues</span><span class="params">(T* memoryBuffer)</span> </span>&#123;</span><br><span class="line">    <span class="function">myValues&lt;T&gt; <span class="title">myLocation</span><span class="params">(<span class="number">0</span>)</span></span>;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__device__ <span class="type">void</span>* buffer;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    useValues&lt;<span class="type">int</span>&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(buffer);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="I-7-4-Function-Template"><a href="#I-7-4-Function-Template" class="headerlink" title="I.7.4. Function Template"></a>I.7.4. Function Template</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; </span><br><span class="line"><span class="function">__device__ <span class="type">bool</span> <span class="title">func</span><span class="params">(T x)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   ...</span><br><span class="line">   <span class="keyword">return</span> (...);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;&gt; </span><br><span class="line">__device__ <span class="type">bool</span> <span class="built_in">func</span>&lt;<span class="type">int</span>&gt;(T x) <span class="comment">// Specialization</span></span><br><span class="line">&#123;</span><br><span class="line">   <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Explicit argument specification</span></span><br><span class="line"><span class="type">bool</span> result = <span class="built_in">func</span>&lt;<span class="type">double</span>&gt;(<span class="number">0.5</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Implicit argument deduction</span></span><br><span class="line"><span class="type">int</span> x = <span class="number">1</span>;</span><br><span class="line"><span class="type">bool</span> result = <span class="built_in">func</span>(x);</span><br></pre></td></tr></table></figure>
<h3 id="I-7-5-Functor-Class"><a href="#I-7-5-Functor-Class" class="headerlink" title="I.7.5. Functor Class"></a>I.7.5. Functor Class</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Add</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">__device__  <span class="type">float</span> <span class="title">operator</span><span class="params">()</span> <span class="params">(<span class="type">float</span> a, <span class="type">float</span> b)</span> <span class="type">const</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> a + b;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Sub</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function">__device__  <span class="type">float</span> <span class="title">operator</span><span class="params">()</span> <span class="params">(<span class="type">float</span> a, <span class="type">float</span> b)</span> <span class="type">const</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> a - b;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Device code</span></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">class</span> O&gt; __global__ </span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">VectorOperation</span><span class="params">(<span class="type">const</span> <span class="type">float</span> * A, <span class="type">const</span> <span class="type">float</span> * B, <span class="type">float</span> * C,</span></span></span><br><span class="line"><span class="params"><span class="function">                     <span class="type">unsigned</span> <span class="type">int</span> N, O op)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> iElement = blockDim.x * blockIdx.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (iElement &lt; N)</span><br><span class="line">        C[iElement] = <span class="built_in">op</span>(A[iElement], B[iElement]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Host code</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    VectorOperation&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(v1, v2, v3, N, <span class="built_in">Add</span>());</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="附录J-纹理获取"><a href="#附录J-纹理获取" class="headerlink" title="附录J 纹理获取"></a>附录J 纹理获取</h1><p>本附录给出了用于计算 Texture Functions 的纹理函数返回值的公式，具体取决于纹理引用的各种属性（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-and-surface-memory">纹理和表面内存</a>）。</p>
<p>绑定到纹理引用的纹理表示为一个数组 T</p>
<ul>
<li>一维纹理的 N 个texels，</li>
<li>二维纹理的 N x M texels，</li>
<li>三维纹理的 N x M x L texels。</li>
</ul>
<p>它是使用非归一化纹理坐标 x、y 和 z 或归一化纹理坐标 x/N、y/M 和 z/L 获取的，如<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-memory">纹理内存</a>中所述。 在本附录中，假定坐标在有效范围内。 纹理内存解释了如何根据寻址模式将超出范围的坐标重新映射到有效范围。</p>
<h2 id="J-1-Nearest-Point-Sampling"><a href="#J-1-Nearest-Point-Sampling" class="headerlink" title="J.1. Nearest-Point Sampling"></a>J.1. Nearest-Point Sampling</h2><p>在这种过滤模式下，纹理获取返回的值是</p>
<ul>
<li>tex(x)=T[i] 对于一维纹理，</li>
<li>tex(x,y)=T[i,j] 对于二维纹理，</li>
<li>tex(x,y,z)=T[i,j,k] 对于三维纹理，</li>
</ul>
<p>其中 i=floor(x)，j=floor(y)，k=floor(z)。</p>
<p>下图 说明了 N=4 的一维纹理的最近点采样。</p>
<p>对于整数纹理，纹理获取返回的值可以选择重新映射到 [0.0, 1.0]（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-memory">纹理内存</a>）。</p>
<p><img src="/img/nearest-point-sampling-of-1-d-texture-of-4-texels.png" alt="near"></p>
<h2 id="J-2-Linear-Filtering"><a href="#J-2-Linear-Filtering" class="headerlink" title="J.2. Linear Filtering"></a>J.2. Linear Filtering</h2><p>在这种仅适用于浮点纹理的过滤模式下，纹理获取返回的值是</p>
<ul>
<li><p>tex(x)=(1−α)T[i]+αT[i+1] for a one-dimensional texture,</p>
</li>
<li><p>tex(x,y)=(1−α)(1−β)T[i,j]+α(1−β)T[i+1,j]+(1−α)βT[i,j+1]+αβT[i+1,j+1] for a two-dimensional texture,</p>
</li>
<li><p>tex(x,y,z) =</p>
<p>  (1−α)(1−β)(1−γ)T[i,j,k]+α(1−β)(1−γ)T[i+1,j,k]+</p>
<p>  (1−α)β(1−γ)T[i,j+1,k]+αβ(1−γ)T[i+1,j+1,k]+</p>
<p>  (1−α)(1−β)γT[i,j,k+1]+α(1−β)γT[i+1,j,k+1]+</p>
<p>  (1−α)βγT[i,j+1,k+1]+αβγT[i+1,j+1,k+1]</p>
<p>  for a three-dimensional texture,</p>
</li>
</ul>
<p>其中:</p>
<ul>
<li>i=floor(xB), α=frac(xB), xB=x-0.5,</li>
<li>j=floor(yB), β=frac(yB), yB=y-0.5,</li>
<li>k=floor(zB), γ=frac(zB), zB= z-0.5,</li>
</ul>
<p>α、β 和 γ 以 9 位定点格式存储，带有 8 位小数值（因此精确表示 1.0）。</p>
<p>下图 说明了 N=4 的一维纹理的线性过滤。</p>
<p><img src="/img/linear-filtering-of-1-d-texture-of-4-texels.png" alt=""></p>
<h2 id="J-3-Table-Lookup"><a href="#J-3-Table-Lookup" class="headerlink" title="J.3. Table Lookup"></a>J.3. Table Lookup</h2><p>x 跨越区间 [0,R] 的查表 TL(x) 可以实现为 TL(x)=tex((N-1)/R)x+0.5) 以确保 TL(0)= T[0] 和 TL(R)=T[N-1]。</p>
<p>下图 说明了使用纹理过滤从 N=4 的一维纹理中实现 R=4 或 R=1 的表查找。</p>
<p><img src="/img/1-d-table-lookup-using-linear-filtering.png" alt=""></p>
<h1 id="附录K-CUDA计算能力"><a href="#附录K-CUDA计算能力" class="headerlink" title="附录K CUDA计算能力"></a>附录K CUDA计算能力</h1><p>计算设备的一般规格和功能取决于其计算能力（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability">计算能力</a>）。</p>
<p>下面的表格中 显示了与当前支持的每种计算能力相关的特性和技术规格。</p>
<p>浮点标准审查是否符合 IEEE 浮点标准。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-3-0">Compute Capability 3.x</a>、<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-5-x">Compute Capability 5.x</a>、<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-6-x">Compute Capability 6.x</a>、<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-7-x">Compute Capability 7.x</a> 和 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capability-8-x">Compute Capability 8.x</a> 部分提供了有关计算能力 3.x、5.x、6 的设备架构的更多详细信息 .x、7.x 和 8.x 分别。</p>
<h2 id="K-1-Features-and-Technical-Specifications"><a href="#K-1-Features-and-Technical-Specifications" class="headerlink" title="K.1. Features and Technical Specifications"></a>K.1. Features and Technical Specifications</h2><div class="tablenoborder"><a name="features-and-technical-specifications__feature-support-per-compute-capability" shape="rect">
                              <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="features-and-technical-specifications__feature-support-per-compute-capability" class="table" frame="border" border="1" rules="all">
                              <caption><span class="tablecap">Table 14. Feature Support per Compute Capability</span></caption>
                              <thead class="thead" align="left">
                                 <tr class="row" valign="middle">
                                    <th class="entry" align="left" valign="middle" width="50%" id="d117e33416" rowspan="1" colspan="1">Feature Support</th>
                                    <th class="entry" colspan="5" align="center" valign="middle" id="d117e33419" rowspan="1">
                                       Compute Capability   
                                    </th>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <th class="entry" align="left" valign="middle" width="50%" id="d117e33425" rowspan="1" colspan="1">
                                       (Unlisted features are
                                       supported for all compute capabilities)
                                    </th>
                                    <th class="entry" align="center" valign="middle" width="10%" id="d117e33428" rowspan="1" colspan="1">3.5, 3.7, 5.0, 5.2</th>
                                    <th class="entry" align="center" valign="middle" width="10%" id="d117e33431" rowspan="1" colspan="1">5.3</th>
                                    <th class="entry" align="center" valign="middle" width="10%" id="d117e33434" rowspan="1" colspan="1">6.x</th>
                                    <th class="entry" align="center" valign="middle" width="10%" id="d117e33437" rowspan="1" colspan="1">7.x</th>
                                    <th class="entry" align="center" valign="middle" width="10%" id="d117e33441" rowspan="1" colspan="1">8.x</th>
                                 </tr>
                              </thead>
                              <tbody class="tbody">
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">
                                       Atomic functions operating on 32-bit integer values in global memory
                                       (<a class="xref" href="index.html#atomic-functions" shape="rect">Atomic Functions</a>)  
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33419 d117e33428 d117e33431 d117e33434 d117e33437 d117e33441" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">
                                       Atomic functions operating on 32-bit integer values in shared memory
                                       (<a class="xref" href="index.html#atomic-functions" shape="rect">Atomic Functions</a>)
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33419 d117e33428 d117e33431 d117e33434 d117e33437 d117e33441" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">
                                       Atomic functions operating on 64-bit integer values in global memory
                                       (<a class="xref" href="index.html#atomic-functions" shape="rect">Atomic Functions</a>)  
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33419 d117e33428 d117e33431 d117e33434 d117e33437 d117e33441" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">
                                       Atomic functions operating on 64-bit integer values in shared memory
                                       (<a class="xref" href="index.html#atomic-functions" shape="rect">Atomic Functions</a>) 
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33419 d117e33428 d117e33431 d117e33434 d117e33437 d117e33441" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">
                                       Atomic addition operating on 32-bit floating point values in global
                                       and shared memory (<a class="xref" href="index.html#atomicadd" shape="rect">atomicAdd()</a>)  
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33419 d117e33428 d117e33431 d117e33434 d117e33437 d117e33441" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">
                                       Atomic addition operating on 64-bit floating point values in global
                                       memory and shared memory (<a class="xref" href="index.html#atomicadd" shape="rect">atomicAdd()</a>)
                                    </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33419 d117e33428 d117e33431" rowspan="1">No</td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d117e33419 d117e33434 d117e33437 d117e33441" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">
                                       Warp vote functions (<a class="xref" href="index.html#warp-vote-functions" shape="rect">Warp Vote Functions</a>) 
                                    </td>
                                    <td class="entry" rowspan="6" colspan="5" align="center" valign="middle" headers="d117e33419 d117e33428 d117e33431 d117e33434 d117e33437 d117e33441">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">
                                       Memory fence functions (<a class="xref" href="index.html#memory-fence-functions" shape="rect">Memory Fence Functions</a>)
                                    </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">
                                       Synchronization functions (<a class="xref" href="index.html#synchronization-functions" shape="rect">Synchronization Functions</a>) 
                                    </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">
                                       Surface functions (<a class="xref" href="index.html#surface-functions" shape="rect">Surface Functions</a>)
                                    </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">Unified Memory Programming (<a class="xref" href="index.html#um-unified-memory-programming-hd" shape="rect">Unified Memory Programming</a>)
                                    </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">Dynamic Parallelism (<a class="xref" href="index.html#cuda-dynamic-parallelism" shape="rect">CUDA Dynamic Parallelism</a>)
                                    </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">
                                       Half-precision floating-point operations: addition, subtraction, multiplication, comparison, warp shuffle functions, conversion
                                    </td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33419 d117e33428" rowspan="1">No</td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d117e33419 d117e33431 d117e33434 d117e33437 d117e33441" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">
                                       Bfloat16-precision floating-point operations: addition, subtraction, multiplication, comparison, warp shuffle functions, conversion
                                    </td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d117e33419 d117e33428 d117e33431 d117e33434 d117e33437" rowspan="1">No</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33419 d117e33441" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">
                                       Tensor Cores
                                    </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d117e33419 d117e33428 d117e33431 d117e33434" rowspan="1">No</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33419 d117e33437 d117e33441" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1">Mixed Precision Warp-Matrix Functions
                                       (<a class="xref" href="index.html#wmma" shape="rect">Warp matrix functions</a>)
                                    </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d117e33419 d117e33428 d117e33431 d117e33434" rowspan="1">No</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33419 d117e33437 d117e33441" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1"> Hardware-accelerated <samp class="ph codeph">memcpy_async</samp>
                                       (<a class="xref" href="index.html#memcpy_async_pipeline" shape="rect">Asynchronous Data Copies using cuda::pipeline</a>)
                                    </td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d117e33419 d117e33428 d117e33431 d117e33434 d117e33437" rowspan="1">No</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33419 d117e33441" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1"> Hardware-accelerated Split Arrive/Wait Barrier
                                       (<a class="xref" href="index.html#aw-barrier" shape="rect">Asynchronous Barrier</a>)
                                    </td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d117e33419 d117e33428 d117e33431 d117e33434 d117e33437" rowspan="1">No</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33419 d117e33441" rowspan="1">Yes</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="50%" headers="d117e33416 d117e33425" rowspan="1" colspan="1"> L2 Cache Residency Management
                                       (<a class="xref" href="index.html#L2_access_intro" shape="rect">Device Memory L2 Access Management</a>)
                                    </td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d117e33419 d117e33428 d117e33431 d117e33434 d117e33437" rowspan="1">No</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33419 d117e33441" rowspan="1">Yes</td>
                                 </tr>
                              </tbody>
                           </table>
                        </div>



<p>请注意，下表中使用的 KB 和 K 单位分别对应于 1024 字节（即 KiB）和 1024。</p>
<div class="tablenoborder"><a name="features-and-technical-specifications__technical-specifications-per-compute-capability" shape="rect">
                              <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="features-and-technical-specifications__technical-specifications-per-compute-capability" class="table" frame="border" border="1" rules="all">
                              <caption><span class="tablecap">Table 15. Technical Specifications per Compute Capability</span></caption>
                              <thead class="thead" align="left">
                                 <tr class="row" valign="middle">
                                    <th class="entry" align="left" valign="middle" width="26.31578947368421%" id="d117e33731" rowspan="1" colspan="1">&nbsp;</th>
                                    <th class="entry" colspan="14" align="center" valign="middle" id="d117e33733" rowspan="1"> Compute Capability </th>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <th class="entry" align="left" valign="middle" width="26.31578947368421%" id="d117e33739" rowspan="1" colspan="1">Technical Specifications</th>
                                    <th class="entry" align="center" valign="middle" width="5.263157894736842%" id="d117e33742" rowspan="1" colspan="1">3.5</th>
                                    <th class="entry" align="center" valign="middle" width="5.263157894736842%" id="d117e33745" rowspan="1" colspan="1">3.7</th>
                                    <th class="entry" align="center" valign="middle" width="5.263157894736842%" id="d117e33748" rowspan="1" colspan="1">5.0</th>
                                    <th class="entry" align="center" valign="middle" width="5.263157894736842%" id="d117e33751" rowspan="1" colspan="1">5.2</th>
                                    <th class="entry" align="center" valign="middle" width="5.263157894736842%" id="d117e33755" rowspan="1" colspan="1">5.3</th>
                                    <th class="entry" align="center" valign="middle" width="5.263157894736842%" id="d117e33758" rowspan="1" colspan="1">6.0</th>
                                    <th class="entry" align="center" valign="middle" width="5.263157894736842%" id="d117e33761" rowspan="1" colspan="1">6.1</th>
                                    <th class="entry" align="center" valign="middle" width="5.263157894736842%" id="d117e33764" rowspan="1" colspan="1">6.2</th>
                                    <th class="entry" align="center" valign="middle" width="5.263157894736842%" id="d117e33767" rowspan="1" colspan="1">7.0</th>
                                    <th class="entry" align="center" valign="middle" width="5.263157894736842%" id="d117e33770" rowspan="1" colspan="1">7.2</th>
                                    <th class="entry" align="center" valign="middle" width="5.263157894736842%" id="d117e33774" rowspan="1" colspan="1">7.5</th>
                                    <th class="entry" align="center" valign="middle" width="5.263157894736842%" id="d117e33777" rowspan="1" colspan="1">8.0</th>
                                    <th class="entry" align="center" valign="middle" width="5.263157894736842%" id="d117e33780" rowspan="1" colspan="1">8.6</th>
                                    <th class="entry" align="center" valign="middle" width="5.263157894736842%" id="d117e33783" rowspan="1" colspan="1">8.7</th>
                                 </tr>
                              </thead>
                              <tbody class="tbody">
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum number of resident grids per device (<a class="xref" href="index.html#concurrent-kernel-execution" shape="rect">Concurrent Kernel Execution</a>) 
                                    </td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751" rowspan="1">32</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33755" rowspan="1">16</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33758" rowspan="1">128</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33761" rowspan="1">32</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33764" rowspan="1">16</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33767" rowspan="1">128</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33770" rowspan="1">16</td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d117e33733 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">128</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum dimensionality of grid of thread blocks </td>
                                    <td class="entry" colspan="14" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">3</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum x-dimension of a grid of thread blocks </td>
                                    <td class="entry" colspan="14" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1"> 2<sup class="ph sup">31</sup>-1 
                                    </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum y- or z-dimension of a grid of thread blocks</td>
                                    <td class="entry" colspan="14" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">65535</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum dimensionality of a thread block </td>
                                    <td class="entry" colspan="14" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">3</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum x- or y-dimension of a block </td>
                                    <td class="entry" colspan="14" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">1024</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum z-dimension of a block </td>
                                    <td class="entry" colspan="14" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">64</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum number of threads per block </td>
                                    <td class="entry" colspan="14" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">1024</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Warp size</td>
                                    <td class="entry" colspan="14" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">32</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum number of resident blocks per SM </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745" rowspan="1">16</td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d117e33733 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770" rowspan="1">32</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33774" rowspan="1">16</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33777" rowspan="1">32</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33780 d117e33783" rowspan="1">16</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum number of resident warps per SM </td>
                                    <td class="entry" colspan="10" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770" rowspan="1">64</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33774" rowspan="1">32</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33777" rowspan="1">64</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33780 d117e33783" rowspan="1">48</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum number of resident threads per SM </td>
                                    <td class="entry" colspan="10" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770" rowspan="1">2048</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33774" rowspan="1">1024</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33777" rowspan="1">2048</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33780 d117e33783" rowspan="1">1536</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Number of 32-bit registers per SM </td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33742" rowspan="1">64 K</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33745" rowspan="1">128 K</td>
                                    <td class="entry" colspan="12" align="center" valign="middle" headers="d117e33733 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">64 K</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum number of 32-bit registers per thread block </td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751" rowspan="1">64 K</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33755" rowspan="1">32 K</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761" rowspan="1">64 K</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33764" rowspan="1">32 K</td>
                                    <td class="entry" colspan="6" align="center" valign="middle" headers="d117e33733 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">64 K</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum number of 32-bit registers per thread</td>
                                    <td class="entry" colspan="14" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">255</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum amount of shared memory per SM </td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33742" rowspan="1">48 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33745" rowspan="1">112 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33748" rowspan="1">64 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33751" rowspan="1">96 KB</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33755 d117e33758" rowspan="1">64 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33761" rowspan="1">96 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33764" rowspan="1">64 KB</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33767 d117e33770" rowspan="1">96 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33774" rowspan="1">64 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33777" rowspan="1">164 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33780" rowspan="1">100 KB</td>
                                    <td class="entry" align="center" valign="middle" width="5.263157894736842%" headers="d117e33733 d117e33783" rowspan="1" colspan="1">164 KB</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum amount of shared memory per thread block <a name="fnsrc_33" href="#fntarg_33" shape="rect"><sup>33</sup></a></td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764" rowspan="1">48 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33767" rowspan="1">96 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33770" rowspan="1">96 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33774" rowspan="1">64 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33777" rowspan="1">163 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33780" rowspan="1">99 KB</td>
                                    <td class="entry" align="center" valign="middle" width="5.263157894736842%" headers="d117e33733 d117e33783" rowspan="1" colspan="1">163 KB</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Number of shared memory banks </td>
                                    <td class="entry" colspan="14" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">32</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum amount of local memory per thread </td>
                                    <td class="entry" colspan="14" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">512 KB</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Constant memory size </td>
                                    <td class="entry" colspan="14" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">64 KB</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Cache working set per SM for constant memory </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755" rowspan="1">8 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33758" rowspan="1">4 KB</td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d117e33733 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">8 KB</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Cache working set per SM for texture memory </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755" rowspan="1"> Between 12 KB and 48 KB </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764" rowspan="1">Between 24 KB and 48 KB</td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33767 d117e33770" rowspan="1">32 ~ 128 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33774" rowspan="1">32 or 64 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33777" rowspan="1">28KB ~ 192 KB</td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33780" rowspan="1">28KB ~ 128 KB</td>
                                    <td class="entry" align="center" valign="middle" headers="d117e33733 d117e33783" rowspan="1" colspan="1">28KB ~ 192 KB</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum width for a 1D texture reference bound to a CUDA array </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755" rowspan="1">65536</td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">131072</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum width for a 1D texture reference bound to linear memory </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755" rowspan="1">2<sup class="ph sup">27</sup></td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33758" rowspan="1">2<sup class="ph sup">28</sup></td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33761 d117e33764" rowspan="1">2<sup class="ph sup">27</sup></td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33767" rowspan="1">2<sup class="ph sup">28</sup></td>
                                    <td class="entry" colspan="1" align="center" valign="middle" headers="d117e33733 d117e33770" rowspan="1">2<sup class="ph sup">27</sup></td>
                                    <td class="entry" colspan="4" align="center" valign="middle" headers="d117e33733 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">2<sup class="ph sup">28</sup></td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum width and number of layers for a 1D layered texture reference </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755" rowspan="1"> 16384 x 2048 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">32768 x 2048 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1"> Maximum width and height for a 2D texture reference bound to a CUDA array </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755" rowspan="1"> 65536 x 65536 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">131072 x 65536</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1"> Maximum width and height for a 2D texture reference bound to linear memory </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745" rowspan="1"> 65000 x 65000 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d117e33733 d117e33748 d117e33751 d117e33755" rowspan="1"> 65536 x 65536 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1"> 131072 x 65000 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1"> Maximum width and height for a 2D texture reference bound to a CUDA array
                                       supporting texture gather 
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755" rowspan="1"> 16384 x 16384 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1"> 32768 x 32768 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1"> Maximum width, height, and number of layers for a 2D layered texture reference </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755" rowspan="1"> 16384 x 16384 x 2048 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1"> 32768 x 32768 x 2048 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1"> Maximum width, height, and depth for a 3D texture reference bound to a CUDA
                                       array 
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755" rowspan="1"> 4096 x 4096 x 4096 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1"> 16384 x 16384 x 16384 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum width (and height) for a cubemap texture reference </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755" rowspan="1"> 16384 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1"> 32768 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1"> Maximum width (and height) and number of layers for a cubemap layered texture
                                       reference 
                                    </td>
                                    <td class="entry" colspan="5" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755" rowspan="1">16384 x 2046 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1"> 32768 x 2046 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum number of textures that can be bound to a kernel </td>
                                    <td class="entry" colspan="14" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">256</td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum width for a 1D surface reference bound to a CUDA array </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745" rowspan="1"> 65536 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d117e33733 d117e33748 d117e33751 d117e33755" rowspan="1"> 16384 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1"> 32768 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum width and number of layers for a 1D layered surface reference </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745" rowspan="1"> 65536 x 2048 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d117e33733 d117e33748 d117e33751 d117e33755" rowspan="1"> 16384 x 2048 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1"> 32768 x 2048 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1"> Maximum width and height for a 2D surface reference bound to a CUDA array </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745" rowspan="1"> 65536 x 32768 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d117e33733 d117e33748 d117e33751 d117e33755" rowspan="1"> 65536 x 65536 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1"> 131072 x 65536 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1"> Maximum width, height, and number of layers for a 2D layered surface reference </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745" rowspan="1">65536 x 32768 x 2048 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d117e33733 d117e33748 d117e33751 d117e33755" rowspan="1"> 16384 x 16384 x 2048 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1"> 32768 x 32768 x 2048 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1"> Maximum width, height, and depth for a 3D surface reference bound to a CUDA
                                       array 
                                    </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745" rowspan="1"> 65536 x 32768 x 2048 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d117e33733 d117e33748 d117e33751 d117e33755" rowspan="1"> 4096 x 4096 x 4096 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1"> 16384 x 16384 x 16384 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1"> Maximum width (and height) for a cubemap surface reference bound to a CUDA
                                       array 
                                    </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745" rowspan="1"> 32768 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d117e33733 d117e33748 d117e33751 d117e33755" rowspan="1"> 16384 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1"> 32768 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1"> Maximum width (and height) and number of layers for a cubemap layered surface
                                       reference 
                                    </td>
                                    <td class="entry" colspan="2" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745" rowspan="1"> 32768 x 2046 </td>
                                    <td class="entry" colspan="3" align="center" valign="middle" headers="d117e33733 d117e33748 d117e33751 d117e33755" rowspan="1"> 16384 x 2046 </td>
                                    <td class="entry" colspan="9" align="center" valign="middle" headers="d117e33733 d117e33758 d117e33761 d117e33764 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1"> 32768 x 2046 </td>
                                 </tr>
                                 <tr class="row" valign="middle">
                                    <td class="entry" align="left" valign="middle" width="26.31578947368421%" headers="d117e33731 d117e33739" rowspan="1" colspan="1">Maximum number of surfaces that can be bound to a kernel </td>
                                    <td class="entry" colspan="8" align="center" valign="middle" headers="d117e33733 d117e33742 d117e33745 d117e33748 d117e33751 d117e33755 d117e33758 d117e33761 d117e33764" rowspan="1">16</td>
                                    <td class="entry" colspan="6" align="center" valign="middle" headers="d117e33733 d117e33767 d117e33770 d117e33774 d117e33777 d117e33780 d117e33783" rowspan="1">32</td>
                                 </tr>
                              </tbody>
                           </table>
                        </div>



<h2 id="K-2-Floating-Point-Standard"><a href="#K-2-Floating-Point-Standard" class="headerlink" title="K.2. Floating-Point Standard"></a>K.2. Floating-Point Standard</h2><p>所有计算设备都遵循二进制浮点运算的 IEEE 754-2008 标准，但存在以下偏差：</p>
<ul>
<li>没有动态可配置的舍入模式；但是，大多数操作支持多种 IEEE 舍入模式，通过设备内在函数公开。</li>
<li>没有检测浮点异常发生的机制，并且所有操作都表现得好像 IEEE-754 异常总是被屏蔽，如果出现异常事件，则传递 IEEE-754 定义的屏蔽响应。出于同样的原因，虽然支持 SNaN 编码，但它们不是发信号的，而是作为静默处理的。</li>
<li>涉及一个或多个输入 NaN 的单精度浮点运算的结果是位模式 0x7fffffff 的安静 NaN。</li>
<li>双精度浮点绝对值和求反在 NaN 方面不符合 IEEE-754；这些通过不变。</li>
</ul>
<p>必须使用 <code>-ftz=false</code>、<code>-prec-div=true</code> 和 <code>-prec-sqrt=true</code> 编译代码以确保符合 IEEE 标准（这是默认设置；有关这些编译标志的说明，请参阅 nvcc 用户手册）。</p>
<p>无论编译器标志 <code>-ftz</code> 的设置如何，</p>
<ul>
<li>全局内存上的原子单精度浮点加法始终以清零模式运行，即，行为等同于 <code>FADD.F32.FTZ.RN</code>，</li>
<li>共享内存上的原子单精度浮点加法始终在非规范支持下运行，即，行为等同于 <code>FADD.F32.RN</code>。</li>
</ul>
<p>根据 IEEE-754R 标准，如果 <code>fminf()</code>、<code>fmin()</code>、<code>fmaxf()</code> 或 <code>fmax()</code> 的输入参数之一是 NaN，而另一个不是，则结果是<code>non-NaN</code> 参数。</p>
<p>IEEE-754 未定义在浮点值超出整数格式范围的情况下将浮点值转换为整数值。对于计算设备，行为是钳制到支持范围的末尾。这与 x86 架构行为不同。</p>
<p>IEEE-754 未定义整数除以零和整数溢出的行为。对于计算设备，没有机制可以检测是否发生了此类整数运算异常。整数除以零会产生一个未指定的、特定于机器的值。</p>
<p><a href="https://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus">https://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus</a> 包含有关 NVIDIA GPU 的浮点精度和合规性的更多信息。</p>
<h2 id="K-3-Compute-Capability-3-x"><a href="#K-3-Compute-Capability-3-x" class="headerlink" title="K.3. Compute Capability 3.x"></a>K.3. Compute Capability 3.x</h2><h3 id="K-3-1-Architecture"><a href="#K-3-1-Architecture" class="headerlink" title="K.3.1. Architecture"></a>K.3.1. Architecture</h3><p>一个 SM 包括：</p>
<ul>
<li>192 个用于算术运算的 CUDA 内核（请参阅算术指令以了解算术运算的吞吐量），</li>
<li>32个单精度浮点先验函数的特殊函数单元，</li>
<li>4个warp调度器。</li>
</ul>
<p>当一个 SM 被赋予执行 warp 时，它首先将它们分配给四个调度程序。然后，在每个指令发布时间，每个调度程序都会为其分配的一个已准备好执行的warp（如果有的话）发布两条独立的指令。</p>
<p>一个 SM 有一个只读常量缓存，它被所有功能单元共享，并加快了从驻留在设备内存中的常量内存空间的读取速度。</p>
<p><strong>每个 SM 都有一个 L1 缓存，所有 SM 共享一个 L2 缓存</strong>。 L1 缓存用于缓存对本地内存的访问，包括临时寄存器溢出。 L2 缓存用于缓存对本地和全局内存的访问。缓存行为（例如，读取是在 L1 和 L2 中缓存还是仅在 L2 中缓存）可以使用加载或存储指令的修饰符在每次访问的基础上进行部分配置。某些计算能力为 3.5 的设备和计算能力为 3.7 的设备允许通过编译器选项选择在 L1 和 L2 中缓存全局内存。</p>
<p>相同的片上存储器用于 L1 和共享内存：它可以配置为 48 KB 共享内存和 16 KB 一级缓存或 16 KB 共享内存和 48 KB 一级缓存或 32 KB 共享内存和 32 KB 的 L1 缓存，使用 <code>cudaFuncSetCacheConfig()/cuFuncSetCacheConfig()</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Device code</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MyKernel</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Host code</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Runtime API</span></span><br><span class="line"><span class="comment">// cudaFuncCachePreferShared: shared memory is 48 KB</span></span><br><span class="line"><span class="comment">// cudaFuncCachePreferEqual: shared memory is 32 KB</span></span><br><span class="line"><span class="comment">// cudaFuncCachePreferL1: shared memory is 16 KB</span></span><br><span class="line"><span class="comment">// cudaFuncCachePreferNone: no preference</span></span><br><span class="line"><span class="built_in">cudaFuncSetCacheConfig</span>(MyKernel, cudaFuncCachePreferShared)</span><br></pre></td></tr></table></figure>
<p>默认的缓存配置是“prefer none”，意思是“无偏好”。如果内核被配置为没有首选项，那么它将默认为当前线程/上下文的首选项，这是使用 <code>cudaDeviceSetCacheConfig()/cuCtxSetCacheConfig()</code> 设置的（有关详细信息，请参阅参考手册）。如果当前线程/上下文也没有首选项（这又是默认设置），那么任何内核最近使用的缓存配置都将被使用，除非需要不同的缓存配置来启动内核（例如，由于共享内存要求）。初始配置是 48 KB 的共享内存和 16 KB 的 L1 高速缓存。</p>
<p>注意：计算能力为 3.7 的设备为上述每个配置添加了额外的 64 KB 共享内存，每个 SM 分别产生 112 KB、96 KB 和 80 KB 共享内存。但是，每个线程块的最大共享内存仍为 48 KB。<br>应用程序可以通过检查 <code>l2CacheSize</code> 设备属性来查询 L2 缓存大小（请参阅设备枚举）。最大二级缓存大小为 1.5 MB。</p>
<p>每个 SM 都有一个 48 KB 的只读数据缓存，以加快从设备内存中读取的速度。它直接访问此缓存（对于计算能力为 3.5 或 3.7 的设备），或通过实现<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-and-surface-memory">纹理和表面</a>内存中提到的各种寻址模式和数据过滤的纹理单元。当通过纹理单元访问时，只读数据缓存也称为纹理缓存。</p>
<h3 id="K-3-2-Global-Memory"><a href="#K-3-2-Global-Memory" class="headerlink" title="K.3.2. Global Memory"></a>K.3.2. Global Memory</h3><p>计算能力 3.x 的设备的全局内存访问缓存在 L2 中，计算能力 3.5 或 3.7 的设备也可以缓存在上一节中描述的只读数据缓存中；它们通常不缓存在 L1 中。某些计算能力为 3.5 的设备和计算能力为 3.7 的设备允许通过 nvcc 的 <code>-Xptxas -dlcm=ca</code> 选项选择缓存 L1 中的全局内存访问。</p>
<p>高速缓存行是 128 字节，并映射到设备内存中 128 字节对齐的段。缓存在 L1 和 L2 中的内存访问使用 128 字节内存事务处理，而仅缓存在 L2 中的内存访问使用 32 字节内存事务处理。因此，仅在 L2 中进行缓存可以减少过度获取，例如，在分散内存访问的情况下。</p>
<p>如果每个线程访问的字的大小超过 4 字节，则 warp 的内存请求首先被拆分为独立发出的单独的 128 字节内存请求：</p>
<ul>
<li>两个内存请求，每个半warp一个，如果大小为 8 字节，</li>
<li>如果大小为 16 字节，则四个内存请求，每个四分之一warp一个。</li>
</ul>
<p>然后将每个内存请求分解为独立发出的高速缓存行请求。在缓存命中的情况下，以 L1 或 L2 缓存的吞吐量为缓存行请求提供服务，否则以设备内存的吞吐量提供服务。</p>
<p>请注意，线程可以以任何顺序访问任何字，包括相同的字。</p>
<p>如果 warp 执行的非原子指令为该 warp 的多个线程写入全局内存中的同一位置，则只有一个线程执行写入，并且未定义哪个线程执行写入。</p>
<p>在内核的整个生命周期内只读的数据也可以通过使用 <code>__ldg()</code> 函数读取它来缓存在上一节中描述的只读数据缓存中（请参阅只读数据缓存加载函数）。当编译器检测到某些数据满足只读条件时，它会使用<code>__ldg()</code> 来读取它。编译器可能并不总是能够检测到某些数据满足只读条件。使用 <code>const</code> 和 <code>__restrict__</code> 限定符标记用于加载此类数据的指针会增加编译器检测到只读条件的可能性。</p>
<p>下图显示了全局内存访问和相应内存事务的一些示例。</p>
<p><img src="/img/examples-of-global-memory-accesses.png" alt=""></p>
<h3 id="K-3-3-Shared-Memory"><a href="#K-3-3-Shared-Memory" class="headerlink" title="K.3.3. Shared Memory"></a>K.3.3. Shared Memory</h3><p>下图中显示了一些跨步访问的示例。</p>
<p><img src="/img/examples-of-strided-shared-memory-accesses.png" alt=""></p>
<p>下图显示了一些涉及广播机制的内存读取访问示例。</p>
<p><img src="/img/examples-of-irregular-shared-memory-accesses.png" alt=""></p>
<p><strong>64 位模式</strong></p>
<p>连续的 64 位字映射到连续的存储区。</p>
<p>对 warp 的共享内存请求不会在访问同一 64 位字中的任何子字的两个线程之间产生bank冲突（即使两个子字的地址位于同一bank中）。在这种情况下，对于读取访问，64 位字被广播到请求线程，对于写入访问，每个子字仅由其中一个线程写入（哪个线程执行写入未定义）。</p>
<p><strong>32 位模式</strong></p>
<p>连续的 32 位字映射到连续的存储区。</p>
<p>对warp 的共享内存请求不会在访问同一32 位字或索引i 和j 在同一64 字对齐段中的两个32 位字内的任何子字的两个线程之间产生bank冲突（即，第一个索引是 64 的倍数的段）并且使得 j=i+32（即使两个子字的地址在同一个库中）。在这种情况下，对于读访问，32 位字被广播到请求线程，对于写访问，每个子字仅由其中一个线程写入（哪个线程执行写入未定义）。</p>
<h2 id="K-4-Compute-Capability-5-x"><a href="#K-4-Compute-Capability-5-x" class="headerlink" title="K.4. Compute Capability 5.x"></a>K.4. Compute Capability 5.x</h2><h3 id="K-4-1-Architecture"><a href="#K-4-1-Architecture" class="headerlink" title="K.4.1. Architecture"></a>K.4.1. Architecture</h3><p>一个 SM 包括：</p>
<ul>
<li>128 个用于算术运算的 CUDA 内核（请参阅算术指令以了解算术运算的吞吐量），</li>
<li>32个单精度浮点先验函数的特殊函数单元，</li>
<li>4个warp调度器。</li>
</ul>
<p>当一个 SM 被赋予执行 warp 时，它首先将它们分配给四个调度程序。然后，在每个指令发布时间，每个调度程序都会为其分配的经准备好执行的warp之一发布一条指令（如果有的话）。</p>
<p>SM 具有：</p>
<ul>
<li>由所有功能单元共享的只读常量缓存，可加快从驻留在设备内存中的常量内存空间的读取速度，</li>
<li>一个 24 KB 的统一 L1/纹理缓存，用于缓存来自全局内存的读取，</li>
<li>64 KB 共享内存用于计算能力为 5.0 的设备或 96 KB 共享内存用于计算能力为 5.2 的设备。</li>
</ul>
<p>纹理单元也使用统一的 L1/纹理缓存，实现<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-and-surface-memory">纹理和表面</a>内存中提到的各种寻址模式和数据过滤。</p>
<p>还有一个由所有 SM 共享的 L2 缓存，用于缓存对本地或全局内存的访问，包括<strong>临时寄存器溢出</strong>。应用程序可以通过检查 <code>l2CacheSize</code> 设备属性来查询 L2 缓存大小（请参阅设备枚举）。</p>
<p>缓存行为（例如，读取是否缓存在统一的 L1/纹理缓存和 L2 中或仅在 L2 中）可以使用加载指令的修饰符在每次访问的基础上进行部分配置。</p>
<h3 id="K-4-2-Global-Memory"><a href="#K-4-2-Global-Memory" class="headerlink" title="K.4.2. Global Memory"></a>K.4.2. Global Memory</h3><p>全局内存访问始终缓存在 L2 中，并且 L2 中的缓存行为与计算能力 3.x 的设备相同（请参阅全局内存）。</p>
<p>在内核的整个生命周期内只读的数据也可以通过使用 <code>__ldg()</code> 函数读取它来缓存在上一节中描述的统一 L1/纹理缓存中（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#ldg-function">只读数据缓存加载函数</a>）。当编译器检测到某些数据满足只读条件时，它会使用<code>__ldg()</code> 来读取它。编译器可能并不总是能够检测到某些数据满足只读条件。使用 <code>const</code> 和 <code>__restrict__</code> 限定符标记用于加载此类数据的指针会增加编译器检测到只读条件的可能性。</p>
<p>对于计算能力 5.0 的设备，在内核的整个生命周期内不是只读的数据不能缓存在统一的 L1/纹理缓存中。对于计算能力为 5.2 的设备，默认情况下不缓存在统一的 L1/纹理缓存中，但可以使用以下机制启用缓存：</p>
<ul>
<li>如 PTX 参考手册中所述，使用带有适当修饰符的内联汇编执行读取；</li>
<li>使用 <code>-Xptxas -dlcm=ca</code> 编译标志进行编译，在这种情况下，所有读取都被缓存，除了使用带有禁用缓存的修饰符的内联汇编执行的读取；</li>
<li>使用 <code>-Xptxas -fscm=ca</code> 编译标志进行编译，在这种情况下，所有读取都被缓存，包括使用内联汇编执行的读取，无论使用何种修饰符。</li>
</ul>
<p>当使用上面列出的三种机制之一启用缓存时，计算能力 5.2 的设备将为所有内核启动缓存全局内存读取到统一的 L1/纹理缓存中，除了线程块消耗过多 SM 寄存器的内核启动文件。这些异常由分析器报告。</p>
<h3 id="K-4-3-Shared-Memory"><a href="#K-4-3-Shared-Memory" class="headerlink" title="K.4.3. Shared Memory"></a>K.4.3. Shared Memory</h3><p>共享内存有 32 个bank，这些bank被组织成连续的 32 位字映射到连续的bank。 每个bank的带宽为每个时钟周期 32 位。</p>
<p>对 warp 的共享内存请求不会在访问同一 32 位字内的任何地址的两个线程之间产生bank冲突（即使两个地址位于同一存储库中）。 在这种情况下，对于读取访问，该字被广播到请求线程，对于写入访问，每个地址仅由一个线程写入（哪个线程执行写入未定义）。</p>
<p>下显示了一些跨步访问的示例。</p>
<p><img src="/img/examples-of-strided-shared-memory-accesses.png" alt=""></p>
<p>左边</p>
<p>步长为一个 32 位字的线性寻址（无bank冲突）。</p>
<p>中间</p>
<p>跨两个 32 位字的线性寻址（双向bank冲突）。</p>
<p>右边</p>
<p>跨度为三个 32 位字的线性寻址（无bank冲突）。</p>
<p>下图显示了一些涉及广播机制的内存读取访问示例。</p>
<p><img src="/img/examples-of-irregular-shared-memory-accesses.png" alt=""></p>
<p>左边</p>
<p>通过随机排列实现无冲突访问。</p>
<p>中间</p>
<p>由于线程 3、4、6、7 和 9 访问存储区 5 中的同一个字，因此无冲突访问。</p>
<p>右边</p>
<p>无冲突广播访问（线程访问bank内的同一个词）。</p>
<h2 id="K-5-Compute-Capability-6-x"><a href="#K-5-Compute-Capability-6-x" class="headerlink" title="K.5. Compute Capability 6.x"></a>K.5. Compute Capability 6.x</h2><h3 id="K-5-1-Architecture"><a href="#K-5-1-Architecture" class="headerlink" title="K.5.1. Architecture"></a>K.5.1. Architecture</h3><p>一个 SM 包括：</p>
<ul>
<li>64 个（计算能力 6.0）或 128 个（6.1 和 6.2）用于算术运算的 CUDA 内核，</li>
<li>16 个 (6.0) 或 32 个 (6.1 和 6.2) 用于单精度浮点超越函数的特殊函数单元，</li>
<li>2 个（6.0）或 4 个（6.1 和 6.2）warp 调度器。</li>
</ul>
<p>当一个 SM 被指定执行 warp 时，它首先将它们分配给它的调度程序。然后，在每个指令发布时间，每个调度程序都会为其分配的经准备好执行的warp之一发布一条指令（如果有的话）。</p>
<p>SM 具有：</p>
<ul>
<li>由所有功能单元共享的只读常量缓存，可加快从驻留在设备内存中的常量内存空间的读取速度，</li>
<li>一个统一的 L1/纹理缓存，用于从大小为 24 KB（6.0 和 6.2）或 48 KB（6.1）的全局内存中读取，</li>
<li>大小为 64 KB（6.0 和 6.2）或 96 KB（6.1）的共享内存。</li>
</ul>
<p>纹理单元也使用统一的 L1/纹理缓存，实现纹理和表面内存中提到的各种寻址模式和数据过滤。</p>
<p>还有一个由所有 SM 共享的 L2 缓存，用于缓存对本地或全局内存的访问，包括临时寄存器溢出。应用程序可以通过检查 <code>l2CacheSize</code> 设备属性来查询 L2 缓存大小（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-enumeration">设备枚举</a>）。</p>
<p>缓存行为（例如，读取是否缓存在统一的 L1/纹理缓存和 L2 中或仅在 L2 中）可以使用加载指令的修饰符在每次访问的基础上进行部分配置。</p>
<h3 id="K-5-2-Global-Memory"><a href="#K-5-2-Global-Memory" class="headerlink" title="K.5.2. Global Memory"></a>K.5.2. Global Memory</h3><p>全局内存的行为方式与计算能力 5.x 的设备相同（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#global-memory-5-x">全局内存</a>）。</p>
<h3 id="K-5-3-Shared-Memory"><a href="#K-5-3-Shared-Memory" class="headerlink" title="K.5.3. Shared Memory"></a>K.5.3. Shared Memory</h3><p>共享内存的行为方式与计算能力 5.x 的设备相同（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-5-x">共享内存</a>）。</p>
<h2 id="K-6-Compute-Capability-7-x"><a href="#K-6-Compute-Capability-7-x" class="headerlink" title="K.6. Compute Capability 7.x"></a>K.6. Compute Capability 7.x</h2><p>一个 SM 包括：</p>
<ul>
<li>64 个 FP32 内核，用于单精度算术运算，</li>
<li>32 个用于双精度算术运算的 FP64 内核，</li>
<li>64 个 INT32 内核用于整数数学，</li>
<li>8 个混合精度张量核，用于深度学习矩阵算术</li>
<li>16个单精度浮点超越函数的特殊函数单元，</li>
<li>4个warp调度器。</li>
</ul>
<p>一个 SM 在它的调度器之间静态地分配它的 warp。然后，在每个指令发布时间，每个调度程序都会为其分配的warp准备好执行的warp之一发布一条指令（如果有的话）。</p>
<p>SM 具有：</p>
<ul>
<li>由所有功能单元共享的只读常量缓存，可加快从驻留在设备内存中的常量内存空间的读取速度，</li>
<li>一个统一的数据缓存和共享内存，总大小为 128 KB (Volta) 或 96 KB (Turing)。</li>
</ul>
<p>共享内存从统一的数据缓存中分割出来，并且可以配置为各种大小（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-7-x">共享内存</a>。）剩余的数据缓存用作 L1 缓存，也由实现上述各种寻址和数据过滤模式的纹理单元使用在纹理和表面内存。</p>
<h3 id="K-6-2-Independent-Thread-Scheduling"><a href="#K-6-2-Independent-Thread-Scheduling" class="headerlink" title="K.6.2. Independent Thread Scheduling"></a>K.6.2. Independent Thread Scheduling</h3><p>1.Volta 架构在 warp 中的线程之间引入了独立线程调度，启用了以前不可用的内部 warp 同步模式，并在移植 CPU 代码时简化了代码更改。 但是，如果开发人员对先前硬件架构的warp同步性做出假设，这可能会导致参与执行代码的线程集与预期的完全不同。</p>
<p>以下是 Volta 安全代码的关注代码模式和建议的纠正措施。</p>
<p>对于使用 warp 内在函数（<code>__shfl*、__any、__all、__ballot</code>）的应用程序，开发人员有必要将他们的代码移植到具有 <code>*_sync</code> 后缀的新的、安全的同步对应方。 新的warp内在函数采用线程掩码，明确定义哪些通道（warp的线程）必须参与warp内在函数。 有关详细信息，请参阅 Warp Vote 函数和 Warp Shuffle 函数。</p>
<p>由于内在函数可用于 CUDA 9.0+，因此（如有必要）可以使用以下预处理器宏有条件地执行代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">if</span> defined(CUDART_VERSION) &amp;&amp; CUDART_VERSION &gt;= 9000</span></span><br><span class="line"><span class="comment">// *_sync intrinsic</span></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span>      </span></span><br></pre></td></tr></table></figure>
<p>这些内在函数可用于所有架构，而不仅仅是 Volta 或 Turing，并且在大多数情况下，单个代码库就足以满足所有架构的需求。 但是请注意，对于 Pascal 和更早的架构，mask 中的所有线程在收敛时必须执行相同的 warp 内在指令，并且 mask 中所有值的并集必须等于 warp 的活动掩码。 以下代码模式在 Volta 上有效，但在 Pascal 或更早的架构上无效。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (tid % warpSize &lt; <span class="number">16</span>) &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="type">float</span> swapped = __shfl_xor_sync(<span class="number">0xffffffff</span>, val, <span class="number">16</span>);</span><br><span class="line">    ...</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="type">float</span> swapped = __shfl_xor_sync(<span class="number">0xffffffff</span>, val, <span class="number">16</span>);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>__ballot(1)</code> 的替代品是 <code>__activemask()</code>。 请注意，即使在单个代码路径中，warp 中的线程也可以发散。 因此，<code>__activemask()</code> 和 <code>__ballot(1)</code> 可能只返回当前代码路径上的线程子集。 以下无效代码示例在 <code>data[i]</code> 大于阈值时将输出的位<code>i</code> 设置为 1。 <code>__activemask()</code> 用于尝试启用 <code>dataLen</code> 不是 32 的倍数的情况。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Sets bit in output[] to 1 if the correspond element in data[i]</span></span><br><span class="line"><span class="comment">// is greater than ‘threshold’, using 32 threads in a warp.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = warpLane; i &lt; dataLen; i += warpSize) &#123;</span><br><span class="line">    <span class="type">unsigned</span> active = __activemask();</span><br><span class="line">    <span class="type">unsigned</span> bitPack = __ballot_sync(active, data[i] &gt; threshold);</span><br><span class="line">    <span class="keyword">if</span> (warpLane == <span class="number">0</span>) &#123;</span><br><span class="line">        output[i / <span class="number">32</span>] = bitPack;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此代码无效，因为 CUDA 不保证warp只会在循环条件下发散。 当由于其他原因发生分歧时，将由 warp 中的不同线程子集为相同的 32 位输出元素计算冲突的结果。 正确的代码可能会使用非发散循环条件和 <code>__ballot_sync()</code> 来安全地枚举 warp 中参与阈值计算的线程集，如下所示。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = warpLane; i - warpLane &lt; dataLen; i += warpSize) &#123;</span><br><span class="line">    <span class="type">unsigned</span> active = __ballot_sync(<span class="number">0xFFFFFFFF</span>, i &lt; dataLen);</span><br><span class="line">    <span class="keyword">if</span> (i &lt; dataLen) &#123;</span><br><span class="line">        <span class="type">unsigned</span> bitPack = __ballot_sync(active, data[i] &gt; threshold);</span><br><span class="line">        <span class="keyword">if</span> (warpLane == <span class="number">0</span>) &#123;</span><br><span class="line">            output[i / <span class="number">32</span>] = bitPack;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#discovery-pattern-cg">Discovery Pattern</a> 演示了 <code>__activemask()</code> 的有效用例。</p>
<p>2.如果应用程序有warp同步代码，他们将需要在通过全局或共享内存在线程之间交换数据的任何步骤之间插入新的 <code>__syncwarp()</code> warp范围屏障同步指令。 假设代码以锁步方式执行，或者来自不同线程的读/写在没有同步的情况下在 warp 中可见是无效的。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">__shared__ <span class="type">float</span> s_buff[BLOCK_SIZE];</span><br><span class="line">s_buff[tid] = val;</span><br><span class="line">__syncthreads();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Inter-warp reduction</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = BLOCK_SIZE / <span class="number">2</span>; i &gt;= <span class="number">32</span>; i /= <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (tid &lt; i) &#123;</span><br><span class="line">        s_buff[tid] += s_buff[tid+i];</span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Intra-warp reduction</span></span><br><span class="line"><span class="comment">// Butterfly reduction simplifies syncwarp mask</span></span><br><span class="line"><span class="keyword">if</span> (tid &lt; <span class="number">32</span>) &#123;</span><br><span class="line">    <span class="type">float</span> temp;</span><br><span class="line">    temp = s_buff[tid ^ <span class="number">16</span>]; __syncwarp();</span><br><span class="line">    s_buff[tid] += temp;     __syncwarp();</span><br><span class="line">    temp = s_buff[tid ^ <span class="number">8</span>];  __syncwarp();</span><br><span class="line">    s_buff[tid] += temp;     __syncwarp();</span><br><span class="line">    temp = s_buff[tid ^ <span class="number">4</span>];  __syncwarp();</span><br><span class="line">    s_buff[tid] += temp;     __syncwarp();</span><br><span class="line">    temp = s_buff[tid ^ <span class="number">2</span>];  __syncwarp();</span><br><span class="line">    s_buff[tid] += temp;     __syncwarp();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (tid == <span class="number">0</span>) &#123;</span><br><span class="line">    *output = s_buff[<span class="number">0</span>] + s_buff[<span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line">__syncthreads();</span><br></pre></td></tr></table></figure>
<p>3.尽管 <code>__syncthreads()</code> 一直被记录为同步线程块中的所有线程，但 Pascal 和以前的体系结构只能在 warp 级别强制同步。 在某些情况下，只要每个 warp 中至少有一些线程到达屏障，这就会允许屏障成功，而不会被每个线程执行。 从 Volta 开始，CUDA 内置的 <code>__syncthreads()</code> 和 PTX 指令 <code>bar.sync</code>（及其派生类）在每个线程中强制执行，因此在块中所有未退出的线程到达之前不会成功。 利用先前行为的代码可能会死锁，必须进行修改以确保所有未退出的线程都到达屏障。</p>
<p><code>cuda-memcheck</code> 提供的 <code>racecheck</code> 和 <code>synccheck</code> 工具可以帮助定位第 2 点和第 3 点的违规行为。</p>
<p>为了在实现上述纠正措施的同时帮助迁移，开发人员可以选择加入不支持独立线程调度的 Pascal 调度模型。 有关详细信息，请参阅应用<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#application-compatibility">程序兼容性</a>。</p>
<h3 id="K-6-3-Global-Memory"><a href="#K-6-3-Global-Memory" class="headerlink" title="K.6.3. Global Memory"></a>K.6.3. Global Memory</h3><p>全局内存的行为方式与计算能力 5.x 的设备相同（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#global-memory-5-x">全局内存</a>）。</p>
<h3 id="K-6-4-Shared-Memory"><a href="#K-6-4-Shared-Memory" class="headerlink" title="K.6.4. Shared Memory"></a>K.6.4. Shared Memory</h3><p>与 Kepler 架构类似，为共享内存保留的统一数据缓存的数量可以在每个内核的基础上进行配置。对于 Volta 架构（计算能力 7.0），统一数据缓存大小为 128 KB，共享内存容量可设置为 0、8、16、32、64 或 96 KB。对于图灵架构（计算能力 7.5），统一数据缓存大小为 96 KB，共享内存容量可以设置为 32 KB 或 64 KB。与 Kepler 不同，驱动程序自动为每个内核配置共享内存容量以避免共享内存占用瓶颈，同时还允许在可能的情况下与已启动的内核并发执行。在大多数情况下，驱动程序的默认行为应该提供最佳性能。</p>
<p>因为驱动程序并不总是知道全部工作负载，所以有时应用程序提供有关所需共享内存配置的额外提示很有用。例如，很少或没有使用共享内存的内核可能会请求更大的分割，以鼓励与需要更多共享内存的后续内核并发执行。新的 <code>cudaFuncSetAttribute()</code> API 允许应用程序设置首选共享内存容量或分割，作为支持的最大共享内存容量的百分比（Volta 为 96 KB，Turing 为 64 KB）。</p>
<p>与 Kepler 引入的传统 <code>cudaFuncSetCacheConfig()</code> API 相比，<code>cudaFuncSetAttribute()</code> 放宽了首选共享容量的执行。旧版 API 将共享内存容量视为内核启动的硬性要求。结果，具有不同共享内存配置的交错内核将不必要地序列化共享内存重新配置之后的启动。使用新 API，分割被视为提示。如果需要执行功能或避免颠簸，驱动程序可以选择不同的配置。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Device code</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MyKernel</span><span class="params">(...)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    __shared__ <span class="type">float</span> buffer[BLOCK_DIM];</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Host code</span></span><br><span class="line"><span class="type">int</span> carveout = <span class="number">50</span>; <span class="comment">// prefer shared memory capacity 50% of maximum</span></span><br><span class="line"><span class="comment">// Named Carveout Values:</span></span><br><span class="line"><span class="comment">// carveout = cudaSharedmemCarveoutDefault;   //  (-1)</span></span><br><span class="line"><span class="comment">// carveout = cudaSharedmemCarveoutMaxL1;     //   (0)</span></span><br><span class="line"><span class="comment">// carveout = cudaSharedmemCarveoutMaxShared; // (100)</span></span><br><span class="line"><span class="built_in">cudaFuncSetAttribute</span>(MyKernel, cudaFuncAttributePreferredSharedMemoryCarveout, carveout);</span><br><span class="line">MyKernel &lt;&lt;&lt;gridDim, BLOCK_DIM&gt;&gt;&gt;(...);</span><br></pre></td></tr></table></figure>
<p>除了整数百分比之外，还提供了几个方便的枚举，如上面的代码注释中所列。 如果选择的整数百分比不完全映射到支持的容量（SM 7.0 设备支持 0、8、16、32、64 或 96 KB 的共享容量），则使用下一个更大的容量。 例如，在上面的示例中，最大 96 KB 的 50% 是 48 KB，这不是受支持的共享内存容量。 因此，首选项向上舍入为 64 KB。</p>
<p>计算能力 7.x 设备允许单个线程块来处理共享内存的全部容量：Volta 上为 96 KB，Turing 上为 64 KB。 依赖于每个块超过 48 KB 的共享内存分配的内核是特定于体系结构的，因此它们必须使用动态共享内存（而不是静态大小的数组），并且需要使用 <code>cudaFuncSetAttribute()</code> 显式选择加入，如下所示。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Device code</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MyKernel</span><span class="params">(...)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Host code</span></span><br><span class="line"><span class="type">int</span> maxbytes = <span class="number">98304</span>; <span class="comment">// 96 KB</span></span><br><span class="line"><span class="built_in">cudaFuncSetAttribute</span>(MyKernel, cudaFuncAttributeMaxDynamicSharedMemorySize, maxbytes);</span><br><span class="line">MyKernel &lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(...);</span><br></pre></td></tr></table></figure>
<p>否则，共享内存的行为方式与计算能力 5.x 的设备相同（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-5-x">共享内存</a>）。</p>
<h2 id="K-7-Compute-Capability-8-x"><a href="#K-7-Compute-Capability-8-x" class="headerlink" title="K.7. Compute Capability 8.x"></a>K.7. Compute Capability 8.x</h2><h3 id="K-7-1-Architecture"><a href="#K-7-1-Architecture" class="headerlink" title="K.7.1. Architecture"></a>K.7.1. Architecture</h3><p>流式多处理器 (SM) 包括：</p>
<ul>
<li>计算能力为 8.0 的设备中用于单精度算术运算的 64 个 FP32 内核和计算能力为 8.6 的设备中的 128 个 FP32 内核，</li>
<li>计算能力 8.0 的设备中用于双精度算术运算的 32 个 FP64 内核和计算能力 8.6 的设备中的 2 个 FP64 内核</li>
<li>64 个 INT32 内核用于整数数学，</li>
<li>4 个混合精度第三代张量核心，支持半精度 (fp16)、__nv_bfloat16、tf32、子字节和双精度 (fp64) 矩阵运算（详见 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#wmma">Warp 矩阵函数</a>），</li>
<li>16个单精度浮点超越函数的特殊函数单元，</li>
<li>4个warp调度器。</li>
</ul>
<p>一个 SM 在它的调度器之间静态地分配它的 warp。然后，在每个指令发布时间，每个调度程序都会为其分配的warp准备好执行的warp之一发布一条指令（如果有的话）。</p>
<p>SM 具有：</p>
<ul>
<li>由所有功能单元共享的只读常量缓存，可加快从驻留在设备内存中的常量内存空间的读取速度，</li>
<li>一个统一的数据缓存和共享内存，总大小为 192 KB，用于计算能力 8.0 的设备（1.5 倍 Volta 的 128 KB 容量）和 128 KB，用于计算能力 8.6 的设备。</li>
</ul>
<p>共享内存从统一数据缓存中分割出来，并且可以配置为各种大小（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-8-x">共享内存</a>部分）。剩余的数据缓存用作 L1 缓存，也由实现<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-and-surface-memory">纹理和表面内存</a>中提到的各种寻址和数据过滤模式的纹理单元使用。</p>
<h3 id="K-7-2-Global-Memory"><a href="#K-7-2-Global-Memory" class="headerlink" title="K.7.2. Global Memory"></a>K.7.2. Global Memory</h3><p>全局内存的行为方式与计算能力 5.x 的设备相同（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#global-memory-5-x">全局内存</a>）。</p>
<h3 id="K-7-3-Shared-Memory"><a href="#K-7-3-Shared-Memory" class="headerlink" title="K.7.3. Shared Memory"></a>K.7.3. Shared Memory</h3><p>与 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#architecture-7-x">Volta</a> 架构类似，为共享内存保留的统一数据缓存的数量可在每个内核的基础上进行配置。对于 NVIDIA Ampere GPU 架构，计算能力为 8.0 的设备的统一数据缓存大小为 192 KB，计算能力为 8.6 的设备为 128 KB。对于计算能力为 8.0 的设备，共享内存容量可以设置为 0、8、16、32、64、100、132 或 164 KB，对于计算能力的设备，可以设置为 0、8、16、32、64 或 100 KB 8.6.</p>
<p>应用程序可以使用 <code>cudaFuncSetAttribute()</code> 设置<code>carveout</code>，即首选共享内存容量。</p>
<p><code>cudaFuncSetAttribute(kernel_name, cudaFuncAttributePreferredSharedMemoryCarveout, carveout);</code></p>
<p>API 可以分别指定计算能力为 8.0 的设备的最大支持共享内存容量 164 KB 和计算能力为 8.6 的设备的 100 KB 的整数百分比，或以下值之一：<code>cudaSharedmemCarveoutDefault, cudaSharedmemCarveoutMaxL1 ，或 cudaSharedmemCarveoutMaxShared</code>。使用百分比时，分拆四舍五入到最接近的受支持共享内存容量。例如，对于计算能力为 8.0 的设备，50% 将映射到 100 KB 的分割，而不是 82 KB 的分割。设置 <code>cudaFuncAttributePreferredSharedMemoryCarveout</code> 被驱动程序视为提示；如果需要，驱动程序可以选择不同的配置。</p>
<p>计算能力 8.0 的设备允许单个线程块寻址多达 163 KB 的共享内存，而计算能力 8.6 的设备允许多达 99 KB 的共享内存。依赖于每个块超过 48 KB 的共享内存分配的内核是特定于体系结构的，并且必须使用动态共享内存而不是静态大小的共享内存数组。这些内核需要通过使用 <code>cudaFuncSetAttribute()</code> 来设置 <code>cudaFuncAttributeMaxDynamicSharedMemorySize</code> 来明确选择加入；请参阅 Volta 架构的<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/topics/compute-capabilities.html#shared-memory-7.x">共享内存</a>。</p>
<p><strong>请注意，每个线程块的最大共享内存量小于每个 SM 可用的最大共享内存分区。未提供给线程块的 1 KB 共享内存保留给系统使用。</strong></p>
<h1 id="附录L-CUDA底层驱动API"><a href="#附录L-CUDA底层驱动API" class="headerlink" title="附录L CUDA底层驱动API"></a>附录L CUDA底层驱动API</h1><p>本附录假定您了解 CUDA 运行时中描述的概念。</p>
<p>驱动程序 API 在 cuda 动态库（<code>cuda.dll</code> 或 <code>cuda.so</code>）中实现，该库在安装设备驱动程序期间复制到系统上。 它的所有入口点都以 cu 为前缀。</p>
<p>它是一个基于句柄的命令式 API：大多数对象都由不透明的句柄引用，这些句柄可以指定给函数来操作对象。</p>
<p>驱动程序 API 中可用的对象汇总在下表中。</p>
<div class="tablenoborder"><a name="driver-api__objects-available-in-cuda-driver-api" shape="rect">
                           <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="driver-api__objects-available-in-cuda-driver-api" class="table" frame="border" border="1" rules="all">
                           <caption><span class="tablecap">Table 16. Objects Available in the CUDA Driver API</span></caption>
                           <thead class="thead" align="left">
                              <tr class="row">
                                 <th class="entry" valign="top" width="33.33333333333333%" id="d117e35648" rowspan="1" colspan="1">Object</th>
                                 <th class="entry" valign="top" width="16.666666666666664%" id="d117e35651" rowspan="1" colspan="1">Handle</th>
                                 <th class="entry" valign="top" width="50%" id="d117e35654" rowspan="1" colspan="1">Description</th>
                              </tr>
                           </thead>
                           <tbody class="tbody">
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d117e35648" rowspan="1" colspan="1">Device</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d117e35651" rowspan="1" colspan="1">CUdevice</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e35654" rowspan="1" colspan="1">CUDA-enabled device</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d117e35648" rowspan="1" colspan="1">Context</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d117e35651" rowspan="1" colspan="1">CUcontext</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e35654" rowspan="1" colspan="1">Roughly equivalent to a CPU process</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d117e35648" rowspan="1" colspan="1">Module</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d117e35651" rowspan="1" colspan="1">CUmodule</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e35654" rowspan="1" colspan="1">Roughly equivalent to a dynamic library</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d117e35648" rowspan="1" colspan="1">Function</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d117e35651" rowspan="1" colspan="1">CUfunction</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e35654" rowspan="1" colspan="1">Kernel</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d117e35648" rowspan="1" colspan="1">Heap memory</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d117e35651" rowspan="1" colspan="1">CUdeviceptr</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e35654" rowspan="1" colspan="1">Pointer to device memory</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d117e35648" rowspan="1" colspan="1">CUDA array</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d117e35651" rowspan="1" colspan="1">CUarray</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e35654" rowspan="1" colspan="1">Opaque container for one-dimensional or two-dimensional data on the
                                    device, readable via texture or surface references
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d117e35648" rowspan="1" colspan="1">Texture reference</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d117e35651" rowspan="1" colspan="1">CUtexref</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e35654" rowspan="1" colspan="1">Object that describes how to interpret texture memory data</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d117e35648" rowspan="1" colspan="1">Surface reference</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d117e35651" rowspan="1" colspan="1">CUsurfref</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e35654" rowspan="1" colspan="1">Object that describes how to read or write CUDA arrays</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d117e35648" rowspan="1" colspan="1">Stream</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d117e35651" rowspan="1" colspan="1">CUstream</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e35654" rowspan="1" colspan="1">Object that describes a CUDA stream</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="33.33333333333333%" headers="d117e35648" rowspan="1" colspan="1">Event</td>
                                 <td class="entry" valign="top" width="16.666666666666664%" headers="d117e35651" rowspan="1" colspan="1">CUevent</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e35654" rowspan="1" colspan="1">Object that describes a CUDA event</td>
                              </tr>
                           </tbody>
                        </table>
                     </div>

<p>在调用驱动程序 API 的任何函数之前，必须使用 <code>cuInit()</code> 初始化驱动程序 API。 然后必须创建一个附加到特定设备的 CUDA 上下文，并使其成为当前调用主机线程，如上下文中所述。</p>
<p>在 CUDA 上下文中，内核作为 PTX 或二进制对象由主机代码显式加载，如模块中所述。 因此，用 C++ 编写的内核必须单独编译成 PTX 或二进制对象。 内核使用 API 入口点启动，如内核执行中所述。</p>
<p>任何想要在未来设备架构上运行的应用程序都必须加载 PTX，而不是二进制代码。 这是因为二进制代码是特定于体系结构的，因此与未来的体系结构不兼容，而 PTX 代码在加载时由设备驱动程序编译为二进制代码。</p>
<p>以下是使用驱动程序 API 编写的内核示例的主机代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> N = ...;</span><br><span class="line">    <span class="type">size_t</span> size = N * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate input vectors h_A and h_B in host memory</span></span><br><span class="line">    <span class="type">float</span>* h_A = (<span class="type">float</span>*)<span class="built_in">malloc</span>(size);</span><br><span class="line">    <span class="type">float</span>* h_B = (<span class="type">float</span>*)<span class="built_in">malloc</span>(size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialize input vectors</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialize</span></span><br><span class="line">    <span class="built_in">cuInit</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Get number of devices supporting CUDA</span></span><br><span class="line">    <span class="type">int</span> deviceCount = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cuDeviceGetCount</span>(&amp;deviceCount);</span><br><span class="line">    <span class="keyword">if</span> (deviceCount == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;There is no device supporting CUDA.\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span> (<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Get handle for device 0</span></span><br><span class="line">    CUdevice cuDevice;</span><br><span class="line">    <span class="built_in">cuDeviceGet</span>(&amp;cuDevice, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create context</span></span><br><span class="line">    CUcontext cuContext;</span><br><span class="line">    <span class="built_in">cuCtxCreate</span>(&amp;cuContext, <span class="number">0</span>, cuDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create module from binary file</span></span><br><span class="line">    CUmodule cuModule;</span><br><span class="line">    <span class="built_in">cuModuleLoad</span>(&amp;cuModule, <span class="string">&quot;VecAdd.ptx&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate vectors in device memory</span></span><br><span class="line">    CUdeviceptr d_A;</span><br><span class="line">    <span class="built_in">cuMemAlloc</span>(&amp;d_A, size);</span><br><span class="line">    CUdeviceptr d_B;</span><br><span class="line">    <span class="built_in">cuMemAlloc</span>(&amp;d_B, size);</span><br><span class="line">    CUdeviceptr d_C;</span><br><span class="line">    <span class="built_in">cuMemAlloc</span>(&amp;d_C, size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Copy vectors from host memory to device memory</span></span><br><span class="line">    <span class="built_in">cuMemcpyHtoD</span>(d_A, h_A, size);</span><br><span class="line">    <span class="built_in">cuMemcpyHtoD</span>(d_B, h_B, size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Get function handle from module</span></span><br><span class="line">    CUfunction vecAdd;</span><br><span class="line">    <span class="built_in">cuModuleGetFunction</span>(&amp;vecAdd, cuModule, <span class="string">&quot;VecAdd&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Invoke kernel</span></span><br><span class="line">    <span class="type">int</span> threadsPerBlock = <span class="number">256</span>;</span><br><span class="line">    <span class="type">int</span> blocksPerGrid =</span><br><span class="line">            (N + threadsPerBlock - <span class="number">1</span>) / threadsPerBlock;</span><br><span class="line">    <span class="type">void</span>* args[] = &#123; &amp;d_A, &amp;d_B, &amp;d_C, &amp;N &#125;;</span><br><span class="line">    <span class="built_in">cuLaunchKernel</span>(vecAdd,</span><br><span class="line">                   blocksPerGrid, <span class="number">1</span>, <span class="number">1</span>, threadsPerBlock, <span class="number">1</span>, <span class="number">1</span>,</span><br><span class="line">                   <span class="number">0</span>, <span class="number">0</span>, args, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>完整的代码可以在 <code>vectorAddDrv CUDA</code> 示例中找到。</p>
<h2 id="L-1-Context"><a href="#L-1-Context" class="headerlink" title="L.1. Context"></a>L.1. Context</h2><p>CUDA 上下文类似于 CPU 进程。驱动 API 中执行的所有资源和操作都封装在 CUDA 上下文中，当上下文被销毁时，系统会自动清理这些资源。除了模块和纹理或表面引用等对象外，每个上下文都有自己独特的地址空间。因此，来自不同上下文的 <code>CUdeviceptr</code> 值引用不同的内存位置。</p>
<p>主机线程一次可能只有一个设备上下文当前。当使用 <code>cuCtxCreate()</code> 创建上下文时，它对调用主机线程是当前的。如果有效上下文不是线程当前的，则在上下文中操作的 CUDA 函数（大多数不涉及设备枚举或上下文管理的函数）将返回 <code>CUDA_ERROR_INVALID_CONTEXT</code>。</p>
<p>每个主机线程都有一堆当前上下文。 <code>cuCtxCreate()</code> 将新上下文推送到堆栈顶部。可以调用 <code>cuCtxPopCurrent()</code> 将上下文与主机线程分离。然后上下文是“浮动的”，并且可以作为任何主机线程的当前上下文推送。 <code>cuCtxPopCurrent()</code> 还会恢复先前的当前上下文（如果有）。</p>
<p>还为每个上下文维护使用计数。 <code>cuCtxCreate()</code> 创建使用计数为 1 的上下文。<code>cuCtxAttach()</code> 增加使用计数，而 <code>cuCtxDetach()</code> 减少使用计数。当调用 <code>cuCtxDetach()</code> 或 <code>cuCtxDestroy()</code> 时使用计数变为 0，上下文将被销毁。</p>
<p>驱动程序 API 可与运行时互操作，并且可以通过 <code>cuDevicePrimaryCtxRetain()</code> 从驱动程序 API 访问由运行时管理的主上下文（参见<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#initialization">初始化</a>）。</p>
<p>使用计数有助于在相同上下文中运行的第三方编写的代码之间的互操作性。例如，如果加载三个库以使用相同的上下文，则每个库将调用 <code>cuCtxAttach()</code>来增加使用计数，并在库使用上下文完成时调用 <code>cuCtxDetach()</code> 来减少使用计数。对于大多数库，预计应用程序会在加载或初始化库之前创建上下文；这样，应用程序可以使用自己的启发式方法创建上下文，并且库只需对传递给它的上下文进行操作。希望创建自己的上下文的库（可能会或可能没有创建自己的上下文的 API 客户端不知道）将使用 <code>cuCtxPushCurrent()</code> 和 <code>cuCtxPopCurrent()</code>，如下图所示。</p>
<p><img src="/img/library-context-management.png" alt=""></p>
<h2 id="L-2-Module"><a href="#L-2-Module" class="headerlink" title="L.2. Module"></a>L.2. Module</h2><p>模块是设备代码和数据的动态可加载包，类似于 Windows 中的 DLL，由 nvcc 输出（请参阅使用 NVCC 编译）。 所有符号的名称，包括函数、全局变量和纹理或表面引用，都在模块范围内维护，以便独立第三方编写的模块可以在相同的 CUDA 上下文中互操作。</p>
<p>此代码示例加载一个模块并检索某个内核的句柄：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CUmodule cuModule;</span><br><span class="line"><span class="built_in">cuModuleLoad</span>(&amp;cuModule, <span class="string">&quot;myModule.ptx&quot;</span>);</span><br><span class="line">CUfunction myKernel;</span><br><span class="line"><span class="built_in">cuModuleGetFunction</span>(&amp;myKernel, cuModule, <span class="string">&quot;MyKernel&quot;</span>);</span><br></pre></td></tr></table></figure><br>此代码示例从 PTX 代码编译和加载新模块并解析编译错误：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> BUFFER_SIZE 8192</span></span><br><span class="line">CUmodule cuModule;</span><br><span class="line">CUjit_option options[<span class="number">3</span>];</span><br><span class="line"><span class="type">void</span>* values[<span class="number">3</span>];</span><br><span class="line"><span class="type">char</span>* PTXCode = <span class="string">&quot;some PTX code&quot;</span>;</span><br><span class="line"><span class="type">char</span> error_log[BUFFER_SIZE];</span><br><span class="line"><span class="type">int</span> err;</span><br><span class="line">options[<span class="number">0</span>] = CU_JIT_ERROR_LOG_BUFFER;</span><br><span class="line">values[<span class="number">0</span>]  = (<span class="type">void</span>*)error_log;</span><br><span class="line">options[<span class="number">1</span>] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;</span><br><span class="line">values[<span class="number">1</span>]  = (<span class="type">void</span>*)BUFFER_SIZE;</span><br><span class="line">options[<span class="number">2</span>] = CU_JIT_TARGET_FROM_CUCONTEXT;</span><br><span class="line">values[<span class="number">2</span>]  = <span class="number">0</span>;</span><br><span class="line">err = <span class="built_in">cuModuleLoadDataEx</span>(&amp;cuModule, PTXCode, <span class="number">3</span>, options, values);</span><br><span class="line"><span class="keyword">if</span> (err != CUDA_SUCCESS)</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Link error:\n%s\n&quot;</span>, error_log);</span><br></pre></td></tr></table></figure>
<p>此代码示例从多个 PTX 代码编译、链接和加载新模块，并解析链接和编译错误：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> BUFFER_SIZE 8192</span></span><br><span class="line">CUmodule cuModule;</span><br><span class="line">CUjit_option options[<span class="number">6</span>];</span><br><span class="line"><span class="type">void</span>* values[<span class="number">6</span>];</span><br><span class="line"><span class="type">float</span> walltime;</span><br><span class="line"><span class="type">char</span> error_log[BUFFER_SIZE], info_log[BUFFER_SIZE];</span><br><span class="line"><span class="type">char</span>* PTXCode0 = <span class="string">&quot;some PTX code&quot;</span>;</span><br><span class="line"><span class="type">char</span>* PTXCode1 = <span class="string">&quot;some other PTX code&quot;</span>;</span><br><span class="line">CUlinkState linkState;</span><br><span class="line"><span class="type">int</span> err;</span><br><span class="line"><span class="type">void</span>* cubin;</span><br><span class="line"><span class="type">size_t</span> cubinSize;</span><br><span class="line">options[<span class="number">0</span>] = CU_JIT_WALL_TIME;</span><br><span class="line">values[<span class="number">0</span>] = (<span class="type">void</span>*)&amp;walltime;</span><br><span class="line">options[<span class="number">1</span>] = CU_JIT_INFO_LOG_BUFFER;</span><br><span class="line">values[<span class="number">1</span>] = (<span class="type">void</span>*)info_log;</span><br><span class="line">options[<span class="number">2</span>] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES;</span><br><span class="line">values[<span class="number">2</span>] = (<span class="type">void</span>*)BUFFER_SIZE;</span><br><span class="line">options[<span class="number">3</span>] = CU_JIT_ERROR_LOG_BUFFER;</span><br><span class="line">values[<span class="number">3</span>] = (<span class="type">void</span>*)error_log;</span><br><span class="line">options[<span class="number">4</span>] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;</span><br><span class="line">values[<span class="number">4</span>] = (<span class="type">void</span>*)BUFFER_SIZE;</span><br><span class="line">options[<span class="number">5</span>] = CU_JIT_LOG_VERBOSE;</span><br><span class="line">values[<span class="number">5</span>] = (<span class="type">void</span>*)<span class="number">1</span>;</span><br><span class="line"><span class="built_in">cuLinkCreate</span>(<span class="number">6</span>, options, values, &amp;linkState);</span><br><span class="line">err = <span class="built_in">cuLinkAddData</span>(linkState, CU_JIT_INPUT_PTX,</span><br><span class="line">                    (<span class="type">void</span>*)PTXCode0, <span class="built_in">strlen</span>(PTXCode0) + <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line"><span class="keyword">if</span> (err != CUDA_SUCCESS)</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Link error:\n%s\n&quot;</span>, error_log);</span><br><span class="line">err = <span class="built_in">cuLinkAddData</span>(linkState, CU_JIT_INPUT_PTX,</span><br><span class="line">                    (<span class="type">void</span>*)PTXCode1, <span class="built_in">strlen</span>(PTXCode1) + <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line"><span class="keyword">if</span> (err != CUDA_SUCCESS)</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Link error:\n%s\n&quot;</span>, error_log);</span><br><span class="line"><span class="built_in">cuLinkComplete</span>(linkState, &amp;cubin, &amp;cubinSize);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;Link completed in %fms. Linker Output:\n%s\n&quot;</span>, walltime, info_log);</span><br><span class="line"><span class="built_in">cuModuleLoadData</span>(cuModule, cubin);</span><br><span class="line"><span class="built_in">cuLinkDestroy</span>(linkState);</span><br></pre></td></tr></table></figure>
<p>完整的代码可以在 <code>ptxjit</code> CUDA 示例中找到。</p>
<h2 id="L-3-Kernel-Execution"><a href="#L-3-Kernel-Execution" class="headerlink" title="L.3. Kernel Execution"></a>L.3. Kernel Execution</h2><p><code>cuLaunchKernel()</code> 启动具有给定执行配置的内核。</p>
<p>参数可以作为指针数组（在 <code>cuLaunchKernel()</code> 的最后一个参数旁边）传递，其中第 n 个指针对应于第 n 个参数并指向从中复制参数的内存区域，或者作为额外选项之一（ <code>cuLaunchKernel()</code>) 的最后一个参数。</p>
<p>当参数作为额外选项（<code>CU_LAUNCH_PARAM_BUFFER_POINTER</code> 选项）传递时，它们作为指向单个缓冲区的指针传递，在该缓冲区中，通过匹配设备代码中每个参数类型的对齐要求，参数被假定为彼此正确偏移。</p>
<p><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#vector-types__alignment-requirements-in-device-code">表 4</a> 列出了内置向量类型的设备代码中的对齐要求。对于所有其他基本类型，设备代码中的对齐要求与主机代码中的对齐要求相匹配，因此可以使用 <code>__alignof()</code> 获得。唯一的例外是当宿主编译器在一个字边界而不是两个字边界上对齐 <code>doubl</code>e 和 <code>long long</code>（在 64 位系统上为 long）（例如，使用 gcc 的编译标志 <code>-mno-align-double</code> ) 因为在设备代码中，这些类型总是在两个字的边界上对齐。</p>
<p><code>CUdeviceptr</code>是一个整数，但是代表一个指针，所以它的对齐要求是<code>__alignof(void*)</code>。</p>
<p>以下代码示例使用宏 (<code>ALIGN_UP()</code>) 调整每个参数的偏移量以满足其对齐要求，并使用另一个宏 (<code>ADD_TO_PARAM_BUFFER()</code>) 将每个参数添加到传递给 <code>CU_LAUNCH_PARAM_BUFFER_POINTER</code> 选项的参数缓冲区。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> ALIGN_UP(offset, alignment) \</span></span><br><span class="line"><span class="meta">      (offset) = ((offset) + (alignment) - 1) &amp; ~((alignment) - 1)</span></span><br><span class="line"></span><br><span class="line"><span class="type">char</span> paramBuffer[<span class="number">1024</span>];</span><br><span class="line"><span class="type">size_t</span> paramBufferSize = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> ADD_TO_PARAM_BUFFER(value, alignment)                   \</span></span><br><span class="line"><span class="meta">    do &#123;                                                        \</span></span><br><span class="line"><span class="meta">        paramBufferSize = ALIGN_UP(paramBufferSize, alignment); \</span></span><br><span class="line"><span class="meta">        memcpy(paramBuffer + paramBufferSize,                   \</span></span><br><span class="line"><span class="meta">               &amp;(value), sizeof(value));                        \</span></span><br><span class="line"><span class="meta">        paramBufferSize += sizeof(value);                       \</span></span><br><span class="line"><span class="meta">    &#125; while (0)</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> i;</span><br><span class="line"><span class="built_in">ADD_TO_PARAM_BUFFER</span>(i, __alignof(i));</span><br><span class="line">float4 f4;</span><br><span class="line"><span class="built_in">ADD_TO_PARAM_BUFFER</span>(f4, <span class="number">16</span>); <span class="comment">// float4&#x27;s alignment is 16</span></span><br><span class="line"><span class="type">char</span> c;</span><br><span class="line"><span class="built_in">ADD_TO_PARAM_BUFFER</span>(c, __alignof(c));</span><br><span class="line"><span class="type">float</span> f;</span><br><span class="line"><span class="built_in">ADD_TO_PARAM_BUFFER</span>(f, __alignof(f));</span><br><span class="line">CUdeviceptr devPtr;</span><br><span class="line"><span class="built_in">ADD_TO_PARAM_BUFFER</span>(devPtr, __alignof(devPtr));</span><br><span class="line">float2 f2;</span><br><span class="line"><span class="built_in">ADD_TO_PARAM_BUFFER</span>(f2, <span class="number">8</span>); <span class="comment">// float2&#x27;s alignment is 8</span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span>* extra[] = &#123;</span><br><span class="line">    CU_LAUNCH_PARAM_BUFFER_POINTER, paramBuffer,</span><br><span class="line">    CU_LAUNCH_PARAM_BUFFER_SIZE,    &amp;paramBufferSize,</span><br><span class="line">    CU_LAUNCH_PARAM_END</span><br><span class="line">&#125;;</span><br><span class="line"><span class="built_in">cuLaunchKernel</span>(cuFunction,</span><br><span class="line">               blockWidth, blockHeight, blockDepth,</span><br><span class="line">               gridWidth, gridHeight, gridDepth,</span><br><span class="line">               <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, extra);</span><br></pre></td></tr></table></figure>
<p>结构的对齐要求等于其字段的对齐要求的最大值。 因此，包含内置向量类型 <code>CUdeviceptr</code> 或未对齐的 <code>double</code> 和 <code>long long</code> 的结构的对齐要求可能在设备代码和主机代码之间有所不同。 这种结构也可以用不同的方式填充。 例如，以下结构在主机代码中根本不填充，但在设备代码中填充了字段 <code>f</code>之后的 12 个字节，因为字段 <code>f4</code> 的对齐要求是 16。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="keyword">struct</span> &#123;</span><br><span class="line">    <span class="type">float</span>  f;</span><br><span class="line">    float4 f4;</span><br><span class="line">&#125; myStruct;</span><br></pre></td></tr></table></figure>
<h2 id="L-4-Interoperability-between-Runtime-and-Driver-APIs"><a href="#L-4-Interoperability-between-Runtime-and-Driver-APIs" class="headerlink" title="L.4. Interoperability between Runtime and Driver APIs"></a>L.4. Interoperability between Runtime and Driver APIs</h2><p>应用程序可以将运行时 API 代码与驱动程序 API 代码混合。</p>
<p>如果通过驱动程序 API 创建上下文并使其成为当前上下文，则后续运行时调用将获取此上下文，而不是创建新上下文。</p>
<p>如果运行时已初始化（如 CUDA 运行时中提到的那样），<code>cuCtxGetCurrent()</code> 可用于检索在初始化期间创建的上下文。 后续驱动程序 API 调用可以使用此上下文。</p>
<p>从运行时隐式创建的上下文称为主上下文（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#initialization">初始化</a>）。 它可以通过具有<a href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__PRIMARY__CTX.html">主要上下文管理</a>功能的驱动程序 API 进行管理。</p>
<p>可以使用任一 API 分配和释放设备内存。 <code>CUdeviceptr</code> 可以转换为常规指针，反之亦然：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CUdeviceptr devPtr;</span><br><span class="line"><span class="type">float</span>* d_data;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Allocation using driver API</span></span><br><span class="line"><span class="built_in">cuMemAlloc</span>(&amp;devPtr, size);</span><br><span class="line">d_data = (<span class="type">float</span>*)devPtr;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Allocation using runtime API</span></span><br><span class="line"><span class="built_in">cudaMalloc</span>(&amp;d_data, size);</span><br><span class="line">devPtr = (CUdeviceptr)d_data;</span><br></pre></td></tr></table></figure>
<p>特别是，这意味着使用驱动程序 API 编写的应用程序可以调用使用运行时 API 编写的库（例如 cuFFT、cuBLAS…）。</p>
<p>参考手册的设备和版本管理部分的所有功能都可以互换使用。</p>
<h2 id="L-5-Driver-Entry-Point-Access"><a href="#L-5-Driver-Entry-Point-Access" class="headerlink" title="L.5. Driver Entry Point Access"></a>L.5. Driver Entry Point Access</h2><h3 id="L-5-1-Introduction"><a href="#L-5-1-Introduction" class="headerlink" title="L.5.1. Introduction"></a>L.5.1. Introduction</h3><p><a href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__DRIVER__ENTRY__POINT.html">驱动程序入口点访问</a> API 提供了一种检索 CUDA 驱动程序函数地址的方法。 从 CUDA 11.3 开始，用户可以使用从这些 API 获得的函数指针调用可用的 CUDA 驱动程序 API。</p>
<p>这些 API 提供的功能类似于它们的对应物，POSIX 平台上的 <code>dlsym</code> 和 Windows 上的 <code>GetProcAddress</code>。 提供的 API 将允许用户：</p>
<ul>
<li><p>使用 CUDA 驱动程序 API 检索<a href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__DRIVER__ENTRY__POINT.html">驱动程序</a>函数的地址。</p>
</li>
<li><p>使用 CUDA <a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DRIVER__ENTRY__POINT.html">运行时</a> API 检索驱动程序函数的地址。</p>
</li>
<li><p>请求 CUDA 驱动程序函数的每线程默认流版本。 有关更多详细信息，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#retrieve-per-thread-default-stream-versions">检索每个线程的默认流版本</a></p>
</li>
<li><p>使用较新的驱动程序访问旧工具包上的新 CUDA 功能。</p>
</li>
</ul>
<h3 id="L-5-2-Driver-Function-Typedefs"><a href="#L-5-2-Driver-Function-Typedefs" class="headerlink" title="L.5.2. Driver Function Typedefs"></a>L.5.2. Driver Function Typedefs</h3><p>为了帮助检索 CUDA 驱动程序 API 入口点，CUDA 工具包提供对包含所有 CUDA 驱动程序 API 的函数指针定义的头文件的访问。 这些头文件与 CUDA Toolkit 一起安装，并且在工具包的 <code>include/</code> 目录中可用。 下表总结了包含每个 CUDA API 头文件的 <code>typedef</code> 的头文件。</p>
<div class="tablenoborder"><a name="driver-function-typedefs__driver-api-typedefs-header-files" shape="rect">
                                 <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="driver-function-typedefs__driver-api-typedefs-header-files" class="table" frame="border" border="1" rules="all">
                                 <caption><span class="tablecap">Table 17. Typedefs header files for CUDA driver APIs</span></caption>
                                 <thead class="thead" align="left">
                                    <tr class="row">
                                       <th class="entry" valign="top" width="50%" id="d117e36211" rowspan="1" colspan="1">API header file</th>
                                       <th class="entry" valign="top" width="50%" id="d117e36214" rowspan="1" colspan="1">API Typedef header file</th>
                                    </tr>
                                 </thead>
                                 <tbody class="tbody">
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e36211" rowspan="1" colspan="1"><samp class="ph codeph">cuda.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e36214" rowspan="1" colspan="1"><samp class="ph codeph">cudaTypedefs.h</samp></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e36211" rowspan="1" colspan="1"><samp class="ph codeph">cudaGL.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e36214" rowspan="1" colspan="1"><samp class="ph codeph">cudaGLTypedefs.h</samp></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e36211" rowspan="1" colspan="1"><samp class="ph codeph">cudaProfiler.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e36214" rowspan="1" colspan="1"><samp class="ph codeph">cudaProfilerTypedefs.h</samp></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e36211" rowspan="1" colspan="1"><samp class="ph codeph">cudaVDPAU.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e36214" rowspan="1" colspan="1"><samp class="ph codeph">cudaVDPAUTypedefs.h</samp></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e36211" rowspan="1" colspan="1"><samp class="ph codeph">cudaEGL.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e36214" rowspan="1" colspan="1"><samp class="ph codeph">cudaEGLTypedefs.h</samp></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e36211" rowspan="1" colspan="1"><samp class="ph codeph">cudaD3D9.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e36214" rowspan="1" colspan="1"><samp class="ph codeph">cudaD3D9Typedefs.h</samp></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e36211" rowspan="1" colspan="1"><samp class="ph codeph">cudaD3D10.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e36214" rowspan="1" colspan="1"><samp class="ph codeph">cudaD3D10Typedefs.h</samp></td>
                                    </tr>
                                    <tr class="row">
                                       <td class="entry" valign="top" width="50%" headers="d117e36211" rowspan="1" colspan="1"><samp class="ph codeph">cudaD3D11.h</samp></td>
                                       <td class="entry" valign="top" width="50%" headers="d117e36214" rowspan="1" colspan="1"><samp class="ph codeph">cudaD3D11Typedefs.h</samp></td>
                                    </tr>
                                 </tbody>
                              </table>
                           </div>


<p>上面的头文件本身并没有定义实际的函数指针； 他们为函数指针定义了 <code>typedef</code>。 例如，<code>cudaTypedefs.h</code> 具有驱动 API <code>cuMemAlloc</code> 的以下 <code>typedef</code>：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">typedef</span> <span class="title">CUresult</span> <span class="params">(CUDAAPI *PFN_cuMemAlloc_v3020)</span><span class="params">(CUdeviceptr_v2 *dptr, <span class="type">size_t</span> bytesize)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">typedef</span> <span class="title">CUresult</span> <span class="params">(CUDAAPI *PFN_cuMemAlloc_v2000)</span><span class="params">(CUdeviceptr_v1 *dptr, <span class="type">unsigned</span> <span class="type">int</span> bytesize)</span></span>;</span><br></pre></td></tr></table></figure><br>CUDA 驱动程序符号具有基于版本的命名方案，其名称中带有 <code>_v*</code> 扩展名，但第一个版本除外。 当特定 CUDA 驱动程序 API 的签名或语义发生变化时，我们会增加相应驱动程序符号的版本号。 对于 <code>cuMemAlloc</code> 驱动程序 API，第一个驱动程序符号名称是 <code>cuMemAlloc</code>，下一个符号名称是 <code>cuMemAlloc_v2</code>。 CUDA 2.0 (2000) 中引入的第一个版本的 <code>typedef</code> 是 <code>PFN_cuMemAlloc_v2000</code>。 CUDA 3.2 (3020) 中引入的下一个版本的 <code>typedef</code> 是 <code>PFN_cuMemAlloc_v3020</code>。</p>
<p>typedef 可用于更轻松地在代码中定义适当类型的函数指针：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PFN_cuMemAlloc_v3020 pfn_cuMemAlloc_v2;</span><br><span class="line">PFN_cuMemAlloc_v2000 pfn_cuMemAlloc_v1;</span><br></pre></td></tr></table></figure></p>
<p>如果用户对 API 的特定版本感兴趣，则上述方法更可取。 此外，头文件中包含所有驱动程序符号的最新版本的预定义宏，这些驱动程序符号在安装的 CUDA 工具包发布时可用； 这些 <code>typedef</code> 没有 <code>_v*</code> 后缀。 对于 CUDA 11.3 工具包，<code>cuMemAlloc_v2</code> 是最新版本，所以我们也可以定义它的函数指针如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PFN_cuMemAlloc pfn_cuMemAlloc;</span><br></pre></td></tr></table></figure>
<h3 id="L-5-3-Driver-Function-Retrieval"><a href="#L-5-3-Driver-Function-Retrieval" class="headerlink" title="L.5.3. Driver Function Retrieval"></a>L.5.3. Driver Function Retrieval</h3><p>使用驱动程序入口点访问 API 和适当的 <code>typedef</code>，我们可以获得指向任何 CUDA 驱动程序 API 的函数指针。</p>
<h4 id="L-5-3-1-Using-the-driver-API"><a href="#L-5-3-1-Using-the-driver-API" class="headerlink" title="L.5.3.1. Using the driver API"></a>L.5.3.1. Using the driver API</h4><p>驱动程序 API 需要 CUDA 版本作为参数来获取请求的驱动程序符号的 ABI 兼容版本。 CUDA 驱动程序 API 有一个以 <code>_v*</code> 扩展名表示的按功能 ABI。 例如，考虑 <code>cudaTypedefs.h</code> 中 <code>cuStreamBeginCapture</code> 的版本及其对应的 <code>typedef</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cuda.h</span></span><br><span class="line"><span class="function">CUresult CUDAAPI <span class="title">cuStreamBeginCapture</span><span class="params">(CUstream hStream)</span></span>;</span><br><span class="line"><span class="function">CUresult CUDAAPI <span class="title">cuStreamBeginCapture_v2</span><span class="params">(CUstream hStream, CUstreamCaptureMode mode)</span></span>;</span><br><span class="line">        </span><br><span class="line"><span class="comment">// cudaTypedefs.h</span></span><br><span class="line"><span class="function"><span class="keyword">typedef</span> <span class="title">CUresult</span> <span class="params">(CUDAAPI *PFN_cuStreamBeginCapture_v10000)</span><span class="params">(CUstream hStream)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">typedef</span> <span class="title">CUresult</span> <span class="params">(CUDAAPI *PFN_cuStreamBeginCapture_v10010)</span><span class="params">(CUstream hStream, CUstreamCaptureMode mode)</span></span>;</span><br></pre></td></tr></table></figure>
<p>从上述代码片段中的typedefs，版本后缀<code>_v10000</code>和<code>_v10010</code>表示上述API分别在<code>CUDA 10.0</code>和<code>CUDA 10.1</code>中引入。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cudaTypedefs.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Declare the entry points for cuStreamBeginCapture</span></span><br><span class="line">PFN_cuStreamBeginCapture_v10000 pfn_cuStreamBeginCapture_v1;</span><br><span class="line">PFN_cuStreamBeginCapture_v10010 pfn_cuStreamBeginCapture_v2;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get the function pointer to the cuStreamBeginCapture driver symbol</span></span><br><span class="line"><span class="built_in">cuGetProcAddress</span>(<span class="string">&quot;cuStreamBeginCapture&quot;</span>, &amp;pfn_cuStreamBeginCapture_v1, <span class="number">10000</span>, CU_GET_PROC_ADDRESS_DEFAULT);</span><br><span class="line"><span class="comment">// Get the function pointer to the cuStreamBeginCapture_v2 driver symbol</span></span><br><span class="line"><span class="built_in">cuGetProcAddress</span>(<span class="string">&quot;cuStreamBeginCapture&quot;</span>, &amp;pfn_cuStreamBeginCapture_v2, <span class="number">10010</span>, CU_GET_PROC_ADDRESS_DEFAULT);</span><br></pre></td></tr></table></figure>
<p>参考上面的代码片段，要检索到驱动程序 API <code>cuStreamBeginCapture</code> 的 _v1 版本的地址，CUDA 版本参数应该正好是 <code>10.0 (10000)</code>。同样，用于检索 _v2 版本 API 的地址的 CUDA 版本应该是 <code>10.1 (10010</code>)。为检索特定版本的驱动程序 API 指定更高的 CUDA 版本可能并不总是可移植的。例如，在此处使用 <code>11030</code> 仍会返回 <code>_v2</code> 符号，但如果在 CUDA 11.3 中发布假设的 <code>_v3</code> 版本，则当与 CUDA 11.3 驱动程序配对时，<code>cuGetProcAddress</code> API 将开始返回较新的 _v3 符号。由于 <code>_v2</code>和 <code>_v3</code> 符号的 ABI 和函数签名可能不同，使用用于 <code>_v2</code> 符号的 <code>_v10010 typedef</code> 调用 <code>_v3</code> 函数将表现出未定义的行为。</p>
<p>要检索给定 CUDA 工具包的驱动程序 API 的最新版本，我们还可以指定 <code>CUDA_VERSION</code> 作为版本参数，并使用未版本化的 <code>typedef</code> 来定义函数指针。由于 <code>_v2</code> 是 CUDA 11.3 中驱动程序 API <code>cuStreamBeginCapture</code> 的最新版本，因此下面的代码片段显示了检索它的不同方法。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Assuming we are using CUDA 11.3 Toolkit</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cudaTypedefs.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Declare the entry point</span></span><br><span class="line">PFN_cuStreamBeginCapture pfn_cuStreamBeginCapture_latest;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Intialize the entry point. Specifying CUDA_VERSION will give the function pointer to the</span></span><br><span class="line"><span class="comment">// cuStreamBeginCapture_v2 symbol since it is latest version on CUDA 11.3.</span></span><br><span class="line"><span class="built_in">cuGetProcAddress</span>(<span class="string">&quot;cuStreamBeginCapture&quot;</span>, &amp;pfn_cuStreamBeginCapture_latest, CUDA_VERSION, CU_GET_PROC_ADDRESS_DEFAULT);</span><br><span class="line">  </span><br></pre></td></tr></table></figure><br>请注意，请求具有无效 CUDA 版本的驱动程序 API 将返回错误 <code>CUDA_ERROR_NOT_FOUND。</code> 在上面的代码示例中，传入小于 <code>10000 (CUDA 10.0)</code> 的版本将是无效的。</p>
<h4 id="L-5-3-2-Using-the-runtime-API"><a href="#L-5-3-2-Using-the-runtime-API" class="headerlink" title="L.5.3.2. Using the runtime API"></a>L.5.3.2. Using the runtime API</h4><p>运行时 API 使用 CUDA 运行时版本来获取请求的驱动程序符号的 ABI 兼容版本。 在下面的代码片段中，所需的最低 CUDA 运行时版本将是 CUDA 11.2，因为当时引入了 <code>cuMemAllocAsync</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cudaTypedefs.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Declare the entry point</span></span><br><span class="line">PFN_cuMemAllocAsync pfn_cuMemAllocAsync;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Intialize the entry point. Assuming CUDA runtime version &gt;= 11.2</span></span><br><span class="line"><span class="built_in">cudaGetDriverEntryPoint</span>(<span class="string">&quot;cuMemAllocAsync&quot;</span>, &amp;pfn_cuMemAllocAsync, cudaEnableDefault);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Call the entry point</span></span><br><span class="line"><span class="built_in">pfn_cuMemAllocAsync</span>(...);</span><br></pre></td></tr></table></figure>
<h4 id="L-5-3-3-Retrieve-per-thread-default-stream-versions"><a href="#L-5-3-3-Retrieve-per-thread-default-stream-versions" class="headerlink" title="L.5.3.3. Retrieve per-thread default stream versions"></a>L.5.3.3. Retrieve per-thread default stream versions</h4><p>一些 CUDA 驱动程序 API 可以配置为具有默认流或每线程默认流语义。具有每个线程默认流语义的驱动程序 API 在其名称中以 <code>_ptsz</code> 或 <code>_ptds</code> <code>为后缀。例如，cuLaunchKernel</code> 有一个名为 <code>cuLaunchKernel_ptsz</code> 的每线程默认流变体。使用驱动程序入口点访问 API，用户可以请求驱动程序 API <code>cuLaunchKernel</code> 的每线程默认流版本，而不是默认流版本。为默认流或每线程默认流语义配置 CUDA 驱动程序 API 会影响同步行为。更多详细信息可以在这里找到。</p>
<p>驱动API的默认流或每线程默认流版本可以通过以下方式之一获得：</p>
<ul>
<li>使用编译标志 <code>--default-stream per-thread</code> 或定义宏 <code>CUDA_API_PER_THREAD_DEFAULT_STREAM</code> 以获取每个线程的默认流行为。</li>
<li>分别使用标志 <code>CU_GET_PROC_ADDRESS_LEGACY_STREAM/cudaEnableLegacyStream</code> 或 <code>CU_GET_PROC_ADDRESS_PER_THREAD_DEFAULT_STREAM/cudaEnablePerThreadDefaultStream</code> 强制默认流或每个线程的默认流行为。</li>
</ul>
<h4 id="L-5-3-4-Access-new-CUDA-features"><a href="#L-5-3-4-Access-new-CUDA-features" class="headerlink" title="L.5.3.4. Access new CUDA features"></a>L.5.3.4. Access new CUDA features</h4><p>始终建议安装最新的 CUDA 工具包以访问新的 CUDA 驱动程序功能，但如果出于某种原因，用户不想更新或无法访问最新的工具包，则可以使用 API 来访问新的 CUDA 功能 只有更新的 CUDA 驱动程序。 为了讨论，让我们假设用户使用 CUDA 11.3，并希望使用 CUDA 12.0 驱动程序中提供的新驱动程序 API <code>cuFoo</code>。 下面的代码片段说明了这个用例：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Assuming we have CUDA 12.0 driver installed.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Manually define the prototype as cudaTypedefs.h in CUDA 11.3 does not have the cuFoo typedef</span></span><br><span class="line">    <span class="function"><span class="keyword">typedef</span> <span class="title">CUresult</span> <span class="params">(CUDAAPI *PFN_cuFoo)</span><span class="params">(...)</span></span>;</span><br><span class="line">    PFN_cuFoo pfn_cuFoo = <span class="literal">NULL</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// Get the address for cuFoo API using cuGetProcAddress. Specify CUDA version as</span></span><br><span class="line">    <span class="comment">// 12000 since cuFoo was introduced then or get the driver version dynamically</span></span><br><span class="line">    <span class="comment">// using cuDriverGetVersion </span></span><br><span class="line">    <span class="type">int</span> driverVersion;</span><br><span class="line">    <span class="built_in">cuDriverGetVersion</span>(&amp;driverVersion);</span><br><span class="line">    <span class="built_in">cuGetProcAddress</span>(<span class="string">&quot;cuFoo&quot;</span>, &amp;pfn_cuFoo, driverVersion, CU_GET_PROC_ADDRESS_DEFAULT);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (pfn_cuFoo) &#123;</span><br><span class="line">        <span class="built_in">pfn_cuFoo</span>(...);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Cannot retrieve the address to cuFoo. Check if the latest driver for CUDA 12.0 is installed.\n&quot;</span>);</span><br><span class="line">        <span class="built_in">assert</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// rest of code here</span></span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<div class="tablenoborder"><a name="env-vars__cuda-environment-variables" shape="rect">
                           <!-- --></a><table cellpadding="4" cellspacing="0" summary="" id="env-vars__cuda-environment-variables" class="table" frame="border" border="1" rules="all">
                           <caption><span class="tablecap">Table 18. CUDA Environment Variables</span></caption>
                           <thead class="thead" align="left">
                              <tr class="row">
                                 <th class="entry" valign="top" width="30%" id="d117e36704" rowspan="1" colspan="1">Variable</th>
                                 <th class="entry" valign="top" width="20%" id="d117e36707" rowspan="1" colspan="1">Values</th>
                                 <th class="entry" valign="top" width="50%" id="d117e36710" rowspan="1" colspan="1">Description</th>
                              </tr>
                           </thead>
                           <tbody class="tbody">
                              <tr class="row">
                                 <td class="entry" colspan="3" valign="top" headers="d117e36704 d117e36707 d117e36710" rowspan="1"><strong class="ph b">Device Enumeration and Properties</strong></td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d117e36704" rowspan="1" colspan="1">CUDA_VISIBLE_DEVICES </td>
                                 <td class="entry" valign="top" width="20%" headers="d117e36707" rowspan="1" colspan="1">A comma-separated sequence of GPU identifiers<br clear="none"></br> MIG support:
                                    <samp class="ph codeph">MIG-&lt;GPU-UUID&gt;/&lt;GPU instance ID&gt;/&lt;compute instance ID&gt;</samp></td>
                                 <td class="entry" valign="top" width="50%" headers="d117e36710" rowspan="1" colspan="1">GPU identifiers are given as integer indices or as UUID strings. GPU UUID
                                    strings should follow the same format as given by <dfn class="term">nvidia-smi</dfn>, such as
                                    GPU-8932f937-d72c-4106-c12f-20bd9faed9f6. However, for convenience, abbreviated
                                    forms are allowed; simply specify enough digits from the beginning of the GPU UUID
                                    to uniquely identify that GPU in the target system. For example,
                                    CUDA_VISIBLE_DEVICES=GPU-8932f937 may be a valid way to refer to the above GPU UUID,
                                    assuming no other GPU in the system shares this prefix.<br clear="none"></br> Only the devices whose
                                    index is present in the sequence are visible to CUDA applications and they are
                                    enumerated in the order of the sequence. If one of the indices is invalid, only the
                                    devices whose index precedes the invalid index are visible to CUDA applications. For
                                    example, setting CUDA_VISIBLE_DEVICES to 2,1 causes device 0 to be invisible and
                                    device 2 to be enumerated before device 1. Setting CUDA_VISIBLE_DEVICES to 0,2,-1,1
                                    causes devices 0 and 2 to be visible and device 1 to be invisible.<br clear="none"></br> MIG format
                                    starts with MIG keyword and GPU UUID should follow the same format as given by
                                    <dfn class="term">nvidia-smi</dfn>. For example,
                                    MIG-GPU-8932f937-d72c-4106-c12f-20bd9faed9f6/1/2. Only single MIG instance
                                    enumeration is supported.
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d117e36704" rowspan="1" colspan="1">CUDA_MANAGED_FORCE_DEVICE_ALLOC </td>
                                 <td class="entry" valign="top" width="20%" headers="d117e36707" rowspan="1" colspan="1">0 or 1 (default is 0) </td>
                                 <td class="entry" valign="top" width="50%" headers="d117e36710" rowspan="1" colspan="1">Forces the driver to place all managed allocations in device memory.</td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d117e36704" rowspan="1" colspan="1">CUDA_DEVICE_ORDER </td>
                                 <td class="entry" valign="top" width="20%" headers="d117e36707" rowspan="1" colspan="1">FASTEST_FIRST, PCI_BUS_ID, (default is FASTEST_FIRST) </td>
                                 <td class="entry" valign="top" width="50%" headers="d117e36710" rowspan="1" colspan="1">FASTEST_FIRST causes CUDA to enumerate the available devices in fastest to
                                    slowest order using a simple heuristic. PCI_BUS_ID orders devices by PCI bus ID in
                                    ascending order.
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" colspan="3" valign="top" headers="d117e36704 d117e36707 d117e36710" rowspan="1"><strong class="ph b">Compilation</strong></td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d117e36704" rowspan="1" colspan="1">CUDA_CACHE_DISABLE </td>
                                 <td class="entry" valign="top" width="20%" headers="d117e36707" rowspan="1" colspan="1">0 or 1 (default is 0)</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e36710" rowspan="1" colspan="1">Disables caching (when set to 1) or enables caching (when set to 0) for
                                    just-in-time-compilation. When disabled, no binary code is added to or retrieved
                                    from the cache.
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d117e36704" rowspan="1" colspan="1">CUDA_CACHE_PATH </td>
                                 <td class="entry" valign="top" width="20%" headers="d117e36707" rowspan="1" colspan="1">filepath </td>
                                 <td class="entry" valign="top" width="50%" headers="d117e36710" rowspan="1" colspan="1">Specifies the folder where the just-in-time compiler caches binary codes; the
                                    default values are: 
                                    <ul class="ul">
                                       <li class="li">on Windows, <samp class="ph codeph">%APPDATA%\NVIDIA\ComputeCache</samp></li>
                                       <li class="li">on Linux, <samp class="ph codeph">~/.nv/ComputeCache</samp></li>
                                    </ul>
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d117e36704" rowspan="1" colspan="1">CUDA_CACHE_MAXSIZE</td>
                                 <td class="entry" valign="top" width="20%" headers="d117e36707" rowspan="1" colspan="1">integer (default is 268435456 (256 MiB) and maximum is 4294967296 (4
                                    GiB))
                                 </td>
                                 <td class="entry" valign="top" width="50%" headers="d117e36710" rowspan="1" colspan="1">Specifies the size in bytes of the cache used by the just-in-time compiler.
                                    Binary codes whose size exceeds the cache size are not cached. Older binary codes
                                    are evicted from the cache to make room for newer binary codes if needed. 
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d117e36704" rowspan="1" colspan="1">CUDA_FORCE_PTX_JIT </td>
                                 <td class="entry" valign="top" width="20%" headers="d117e36707" rowspan="1" colspan="1">0 or 1 (default is 0)</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e36710" rowspan="1" colspan="1">When set to 1, forces the device driver to ignore any binary code embedded in
                                    an application (see <a class="xref" href="index.html#application-compatibility" shape="rect">Application Compatibility</a>) and to just-in-time
                                    compile embedded <dfn class="term">PTX</dfn> code instead. If a kernel does not have embedded
                                    <dfn class="term">PTX</dfn> code, it will fail to load. This environment variable can be used
                                    to validate that <dfn class="term">PTX</dfn> code is embedded in an application and that its
                                    just-in-time compilation works as expected to guarantee application forward
                                    compatibility with future architectures (see <a class="xref" href="index.html#just-in-time-compilation" shape="rect">Just-in-Time Compilation</a>).
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d117e36704" rowspan="1" colspan="1">CUDA_DISABLE_PTX_JIT </td>
                                 <td class="entry" valign="top" width="20%" headers="d117e36707" rowspan="1" colspan="1">0 or 1 (default is 0)</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e36710" rowspan="1" colspan="1">When set to 1, disables the just-in-time compilation of embedded
                                    <dfn class="term">PTX</dfn> code and use the compatible binary code embedded in an
                                    application (see <a class="xref" href="index.html#application-compatibility" shape="rect">Application Compatibility</a>). If a kernel does not have embedded binary code or the embedded binary was
                                    compiled for an incompatible architecture, then it will fail to load. This
                                    environment variable can be used to validate that an application has the compatible
                                    <dfn class="term">SASS</dfn> code generated for each kernel.(see <a class="xref" href="index.html#binary-compatibility" shape="rect">Binary Compatibility</a>).
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" colspan="3" valign="top" headers="d117e36704 d117e36707 d117e36710" rowspan="1"><strong class="ph b">Execution</strong></td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d117e36704" rowspan="1" colspan="1">CUDA_LAUNCH_BLOCKING </td>
                                 <td class="entry" valign="top" width="20%" headers="d117e36707" rowspan="1" colspan="1">0 or 1 (default is 0)</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e36710" rowspan="1" colspan="1">Disables (when set to 1) or enables (when set to 0) asynchronous kernel
                                    launches. 
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d117e36704" rowspan="1" colspan="1">CUDA_DEVICE_MAX_CONNECTIONS </td>
                                 <td class="entry" valign="top" width="20%" headers="d117e36707" rowspan="1" colspan="1">1 to 32 (default is 8)</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e36710" rowspan="1" colspan="1">Sets the number of compute and copy engine concurrent connections (work queues)
                                    from the host to each device of compute capability 3.5 and above. 
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d117e36704" rowspan="1" colspan="1">CUDA_AUTO_BOOST </td>
                                 <td class="entry" valign="top" width="20%" headers="d117e36707" rowspan="1" colspan="1">0 or 1</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e36710" rowspan="1" colspan="1">Overrides the autoboost behavior set by the --auto-boost-default option of
                                    nvidia-smi. If an application requests via this environment variable a behavior that
                                    is different from nvidia-smi's, its request is honored if there is no other
                                    application currently running on the same GPU that successfully requested a
                                    different behavior, otherwise it is ignored. 
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" colspan="3" valign="top" headers="d117e36704 d117e36707 d117e36710" rowspan="1"><strong class="ph b">cuda-gdb (on Linux platform)</strong></td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d117e36704" rowspan="1" colspan="1">CUDA_DEVICE_WAITS_ON_EXCEPTION</td>
                                 <td class="entry" valign="top" width="20%" headers="d117e36707" rowspan="1" colspan="1">0 or 1 (default is 0)</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e36710" rowspan="1" colspan="1">When set to 1, a CUDA application will halt when a device exception occurs,
                                    allowing a debugger to be attached for further debugging.
                                 </td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" colspan="3" valign="top" headers="d117e36704 d117e36707 d117e36710" rowspan="1"><strong class="ph b">MPS service (on Linux platform)</strong></td>
                              </tr>
                              <tr class="row">
                                 <td class="entry" valign="top" width="30%" headers="d117e36704" rowspan="1" colspan="1">CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT</td>
                                 <td class="entry" valign="top" width="20%" headers="d117e36707" rowspan="1" colspan="1">Percentage value (between 0 - 100, default is 0)</td>
                                 <td class="entry" valign="top" width="50%" headers="d117e36710" rowspan="1" colspan="1">Devices of compute capability 8.x allow, a portion of L2 cache to be set-aside
                                    for persisting data accesses to global memory. When using CUDA MPS service, the
                                    set-aside size can only be controlled using this environment variable, before
                                    starting CUDA MPS control daemon. I.e., the environment variable should be set
                                    before running the command <samp class="ph codeph">nvidia-cuda-mps-control -d</samp>.
                                 </td>
                              </tr>
                           </tbody>
                        </table>
                     </div>


<h1 id="附录N-CUDA的统一内存"><a href="#附录N-CUDA的统一内存" class="headerlink" title="附录N CUDA的统一内存"></a>附录N CUDA的统一内存</h1><h2 id="N-1-Unified-Memory-Introduction"><a href="#N-1-Unified-Memory-Introduction" class="headerlink" title="N.1. Unified Memory Introduction"></a>N.1. Unified Memory Introduction</h2><p>统一内存是 CUDA 编程模型的一个组件，在 CUDA 6.0 中首次引入，它定义了一个托管内存空间，在该空间中所有处理器都可以看到具有公共地址空间的单个连贯内存映像。</p>
<p><strong>注意：处理器是指任何具有专用 MMU 的独立执行单元。这包括任何类型和架构的 CPU 和 GPU。</strong></p>
<p>底层系统管理 CUDA 程序中的数据访问和位置，无需显式内存复制调用。这在两个主要方面有利于 GPU 编程：</p>
<ul>
<li>通过统一系统中所有 GPU 和 CPU 的内存空间以及为 CUDA 程序员提供更紧密、更直接的语言集成，可以简化 GPU 编程。</li>
<li>通过透明地将数据迁移到使用它的处理器，可以最大限度地提高数据访问速度。</li>
</ul>
<p>简单来说，统一内存消除了通过 <code>cudaMemcpy*()</code> 例程进行显式数据移动的需要，而<strong>不会因将所有数据放入零拷贝内存而导致性能损失</strong>。当然，数据移动仍然会发生，因此程序的运行时间通常不会减少；相反，统一内存可以编写更简单、更易于维护的代码。</p>
<p>统一内存提供了一个“<code>单指针数据</code>”模型，在概念上类似于 CUDA 的零拷贝内存。两者之间的一个关键区别在于，在零拷贝分配中，内存的物理位置固定在 CPU 系统内存中，因此程序可以快速或慢速地访问它，具体取决于访问它的位置。另一方面，<code>统一内存将内存和执行空间解耦</code>，以便所有数据访问都很快。</p>
<p>统一内存一词描述了一个为各种程序提供内存管理服务的系统，从针对运行时 API 的程序到使用虚拟 ISA (PTX) 的程序。该系统的一部分定义了选择加入统一内存服务的托管内存空间。</p>
<p>托管内存可与特定于设备的分配互操作和互换，例如使用 <code>cudaMalloc()</code> 例程创建的分配。所有在设备内存上有效的 CUDA 操作在托管内存上也有效；主要区别在于程序的主机部分也能够引用和访问内存。</p>
<p><strong>注意：连接到 Tegra 的离散 GPU 不支持统一内存</strong>。</p>
<h3 id="N-1-1-System-Requirements"><a href="#N-1-1-System-Requirements" class="headerlink" title="N.1.1. System Requirements"></a>N.1.1. System Requirements</h3><p>统一内存有两个基本要求：</p>
<ul>
<li>具有 SM 架构 3.0 或更高版本（Kepler 类或更高版本）的 GPU</li>
<li>64 位主机应用程序和非嵌入式操作系统（Linux 或 Windows）<br>  具有 SM 架构 6.x 或更高版本（Pascal 类或更高版本）的 GPU 提供额外的统一内存功能，例如本文档中概述的按需页面迁移和 GPU 内存超额订阅。 请注意，目前这些功能仅在 Linux 操作系统上受支持。 在 Windows 上运行的应用程序（无论是 TCC 还是 WDDM 模式）将使用基本的统一内存模型，就像在 6.x 之前的架构上一样，即使它们在具有 6.x 或更高计算能力的硬件上运行也是如此。 有关详细信息，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-data-migration">数据迁移和一致性</a>。</li>
</ul>
<h3 id="N-1-2-Simplifying-GPU-Programming"><a href="#N-1-2-Simplifying-GPU-Programming" class="headerlink" title="N.1.2. Simplifying GPU Programming"></a>N.1.2. Simplifying GPU Programming</h3><p>内存空间的统一意味着主机和设备之间不再需要显式内存传输。在托管内存空间中创建的任何分配都会自动迁移到需要的位置。</p>
<p>程序通过以下两种方式之一分配托管内存： 通过 <code>cudaMallocManaged()</code> 例程，它在语义上类似于 <code>cudaMalloc()</code>；或者通过定义一个全局 <code>__managed__</code> 变量，它在语义上类似于一个 <code>__device__</code> 变量。在本文档的后面部分可以找到这些的精确定义。<br>注意：在具有计算能力 6.x 及更高版本的设备的支持平台上，统一内存将使应用程序能够使用默认系统分配器分配和共享数据。这允许 GPU 在不使用特殊分配器的情况下访问整个系统虚拟内存。有关更多详细信息，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-system-allocator">系统分配器</a>。<br>以下代码示例说明了托管内存的使用如何改变主机代码的编写方式。首先，一个没有使用统一内存的简单程序：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">AplusB</span><span class="params">(<span class="type">int</span> *ret, <span class="type">int</span> a, <span class="type">int</span> b)</span> </span>&#123;</span><br><span class="line">    ret[threadIdx.x] = a + b + threadIdx.x;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> *ret;</span><br><span class="line">    <span class="built_in">cudaMalloc</span>(&amp;ret, <span class="number">1000</span> * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line">    AplusB&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1000</span> &gt;&gt;&gt;(ret, <span class="number">10</span>, <span class="number">100</span>);</span><br><span class="line">    <span class="type">int</span> *host_ret = (<span class="type">int</span> *)<span class="built_in">malloc</span>(<span class="number">1000</span> * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(host_ret, ret, <span class="number">1000</span> * <span class="built_in">sizeof</span>(<span class="type">int</span>), cudaMemcpyDefault);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d: A+B = %d\n&quot;</span>, i, host_ret[i]); </span><br><span class="line">    <span class="built_in">free</span>(host_ret);</span><br><span class="line">    <span class="built_in">cudaFree</span>(ret); </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第一个示例在 GPU 上将两个数字与每个线程 ID 组合在一起，并以数组形式返回值。 如果没有托管内存，则返回值的主机端和设备端存储都是必需的（示例中为 <code>host_ret</code> 和 <code>ret</code>），使用 <code>cudaMemcpy()</code> 在两者之间显式复制也是如此。</p>
<p>将此与程序的统一内存版本进行比较，后者允许从主机直接访问 GPU 数据。 请注意 <code>cudaMallocManaged()</code> 例程，它从主机和设备代码返回一个有效的指针。 这允许在没有单独的 <code>host_ret</code> 副本的情况下使用 <code>ret</code>，大大简化并减小了程序的大小。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">AplusB</span><span class="params">(<span class="type">int</span> *ret, <span class="type">int</span> a, <span class="type">int</span> b)</span> </span>&#123;</span><br><span class="line">    ret[threadIdx.x] = a + b + threadIdx.x;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> *ret;</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>(&amp;ret, <span class="number">1000</span> * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line">    AplusB&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1000</span> &gt;&gt;&gt;(ret, <span class="number">10</span>, <span class="number">100</span>);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d: A+B = %d\n&quot;</span>, i, ret[i]);</span><br><span class="line">    <span class="built_in">cudaFree</span>(ret); </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后，语言集成允许直接引用 GPU 声明的 <code>__managed__</code> 变量，并在使用全局变量时进一步简化程序。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__device__ __managed__ <span class="type">int</span> ret[<span class="number">1000</span>];</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">AplusB</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span> </span>&#123;</span><br><span class="line">    ret[threadIdx.x] = a + b + threadIdx.x;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    AplusB&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1000</span> &gt;&gt;&gt;(<span class="number">10</span>, <span class="number">100</span>);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d: A+B = %d\n&quot;</span>, i, ret[i]);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>请注意没有明确的 <code>cudaMemcpy()</code> 命令以及返回数组 ret 在 CPU 和 GPU 上都可见的事实。</p>
<p>值得一提的是主机和设备之间的同步。 请注意在非托管示例中，同步 <code>cudaMemcpy()</code> 例程如何用于同步内核（即等待它完成运行）以及将数据传输到主机。 统一内存示例不调用 <code>cudaMemcpy()</code>，因此需要显式 <code>cudaDeviceSynchronize()</code>，然后主机程序才能安全地使用 GPU 的输出。</p>
<h3 id="N-1-3-Data-Migration-and-Coherency"><a href="#N-1-3-Data-Migration-and-Coherency" class="headerlink" title="N.1.3. Data Migration and Coherency"></a>N.1.3. Data Migration and Coherency</h3><p>统一内存尝试通过将数据迁移到正在访问它的设备来优化内存性能（也就是说，如果 CPU 正在访问数据，则将数据移动到主机内存，如果 GPU 将访问它，则将数据移动到设备内存）。数据迁移是统一内存的基础，但对程序是透明的。系统将尝试将数据放置在可以最有效地访问而不违反一致性的位置。</p>
<p>数据的物理位置对程序是不可见的，并且可以随时更改，但对数据的虚拟地址的访问将保持有效并且可以从任何处理器保持一致，无论位置如何。请注意，保持一致性是首要要求，高于性能；在主机操作系统的限制下，系统被允许访问失败或移动数据，以保持处理器之间的全局一致性。</p>
<p>计算能力低于 6.x 的 GPU 架构不支持按需将托管数据细粒度移动到 GPU。每当启动 GPU 内核时，通常必须将所有托管内存转移到 GPU 内存，以避免内存访问出错。计算能力 6.x 引入了一种新的 GPU 页面错误机制，可提供更无缝的统一内存功能。结合系统范围的虚拟地址空间，页面错误提供了几个好处。首先，页面错误意味着 CUDA 系统软件不需要在每次内核启动之前将<strong>所有</strong>托管内存分配同步到 GPU。如果在 GPU 上运行的内核访问了一个不在其内存中的页面，它就会出错，从而允许该页面按需自动迁移到 GPU 内存。或者，可以将页面映射到 GPU 地址空间，以便通过 PCIe 或 NVLink 互连进行访问（访问映射有时可能比迁移更快）。<strong>请注意，统一内存是系统范围的：GPU（和 CPU）可以从 CPU 内存或系统中其他 GPU 的内存中发生故障并迁移内存页面。</strong></p>
<h3 id="N-1-4-GPU-Memory-Oversubscription"><a href="#N-1-4-GPU-Memory-Oversubscription" class="headerlink" title="N.1.4. GPU Memory Oversubscription"></a>N.1.4. GPU Memory Oversubscription</h3><p>计算能力低于 6.x 的设备分配的托管内存不能超过 GPU 内存的物理大小。</p>
<p>计算能力 6.x 的设备扩展了寻址模式以支持 49 位虚拟寻址。 这足以覆盖现代 CPU 的 48 位虚拟地址空间，以及 GPU 自己的内存。 大的虚拟地址空间和页面错误能力使应用程序可以访问整个系统的虚拟内存，而不受任何一个处理器的物理内存大小的限制。 这意味着应用程序可以超额订阅内存系统：换句话说，它们可以分配、访问和共享大于系统总物理容量的数组，从而实现超大数据集的核外处理。 只要有足够的系统内存可用于分配，<code>cudaMallocManaged</code> 就不会耗尽内存。</p>
<h3 id="N-1-5-Multi-GPU"><a href="#N-1-5-Multi-GPU" class="headerlink" title="N.1.5. Multi-GPU"></a>N.1.5. Multi-GPU</h3><p>对于计算能力低于 6.x 的设备，托管内存分配的行为与使用 <code>cudaMalloc()</code> 分配的非托管内存相同：当前活动设备是物理分配的主站，所有其他 GPU 接收到内存的对等映射。这意味着系统中的其他 GPU 将以较低的带宽通过 PCIe 总线访问内存。请注意，如果系统中的 GPU 之间不支持对等映射，则托管内存页面将放置在 CPU 系统内存（“零拷贝”内存）中，并且所有 GPU 都会遇到 PCIe 带宽限制。有关详细信息，请参阅 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-managed-memory">6.x 之前架构上的多 GPU 程序的托管内存</a>。</p>
<p>具有计算能力 6.x 设备的系统上的托管分配对所有 GPU 都是可见的，并且可以按需迁移到任何处理器。统一内存性能提示（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-performance-tuning">性能调优</a>）允许开发人员探索自定义使用模式，例如跨 GPU 读取重复数据和直接访问对等 GPU 内存而无需迁移。</p>
<h3 id="N-1-6-System-Allocator"><a href="#N-1-6-System-Allocator" class="headerlink" title="N.1.6. System Allocator"></a>N.1.6. System Allocator</h3><p>计算能力 7.0 的设备支持 NVLink 上的地址转换服务 (ATS)。 如果主机 CPU 和操作系统支持，ATS 允许 GPU 直接访问 CPU 的页表。 GPU MMU 中的未命中将导致向 CPU 发送地址转换请求 (ATR)。 CPU 在其页表中查找该地址的虚拟到物理映射并将转换提供回 GPU。 ATS 提供 GPU 对系统内存的完全访问权限，例如使用 <code>malloc</code> 分配的内存、在堆栈上分配的内存、全局变量和文件支持的内存。 应用程序可以通过检查新的 <code>pageableMemoryAccessUsesHostPageTables</code> 属性来查询设备是否支持通过 ATS 一致地访问可分页内存。</p>
<p>这是一个适用于任何满足统一内存基本要求的系统的示例代码（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-requirements">系统要求</a>）：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> *data;</span><br><span class="line"><span class="built_in">cudaMallocManaged</span>(&amp;data, <span class="built_in">sizeof</span>(<span class="type">int</span>) * n);</span><br><span class="line">kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(data);</span><br></pre></td></tr></table></figure>
<p>具有 pageableMemoryAccess 属性的系统支持这些新的访问模式：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> *data = (<span class="type">int</span>*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(<span class="type">int</span>) * n);</span><br><span class="line">kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(data);</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> data[<span class="number">1024</span>];</span><br><span class="line">kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(data);</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> <span class="type">int</span> *data;</span><br><span class="line">kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(data);</span><br></pre></td></tr></table></figure>
<p>在上面的示例中，数据可以由第三方 CPU 库初始化，然后由 GPU 内核直接访问。 在具有 <code>pageableMemoryAccess</code> 的系统上，用户还可以使用 <code>cudaMemPrefetchAsync</code> 将可分页内存预取到 GPU。 这可以通过优化数据局部性产生性能优势。</p>
<p><strong>注意：目前仅 IBM Power9 系统支持基于 NVLink 的 ATS</strong>。</p>
<h3 id="N-1-7-Hardware-Coherency"><a href="#N-1-7-Hardware-Coherency" class="headerlink" title="N.1.7. Hardware Coherency"></a>N.1.7. Hardware Coherency</h3><p>第二代 NVLink 允许从 CPU 直接加载/存储/原子访问每个 GPU 的内存。结合新的 CPU 主控功能，NVLink 支持一致性操作，允许从 GPU 内存读取的数据存储在 CPU 的缓存层次结构中。从 CPU 缓存访问的较低延迟是 CPU 性能的关键。计算能力 6.x 的设备仅支持对等 GPU 原子。计算能力 7.x 的设备可以通过 NVLink 发送 GPU 原子并在目标 CPU 上完成它们，因此第二代 NVLink 增加了对由 GPU 或 CPU 发起的原子的支持。</p>
<p>请注意，CPU 无法访问 <code>cudaMalloc</code> 分配。因此，要利用硬件一致性，用户必须使用统一内存分配器，例如 <code>cudaMallocManaged</code> 或支持 ATS 的系统分配器（请参阅系统分配器）。新属性 <code>directManagedMemAccessFromHost</code> 指示主机是否可以直接访问设备上的托管内存而无需迁移。默认情况下，驻留在 GPU 内存中的 <code>cudaMallocManaged</code> 分配的任何 CPU 访问都会触发页面错误和数据迁移。应用程序可以使用带有 <code>cudaCpuDeviceId</code> 的 <code>cudaMemAdviseSetAccessedBy</code> 性能提示来启用对受支持系统上 GPU 内存的直接访问。</p>
<p>考虑下面的示例代码：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">write</span><span class="params">(<span class="type">int</span> *ret, <span class="type">int</span> a, <span class="type">int</span> b)</span> </span>&#123;</span><br><span class="line">    ret[threadIdx.x] = a + b + threadIdx.x;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">append</span><span class="params">(<span class="type">int</span> *ret, <span class="type">int</span> a, <span class="type">int</span> b)</span> </span>&#123;</span><br><span class="line">    ret[threadIdx.x] += a + b + threadIdx.x;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> *ret;</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>(&amp;ret, <span class="number">1000</span> * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line">    <span class="built_in">cudaMemAdvise</span>(ret, <span class="number">1000</span> * <span class="built_in">sizeof</span>(<span class="type">int</span>), cudaMemAdviseSetAccessedBy, cudaCpuDeviceId);  <span class="comment">// set direct access hint</span></span><br><span class="line"></span><br><span class="line">    write&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1000</span> &gt;&gt;&gt;(ret, <span class="number">10</span>, <span class="number">100</span>);            <span class="comment">// pages populated in GPU memory</span></span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d: A+B = %d\n&quot;</span>, i, ret[i]);        <span class="comment">// directManagedMemAccessFromHost=1: CPU accesses GPU memory directly without migrations</span></span><br><span class="line">                                                    <span class="comment">// directManagedMemAccessFromHost=0: CPU faults and triggers device-to-host migrations</span></span><br><span class="line">    append&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1000</span> &gt;&gt;&gt;(ret, <span class="number">10</span>, <span class="number">100</span>);            <span class="comment">// directManagedMemAccessFromHost=1: GPU accesses GPU memory without migrations</span></span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();                        <span class="comment">// directManagedMemAccessFromHost=0: GPU faults and triggers host-to-device migrations</span></span><br><span class="line">    <span class="built_in">cudaFree</span>(ret); </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>写内核完成后，会在GPU内存中创建并初始化<code>ret</code>。 接下来，CPU 将访问 <code>ret</code>，然后再次使用相同的 <code>ret</code>内存追加内核。 此代码将根据系统架构和硬件一致性支持显示不同的行为：</p>
<ul>
<li>在 <code>directManagedMemAccessFromHost=1</code> 的系统上：CPU 访问托管缓冲区不会触发任何迁移； 数据将保留在 GPU 内存中，任何后续的 GPU 内核都可以继续直接访问它，而不会造成故障或迁移。</li>
<li>在 <code>directManagedMemAccessFromHost=0</code> 的系统上：CPU 访问托管缓冲区将出现页面错误并启动数据迁移； 任何第一次尝试访问相同数据的 GPU 内核都会出现页面错误并将页面迁移回 GPU 内存。</li>
</ul>
<h3 id="N-1-8-Access-Counters"><a href="#N-1-8-Access-Counters" class="headerlink" title="N.1.8. Access Counters"></a>N.1.8. Access Counters</h3><p>计算能力 7.0 的设备引入了一个新的访问计数器功能，该功能可以跟踪 GPU 对位于其他处理器上的内存进行的访问频率。 访问计数器有助于确保将内存页面移动到最频繁访问页面的处理器的物理内存中。 访问计数器功能可以指导 CPU 和 GPU 之间以及对等 GPU 之间的迁移。</p>
<p>对于 <code>cudaMallocManaged</code>，访问计数器迁移可以通过使用带有相应设备 ID 的 <code>cudaMemAdviseSetAccessedBy</code> 提示来选择加入。 驱动程序还可以使用访问计数器来实现更有效的抖动缓解或内存超额订阅方案。</p>
<p><strong>注意：访问计数器当前仅在 IBM Power9 系统上启用，并且仅用于 cudaMallocManaged 分配器</strong>。</p>
<h2 id="N-2-Programming-Model"><a href="#N-2-Programming-Model" class="headerlink" title="N.2. Programming Model"></a>N.2. Programming Model</h2><h3 id="N-2-1-Managed-Memory-Opt-In"><a href="#N-2-1-Managed-Memory-Opt-In" class="headerlink" title="N.2.1. Managed Memory Opt In"></a>N.2.1. Managed Memory Opt In</h3><p>大多数平台要求程序通过使用 <code>__managed__</code> 关键字注释 <code>__device__</code> 变量（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-language-integration">语言集成部分</a>）或使用新的 <code>cudaMallocManaged()</code> 调用来分配数据来选择自动数据管理。</p>
<p>计算能力低于 6.x 的设备必须始终在堆上分配托管内存，无论是使用分配器还是通过声明全局存储。 无法将先前分配的内存与统一内存相关联，也无法让统一内存系统管理 CPU 或 GPU 堆栈指针。</p>
<p>从 CUDA 8.0 和具有计算能力 6.x 设备的支持系统开始，可以使用相同的指针从 GPU 代码和 CPU 代码访问使用默认 OS 分配器（例如 <code>malloc</code> 或 <code>new</code>）分配的内存。 在这些系统上，统一内存是默认设置：无需使用特殊分配器或创建专门管理的内存池。</p>
<h4 id="N-2-1-1-Explicit-Allocation-Using-cudaMallocManaged"><a href="#N-2-1-1-Explicit-Allocation-Using-cudaMallocManaged" class="headerlink" title="N.2.1.1. Explicit Allocation Using cudaMallocManaged()"></a>N.2.1.1. Explicit Allocation Using cudaMallocManaged()</h4><p>统一内存最常使用在语义和语法上类似于标准 CUDA 分配器 cudaMalloc() 的分配函数创建。 功能说明如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMallocManaged</span><span class="params">(<span class="type">void</span> **devPtr,</span></span></span><br><span class="line"><span class="params"><span class="function">                              <span class="type">size_t</span> size,</span></span></span><br><span class="line"><span class="params"><span class="function">                              <span class="type">unsigned</span> <span class="type">int</span> flags=<span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure>
<p><code>cudaMallocManaged()</code> 函数保留托管内存的 <code>size</code> 字节，并在 <code>devPtr</code> 中返回一个指针。 请注意各种 GPU 架构之间 <code>cudaMallocManaged()</code> 行为的差异。 默认情况下，计算能力低于 6.x 的设备直接在 GPU 上分配托管内存。 但是，计算能力 6.x 及更高版本的设备在调用 <code>cudaMallocManaged()</code> 时不会分配物理内存：在这种情况下，物理内存会在第一次触摸时填充，并且可能驻留在 CPU 或 GPU 上。 托管指针在系统中的所有 GPU 和 CPU 上都有效，尽管程序访问此指针必须遵守统一内存编程模型的并发规则（请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-coherency-hd">一致性和并发性</a>）。 下面是一个简单的例子，展示了 <code>cudaMallocManaged()</code> 的使用：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">printme</span><span class="params">(<span class="type">char</span> *str)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(str);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Allocate 100 bytes of memory, accessible to both Host and Device code</span></span><br><span class="line">    <span class="type">char</span> *s;</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>(&amp;s, <span class="number">100</span>);</span><br><span class="line">    <span class="comment">// Note direct Host-code use of &quot;s&quot;</span></span><br><span class="line">    <span class="built_in">strncpy</span>(s, <span class="string">&quot;Hello Unified Memory\n&quot;</span>, <span class="number">99</span>);</span><br><span class="line">    <span class="comment">// Here we pass &quot;s&quot; to a kernel without explicitly copying</span></span><br><span class="line">    printme&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1</span> &gt;&gt;&gt;(s);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="comment">// Free as for normal CUDA allocations</span></span><br><span class="line">    <span class="built_in">cudaFree</span>(s); </span><br><span class="line">    <span class="keyword">return</span>  <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当 <code>cudaMalloc()</code> 被 <code>cudaMallocManaged()</code> 替换时，程序的行为在功能上没有改变； 但是，该程序应该继续消除显式内存拷贝并利用自动迁移。 此外，可以消除双指针（一个指向主机，一个指向设备存储器）。</p>
<p>设备代码无法调用 <code>cudaMallocManaged()</code>。 所有托管内存必须从主机或全局范围内分配（请参阅下一节）。 在内核中使用 <code>malloc()</code> 在设备堆上的分配不会在托管内存空间中创建，因此 CPU 代码将无法访问。</p>
<h4 id="N-2-1-2-Global-Scope-Managed-Variables-Using-managed"><a href="#N-2-1-2-Global-Scope-Managed-Variables-Using-managed" class="headerlink" title="N.2.1.2. Global-Scope Managed Variables Using managed"></a>N.2.1.2. Global-Scope Managed Variables Using <strong>managed</strong></h4><p>文件范围和全局范围的 CUDA <code>__device__</code> 变量也可以通过在声明中添加新的 <code>__managed__</code> 注释来选择加入统一内存管理。 然后可以直接从主机或设备代码中引用它们，如下所示：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">__device__ __managed__ <span class="type">int</span> x[<span class="number">2</span>];</span><br><span class="line">__device__ __managed__ <span class="type">int</span> y;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    x[<span class="number">1</span>] = x[<span class="number">0</span>] + y;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    x[<span class="number">0</span>] = <span class="number">3</span>;</span><br><span class="line">    y = <span class="number">5</span>;</span><br><span class="line">    kernel&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1</span> &gt;&gt;&gt;();</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;result = %d\n&quot;</span>, x[<span class="number">1</span>]); </span><br><span class="line">    <span class="keyword">return</span>  <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>原始 <code>__device__</code> 内存空间的所有语义，以及一些额外的统一内存特定约束，都由托管变量继承（请参阅使用 NVCC 编译）。</p>
<p>请注意，标记为 <code>__constant__</code> 的变量可能不会也标记为 <code>__managed__</code>； 此注释仅用于 <code>__device__</code> 变量。 常量内存必须在编译时静态设置，或者在 CUDA 中像往常一样使用 <code>cudaMemcpyToSymbol()</code> 设置。</p>
<h3 id="N-2-2-Coherency-and-Concurrency"><a href="#N-2-2-Coherency-and-Concurrency" class="headerlink" title="N.2.2. Coherency and Concurrency"></a>N.2.2. Coherency and Concurrency</h3><p>在计算能力低于 6.x 的设备上同时访问托管内存是不可能的，因为如果 CPU 在 GPU 内核处于活动状态时访问统一内存分配，则无法保证一致性。 但是，支持操作系统的计算能力 6.x 的设备允许 CPU 和 GPU 通过新的页面错误机制同时访问统一内存分配。 程序可以通过检查新的 <code>concurrentManagedAccess</code> 属性来查询设备是否支持对托管内存的并发访问。 请注意，与任何并行应用程序一样，开发人员需要确保正确同步以避免处理器之间的数据危险。</p>
<h4 id="N-2-2-1-GPU-Exclusive-Access-To-Managed-Memory"><a href="#N-2-2-1-GPU-Exclusive-Access-To-Managed-Memory" class="headerlink" title="N.2.2.1. GPU Exclusive Access To Managed Memory"></a>N.2.2.1. GPU Exclusive Access To Managed Memory</h4><p>为了确保 6.x 之前的 GPU 架构的一致性，统一内存编程模型在 CPU 和 GPU 同时执行时对数据访问施加了限制。实际上，GPU 在执行任何内核操作时对所有托管数据具有独占访问权，无论特定内核是否正在积极使用数据。当托管数据与 <code>cudaMemcpy*()</code> 或 <code>cudaMemset*()</code>一起使用时，系统可能会选择从主机或设备访问源或目标，这将限制并发 CPU 访问该数据，而 <code>cudaMemcpy*()</code>或 <code>cudaMemset*()</code> 正在执行。有关更多详细信息，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-memcpy-memset">使用托管内存的 <code>Memcpy()/Memset()</code> 行为</a>。</p>
<p>不允许 CPU 访问任何托管分配或变量，而 GPU 对 <code>concurrentManagedAccess</code> 属性设置为 0 的设备处于活动状态。在这些系统上，并发 CPU/GPU 访问，即使是不同的托管内存分配，也会导致分段错误，因为该页面被认为是 CPU 无法访问的。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__device__ __managed__ <span class="type">int</span> x, y=<span class="number">2</span>;</span><br><span class="line"><span class="function">__global__  <span class="type">void</span>  <span class="title">kernel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    x = <span class="number">10</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    kernel&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1</span> &gt;&gt;&gt;();</span><br><span class="line">    y = <span class="number">20</span>;            <span class="comment">// Error on GPUs not supporting concurrent access</span></span><br><span class="line">                       </span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span>  <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上面的示例中，当 CPU 接触(这里原文中用的是touch这个词) y 时，GPU 程序内核仍然处于活动状态。 （注意它是如何在 <code>cudaDeviceSynchronize()</code> 之前发生的。）由于 GPU 页面错误功能解除了对同时访问的所有限制，因此代码在计算能力 6.x 的设备上成功运行。 但是，即使 CPU 访问的数据与 GPU 不同，这种内存访问在 6.x 之前的架构上也是无效的。 程序必须在访问 y 之前显式地与 GPU 同步：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">__device__ __managed__ <span class="type">int</span> x, y=<span class="number">2</span>;</span><br><span class="line"><span class="function">__global__  <span class="type">void</span>  <span class="title">kernel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    x = <span class="number">10</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    kernel&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1</span> &gt;&gt;&gt;();</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    y = <span class="number">20</span>;            <span class="comment">//  Success on GPUs not supporing concurrent access</span></span><br><span class="line">    <span class="keyword">return</span>  <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如本例所示，在具有 6.x 之前的 GPU 架构的系统上，CPU 线程可能不会在执行内核启动和后续同步调用之间访问任何托管数据，无论 GPU 内核是否实际接触相同的数据（或 任何托管数据）。 并发 CPU 和 GPU 访问的潜力足以引发进程级异常。</p>
<p>请注意，如果在 GPU 处于活动状态时使用 <code>cudaMallocManaged()</code> 或 <code>cuMemAllocManaged()</code> 动态分配内存，则在启动其他工作或同步 GPU 之前，内存的行为是未指定的。 在此期间尝试访问 CPU 上的内存可能会也可能不会导致分段错误。 这不适用于使用标志 <code>cudaMemAttachHost</code> 或 <code>CU_MEM_ATTACH_HOST</code> 分配的内存。</p>
<h4 id="N-2-2-2-Explicit-Synchronization-and-Logical-GPU-Activity"><a href="#N-2-2-2-Explicit-Synchronization-and-Logical-GPU-Activity" class="headerlink" title="N.2.2.2. Explicit Synchronization and Logical GPU Activity"></a>N.2.2.2. Explicit Synchronization and Logical GPU Activity</h4><p>请注意，即使内核快速运行并在上例中的 CPU 接触 y 之前完成，也需要显式同步。统一内存使用逻辑活动来确定 GPU 是否空闲。这与 CUDA 编程模型一致，该模型指定内核可以在启动后的任何时间运行，并且不保证在主机发出同步调用之前完成。</p>
<p>任何在逻辑上保证 GPU 完成其工作的函数调用都是有效的。这包括 <code>cudaDeviceSynchronize()</code>; <code>cudaStreamSynchronize()</code> 和 <code>cudaStreamQuery()</code>（如果它返回 <code>cudaSuccess</code> 而不是 <code>cudaErrorNotReady</code>），其中指定的流是唯一仍在 GPU 上执行的流； <code>cudaEventSynchronize()</code> 和 <code>cudaEventQuery()</code> 在指定事件之后没有任何设备工作的情况下；以及记录为与主机完全同步的 <code>cudaMemcpy()</code> 和 <code>cudaMemset()</code> 的使用。</p>
<p>将遵循流之间创建的依赖关系，通过在流或事件上同步来推断其他流的完成。依赖关系可以通过 <code>cudaStreamWaitEvent()</code> 或在使用默认 (<code>NULL</code>) 流时隐式创建。</p>
<p>CPU 从流回调中访问托管数据是合法的，前提是 GPU 上没有其他可能访问托管数据的流处于活动状态。此外，没有任何设备工作的回调可用于同步：例如，通过从回调内部发出条件变量的信号；否则，CPU 访问仅在回调期间有效。</p>
<p>有几个重要的注意点：</p>
<ul>
<li>在 GPU 处于活动状态时，始终允许 CPU 访问非托管零拷贝数据。</li>
<li>GPU 在运行任何内核时都被认为是活动的，即使该内核不使用托管数据。如果内核可能使用数据，则禁止访问，除非设备属性 <code>concurrentManagedAccess</code> 为 1。</li>
<li>除了适用于非托管内存的多 GPU 访问之外，托管内存的并发 GPU 间访问没有任何限制。</li>
<li>并发 GPU 内核访问托管数据没有任何限制。</li>
</ul>
<p><strong>请注意最后一点如何允许 GPU 内核之间的竞争，就像当前非托管 GPU 内存的情况一样。如前所述，从 GPU 的角度来看，托管内存的功能与非托管内存相同。以下代码示例说明了这些要点：</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    cudaStream_t stream1, stream2;</span><br><span class="line">    <span class="built_in">cudaStreamCreate</span>(&amp;stream1);</span><br><span class="line">    <span class="built_in">cudaStreamCreate</span>(&amp;stream2);</span><br><span class="line">    <span class="type">int</span> *non_managed, *managed, *also_managed;</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>(&amp;non_managed, <span class="number">4</span>);    <span class="comment">// Non-managed, CPU-accessible memory</span></span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>(&amp;managed, <span class="number">4</span>);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>(&amp;also_managed, <span class="number">4</span>);</span><br><span class="line">    <span class="comment">// Point 1: CPU can access non-managed data.</span></span><br><span class="line">    kernel&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, stream1 &gt;&gt;&gt;(managed);</span><br><span class="line">    *non_managed = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// Point 2: CPU cannot access any managed data while GPU is busy,</span></span><br><span class="line">    <span class="comment">//          unless concurrentManagedAccess = 1</span></span><br><span class="line">    <span class="comment">// Note we have not yet synchronized, so &quot;kernel&quot; is still active.</span></span><br><span class="line">    *also_managed = <span class="number">2</span>;      <span class="comment">// Will issue segmentation fault</span></span><br><span class="line">    <span class="comment">// Point 3: Concurrent GPU kernels can access the same data.</span></span><br><span class="line">    kernel&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, stream2 &gt;&gt;&gt;(managed);</span><br><span class="line">    <span class="comment">// Point 4: Multi-GPU concurrent access is also permitted.</span></span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(<span class="number">1</span>);</span><br><span class="line">    kernel&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1</span> &gt;&gt;&gt;(managed);</span><br><span class="line">    <span class="keyword">return</span>  <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="N-2-2-3-Managing-Data-Visibility-and-Concurrent-CPU-GPU-Access-with-Streams"><a href="#N-2-2-3-Managing-Data-Visibility-and-Concurrent-CPU-GPU-Access-with-Streams" class="headerlink" title="N.2.2.3. Managing Data Visibility and Concurrent CPU + GPU Access with Streams"></a>N.2.2.3. Managing Data Visibility and Concurrent CPU + GPU Access with Streams</h4><p>到目前为止，假设对于 6.x 之前的 SM 架构：1) 任何活动内核都可以使用任何托管内存，以​​及 2) 在内核处于活动状态时使用来自 CPU 的托管内存是无效的。在这里，我们提出了一个用于对托管内存进行更细粒度控制的系统，该系统旨在在所有支持托管内存的设备上工作，包括 <code>concurrentManagedAccess</code> 等于 0 的旧架构。</p>
<p>CUDA 编程模型提供流作为程序指示内核启动之间的依赖性和独立性的机制。启动到同一流中的内核保证连续执行，而启动到不同流中的内核允许并发执行。流描述了工作项之间的独立性，因此可以通过并发实现更高的效率。</p>
<p>统一内存建立在流独立模型之上，允许 CUDA 程序显式地将托管分配与 CUDA 流相关联。通过这种方式，程序员根据内核是否将数据启动到指定的流中来指示内核对数据的使用。这为基于程序特定数据访问模式的并发提供了机会。控制这种行为的函数是：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaStreamAttachMemAsync</span><span class="params">(cudaStream_t stream,</span></span></span><br><span class="line"><span class="params"><span class="function">                                     <span class="type">void</span> *ptr,</span></span></span><br><span class="line"><span class="params"><span class="function">                                     <span class="type">size_t</span> length=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                                     <span class="type">unsigned</span> <span class="type">int</span> flags=<span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure>
<p><code>cudaStreamAttachMemAsync()</code> 函数将从 <code>ptr</code> 开始的内存长度字节与指定的流相关联。 （目前，<code>length</code> 必须始终为 0 以指示应该附加整个区域。）由于这种关联，只要流中的所有操作都已完成，统一内存系统就允许 CPU 访问该内存区域，而不管其他流是否是活跃的。实际上，这将活动 GPU 对托管内存区域的独占所有权限制为每个流活动而不是整个 GPU 活动。</p>
<p>最重要的是，如果分配与特定流无关，则所有正在运行的内核都可以看到它，而不管它们的流如何。这是 <code>cudaMallocManaged()</code> 分配或 <code>__managed__</code>变量的默认可见性；因此，在任何内核运行时 CPU 不得接触数据的简单案例规则。</p>
<p>通过将分配与特定流相关联，程序保证只有启动到该流中的内核才会接触该数据。统一内存系统不执行错误检查：程序员有责任确保兑现保证。</p>
<p>除了允许更大的并发性之外，使用 <code>cudaStreamAttachMemAsync()</code> 可以（并且通常会）启用统一内存系统内的数据传输优化，这可能会影响延迟和其他开销。</p>
<h4 id="N-2-2-4-Stream-Association-Examples"><a href="#N-2-2-4-Stream-Association-Examples" class="headerlink" title="N.2.2.4. Stream Association Examples"></a>N.2.2.4. Stream Association Examples</h4><p>将数据与流相关联允许对 CPU + GPU 并发进行细粒度控制，但在使用计算能力低于 6.x 的设备时，必须牢记哪些数据对哪些流可见。 查看前面的同步示例：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">__device__ __managed__ <span class="type">int</span> x, y=<span class="number">2</span>;</span><br><span class="line"><span class="function">__global__  <span class="type">void</span>  <span class="title">kernel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    x = <span class="number">10</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    cudaStream_t stream1;</span><br><span class="line">    <span class="built_in">cudaStreamCreate</span>(&amp;stream1);</span><br><span class="line">    <span class="built_in">cudaStreamAttachMemAsync</span>(stream1, &amp;y, <span class="number">0</span>, cudaMemAttachHost);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();          <span class="comment">// Wait for Host attachment to occur.</span></span><br><span class="line">    kernel&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, stream1 &gt;&gt;&gt;(); <span class="comment">// Note: Launches into stream1.</span></span><br><span class="line">    y = <span class="number">20</span>;                           <span class="comment">// Success – a kernel is running but “y” </span></span><br><span class="line">                                      <span class="comment">// has been associated with no stream.</span></span><br><span class="line">    <span class="keyword">return</span>  <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这里，我们明确地将 y 与主机可访问性相关联，从而始终可以从 CPU 进行访问。 （和以前一样，请注意在访问之前没有 <code>cudaDeviceSynchronize()</code>。）GPU 运行内核对 y 的访问现在将产生未定义的结果。</p>
<p>请注意，将变量与流关联不会更改任何其他变量的关联。 例如。 将 x 与 stream1 关联并不能确保在 stream1 中启动的内核只能访问 x，因此此代码会导致错误：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">__device__ __managed__ <span class="type">int</span> x, y=<span class="number">2</span>;</span><br><span class="line"><span class="function">__global__  <span class="type">void</span>  <span class="title">kernel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    x = <span class="number">10</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    cudaStream_t stream1;</span><br><span class="line">    <span class="built_in">cudaStreamCreate</span>(&amp;stream1);</span><br><span class="line">    <span class="built_in">cudaStreamAttachMemAsync</span>(stream1, &amp;x);<span class="comment">// Associate “x” with stream1.</span></span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();              <span class="comment">// Wait for “x” attachment to occur.</span></span><br><span class="line">    kernel&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, stream1 &gt;&gt;&gt;();     <span class="comment">// Note: Launches into stream1.</span></span><br><span class="line">    y = <span class="number">20</span>;                               <span class="comment">// ERROR: “y” is still associated globally </span></span><br><span class="line">                                          <span class="comment">// with all streams by default</span></span><br><span class="line">    <span class="keyword">return</span>  <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>请注意访问 y 将如何导致错误，因为即使 x 已与流相关联，我们也没有告诉系统谁可以看到 y。 因此，系统保守地假设内核可能会访问它并阻止 CPU 这样做。</strong></p>
<h4 id="N-2-2-5-Stream-Attach-With-Multithreaded-Host-Programs"><a href="#N-2-2-5-Stream-Attach-With-Multithreaded-Host-Programs" class="headerlink" title="N.2.2.5. Stream Attach With Multithreaded Host Programs"></a>N.2.2.5. Stream Attach With Multithreaded Host Programs</h4><p><code>cudaStreamAttachMemAsync()</code> 的主要用途是使用 CPU 线程启用独立任务并行性。 通常在这样的程序中，CPU 线程为它生成的所有工作创建自己的流，因为使用 CUDA 的 NULL 流会导致线程之间的依赖关系。</p>
<p>托管数据对任何 GPU 流的默认全局可见性使得难以避免多线程程序中 CPU 线程之间的交互。 因此，函数 <code>cudaStreamAttachMemAsync()</code> 用于将线程的托管分配与该线程自己的流相关联，并且该关联通常在线程的生命周期内不会更改。</p>
<p>这样的程序将简单地添加一个对 <code>cudaStreamAttachMemAsync()</code> 的调用，以使用统一内存进行数据访问：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This function performs some task, in its own private stream.</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">run_task</span><span class="params">(<span class="type">int</span> *in, <span class="type">int</span> *out, <span class="type">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Create a stream for us to use.</span></span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    <span class="built_in">cudaStreamCreate</span>(&amp;stream);</span><br><span class="line">    <span class="comment">// Allocate some managed data and associate with our stream.</span></span><br><span class="line">    <span class="comment">// Note the use of the host-attach flag to cudaMallocManaged();</span></span><br><span class="line">    <span class="comment">// we then associate the allocation with our stream so that</span></span><br><span class="line">    <span class="comment">// our GPU kernel launches can access it.</span></span><br><span class="line">    <span class="type">int</span> *data;</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span> **)&amp;data, length, cudaMemAttachHost);</span><br><span class="line">    <span class="built_in">cudaStreamAttachMemAsync</span>(stream, data);</span><br><span class="line">    <span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line">    <span class="comment">// Iterate on the data in some way, using both Host &amp; Device.</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;N; i++) &#123;</span><br><span class="line">        transform&lt;&lt;&lt; <span class="number">100</span>, <span class="number">256</span>, <span class="number">0</span>, stream &gt;&gt;&gt;(in, data, length);</span><br><span class="line">        <span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line">        <span class="built_in">host_process</span>(data, length);    <span class="comment">// CPU uses managed data.</span></span><br><span class="line">        convert&lt;&lt;&lt; <span class="number">100</span>, <span class="number">256</span>, <span class="number">0</span>, stream &gt;&gt;&gt;(out, data, length);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cudaStreamSynchronize</span>(stream);</span><br><span class="line">    <span class="built_in">cudaStreamDestroy</span>(stream);</span><br><span class="line">    <span class="built_in">cudaFree</span>(data);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这个例子中，分配流关联只建立一次，然后主机和设备都重复使用数据。 结果是比在主机和设备之间显式复制数据时更简单的代码，尽管结果是相同的。</p>
<h4 id="N-2-2-6-Advanced-Topic-Modular-Programs-and-Data-Access-Constraints"><a href="#N-2-2-6-Advanced-Topic-Modular-Programs-and-Data-Access-Constraints" class="headerlink" title="N.2.2.6. Advanced Topic: Modular Programs and Data Access Constraints"></a>N.2.2.6. Advanced Topic: Modular Programs and Data Access Constraints</h4><p>在前面的示例中，<code>cudaMallocManaged()</code> 指定了 <code>cudaMemAttachHost</code> 标志，它创建了一个最初对设备端执行不可见的分配。 （默认分配对所有流上的所有 GPU 内核都是可见的。）这可确保在数据分配和为特定流获取数据之间的时间间隔内，不会与另一个线程的执行发生意外交互。</p>
<p>如果没有这个标志，如果另一个线程启动的内核恰好正在运行，则新分配将被视为在 GPU 上使用。这可能会影响线程在能够将其显式附加到私有流之前从 CPU 访问新分配的数据的能力（例如，在基类构造函数中）。因此，为了启用线程之间的安全独立性，应指定此标志进行分配。</p>
<p>注意：另一种方法是在分配附加到流之后在所有线程上放置一个进程范围的屏障。这将确保所有线程在启动任何内核之前完成其数据/流关联，从而避免危险。在销毁流之前需要第二个屏障，因为流销毁会导致分配恢复到其默认可见性。 <code>cudaMemAttachHost</code> 标志的存在既是为了简化此过程，也是因为并非总是可以在需要的地方插入全局屏障。</p>
<h4 id="N-2-2-7-Memcpy-Memset-Behavior-With-Managed-Memory"><a href="#N-2-2-7-Memcpy-Memset-Behavior-With-Managed-Memory" class="headerlink" title="N.2.2.7. Memcpy()/Memset() Behavior With Managed Memory"></a>N.2.2.7. Memcpy()/Memset() Behavior With Managed Memory</h4><p>由于可以从主机或设备访问托管内存，因此 <code>cudaMemcpy*()</code> 依赖于使用 <code>cudaMemcpyKind</code> 指定的传输类型来确定数据应该作为主机指针还是设备指针访问。</p>
<p>如果指定了 <code>cudaMemcpyHostTo*</code> 并且管理了源数据，那么如果在复制流 (1) 中可以从主机连贯地访问它，那么它将从主机访问；否则将从设备访问。当指定 <code>cudaMemcpy*ToHost</code> 并且目标是托管内存时，类似的规则适用于目标。</p>
<p>如果指定了 <code>cudaMemcpyDeviceTo*</code> 并管理源数据，则将从设备访问它。源必须可以从复制流中的设备连贯地访问 (2)；否则，返回错误。当指定 <code>cudaMemcpy*ToDevice</code> 并且目标是托管内存时，类似的规则适用于目标。</p>
<p>如果指定了 <code>cudaMemcpyDefault</code>，则如果无法从复制流中的设备一致地访问托管数据 (2)，或者如果数据的首选位置是 <code>cudaCpuDeviceId</code> 并且可以从主机一致地访问，则将从主机访问托管数据在复制流 (1) 中；否则，它将从设备访问。</p>
<p>将 <code>cudaMemset*()</code> 与托管内存一起使用时，始终从设备访问数据。数据必须可以从用于 <code>cudaMemset()</code> 操作的流中的设备连贯地访问 (2)；否则，返回错误。</p>
<p>当通过 <code>cudaMemcpy*</code> 或 <code>cudaMemset*</code> 从设备访问数据时，操作流被视为在 GPU 上处于活动状态。在此期间，如果 GPU 的设备属性 <code>concurrentManagedAccess</code> 为零值，则任何与该流相关联的数据或具有全局可见性的数据的 CPU 访问都将导致分段错误。在从 CPU 访问任何相关数据之前，程序必须适当同步以确保操作已完成。</p>
<p>(1) 要在给定流中从主机连贯地访问托管内存，必须至少满足以下条件之一：</p>
<ul>
<li>给定流与设备属性 <code>concurrentManagedAccess</code> 具有非零值的设备相关联。</li>
<li>内存既不具有全局可见性，也不与给定流相关联。</li>
</ul>
<p>(2) 要在给定流中从设备连贯地访问托管内存，必须至少满足以下条件之一：</p>
<ul>
<li>设备的设备属性 <code>concurrentManagedAccess</code> 具有非零值。</li>
<li>内存要么具有全局可见性，要么与给定的流相关联。</li>
</ul>
<h3 id="N-2-3-Language-Integration"><a href="#N-2-3-Language-Integration" class="headerlink" title="N.2.3. Language Integration"></a>N.2.3. Language Integration</h3><p>使用 nvcc 编译主机代码的 CUDA 运行时 API 用户可以访问其他语言集成功能，例如共享符号名称和通过 <code>&lt;&lt;&lt;...&gt;&gt;&gt;</code> 运算符启动内联内核。 统一内存为 CUDA 的语言集成添加了一个附加元素：使用 <code>__managed__</code> 关键字注释的变量可以直接从主机和设备代码中引用。</p>
<p>下面的例子在前面的 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-simplifying">Simplifying GPU Programming</a> 中看到，说明了 <code>__managed__</code> 全局声明的简单使用：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Managed variable declaration is an extra annotation with __device__</span></span><br><span class="line">__device__ __managed__  <span class="type">int</span>  x;</span><br><span class="line"><span class="function">__global__  <span class="type">void</span>  <span class="title">kernel</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Reference &quot;x&quot; directly - it&#x27;s a normal variable on the GPU.</span></span><br><span class="line">    <span class="built_in">printf</span>( <span class="string">&quot;GPU sees: x = %d\n&quot;</span> , x);</span><br><span class="line">&#125; </span><br><span class="line"><span class="function"><span class="type">int</span>  <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Set &quot;x&quot; from Host code. Note it&#x27;s just a normal variable on the CPU.</span></span><br><span class="line">    x = <span class="number">1234</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// Launch a kernel which uses &quot;x&quot; from the GPU.</span></span><br><span class="line">    kernel&lt;&lt;&lt; <span class="number">1</span>, <span class="number">1</span> &gt;&gt;&gt;(); </span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>(); </span><br><span class="line">    <span class="keyword">return</span>  <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>__managed__</code> 变量的可用功能是该符号在设备代码和主机代码中都可用，而无需取消引用指针，并且数据由所有人共享。这使得在主机和设备程序之间交换数据变得特别容易，而无需显式分配或复制。</p>
<p>从语义上讲，<code>__managed__</code> 变量的行为与通过 <code>cudaMallocManaged()</code> 分配的存储相同。有关详细说明，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-explicit-allocation">使用 cudaMallocManaged() 进行显式分配</a>。流可见性默认为 <code>cudaMemAttachGlobal</code>，但可以使用 <code>cudaStreamAttachMemAsync()</code> 进行限制。</p>
<p><code>__managed__</code> 变量的正确操作需要有效的 CUDA 上下文。如果当前设备的上下文尚未创建，则访问 <code>__managed__</code>变量可以触发 CUDA 上下文创建。在上面的示例中，在内核启动之前访问 x 会触发设备 0 上的上下文创建。如果没有该访问，内核启动将触发上下文创建。</p>
<p>声明为 <code>__managed__</code>的 C++ 对象受到某些特定约束，尤其是在涉及静态初始化程序的情况下。有关这些约束的列表，请参阅 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#c-cplusplus-language-support">CUDA C++ 编程指南中的 C++ 语言支持</a>。</p>
<h4 id="N-2-3-1-Host-Program-Errors-with-managed-Variables"><a href="#N-2-3-1-Host-Program-Errors-with-managed-Variables" class="headerlink" title="N.2.3.1. Host Program Errors with managed Variables"></a>N.2.3.1. Host Program Errors with <strong>managed</strong> Variables</h4><p><code>__managed__</code> 变量的使用取决于底层统一内存系统是否正常运行。 例如，如果 CUDA 安装失败或 CUDA 上下文创建不成功，则可能会出现不正确的功能。</p>
<p>当特定于 CUDA 的操作失败时，通常会返回一个错误，指出失败的根源。 使用 <code>__managed__</code> 变量引入了一种新的故障模式，如果统一内存系统运行不正确，非 CUDA 操作（例如，CPU 访问应该是有效的主机内存地址）可能会失败。 这种无效的内存访问不能轻易地归因于底层的 CUDA 子系统，尽管诸如 <code>cuda-gdb</code> 之类的调试器会指示托管内存地址是故障的根源。</p>
<h3 id="N-2-4-Querying-Unified-Memory-Support"><a href="#N-2-4-Querying-Unified-Memory-Support" class="headerlink" title="N.2.4. Querying Unified Memory Support"></a>N.2.4. Querying Unified Memory Support</h3><h4 id="N-2-4-1-Device-Properties"><a href="#N-2-4-1-Device-Properties" class="headerlink" title="N.2.4.1. Device Properties"></a>N.2.4.1. Device Properties</h4><p>统一内存仅在具有 3.0 或更高计算能力的设备上受支持。程序可以通过使用 <code>cudaGetDeviceProperties()</code> 并检查新的 <code>managedMemory</code> 属性来查询 GPU 设备是否支持托管内存。也可以使用具有属性 <code>cudaDevAttrManagedMemory</code> 的单个属性查询函数 <code>cudaDeviceGetAttribute()</code> 来确定能力。</p>
<p>如果在 GPU 和当前操作系统下允许托管内存分配，则任一属性都将设置为 1。请注意，32 位应用程序不支持统一内存（除非在 Android 上），即使 GPU 有足够的能力。</p>
<p>支持平台上计算能力 6.x 的设备无需调用 <code>cudaHostRegister</code> 即可访问可分页内存。应用程序可以通过检查新的 <code>pageableMemoryAccess</code> 属性来查询设备是否支持连贯访问可分页内存。</p>
<p>通过新的缺页机制，统一内存保证了全局数据的一致性。这意味着 CPU 和 GPU 可以同时访问统一内存分配。这在计算能力低于 6.x 的设备上是非法的，因为如果 CPU 在 GPU 内核处于活动状态时访问统一内存分配，则无法保证一致性。程序可以通过检查 <code>concurrentManagedAccess</code> 属性来查询并发访问支持。有关详细信息，请参阅<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-coherency-hd">一致性和并发性</a>。</p>
<h3 id="N-2-5-Advanced-Topics"><a href="#N-2-5-Advanced-Topics" class="headerlink" title="N.2.5. Advanced Topics"></a>N.2.5. Advanced Topics</h3><h4 id="N-2-5-1-Managed-Memory-with-Multi-GPU-Programs-on-pre-6-x-Architectures"><a href="#N-2-5-1-Managed-Memory-with-Multi-GPU-Programs-on-pre-6-x-Architectures" class="headerlink" title="N.2.5.1. Managed Memory with Multi-GPU Programs on pre-6.x Architectures"></a>N.2.5.1. Managed Memory with Multi-GPU Programs on pre-6.x Architectures</h4><p>在计算能力低于 6.x 的设备的系统上，托管分配通过 GPU 的对等能力自动对系统中的所有 GPU 可见。</p>
<p>在 Linux 上，只要程序正在使用的所有 GPU 都具有点对点支持，托管内存就会在 GPU 内存中分配。如果在任何时候应用程序开始使用不支持对等支持的 GPU 与任何其他对其进行了托管分配的 GPU，则驱动程序会将所有托管分配迁移到系统内存。</p>
<p>在 Windows 上，如果对等映射不可用（例如，在不同架构的 GPU 之间），那么系统将自动回退到使用零拷贝内存，无论两个 GPU 是否都被程序实际使用。如果实际只使用一个 GPU，则需要在启动程序之前设置 <code>CUDA_VISIBLE_DEVICES</code> 环境变量。这限制了哪些 GPU 是可见的，并允许在 GPU 内存中分配托管内存。</p>
<p>或者，在 Windows 上，用户还可以将 <code>CUDA_MANAGED_FORCE_DEVICE_ALLOC</code> 设置为非零值，以强制驱动程序始终使用设备内存进行物理存储。当此环境变量设置为非零值时，该进程中使用的所有支持托管内存的设备必须彼此对等兼容。如果使用支持托管内存的设备并且它与之前在该进程中使用的任何其他托管内存支持设备不兼容，则将返回错误 <code>::cudaErrorInvalidDevic</code>e，即使 <code>::cudaDeviceReset</code> 具有在这些设备上被调用。这些环境变量在附录 <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars">CUDA 环境变量中进行了描述</a>。请注意，从 CUDA 8.0 开始，<code>CUDA_MANAGED_FORCE_DEVICE_ALLOC</code> 对 Linux 操作系统没有影响。</p>
<h4 id="N-2-5-2-Using-fork-with-Managed-Memory"><a href="#N-2-5-2-Using-fork-with-Managed-Memory" class="headerlink" title="N.2.5.2. Using fork() with Managed Memory"></a>N.2.5.2. Using fork() with Managed Memory</h4><p>统一内存系统不允许在进程之间共享托管内存指针。 它不会正确管理通过 <code>fork()</code> 操作复制的内存句柄。 如果子级或父级在 <code>fork()</code> 之后访问托管数据，则结果将不确定。</p>
<p>然而，<code>fork()</code> 一个子进程然后通过 <code>exec()</code> 调用立即退出是安全的，因为子进程丢弃了内存句柄并且父进程再次成为唯一的所有者。 父母离开并让孩子接触句柄是不安全的。</p>
<h2 id="N-3-Performance-Tuning"><a href="#N-3-Performance-Tuning" class="headerlink" title="N.3. Performance Tuning"></a>N.3. Performance Tuning</h2><p>为了使用统一内存实现良好的性能，必须满足以下目标：</p>
<ul>
<li>应避免错误：虽然可重放错误是启用更简单的编程模型的基础，但它们可能严重损害应用程序性能。故障处理可能需要几十微秒，因为它可能涉及 TLB 无效、数据迁移和页表更新。与此同时，应用程序某些部分的执行将停止，从而可能影响整体性能。</li>
<li>数据应该位于访问处理器的本地：如前所述，当数据位于访问它的处理器本地时，内存访问延迟和带宽明显更好。因此，应适当迁移数据以利用较低的延迟和较高的带宽。</li>
<li>应该防止内存抖动：如果数据被多个处理器频繁访问并且必须不断迁移以实现数据局部性，那么迁移的开销可能会超过局部性的好处。应尽可能防止内存抖动。如果无法预防，则必须进行适当的检测和解决。</li>
</ul>
<p>为了达到与不使用统一内存相同的性能水平，应用程序必须引导统一内存驱动子系统避免上述陷阱。值得注意的是，统一内存驱动子系统可以检测常见的数据访问模式并自动实现其中一些目标，而无需应用程序参与。但是，当数据访问模式不明显时，来自应用程序的明确指导至关重要。 CUDA 8.0 引入了有用的 API，用于为运行时提供内存使用提示 (<code>cudaMemAdvise()</code>) 和显式预取 (<code>cudaMemPrefetchAsync()</code>)。这些工具允许与显式内存复制和固定 API 相同的功能，而不会恢复到显式 GPU 内存分配的限制。</p>
<p>注意：Tegra 设备不支持 <code>cudaMemPrefetchAsync()</code>。</p>
<h3 id="N-3-1-Data-Prefetching"><a href="#N-3-1-Data-Prefetching" class="headerlink" title="N.3.1. Data Prefetching"></a>N.3.1. Data Prefetching</h3><p>数据预取意味着将数据迁移到处理器的内存中，并在处理器开始访问该数据之前将其映射到该处理器的页表中。 数据预取的目的是在建立数据局部性的同时避免故障。 这对于在任何给定时间主要从单个处理器访问数据的应用程序来说是最有价值的。 由于访问处理器在应用程序的生命周期中发生变化，因此可以相应地预取数据以遵循应用程序的执行流程。 由于工作是在 CUDA 中的流中启动的，因此预计数据预取也是一种流操作，如以下 API 所示：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemPrefetchAsync</span><span class="params">(<span class="type">const</span> <span class="type">void</span> *devPtr, </span></span></span><br><span class="line"><span class="params"><span class="function">                                 <span class="type">size_t</span> count, </span></span></span><br><span class="line"><span class="params"><span class="function">                                 <span class="type">int</span> dstDevice, </span></span></span><br><span class="line"><span class="params"><span class="function">                                 cudaStream_t stream)</span></span>;</span><br></pre></td></tr></table></figure>
<p>其中由 <code>devPtr</code> 指针和 <code>count</code> <code>字节数指定的内存区域，ptr</code> 向下舍入到最近的页面边界，<code>count</code> 向上舍入到最近的页面边界，通过在流中排队迁移操作迁移到 <code>dstDevice</code>。 为 <code>dstDevice</code> 传入 <code>cudaCpuDeviceId</code> 会导致数据迁移到 CPU 内存。<br>考虑下面的一个简单代码示例：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">foo</span><span class="params">(cudaStream_t s)</span> </span>&#123;</span><br><span class="line">  <span class="type">char</span> *data;</span><br><span class="line">  <span class="built_in">cudaMallocManaged</span>(&amp;data, N);</span><br><span class="line">  <span class="built_in">init_data</span>(data, N);                                   <span class="comment">// execute on CPU</span></span><br><span class="line">  <span class="built_in">cudaMemPrefetchAsync</span>(data, N, myGpuId, s);            <span class="comment">// prefetch to GPU</span></span><br><span class="line">  mykernel&lt;&lt;&lt;..., s&gt;&gt;&gt;(data, N, <span class="number">1</span>, compare);            <span class="comment">// execute on GPU</span></span><br><span class="line">  <span class="built_in">cudaMemPrefetchAsync</span>(data, N, cudaCpuDeviceId, s);    <span class="comment">// prefetch to CPU</span></span><br><span class="line">  <span class="built_in">cudaStreamSynchronize</span>(s);</span><br><span class="line">  <span class="built_in">use_data</span>(data, N);</span><br><span class="line">  <span class="built_in">cudaFree</span>(data);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果没有性能提示，内核 <code>mykernel</code> 将在首次访问数据时出错，这会产生额外的故障处理开销，并且通常会减慢应用程序的速度。 通过提前预取数据，可以避免页面错误并获得更好的性能。<br>此 API 遵循流排序语义，即迁移在流中的所有先前操作完成之前不会开始，并且流中的任何后续操作在迁移完成之前不会开始。</p>
<h3 id="N-3-2-Data-Usage-Hints"><a href="#N-3-2-Data-Usage-Hints" class="headerlink" title="N.3.2. Data Usage Hints"></a>N.3.2. Data Usage Hints</h3><p>当多个处理器需要同时访问相同的数据时，单独的数据预取是不够的。 在这种情况下，应用程序提供有关如何实际使用数据的提示很有用。 以下咨询 API 可用于指定数据使用情况：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemAdvise</span><span class="params">(<span class="type">const</span> <span class="type">void</span> *devPtr, </span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">size_t</span> count, </span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="keyword">enum</span> cudaMemoryAdvise advice, </span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">int</span> device)</span></span>;</span><br></pre></td></tr></table></figure>
<p>其中，为从 <code>devPtr</code> 地址开始的区域中包含的数据指定的通知和计数字节的长度，四舍五入到最近的页面边界，可以采用以下值：</p>
<ul>
<li><code>cudaMemAdviseSetReadMostly</code>：这意味着数据大部分将被读取并且只是偶尔写入。 这允许驱动程序在处理器访问数据时在处理器内存中创建数据的只读拷贝。 同样，如果在此区域上调用 <code>cudaMemPrefetchAsync</code>，它将在目标处理器上创建数据的只读拷贝。 当处理器写入此数据时，相应页面的所有副本都将失效，但发生写入的拷贝除外。 此建议忽略设备参数。 该建议允许多个处理器以最大带宽同时访问相同的数据，如以下代码片段所示：</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">char</span> *dataPtr;</span><br><span class="line"><span class="type">size_t</span> dataSize = <span class="number">4096</span>;</span><br><span class="line"><span class="comment">// Allocate memory using malloc or cudaMallocManaged</span></span><br><span class="line">dataPtr = (<span class="type">char</span> *)<span class="built_in">malloc</span>(dataSize);</span><br><span class="line"><span class="comment">// Set the advice on the memory region</span></span><br><span class="line"><span class="built_in">cudaMemAdvise</span>(dataPtr, dataSize, cudaMemAdviseSetReadMostly, <span class="number">0</span>);</span><br><span class="line"><span class="type">int</span> outerLoopIter = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (outerLoopIter &lt; maxOuterLoopIter) &#123;</span><br><span class="line">    <span class="comment">// The data is written to in the outer loop on the CPU</span></span><br><span class="line">    <span class="built_in">initializeData</span>(dataPtr, dataSize);</span><br><span class="line">    <span class="comment">// The data is made available to all GPUs by prefetching.</span></span><br><span class="line">    <span class="comment">// Prefetching here causes read duplication of data instead</span></span><br><span class="line">    <span class="comment">// of data migration</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> device = <span class="number">0</span>; device &lt; maxDevices; device++) &#123;</span><br><span class="line">        <span class="built_in">cudaMemPrefetchAsync</span>(dataPtr, dataSize, device, stream);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// The kernel only reads this data in the inner loop</span></span><br><span class="line">    <span class="type">int</span> innerLoopIter = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (innerLoopIter &lt; maxInnerLoopIter) &#123;</span><br><span class="line">        kernel&lt;&lt;&lt;<span class="number">32</span>,<span class="number">32</span>&gt;&gt;&gt;((<span class="type">const</span> <span class="type">char</span> *)dataPtr);</span><br><span class="line">        innerLoopIter++;</span><br><span class="line">    &#125;</span><br><span class="line">    outerLoopIter++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>cudaMemAdviseSetPreferredLocation</code>：此建议将数据的首选位置设置为属于设备的内存。传入设备的 <code>cudaCpuDeviceId</code> 值会将首选位置设置为 CPU 内存。设置首选位置不会导致数据立即迁移到该位置。相反，它会在该内存区域发生故障时指导迁移策略。如果数据已经在它的首选位置并且故障处理器可以建立映射而不需要迁移数据，那么迁移将被避免。另一方面，如果数据不在其首选位置，或者无法建立直接映射，那么它将被迁移到访问它的处理器。请务必注意，设置首选位置不会阻止使用 <code>cudaMemPrefetchAsync</code> 完成数据预取。</li>
<li><code>cudaMemAdviseSetAccessedBy</code>：这个advice意味着数据将被设备访问。这不会导致数据迁移，并且对数据本身的位置没有影响。相反，只要数据的位置允许建立映射，它就会使数据始终映射到指定处理器的页表中。如果数据因任何原因被迁移，映射会相应更新。此advice在数据局部性不重要但避免故障很重要的情况下很有用。例如，考虑一个包含多个启用对等访问的 GPU 的系统，其中位于一个 GPU 上的数据偶尔会被其他 GPU 访问。在这种情况下，将数据迁移到其他 GPU 并不那么重要，因为访问不频繁并且迁移的开销可能太高。但是防止故障仍然有助于提高性能，因此提前设置映射很有用。请注意，在 CPU 访问此数据时，由于 CPU 无法直接访问 GPU 内存，因此数据可能会迁移到 CPU 内存。任何为此数据设置了 <code>cudaMemAdviceSetAccessedBy</code> 标志的 GPU 现在都将更新其映射以指向 CPU 内存中的页面。</li>
</ul>
<p>每个advice也可以使用以下值之一取消设置：<code>cudaMemAdviseUnsetReadMostly</code>、<code>cudaMemAdviseUnsetPreferredLocation</code> 和 <code>cudaMemAdviseUnsetAccessedBy</code>。</p>
<h3 id="N-3-3-Querying-Usage-Attributes"><a href="#N-3-3-Querying-Usage-Attributes" class="headerlink" title="N.3.3. Querying Usage Attributes"></a>N.3.3. Querying Usage Attributes</h3><p>程序可以使用以下 API 查询通过 <code>cudaMemAdvise</code> 或 <code>cudaMemPrefetchAsync</code> 分配的内存范围属性：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaMemRangeGetAttribute</span>(<span class="type">void</span> *data, </span><br><span class="line">                         <span class="type">size_t</span> dataSize, </span><br><span class="line">                         <span class="keyword">enum</span> cudaMemRangeAttribute attribute, </span><br><span class="line">                         <span class="type">const</span> <span class="type">void</span> *devPtr, </span><br><span class="line">                         <span class="type">size_t</span> count);</span><br></pre></td></tr></table></figure>
<p>此函数查询从 <code>devPtr</code> 开始的内存范围的属性，大小为 <code>count</code> 字节。内存范围必须引用通过 <code>cudaMallocManaged</code> 分配或通过 <code>__managed__</code> 变量声明的托管内存。可以查询以下属性：</p>
<ul>
<li><code>cudaMemRangeAttributeReadMostly</code>：如果给定内存范围内的所有页面都启用了重复读取，则返回的结果将为 1，否则返回 0。</li>
<li><code>cudaMemRangeAttributePreferredLocation</code>：如果内存范围内的所有页面都将相应的处理器作为首选位置，则返回结果将是 GPU 设备 ID 或 <code>cudaCpuDeviceId</code>，否则将返回 <code>cudaInvalidDeviceId</code>。应用程序可以使用此查询 API 来决定通过 CPU 或 GPU 暂存数据，具体取决于托管指针的首选位置属性。请注意，查询时内存范围内页面的实际位置可能与首选位置不同。</li>
<li><code>cudaMemRangeAttributeAccessedBy</code>: 将返回为该内存范围设置了该建议的设备列表。</li>
<li><code>cudaMemRangeAttributeLastPrefetchLocation</code>：将返回使用 <code>cudaMemPrefetchAsync</code> 显式预取内存范围内所有页面的最后位置。请注意，这只是返回应用程序请求将内存范围预取到的最后一个位置。它没有指示对该位置的预取操作是否已经完成或什至开始。</li>
</ul>
<p>此外，还可以使用对应的 <code>cudaMemRangeGetAttributes</code> 函数查询多个属性。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/C/" rel="tag"># C++</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/12/08/sgemm/" rel="prev" title="SGEMM实施的完整演练">
      <i class="fa fa-chevron-left"></i> SGEMM实施的完整演练
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/12/23/%E8%AF%BBcuda_best_practices_guide/" rel="next" title="读《CUDA C++ Best Practices Guide》">
      读《CUDA C++ Best Practices Guide》 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-CUDA%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">1.CUDA简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E6%88%91%E4%BB%AC%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8GPU"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 我们为什么要使用GPU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-CUDA%C2%AE%EF%BC%9A%E9%80%9A%E7%94%A8%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E5%B9%B3%E5%8F%B0%E5%92%8C%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 CUDA®：通用并行计算平台和编程模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 可扩展的编程模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">2.编程模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E5%86%85%E6%A0%B8"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 内核</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E7%BA%BF%E7%A8%8B%E5%B1%82%E6%AC%A1"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 线程层次</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-%E5%AD%98%E5%82%A8%E5%8D%95%E5%85%83%E5%B1%82%E6%AC%A1"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 存储单元层次</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-%E5%BC%82%E6%9E%84%E7%BC%96%E7%A8%8B"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 异构编程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-%E5%BC%82%E6%AD%A5SIMT%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.5.</span> <span class="nav-text">2.5 异步SIMT编程模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-1-%E5%BC%82%E6%AD%A5%E6%93%8D%E4%BD%9C"><span class="nav-number">2.5.1.</span> <span class="nav-text">2.5.1 异步操作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-Compute-Capability"><span class="nav-number">2.6.</span> <span class="nav-text">2.6 Compute Capability</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0%E7%BC%96%E7%A8%8B%E6%8E%A5%E5%8F%A3"><span class="nav-number">3.</span> <span class="nav-text">第三章编程接口</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1%E5%88%A9%E7%94%A8NVCC%E7%BC%96%E8%AF%91"><span class="nav-number">3.1.</span> <span class="nav-text">3.1利用NVCC编译</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1%E7%BC%96%E8%AF%91%E6%B5%81%E7%A8%8B"><span class="nav-number">3.1.1.</span> <span class="nav-text">3.1.1编译流程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-1-%E7%A6%BB%E7%BA%BF%E7%BC%96%E8%AF%91"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">3.1.1.1 离线编译</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-2-%E5%8D%B3%E6%97%B6%E7%BC%96%E8%AF%91"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">3.1.1.2 即时编译</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-Binary-%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="nav-number">3.1.2.</span> <span class="nav-text">3.1.2 Binary 兼容性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-3-PTX-%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="nav-number">3.1.3.</span> <span class="nav-text">3.1.3 PTX 兼容性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-4-%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="nav-number">3.1.4.</span> <span class="nav-text">3.1.4 应用程序兼容性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-5-C-%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="nav-number">3.1.5.</span> <span class="nav-text">3.1.5 C++兼容性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-6-64%E4%BD%8D%E6%94%AF%E6%8C%81"><span class="nav-number">3.1.6.</span> <span class="nav-text">3.1.6 64位支持</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-CUDA%E8%BF%90%E8%A1%8C%E6%97%B6"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 CUDA运行时</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">3.2.0.1.</span> <span class="nav-text">3.2.1 初始化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-%E8%AE%BE%E5%A4%87%E5%AD%98%E5%82%A8"><span class="nav-number">3.2.1.</span> <span class="nav-text">3.2.2 设备存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-3-L2%E7%BA%A7%E8%AE%BE%E5%A4%87%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="nav-number">3.2.2.</span> <span class="nav-text">3.2.3 L2级设备内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-1-%E4%B8%BA%E6%8C%81%E4%B9%85%E8%AE%BF%E9%97%AE%E9%A2%84%E7%95%99L2%E7%BC%93%E5%AD%98"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">3.2.3.1 为持久访问预留L2缓存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-2-L2%E6%8C%81%E4%B9%85%E5%8C%96%E8%AE%BF%E9%97%AE%E7%AD%96%E7%95%A5"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">3.2.3.2 L2持久化访问策略</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-3-L2%E8%AE%BF%E9%97%AE%E5%B1%9E%E6%80%A7"><span class="nav-number">3.2.2.3.</span> <span class="nav-text">3.2.3.3 L2访问属性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-4-L2%E6%8C%81%E4%B9%85%E6%80%A7%E7%A4%BA%E4%BE%8B"><span class="nav-number">3.2.2.4.</span> <span class="nav-text">3.2.3.4 L2持久性示例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-5-%E5%B0%86L2-Access%E9%87%8D%E7%BD%AE%E4%B8%BANormal"><span class="nav-number">3.2.2.5.</span> <span class="nav-text">3.2.3.5 将L2 Access重置为Normal</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-6-%E7%AE%A1%E7%90%86L2%E9%A2%84%E7%95%99%E7%BC%93%E5%AD%98%E7%9A%84%E5%88%A9%E7%94%A8%E7%8E%87"><span class="nav-number">3.2.2.6.</span> <span class="nav-text">3.2.3.6 管理L2预留缓存的利用率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-7-%E6%9F%A5%E8%AF%A2L2%E7%BC%93%E5%AD%98%E5%B1%9E%E6%80%A7"><span class="nav-number">3.2.2.7.</span> <span class="nav-text">3.2.3.7 查询L2缓存属性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-8-%E6%8E%A7%E5%88%B6L2%E7%BC%93%E5%AD%98%E9%A2%84%E7%95%99%E5%A4%A7%E5%B0%8F%E7%94%A8%E4%BA%8E%E6%8C%81%E4%B9%85%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE"><span class="nav-number">3.2.2.8.</span> <span class="nav-text">3.2.3.8 控制L2缓存预留大小用于持久内存访问</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-4%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98"><span class="nav-number">3.2.3.</span> <span class="nav-text">3.2.4共享内存</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-5-Page-Locked%E4%B8%BB%E6%9C%BA%E5%86%85%E5%AD%98"><span class="nav-number">3.2.4.</span> <span class="nav-text">3.2.5 Page-Locked主机内存</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-5-1-Portable-Memory"><span class="nav-number">3.2.4.1.</span> <span class="nav-text">3.2.5.1 Portable Memory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-5-2-%E5%86%99%E5%90%88%E5%B9%B6%E5%86%85%E5%AD%98"><span class="nav-number">3.2.4.2.</span> <span class="nav-text">3.2.5.2 写合并内存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-5-3-Mapped-Memory"><span class="nav-number">3.2.4.3.</span> <span class="nav-text">3.2.5.3 Mapped Memory</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-6-%E5%BC%82%E6%AD%A5%E5%B9%B6%E5%8F%91%E6%89%A7%E8%A1%8C"><span class="nav-number">3.2.5.</span> <span class="nav-text">3.2.6 异步并发执行</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-6-1-%E4%B8%BB%E6%9C%BA%E5%92%8C%E8%AE%BE%E5%A4%87%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B9%B6%E5%8F%91%E6%89%A7%E8%A1%8C"><span class="nav-number">3.2.5.1.</span> <span class="nav-text">3.2.6.1 主机和设备之间的并发执行</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-6-2-%E5%B9%B6%E8%A1%8C%E6%89%A7%E8%A1%8C%E5%86%85%E6%A0%B8"><span class="nav-number">3.2.5.2.</span> <span class="nav-text">3.2.6.2 并行执行内核</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-6-3-%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93%E5%92%8C%E5%86%85%E6%A0%B8%E6%89%A7%E8%A1%8C%E7%9A%84%E9%87%8D%E5%8F%A0"><span class="nav-number">3.2.5.3.</span> <span class="nav-text">3.2.6.3 数据传输和内核执行的重叠</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-6-4-%E5%B9%B6%E8%A1%8C%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93"><span class="nav-number">3.2.5.4.</span> <span class="nav-text">3.2.6.4 并行数据传输</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-6-5-%E6%B5%81"><span class="nav-number">3.2.5.5.</span> <span class="nav-text">3.2.6.5 流</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-5-1-%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%94%80%E6%AF%81"><span class="nav-number">3.2.5.5.1.</span> <span class="nav-text">3.2.6.5.1 创建与销毁</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-5-2-%E9%BB%98%E8%AE%A4%E6%B5%81"><span class="nav-number">3.2.5.5.2.</span> <span class="nav-text">3.2.6.5.2 默认流</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-5-3-%E6%98%BE%E5%BC%8F%E5%90%8C%E6%AD%A5"><span class="nav-number">3.2.5.5.3.</span> <span class="nav-text">3.2.6.5.3 显式同步</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-6-5-4-%E9%9A%90%E5%BC%8F%E5%90%8C%E6%AD%A5"><span class="nav-number">3.2.5.6.</span> <span class="nav-text">3.2.6.5.4 隐式同步</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-5-5-%E9%87%8D%E5%8F%A0%E8%A1%8C%E4%B8%BA"><span class="nav-number">3.2.5.6.1.</span> <span class="nav-text">3.2.6.5.5 重叠行为</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-5-6-Host%E5%87%BD%E6%95%B0-%E5%9B%9E%E8%B0%83"><span class="nav-number">3.2.5.6.2.</span> <span class="nav-text">3.2.6.5.6 Host函数(回调)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-5-7-%E6%B5%81%E4%BC%98%E5%85%88%E7%BA%A7"><span class="nav-number">3.2.5.6.3.</span> <span class="nav-text">3.2.6.5.7 流优先级</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-6-6-CUDA%E5%9B%BE"><span class="nav-number">3.2.5.7.</span> <span class="nav-text">3.2.6.6 CUDA图</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-6-1%E5%9B%BE%E6%9E%B6%E6%9E%84"><span class="nav-number">3.2.5.7.1.</span> <span class="nav-text">3.2.6.6.1图架构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-6-1-1-%E8%8A%82%E7%82%B9%E7%B1%BB%E5%9E%8B"><span class="nav-number">3.2.5.7.2.</span> <span class="nav-text">3.2.6.6.1.1 节点类型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-6-2%E5%88%A9%E7%94%A8API%E5%88%9B%E5%BB%BA%E5%9B%BE"><span class="nav-number">3.2.5.7.3.</span> <span class="nav-text">3.2.6.6.2利用API创建图</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-6-3-%E4%BD%BF%E7%94%A8%E6%B5%81%E6%8D%95%E8%8E%B7%E5%88%9B%E5%BB%BA%E5%9B%BE"><span class="nav-number">3.2.5.7.4.</span> <span class="nav-text">3.2.6.6.3 使用流捕获创建图</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-6-3-1-%E8%B7%A8%E6%B5%81%E4%BE%9D%E8%B5%96%E6%80%A7%E5%92%8C%E4%BA%8B%E4%BB%B6"><span class="nav-number">3.2.5.7.5.</span> <span class="nav-text">3.2.6.6.3.1 跨流依赖性和事件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-6-3-2-%E7%A6%81%E6%AD%A2%E5%92%8C%E6%9C%AA%E5%A4%84%E7%90%86%E7%9A%84%E6%93%8D%E4%BD%9C"><span class="nav-number">3.2.5.7.6.</span> <span class="nav-text">3.2.6.6.3.2 禁止和未处理的操作</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-6-3-3%E5%A4%B1%E6%95%88"><span class="nav-number">3.2.5.7.7.</span> <span class="nav-text">3.2.6.6.3.3失效</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-6-4-%E6%9B%B4%E6%96%B0%E5%AE%9E%E4%BE%8B%E5%8C%96%E5%9B%BE"><span class="nav-number">3.2.5.7.8.</span> <span class="nav-text">3.2.6.6.4 更新实例化图</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-6-4-1-%E5%9B%BE%E6%9B%B4%E6%96%B0%E9%99%90%E5%88%B6"><span class="nav-number">3.2.5.7.9.</span> <span class="nav-text">3.2.6.6.4.1 图更新限制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-6-4-2%E5%85%A8%E5%9B%BE%E6%9B%B4%E6%96%B0"><span class="nav-number">3.2.5.7.10.</span> <span class="nav-text">3.2.6.6.4.2全图更新</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-6-4-3-%E5%8D%95%E4%B8%AA%E8%8A%82%E7%82%B9%E6%9B%B4%E6%96%B0"><span class="nav-number">3.2.5.7.11.</span> <span class="nav-text">3.2.6.6.4.3 单个节点更新</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-6-5-%E4%BD%BF%E7%94%A8%E5%9B%BEAPI"><span class="nav-number">3.2.5.7.12.</span> <span class="nav-text">3.2.6.6.5 使用图API</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-6-7-%E4%BA%8B%E4%BB%B6"><span class="nav-number">3.2.5.8.</span> <span class="nav-text">3.2.6.7 事件</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-7-1-%E5%88%9B%E5%BB%BA%E5%92%8C%E9%94%80%E6%AF%81"><span class="nav-number">3.2.5.8.1.</span> <span class="nav-text">3.2.6.7.1 创建和销毁</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-6-7-2-%E8%AE%A1%E7%AE%97%E6%97%B6%E9%97%B4"><span class="nav-number">3.2.5.8.2.</span> <span class="nav-text">3.2.6.7.2 计算时间</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-6-8%E5%90%8C%E6%AD%A5%E8%B0%83%E7%94%A8"><span class="nav-number">3.2.5.9.</span> <span class="nav-text">3.2.6.8同步调用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-7-%E5%A4%9A%E8%AE%BE%E5%A4%87%E7%B3%BB%E7%BB%9F"><span class="nav-number">3.2.6.</span> <span class="nav-text">3.2.7 多设备系统</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-7-1%E8%AE%BE%E5%A4%87%E6%9E%9A%E4%B8%BE"><span class="nav-number">3.2.6.1.</span> <span class="nav-text">3.2.7.1设备枚举</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-7-2-%E8%AE%BE%E5%A4%87%E9%80%89%E6%8B%A9"><span class="nav-number">3.2.6.2.</span> <span class="nav-text">3.2.7.2 设备选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-7-3-%E6%B5%81%E5%92%8C%E4%BA%8B%E4%BB%B6%E8%A1%8C%E4%B8%BA"><span class="nav-number">3.2.6.3.</span> <span class="nav-text">3.2.7.3 流和事件行为</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-7-4-Peer-to-Peer%E7%9A%84%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE"><span class="nav-number">3.2.6.4.</span> <span class="nav-text">3.2.7.4 Peer-to-Peer的内存访问</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-7-4-1-Linux%E4%B8%8A%E7%9A%84IOMMU"><span class="nav-number">3.2.6.4.1.</span> <span class="nav-text">3.2.7.4.1 Linux上的IOMMU</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-7-5-Peer-to-Peer%E5%86%85%E5%AD%98%E6%8B%B7%E8%B4%9D"><span class="nav-number">3.2.6.5.</span> <span class="nav-text">3.2.7.5 Peer-to-Peer内存拷贝</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%9F%E4%B8%80%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4"><span class="nav-number">3.2.7.</span> <span class="nav-text">统一虚拟地址空间</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-9-%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1"><span class="nav-number">3.2.8.</span> <span class="nav-text">3.2.9 进程间通信</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-10-%E9%94%99%E8%AF%AF%E6%A3%80%E6%9F%A5"><span class="nav-number">3.2.9.</span> <span class="nav-text">3.2.10 错误检查</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-11-%E8%B0%83%E7%94%A8%E6%A0%88"><span class="nav-number">3.2.10.</span> <span class="nav-text">3.2.11 调用栈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-12-%E7%BA%B9%E7%90%86%E5%86%85%E5%AD%98%E5%92%8C%E8%A1%A8%E9%9D%A2%E5%86%85%E5%AD%98-surface-memory"><span class="nav-number">3.2.11.</span> <span class="nav-text">3.2.12 纹理内存和表面内存(surface memory)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-12-1%E7%BA%B9%E7%90%86%E5%86%85%E5%AD%98"><span class="nav-number">3.2.11.1.</span> <span class="nav-text">3.2.12.1纹理内存</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-12-1-1-%E7%BA%B9%E7%90%86%E5%AF%B9%E8%B1%A1API"><span class="nav-number">3.2.11.1.1.</span> <span class="nav-text">3.2.12.1.1 纹理对象API</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-12-1-2-%E5%B7%B2%E5%BC%83%E7%94%A8-%E7%BA%B9%E7%90%86%E5%BC%95%E7%94%A8-API"><span class="nav-number">3.2.11.1.2.</span> <span class="nav-text">3.2.12.1.2 [[已弃用]] 纹理引用 API</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-12-1-3-16%E4%BD%8D%E6%B5%AE%E7%82%B9%E7%B1%BB%E5%9E%8B%E7%BA%B9%E7%90%86"><span class="nav-number">3.2.11.1.3.</span> <span class="nav-text">3.2.12.1.3 16位浮点类型纹理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-12-1-4-%E5%88%86%E5%B1%82%E7%BA%B9%E7%90%86"><span class="nav-number">3.2.11.1.4.</span> <span class="nav-text">3.2.12.1.4 分层纹理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-12-1-5-%E7%AB%8B%E6%96%B9%E4%BD%93%E7%BA%B9%E7%90%86-Cubemap-Textures"><span class="nav-number">3.2.11.1.5.</span> <span class="nav-text">3.2.12.1.5 立方体纹理(Cubemap Textures)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-12-1-6-%E5%88%86%E5%B1%82%E7%9A%84%E7%AB%8B%E6%96%B9%E4%BD%93%E7%BA%B9%E7%90%86%E5%86%85%E5%AD%98-Cubemap-Layered-Textures"><span class="nav-number">3.2.11.1.6.</span> <span class="nav-text">3.2.12.1.6 分层的立方体纹理内存(Cubemap Layered Textures)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-12-1-7-%E7%BA%B9%E7%90%86%E6%94%B6%E9%9B%86-Texture-Gather"><span class="nav-number">3.2.11.1.7.</span> <span class="nav-text">3.2.12.1.7 纹理收集(Texture Gather)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-12-2-%E8%A1%A8%E9%9D%A2%E5%86%85%E5%AD%98-Surface-Memory"><span class="nav-number">3.2.11.2.</span> <span class="nav-text">3.2.12.2 表面内存(Surface Memory)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-12-2-1-%E8%A1%A8%E9%9D%A2%E5%86%85%E5%AD%98%E5%AF%B9%E8%B1%A1API"><span class="nav-number">3.2.11.2.1.</span> <span class="nav-text">3.2.12.2.1 表面内存对象API</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-12-2-3-%E7%AB%8B%E6%96%B9%E4%BD%93%E8%A1%A8%E9%9D%A2%E5%86%85%E5%AD%98"><span class="nav-number">3.2.11.2.2.</span> <span class="nav-text">3.2.12.2.3 立方体表面内存</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-12-2-4-%E7%AB%8B%E6%96%B9%E4%BD%93%E5%88%86%E5%B1%82%E8%A1%A8%E9%9D%A2%E5%86%85%E5%AD%98"><span class="nav-number">3.2.11.2.3.</span> <span class="nav-text">3.2.12.2.4 立方体分层表面内存</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-12-3-CUDA-Array"><span class="nav-number">3.2.11.3.</span> <span class="nav-text">3.2.12.3 CUDA Array</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-12-4-%E8%AF%BB%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">3.2.11.4.</span> <span class="nav-text">3.2.12.4 读写一致性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-13%E5%9B%BE%E5%BD%A2%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">3.2.12.</span> <span class="nav-text">3.2.13图形一致性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-13-1-OpenGL-%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">3.2.12.1.</span> <span class="nav-text">3.2.13.1. OpenGL 一致性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%B7%E6%B3%A8%E6%84%8F%EF%BC%9A%E5%BD%93-OpenGL-%E7%BA%B9%E7%90%86%E8%AE%BE%E7%BD%AE%E4%B8%BA%E6%97%A0%E7%BB%91%E5%AE%9A%E6%97%B6%EF%BC%88%E4%BE%8B%E5%A6%82%EF%BC%8C%E9%80%9A%E8%BF%87%E4%BD%BF%E7%94%A8-glGetTextureHandle-glGetImageHandle-API-%E8%AF%B7%E6%B1%82%E5%9B%BE%E5%83%8F%E6%88%96%E7%BA%B9%E7%90%86%E5%8F%A5%E6%9F%84%EF%BC%89%EF%BC%8C%E5%AE%83%E4%B8%8D%E8%83%BD%E5%9C%A8-CUDA-%E4%B8%AD%E6%B3%A8%E5%86%8C%E3%80%82%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E9%9C%80%E8%A6%81%E5%9C%A8%E8%AF%B7%E6%B1%82%E5%9B%BE%E5%83%8F%E6%88%96%E7%BA%B9%E7%90%86%E5%8F%A5%E6%9F%84%E4%B9%8B%E5%89%8D%E6%B3%A8%E5%86%8C%E7%BA%B9%E7%90%86%E4%BB%A5%E8%BF%9B%E8%A1%8C%E4%BA%92%E6%93%8D%E4%BD%9C%E3%80%82"><span class="nav-number">3.2.12.2.</span> <span class="nav-text">请注意：当 OpenGL 纹理设置为无绑定时（例如，通过使用 glGetTextureHandle*&#x2F;glGetImageHandle* API 请求图像或纹理句柄），它不能在 CUDA 中注册。应用程序需要在请求图像或纹理句柄之前注册纹理以进行互操作。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-13-2-Direct3D-%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">3.2.12.3.</span> <span class="nav-text">3.2.13.2. Direct3D 一致性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-13-3-SLI%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">3.2.13.</span> <span class="nav-text">3.2.13.3 SLI一致性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-14-%E6%89%A9%E5%B1%95%E8%B5%84%E6%BA%90%E4%B8%80%E8%87%B4%E6%80%A7"><span class="nav-number">3.2.14.</span> <span class="nav-text">3.2.14 扩展资源一致性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-15-CUDA%E7%94%A8%E6%88%B7%E5%AF%B9%E8%B1%A1"><span class="nav-number">3.2.15.</span> <span class="nav-text">3.2.15 CUDA用户对象</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-%E7%89%88%E6%9C%AC%E5%92%8C%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 版本和兼容性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-Compute-Modes"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 Compute Modes</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-5-%E6%A8%A1%E5%BC%8F%E5%88%87%E6%8D%A2"><span class="nav-number">3.5.</span> <span class="nav-text">3.5 模式切换</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-6-%E5%9C%A8Windows%E4%B8%8A%E7%9A%84Tesla%E8%AE%A1%E7%AE%97%E9%9B%86%E7%BE%A4"><span class="nav-number">3.6.</span> <span class="nav-text">3.6 在Windows上的Tesla计算集群</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E7%A1%AC%E4%BB%B6%E5%AE%9E%E7%8E%B0"><span class="nav-number">4.</span> <span class="nav-text">第四章 硬件实现</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-SIMT-%E6%9E%B6%E6%9E%84"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 SIMT 架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-%E7%A1%AC%E4%BB%B6%E5%A4%9A%E7%BA%BF%E7%A8%8B"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 硬件多线程</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-%E6%80%A7%E8%83%BD%E6%8C%87%E5%8D%97"><span class="nav-number">5.</span> <span class="nav-text">第五章 性能指南</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-%E6%95%B4%E4%BD%93%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 整体性能优化策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-%E6%9C%80%E5%A4%A7%E5%8C%96%E5%88%A9%E7%94%A8%E7%8E%87"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 最大化利用率</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-1-%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E5%B1%82%E6%AC%A1"><span class="nav-number">5.2.1.</span> <span class="nav-text">5.2.1 应用程序层次</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-2-%E8%AE%BE%E5%A4%87%E5%B1%82%E6%AC%A1"><span class="nav-number">5.2.2.</span> <span class="nav-text">5.2.2 设备层次</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-3-%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E5%B1%82%E6%AC%A1"><span class="nav-number">5.2.3.</span> <span class="nav-text">5.2.3 多处理器层次</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-3-1-%E5%8D%A0%E7%94%A8%E7%8E%87%E8%AE%A1%E7%AE%97"><span class="nav-number">5.2.3.1.</span> <span class="nav-text">5.2.3.1 占用率计算</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-3-%E6%9C%80%E5%A4%A7%E5%8C%96%E5%AD%98%E5%82%A8%E5%90%9E%E5%90%90%E9%87%8F"><span class="nav-number">5.3.</span> <span class="nav-text">5.3 最大化存储吞吐量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-1-%E8%AE%BE%E5%A4%87%E4%B8%8E%E4%B8%BB%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93"><span class="nav-number">5.3.1.</span> <span class="nav-text">5.3.1 设备与主机之间的数据传输</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-2-%E8%AE%BE%E5%A4%87%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE"><span class="nav-number">5.3.2.</span> <span class="nav-text">5.3.2 设备内存访问</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-4%E6%9C%80%E5%A4%A7%E5%8C%96%E6%8C%87%E4%BB%A4%E5%90%9E%E5%90%90%E9%87%8F"><span class="nav-number">5.4.</span> <span class="nav-text">5.4最大化指令吞吐量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-1-%E7%AE%97%E6%95%B0%E6%8C%87%E4%BB%A4"><span class="nav-number">5.4.1.</span> <span class="nav-text">5.4.1 算数指令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-2-%E6%8E%A7%E5%88%B6%E6%B5%81%E6%8C%87%E4%BB%A4"><span class="nav-number">5.4.2.</span> <span class="nav-text">5.4.2 控制流指令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-3-%E5%90%8C%E6%AD%A5%E6%8C%87%E4%BB%A4"><span class="nav-number">5.4.3.</span> <span class="nav-text">5.4.3 同步指令</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-5%E6%9C%80%E5%B0%8F%E5%8C%96%E5%86%85%E5%AD%98%E6%8A%96%E5%8A%A8"><span class="nav-number">5.5.</span> <span class="nav-text">5.5最小化内存抖动</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95A-%E6%94%AF%E6%8C%81GPU%E8%AE%BE%E5%A4%87%E5%88%97%E8%A1%A8"><span class="nav-number">6.</span> <span class="nav-text">附录A 支持GPU设备列表</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95B-%E5%AF%B9C-%E6%89%A9%E5%B1%95%E7%9A%84%E8%AF%A6%E7%BB%86%E6%8F%8F%E8%BF%B0"><span class="nav-number">7.</span> <span class="nav-text">附录B 对C++扩展的详细描述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#B-1-%E5%87%BD%E6%95%B0%E6%89%A7%E8%A1%8C%E7%A9%BA%E9%97%B4%E8%AF%B4%E6%98%8E%E7%AC%A6"><span class="nav-number">7.1.</span> <span class="nav-text">B.1 函数执行空间说明符</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-1-1-global"><span class="nav-number">7.1.1.</span> <span class="nav-text">B.1.1 __global__</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-1-2-device"><span class="nav-number">7.1.2.</span> <span class="nav-text">B.1.2 __device__</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-1-3-host"><span class="nav-number">7.1.3.</span> <span class="nav-text">B.1.3 __host__</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-1-4-Undefined-behavior"><span class="nav-number">7.1.4.</span> <span class="nav-text">B.1.4 Undefined behavior</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B-1-5-noinline-and-forceinline"><span class="nav-number">7.1.4.1.</span> <span class="nav-text">B.1.5 __noinline__ and __forceinline__</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-2-Variable-Memory-Space-Specifiers"><span class="nav-number">7.2.</span> <span class="nav-text">B.2 Variable Memory Space Specifiers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-2-1-device"><span class="nav-number">7.2.1.</span> <span class="nav-text">B.2.1 __device__</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-2-2-constant"><span class="nav-number">7.2.2.</span> <span class="nav-text">B.2.2. __constant__</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-2-3-shared"><span class="nav-number">7.2.3.</span> <span class="nav-text">B.2.3 __shared__</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-2-4-managed"><span class="nav-number">7.2.4.</span> <span class="nav-text">B.2.4. managed</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-2-5-restrict"><span class="nav-number">7.2.5.</span> <span class="nav-text">B.2.5. restrict</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-3-Built-in-Vector-Types"><span class="nav-number">7.3.</span> <span class="nav-text">B.3. Built-in Vector Types</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-3-1-char-short-int-long-longlong-float-double"><span class="nav-number">7.3.1.</span> <span class="nav-text">B.3.1. char, short, int, long, longlong, float, double</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-3-2-dim3"><span class="nav-number">7.3.2.</span> <span class="nav-text">B.3.2. dim3</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-4-Built-in-Variables"><span class="nav-number">7.4.</span> <span class="nav-text">B.4. Built-in Variables</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-4-1-gridDim"><span class="nav-number">7.4.1.</span> <span class="nav-text">B.4.1. gridDim</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-4-2-blockIdx"><span class="nav-number">7.4.2.</span> <span class="nav-text">B.4.2. blockIdx</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-4-3-blockDim"><span class="nav-number">7.4.3.</span> <span class="nav-text">B.4.3. blockDim</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-4-4-threadIdx"><span class="nav-number">7.4.4.</span> <span class="nav-text">B.4.4. threadIdx</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-4-5-warpSize"><span class="nav-number">7.4.5.</span> <span class="nav-text">B.4.5. warpSize</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-5-Memory-Fence-Functions"><span class="nav-number">7.5.</span> <span class="nav-text">B.5. Memory Fence Functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-6-Synchronization-Functions"><span class="nav-number">7.6.</span> <span class="nav-text">B.6. Synchronization Functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-7-Mathematical-Functions"><span class="nav-number">7.7.</span> <span class="nav-text">B.7. Mathematical Functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-8-Texture-Functions"><span class="nav-number">7.8.</span> <span class="nav-text">B.8. Texture Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-8-1-Texture-Object-API"><span class="nav-number">7.8.1.</span> <span class="nav-text">B.8.1. Texture Object API</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-1-tex1Dfetch"><span class="nav-number">7.8.1.1.</span> <span class="nav-text">B.8.1.1. tex1Dfetch()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-2%E3%80%82-tex1D"><span class="nav-number">7.8.1.2.</span> <span class="nav-text">B.8.1.2。 tex1D()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-3%E3%80%82-tex1DLod"><span class="nav-number">7.8.1.3.</span> <span class="nav-text">B.8.1.3。 tex1DLod()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-4%E3%80%82-tex1DGrad"><span class="nav-number">7.8.1.4.</span> <span class="nav-text">B.8.1.4。 tex1DGrad()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-5%E3%80%82-tex2D"><span class="nav-number">7.8.1.5.</span> <span class="nav-text">B.8.1.5。 tex2D()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-6%E3%80%82-tex2DLod"><span class="nav-number">7.8.1.6.</span> <span class="nav-text">B.8.1.6。 tex2DLod()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-7%E3%80%82-tex2DGrad"><span class="nav-number">7.8.1.7.</span> <span class="nav-text">B.8.1.7。 tex2DGrad()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-8%E3%80%82-tex3D"><span class="nav-number">7.8.1.8.</span> <span class="nav-text">B.8.1.8。 tex3D()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-9%E3%80%82-tex3DLod"><span class="nav-number">7.8.1.9.</span> <span class="nav-text">B.8.1.9。 tex3DLod()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-10%E3%80%82-tex3DGrad"><span class="nav-number">7.8.1.10.</span> <span class="nav-text">B.8.1.10。 tex3DGrad()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-11%E3%80%82-tex1DLlayered"><span class="nav-number">7.8.1.11.</span> <span class="nav-text">B.8.1.11。 tex1DLlayered()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-12%E3%80%82-tex1DLlayeredLod"><span class="nav-number">7.8.1.12.</span> <span class="nav-text">B.8.1.12。 tex1DLlayeredLod()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-13%E3%80%82-tex1DLlayeredGrad"><span class="nav-number">7.8.1.13.</span> <span class="nav-text">B.8.1.13。 tex1DLlayeredGrad()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-14%E3%80%82-tex2DLlayered"><span class="nav-number">7.8.1.14.</span> <span class="nav-text">B.8.1.14。 tex2DLlayered()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-15%E3%80%82-tex2DLlayeredLod"><span class="nav-number">7.8.1.15.</span> <span class="nav-text">B.8.1.15。 tex2DLlayeredLod()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-16%E3%80%82-tex2DLlayeredGrad"><span class="nav-number">7.8.1.16.</span> <span class="nav-text">B.8.1.16。 tex2DLlayeredGrad()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-17%E3%80%82-texCubemap"><span class="nav-number">7.8.1.17.</span> <span class="nav-text">B.8.1.17。 texCubemap()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-18%E3%80%82-texCubemapLod"><span class="nav-number">7.8.1.18.</span> <span class="nav-text">B.8.1.18。 texCubemapLod()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-19%E3%80%82-texCubemapLayered"><span class="nav-number">7.8.1.19.</span> <span class="nav-text">B.8.1.19。 texCubemapLayered()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-20%E3%80%82-texCubemapLayeredLod"><span class="nav-number">7.8.1.20.</span> <span class="nav-text">B.8.1.20。 texCubemapLayeredLod()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-1-21%E3%80%82-tex2Dgather"><span class="nav-number">7.8.1.21.</span> <span class="nav-text">B.8.1.21。 tex2Dgather()</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-8-2-Texture-Reference-API"><span class="nav-number">7.8.2.</span> <span class="nav-text">B.8.2. Texture Reference API</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B-8-2-1-tex1Dfetch"><span class="nav-number">7.8.2.1.</span> <span class="nav-text">B.8.2.1. tex1Dfetch()</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-9-Surface-Functions"><span class="nav-number">7.9.</span> <span class="nav-text">B.9. Surface Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-9-1-Surface-Object-API"><span class="nav-number">7.9.1.</span> <span class="nav-text">B.9.1. Surface Object API</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-1-1-surf1Dread"><span class="nav-number">7.9.1.1.</span> <span class="nav-text">B.9.1.1. surf1Dread()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-1-2-surf1Dwrite"><span class="nav-number">7.9.1.2.</span> <span class="nav-text">B.9.1.2. surf1Dwrite</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-1-3-surf2Dread"><span class="nav-number">7.9.1.3.</span> <span class="nav-text">B.9.1.3. surf2Dread()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-1-4-surf2Dwrite"><span class="nav-number">7.9.1.4.</span> <span class="nav-text">B.9.1.4 surf2Dwrite()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-1-5-surf3Dread"><span class="nav-number">7.9.1.5.</span> <span class="nav-text">B.9.1.5. surf3Dread()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-1-6-surf3Dwrite"><span class="nav-number">7.9.1.6.</span> <span class="nav-text">B.9.1.6. surf3Dwrite()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-1-7-surf1DLayeredread"><span class="nav-number">7.9.1.7.</span> <span class="nav-text">B.9.1.7. surf1DLayeredread()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-1-8-surf1DLayeredwrite"><span class="nav-number">7.9.1.8.</span> <span class="nav-text">B.9.1.8. surf1DLayeredwrite()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-1-9-surf2DLayeredread"><span class="nav-number">7.9.1.9.</span> <span class="nav-text">B.9.1.9. surf2DLayeredread()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-1-10-surf2DLayeredwrite"><span class="nav-number">7.9.1.10.</span> <span class="nav-text">B.9.1.10. surf2DLayeredwrite()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-1-11-surfCubemapread"><span class="nav-number">7.9.1.11.</span> <span class="nav-text">B.9.1.11. surfCubemapread()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-1-12-surfCubemapwrite"><span class="nav-number">7.9.1.12.</span> <span class="nav-text">B.9.1.12. surfCubemapwrite()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-1-13-surfCubemapLayeredread"><span class="nav-number">7.9.1.13.</span> <span class="nav-text">B.9.1.13. surfCubemapLayeredread()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-1-14-surfCubemapLayeredwrite"><span class="nav-number">7.9.1.14.</span> <span class="nav-text">B.9.1.14. surfCubemapLayeredwrite()</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-9-2-Surface-Reference-API"><span class="nav-number">7.9.2.</span> <span class="nav-text">B.9.2. Surface Reference API</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-2-1-surf1Dread"><span class="nav-number">7.9.2.1.</span> <span class="nav-text">B.9.2.1. surf1Dread()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-2-2-surf1Dwrite"><span class="nav-number">7.9.2.2.</span> <span class="nav-text">B.9.2.2. surf1Dwrite</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-2-3-surf2Dread"><span class="nav-number">7.9.2.3.</span> <span class="nav-text">B.9.2.3. surf2Dread()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-2-4-surf2Dwrite"><span class="nav-number">7.9.2.4.</span> <span class="nav-text">B.9.2.4. surf2Dwrite()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-2-5-surf3Dread"><span class="nav-number">7.9.2.5.</span> <span class="nav-text">B.9.2.5. surf3Dread()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-2-6-surf3Dwrite"><span class="nav-number">7.9.2.6.</span> <span class="nav-text">B.9.2.6. surf3Dwrite()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-2-7-surf1DLayeredread"><span class="nav-number">7.9.2.7.</span> <span class="nav-text">B.9.2.7. surf1DLayeredread()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-2-8-surf1DLayeredwrite"><span class="nav-number">7.9.2.8.</span> <span class="nav-text">B.9.2.8. surf1DLayeredwrite()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-2-9-surf2DLayeredread"><span class="nav-number">7.9.2.9.</span> <span class="nav-text">B.9.2.9. surf2DLayeredread()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-2-10-surf2DLayeredwrite"><span class="nav-number">7.9.2.10.</span> <span class="nav-text">B.9.2.10. surf2DLayeredwrite()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-2-11-surfCubemapread"><span class="nav-number">7.9.2.11.</span> <span class="nav-text">B.9.2.11. surfCubemapread()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-2-12-surfCubemapwrite"><span class="nav-number">7.9.2.12.</span> <span class="nav-text">B.9.2.12. surfCubemapwrite()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-9-2-14-surfCubemapLayeredwrite"><span class="nav-number">7.9.2.13.</span> <span class="nav-text">B.9.2.14. surfCubemapLayeredwrite()</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-10-Read-Only-Data-Cache-Load-Function"><span class="nav-number">7.10.</span> <span class="nav-text">B.10. Read-Only Data Cache Load Function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-11-Load-Functions-Using-Cache-Hints"><span class="nav-number">7.11.</span> <span class="nav-text">B.11. Load Functions Using Cache Hints</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-12-Store-Functions-Using-Cache-Hints"><span class="nav-number">7.12.</span> <span class="nav-text">B.12. Store Functions Using Cache Hints</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-13-Time-Function"><span class="nav-number">7.13.</span> <span class="nav-text">B.13. Time Function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-14-Atomic-Functions"><span class="nav-number">7.14.</span> <span class="nav-text">B.14. Atomic Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-14-1-Arithmetic-Functions"><span class="nav-number">7.14.1.</span> <span class="nav-text">B.14.1. Arithmetic Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B-14-1-1-atomicAdd"><span class="nav-number">7.14.1.1.</span> <span class="nav-text">B.14.1.1. atomicAdd()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-14-1-2-atomicSub"><span class="nav-number">7.14.1.2.</span> <span class="nav-text">B.14.1.2. atomicSub()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-14-1-3-atomicExch"><span class="nav-number">7.14.1.3.</span> <span class="nav-text">B.14.1.3. atomicExch()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-14-1-4-atomicMin"><span class="nav-number">7.14.1.4.</span> <span class="nav-text">B.14.1.4. atomicMin()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-14-1-5-atomicMax"><span class="nav-number">7.14.1.5.</span> <span class="nav-text">B.14.1.5. atomicMax()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-14-1-6-atomicInc"><span class="nav-number">7.14.1.6.</span> <span class="nav-text">B.14.1.6. atomicInc()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-14-1-7-atomicDec"><span class="nav-number">7.14.1.7.</span> <span class="nav-text">B.14.1.7. atomicDec()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-14-1-8-atomicCAS"><span class="nav-number">7.14.1.8.</span> <span class="nav-text">B.14.1.8. atomicCAS()</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-14-2-Bitwise-Functions"><span class="nav-number">7.14.2.</span> <span class="nav-text">B.14.2. Bitwise Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B-14-2-1-atomicAnd"><span class="nav-number">7.14.2.1.</span> <span class="nav-text">B.14.2.1. atomicAnd()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-14-2-2-atomicOr"><span class="nav-number">7.14.2.2.</span> <span class="nav-text">B.14.2.2. atomicOr()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-14-2-3-atomicXor"><span class="nav-number">7.14.2.3.</span> <span class="nav-text">B.14.2.3. atomicXor()</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-15-Address-Space-Predicate-Functions"><span class="nav-number">7.15.</span> <span class="nav-text">B.15. Address Space Predicate Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-15-1-isGlobal"><span class="nav-number">7.15.1.</span> <span class="nav-text">B.15.1. __isGlobal()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-15-2-isShared"><span class="nav-number">7.15.2.</span> <span class="nav-text">B.15.2. __isShared()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-15-3-isConstant"><span class="nav-number">7.15.3.</span> <span class="nav-text">B.15.3. __isConstant()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-15-4-isLocal"><span class="nav-number">7.15.4.</span> <span class="nav-text">B.15.4. __isLocal()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-16-Address-Space-Conversion-Functions"><span class="nav-number">7.16.</span> <span class="nav-text">B.16. Address Space Conversion Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-16-1-cvta-generic-to-global"><span class="nav-number">7.16.1.</span> <span class="nav-text">B.16.1. __cvta_generic_to_global()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-16-2-cvta-generic-to-shared"><span class="nav-number">7.16.2.</span> <span class="nav-text">B.16.2. __cvta_generic_to_shared()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-16-3-cvta-generic-to-constant"><span class="nav-number">7.16.3.</span> <span class="nav-text">B.16.3. __cvta_generic_to_constant()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-16-4-cvta-generic-to-local"><span class="nav-number">7.16.4.</span> <span class="nav-text">B.16.4. __cvta_generic_to_local()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-16-5-cvta-global-to-generic"><span class="nav-number">7.16.5.</span> <span class="nav-text">B.16.5. __cvta_global_to_generic()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-16-6-cvta-shared-to-generic"><span class="nav-number">7.16.6.</span> <span class="nav-text">B.16.6. __cvta_shared_to_generic()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-16-7-cvta-constant-to-generic"><span class="nav-number">7.16.7.</span> <span class="nav-text">B.16.7. __cvta_constant_to_generic()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-16-8-cvta-local-to-generic"><span class="nav-number">7.16.8.</span> <span class="nav-text">B.16.8. __cvta_local_to_generic()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-17-Alloca-Function"><span class="nav-number">7.17.</span> <span class="nav-text">B.17. Alloca Function</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-17-1-Synopsis"><span class="nav-number">7.17.1.</span> <span class="nav-text">B.17.1. Synopsis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-17-2-Description"><span class="nav-number">7.17.2.</span> <span class="nav-text">B.17.2. Description</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-17-3-Example"><span class="nav-number">7.17.3.</span> <span class="nav-text">B.17.3. Example</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-18-Compiler-Optimization-Hint-Functions"><span class="nav-number">7.18.</span> <span class="nav-text">B.18. Compiler Optimization Hint Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-18-1-builtin-assume-aligned"><span class="nav-number">7.18.1.</span> <span class="nav-text">B.18.1. __builtin_assume_aligned()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-18-2-builtin-assume"><span class="nav-number">7.18.2.</span> <span class="nav-text">B.18.2. __builtin_assume()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-18-3-assume"><span class="nav-number">7.18.3.</span> <span class="nav-text">B.18.3. __assume()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-18-4-builtin-expect"><span class="nav-number">7.18.4.</span> <span class="nav-text">B.18.4. __builtin_expect()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-18-5-builtin-unreachable"><span class="nav-number">7.18.5.</span> <span class="nav-text">B.18.5. __builtin_unreachable()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-18-6-Restrictions"><span class="nav-number">7.18.6.</span> <span class="nav-text">B.18.6. Restrictions</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-19-Warp-Vote-Functions"><span class="nav-number">7.19.</span> <span class="nav-text">B.19. Warp Vote Functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-20-Warp-Match-Functions"><span class="nav-number">7.20.</span> <span class="nav-text">B.20. Warp Match Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-20-1-Synopsis"><span class="nav-number">7.20.1.</span> <span class="nav-text">B.20.1. Synopsis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-20-2-Description"><span class="nav-number">7.20.2.</span> <span class="nav-text">B.20.2. Description</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-21-Warp-Reduce-Functions"><span class="nav-number">7.21.</span> <span class="nav-text">B.21. Warp Reduce Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-21-1-Synopsis"><span class="nav-number">7.21.1.</span> <span class="nav-text">B.21.1. Synopsis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-21-2-Description"><span class="nav-number">7.21.2.</span> <span class="nav-text">B.21.2. Description</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-22-Warp-Shuffle-Functions"><span class="nav-number">7.22.</span> <span class="nav-text">B.22. Warp Shuffle Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-22-1-Synopsis"><span class="nav-number">7.22.1.</span> <span class="nav-text">B.22.1. Synopsis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-22-2-Description"><span class="nav-number">7.22.2.</span> <span class="nav-text">B.22.2. Description</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-22-3-Notes"><span class="nav-number">7.22.3.</span> <span class="nav-text">B.22.3. Notes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-22-4-Examples"><span class="nav-number">7.22.4.</span> <span class="nav-text">B.22.4. Examples</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B-22-4-1-Broadcast-of-a-single-value-across-a-warp"><span class="nav-number">7.22.4.1.</span> <span class="nav-text">B.22.4.1. Broadcast of a single value across a warp</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-22-4-3-Reduction-across-a-warp"><span class="nav-number">7.22.4.2.</span> <span class="nav-text">B.22.4.3. Reduction across a warp</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-23-Nanosleep-Function"><span class="nav-number">7.23.</span> <span class="nav-text">B.23. Nanosleep Function</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-23-1-Synopsis"><span class="nav-number">7.23.1.</span> <span class="nav-text">B.23.1. Synopsis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-23-2-Description"><span class="nav-number">7.23.2.</span> <span class="nav-text">B.23.2. Description</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-23-3-Example"><span class="nav-number">7.24.</span> <span class="nav-text">B.23.3. Example</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-24-Warp-matrix-functions"><span class="nav-number">7.25.</span> <span class="nav-text">B.24. Warp matrix functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-24-1-Description"><span class="nav-number">7.25.1.</span> <span class="nav-text">B.24.1. Description</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-24-2-Alternate-Floating-Point"><span class="nav-number">7.25.2.</span> <span class="nav-text">B.24.2. Alternate Floating Point</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-24-3-Double-Precision"><span class="nav-number">7.25.3.</span> <span class="nav-text">B.24.3. Double Precision</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-24-4-Sub-byte-Operations"><span class="nav-number">7.25.4.</span> <span class="nav-text">B.24.4. Sub-byte Operations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-24-5-Restrictions"><span class="nav-number">7.25.5.</span> <span class="nav-text">B.24.5. Restrictions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-24-6-Element-Types-amp-Matrix-Sizes"><span class="nav-number">7.25.6.</span> <span class="nav-text">B.24.6. Element Types &amp; Matrix Sizes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-24-7-Example"><span class="nav-number">7.25.7.</span> <span class="nav-text">B.24.7. Example</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-25-Asynchronous-Barrier"><span class="nav-number">7.26.</span> <span class="nav-text">B.25. Asynchronous Barrier</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-25-1-Simple-Synchronization-Pattern"><span class="nav-number">7.26.1.</span> <span class="nav-text">B.25.1. Simple Synchronization Pattern</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-25-2-Temporal-Splitting-and-Five-Stages-of-Synchronization"><span class="nav-number">7.26.2.</span> <span class="nav-text">B.25.2. Temporal Splitting and Five Stages of Synchronization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-25-3-Bootstrap-Initialization-Expected-Arrival-Count-and-Participation"><span class="nav-number">7.26.3.</span> <span class="nav-text">B.25.3. Bootstrap Initialization, Expected Arrival Count, and Participation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-25-4-A-Barrier%E2%80%99s-Phase-Arrival-Countdown-Completion-and-Reset"><span class="nav-number">7.26.4.</span> <span class="nav-text">B.25.4. A Barrier’s Phase: Arrival, Countdown, Completion, and Reset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-25-5-Spatial-Partitioning-also-known-as-Warp-Specialization"><span class="nav-number">7.26.5.</span> <span class="nav-text">B.25.5. Spatial Partitioning (also known as Warp Specialization)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-25-6-Early-Exit-Dropping-out-of-Participation"><span class="nav-number">7.26.6.</span> <span class="nav-text">B.25.6. Early Exit (Dropping out of Participation)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-25-7-Memory-Barrier-Primitives-Interface"><span class="nav-number">7.26.7.</span> <span class="nav-text">B.25.7. Memory Barrier Primitives Interface</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B-25-7-1-Data-Types"><span class="nav-number">7.26.7.1.</span> <span class="nav-text">B.25.7.1. Data Types</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-25-7-2-Memory-Barrier-Primitives-API"><span class="nav-number">7.26.7.2.</span> <span class="nav-text">B.25.7.2. Memory Barrier Primitives API</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-26-Asynchronous-Data-Copies"><span class="nav-number">7.27.</span> <span class="nav-text">B.26. Asynchronous Data Copies</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-26-1-memcpy-async-API"><span class="nav-number">7.27.1.</span> <span class="nav-text">B.26.1. memcpy_async API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-26-2-Copy-and-Compute-Pattern-Staging-Data-Through-Shared-Memory"><span class="nav-number">7.27.2.</span> <span class="nav-text">B.26.2. Copy and Compute Pattern - Staging Data Through Shared Memory</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-26-3-Without-memcpy-async"><span class="nav-number">7.27.3.</span> <span class="nav-text">B.26.3. Without memcpy_async</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-26-4-With-memcpy-async"><span class="nav-number">7.27.4.</span> <span class="nav-text">B.26.4. With memcpy_async</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-26-5-Asynchronous-Data-Copies-using-cuda-barrier"><span class="nav-number">7.27.5.</span> <span class="nav-text">B.26.5. Asynchronous Data Copies using cuda::barrier</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-26-6-Performance-Guidance-for-memcpy-async"><span class="nav-number">7.27.6.</span> <span class="nav-text">B.26.6. Performance Guidance for memcpy_async</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B-26-6-1-Alignment"><span class="nav-number">7.27.6.1.</span> <span class="nav-text">B.26.6.1. Alignment</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-26-6-2-Trivially-copyable"><span class="nav-number">7.27.6.2.</span> <span class="nav-text">B.26.6.2. Trivially copyable</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-26-6-3-Warp-Entanglement-Commit"><span class="nav-number">7.27.6.3.</span> <span class="nav-text">B.26.6.3. Warp Entanglement - Commit</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-26-6-4-Warp-Entanglement-Wait"><span class="nav-number">7.27.6.4.</span> <span class="nav-text">B.26.6.4. Warp Entanglement - Wait</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-26-6-5-Warp-Entanglement-Arrive-On"><span class="nav-number">7.27.6.5.</span> <span class="nav-text">B.26.6.5. Warp Entanglement - Arrive-On</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-26-6-6-Keep-Commit-and-Arrive-On-Operations-Converged"><span class="nav-number">7.27.6.6.</span> <span class="nav-text">B.26.6.6. Keep Commit and Arrive-On Operations Converged</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-27-Asynchronous-Data-Copies-using-cuda-pipeline"><span class="nav-number">7.28.</span> <span class="nav-text">B.27. Asynchronous Data Copies using cuda::pipeline</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-27-1-Single-Stage-Asynchronous-Data-Copies-using-cuda-pipeline"><span class="nav-number">7.28.1.</span> <span class="nav-text">B.27.1. Single-Stage Asynchronous Data Copies using cuda::pipeline</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-27-3-Pipeline-Interface"><span class="nav-number">7.28.2.</span> <span class="nav-text">B.27.3. Pipeline Interface</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-27-4-Pipeline-Primitives-Interface"><span class="nav-number">7.28.3.</span> <span class="nav-text">B.27.4. Pipeline Primitives Interface</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-27-4-1-memcpy-async-Primitive"><span class="nav-number">7.28.4.</span> <span class="nav-text">B.27.4.1. memcpy_async Primitive</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B-27-4-2-Commit-Primitive"><span class="nav-number">7.28.4.1.</span> <span class="nav-text">B.27.4.2. Commit Primitive</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-27-4-3-Wait-Primitive"><span class="nav-number">7.28.4.2.</span> <span class="nav-text">B.27.4.3. Wait Primitive</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-27-4-4-Arrive-On-Barrier-Primitive"><span class="nav-number">7.28.4.3.</span> <span class="nav-text">B.27.4.4. Arrive On Barrier Primitive</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-28-Profiler-Counter-Function"><span class="nav-number">7.29.</span> <span class="nav-text">B.28. Profiler Counter Function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-29-Assertion"><span class="nav-number">7.30.</span> <span class="nav-text">B.29. Assertion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-30-Trap-function"><span class="nav-number">7.31.</span> <span class="nav-text">B.30. Trap function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-31-Breakpoint-Function"><span class="nav-number">7.32.</span> <span class="nav-text">B.31. Breakpoint Function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-32-Formatted-Output"><span class="nav-number">7.33.</span> <span class="nav-text">B.32. Formatted Output</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-32-1-Format-Specifiers"><span class="nav-number">7.33.1.</span> <span class="nav-text">B.32.1. Format Specifiers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-32-2-Limitations"><span class="nav-number">7.33.2.</span> <span class="nav-text">B.32.2. Limitations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-32-3-Associated-Host-Side-API"><span class="nav-number">7.33.3.</span> <span class="nav-text">B.32.3. Associated Host-Side API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-32-4-Examples"><span class="nav-number">7.33.4.</span> <span class="nav-text">B.32.4. Examples</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-33-Dynamic-Global-Memory-Allocation-and-Operations"><span class="nav-number">7.34.</span> <span class="nav-text">B.33. Dynamic Global Memory Allocation and Operations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#B-33-1-Heap-Memory-Allocation"><span class="nav-number">7.34.1.</span> <span class="nav-text">B.33.1. Heap Memory Allocation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-33-2-Interoperability-with-Host-Memory-API"><span class="nav-number">7.34.2.</span> <span class="nav-text">B.33.2. Interoperability with Host Memory API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-33-3-Examples"><span class="nav-number">7.34.3.</span> <span class="nav-text">B.33.3. Examples</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B-33-3-1-Per-Thread-Allocation"><span class="nav-number">7.34.3.1.</span> <span class="nav-text">B.33.3.1. Per Thread Allocation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-33-3-2-Per-Thread-Block-Allocation"><span class="nav-number">7.34.3.2.</span> <span class="nav-text">B.33.3.2. Per Thread Block Allocation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#B-33-3-3-Allocation-Persisting-Between-Kernel-Launches"><span class="nav-number">7.34.3.3.</span> <span class="nav-text">B.33.3.3. Allocation Persisting Between Kernel Launches</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-34-Execution-Configuration"><span class="nav-number">7.35.</span> <span class="nav-text">B.34. Execution Configuration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-35-Launch-Bounds"><span class="nav-number">7.36.</span> <span class="nav-text">B.35. Launch Bounds</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-36-pragma-unroll"><span class="nav-number">7.37.</span> <span class="nav-text">B.36. #pragma unroll</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-37-SIMD-Video-Instructions"><span class="nav-number">7.38.</span> <span class="nav-text">B.37. SIMD Video Instructions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#B-38-Diagnostic-Pragmas"><span class="nav-number">7.39.</span> <span class="nav-text">B.38. Diagnostic Pragmas</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95C-%E5%8D%8F%E4%BD%9C%E7%BB%84"><span class="nav-number">8.</span> <span class="nav-text">附录C 协作组</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#C-1-Introduction"><span class="nav-number">8.1.</span> <span class="nav-text">C.1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C-2-What%E2%80%99s-New-in-CUDA-11-0"><span class="nav-number">8.2.</span> <span class="nav-text">C.2. What’s New in CUDA 11.0</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C-3-Programming-Model-Concept"><span class="nav-number">8.3.</span> <span class="nav-text">C.3. Programming Model Concept</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#C-3-1-Composition-Example"><span class="nav-number">8.3.1.</span> <span class="nav-text">C.3.1. Composition Example</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C-4-Group-Types"><span class="nav-number">8.4.</span> <span class="nav-text">C.4. Group Types</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#C-4-1-Implicit-Groups"><span class="nav-number">8.4.1.</span> <span class="nav-text">C.4.1. Implicit Groups</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#C-4-1-1-Thread-Block-Group"><span class="nav-number">8.4.1.1.</span> <span class="nav-text">C.4.1.1. Thread Block Group</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-4-1-2-Grid-Group"><span class="nav-number">8.4.1.2.</span> <span class="nav-text">C.4.1.2. Grid Group</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-4-1-3-Multi-Grid-Group"><span class="nav-number">8.4.2.</span> <span class="nav-text">C.4.1.3. Multi Grid Group</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-4-2-Explicit-Groups"><span class="nav-number">8.4.3.</span> <span class="nav-text">C.4.2. Explicit Groups</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#C-4-2-1-Thread-Block-Tile"><span class="nav-number">8.4.3.1.</span> <span class="nav-text">C.4.2.1. Thread Block Tile</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%EF%BC%9A"><span class="nav-number">8.4.3.2.</span> <span class="nav-text">注意：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-4-2-1-1-Warp-Synchronous-Code-Pattern"><span class="nav-number">8.4.3.3.</span> <span class="nav-text">C.4.2.1.1. Warp-Synchronous Code Pattern</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#C-4-2-1-2-Single-thread-group"><span class="nav-number">8.4.3.3.1.</span> <span class="nav-text">C.4.2.1.2. Single thread group</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#C-4-2-1-3-Thread-Block-Tile-of-size-larger-than-32"><span class="nav-number">8.4.3.3.2.</span> <span class="nav-text">C.4.2.1.3. Thread Block Tile of size larger than 32</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-4-2-2-Coalesced-Groups"><span class="nav-number">8.4.3.4.</span> <span class="nav-text">C.4.2.2. Coalesced Groups</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-4-2-2-1-Discovery-Pattern"><span class="nav-number">8.4.3.5.</span> <span class="nav-text">C.4.2.2.1. Discovery Pattern</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C-5-Group-Partitioning"><span class="nav-number">8.5.</span> <span class="nav-text">C.5. Group Partitioning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#C-5-1-tiled-partition"><span class="nav-number">8.5.1.</span> <span class="nav-text">C.5.1. tiled_partition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-5-2-labeled-partition"><span class="nav-number">8.5.2.</span> <span class="nav-text">C.5.2. labeled_partition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-5-3-binary-partition"><span class="nav-number">8.5.3.</span> <span class="nav-text">C.5.3. binary_partition</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C-6-Group-Collectives"><span class="nav-number">8.6.</span> <span class="nav-text">C.6. Group Collectives</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#C-6-1-Synchronization"><span class="nav-number">8.6.1.</span> <span class="nav-text">C.6.1. Synchronization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#C-6-1-1-sync"><span class="nav-number">8.6.1.1.</span> <span class="nav-text">C.6.1.1. sync</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-6-2-Data-Transfer"><span class="nav-number">8.6.2.</span> <span class="nav-text">C.6.2. Data Transfer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#C-6-2-1-memcpy-async"><span class="nav-number">8.6.2.1.</span> <span class="nav-text">C.6.2.1. memcpy_async</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-6-2-2-wait-and-wait-prior"><span class="nav-number">8.6.2.2.</span> <span class="nav-text">C.6.2.2. wait and wait_prior</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C-6-3-Data-manipulation"><span class="nav-number">8.7.</span> <span class="nav-text">C.6.3. Data manipulation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#C-6-3-1-reduce"><span class="nav-number">8.7.1.</span> <span class="nav-text">C.6.3.1. reduce</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#C-6-3-2-Reduce-Operators"><span class="nav-number">8.7.1.1.</span> <span class="nav-text">C.6.3.2. Reduce Operators</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C-6-3-3-inclusive-scan-and-exclusive-scan"><span class="nav-number">8.7.1.2.</span> <span class="nav-text">C.6.3.3. inclusive_scan and exclusive_scan</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C-7-Grid-Synchronization"><span class="nav-number">8.8.</span> <span class="nav-text">C.7. Grid Synchronization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C-8-Multi-Device-Synchronization"><span class="nav-number">8.9.</span> <span class="nav-text">C.8. Multi-Device Synchronization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95D-CUDA%E7%9A%84%E5%8A%A8%E6%80%81%E5%B9%B6%E8%A1%8C"><span class="nav-number">9.</span> <span class="nav-text">附录D-CUDA的动态并行</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#D-1-Introduction"><span class="nav-number">9.1.</span> <span class="nav-text">D.1. Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#D-1-1-Overview"><span class="nav-number">9.1.1.</span> <span class="nav-text">D.1.1. Overview</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#D-1-2-Glossary"><span class="nav-number">9.1.2.</span> <span class="nav-text">D.1.2. Glossary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#D-2-Execution-Environment-and-Memory-Model"><span class="nav-number">9.2.</span> <span class="nav-text">D.2. Execution Environment and Memory Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#D-2-1-Execution-Environment"><span class="nav-number">9.2.1.</span> <span class="nav-text">D.2.1. Execution Environment</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#D-2-1-1-Parent-and-Child-Grids"><span class="nav-number">9.2.1.1.</span> <span class="nav-text">D.2.1.1. Parent and Child Grids</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-2-1-2-Scope-of-CUDA-Primitives"><span class="nav-number">9.2.1.2.</span> <span class="nav-text">D.2.1.2. Scope of CUDA Primitives</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-2-1-3-Synchronization"><span class="nav-number">9.2.1.3.</span> <span class="nav-text">D.2.1.3. Synchronization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-2-1-4-Streams-and-Events"><span class="nav-number">9.2.1.4.</span> <span class="nav-text">D.2.1.4. Streams and Events</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-2-1-5-Ordering-and-Concurrency"><span class="nav-number">9.2.1.5.</span> <span class="nav-text">D.2.1.5. Ordering and Concurrency</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-2-1-6-Device-Management"><span class="nav-number">9.2.1.6.</span> <span class="nav-text">D.2.1.6. Device Management</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#D-2-2-Memory-Model"><span class="nav-number">9.2.2.</span> <span class="nav-text">D.2.2. Memory Model</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#D-2-2-1-Coherence-and-Consistency"><span class="nav-number">9.2.2.1.</span> <span class="nav-text">D.2.2.1. Coherence and Consistency</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-2-2-1-1-Global-Memory"><span class="nav-number">9.2.2.2.</span> <span class="nav-text">D.2.2.1.1. Global Memory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-2-2-1-2-Zero-Copy-Memory"><span class="nav-number">9.2.2.3.</span> <span class="nav-text">D.2.2.1.2. Zero Copy Memory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-2-2-1-3-Constant-Memory"><span class="nav-number">9.2.2.4.</span> <span class="nav-text">D.2.2.1.3. Constant Memory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-2-2-1-4-Shared-and-Local-Memory"><span class="nav-number">9.2.2.5.</span> <span class="nav-text">D.2.2.1.4. Shared and Local Memory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-2-2-1-5-Local-Memory"><span class="nav-number">9.2.2.6.</span> <span class="nav-text">D.2.2.1.5. Local Memory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-2-2-1-6-Texture-Memory"><span class="nav-number">9.2.2.7.</span> <span class="nav-text">D.2.2.1.6. Texture Memory</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#D-3-Programming-Interface"><span class="nav-number">9.3.</span> <span class="nav-text">D.3. Programming Interface</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#D-3-1-CUDA-C-Reference"><span class="nav-number">9.3.1.</span> <span class="nav-text">D.3.1. CUDA C++ Reference</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3-1-1-1-Launches-are-Asynchronous"><span class="nav-number">9.3.1.1.</span> <span class="nav-text">D.3.1.1.1. Launches are Asynchronous</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3-1-1-2-Launch-Environment-Configuration"><span class="nav-number">9.3.1.2.</span> <span class="nav-text">D.3.1.1.2. Launch Environment Configuration</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3-1-2-Streams"><span class="nav-number">9.3.1.3.</span> <span class="nav-text">D.3.1.2. Streams</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3-1-2-1-The-Implicit-NULL-Stream"><span class="nav-number">9.3.1.4.</span> <span class="nav-text">D.3.1.2.1. The Implicit (NULL) Stream</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3-1-3-Events"><span class="nav-number">9.3.1.5.</span> <span class="nav-text">D.3.1.3. Events</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3-1-4-Synchronization"><span class="nav-number">9.3.1.6.</span> <span class="nav-text">D.3.1.4. Synchronization</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#D-3-1-4-1-Block-Wide-Synchronization"><span class="nav-number">9.3.1.6.1.</span> <span class="nav-text">D.3.1.4.1. Block Wide Synchronization</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3-1-5-Device-Management"><span class="nav-number">9.3.1.7.</span> <span class="nav-text">D.3.1.5. Device Management</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3-1-6-Memory-Declarations"><span class="nav-number">9.3.1.8.</span> <span class="nav-text">D.3.1.6. Memory Declarations</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#D-3-1-6-1-Device-and-Constant-Memory"><span class="nav-number">9.3.1.8.1.</span> <span class="nav-text">D.3.1.6.1. Device and Constant Memory</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#D-3-1-6-2-Textures-amp-Surfaces"><span class="nav-number">9.3.1.8.2.</span> <span class="nav-text">D.3.1.6.2. Textures &amp; Surfaces</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#D-3-1-6-3-Shared-Memory-Variable-Declarations"><span class="nav-number">9.3.1.8.3.</span> <span class="nav-text">D.3.1.6.3. Shared Memory Variable Declarations</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#D-3-1-6-4-Symbol-Addresses"><span class="nav-number">9.3.1.8.4.</span> <span class="nav-text">D.3.1.6.4. Symbol Addresses</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3-1-7-API-Errors-and-Launch-Failures"><span class="nav-number">9.3.1.9.</span> <span class="nav-text">D.3.1.7. API Errors and Launch Failures</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#D-3-1-7-1-Launch-Setup-APIs"><span class="nav-number">9.3.1.9.1.</span> <span class="nav-text">D.3.1.7.1. Launch Setup APIs</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3-1-8-API-Reference"><span class="nav-number">9.3.1.10.</span> <span class="nav-text">D.3.1.8. API Reference</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#D-3-2-Device-side-Launch-from-PTX"><span class="nav-number">9.3.2.</span> <span class="nav-text">D.3.2. Device-side Launch from PTX</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3-2-1-Kernel-Launch-APIs"><span class="nav-number">9.3.2.1.</span> <span class="nav-text">D.3.2.1. Kernel Launch APIs</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#D-3-2-1-1-cudaLaunchDevice"><span class="nav-number">9.3.2.1.1.</span> <span class="nav-text">D.3.2.1.1. cudaLaunchDevice</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#D-3-2-1-2-cudaGetParameterBuffer"><span class="nav-number">9.3.2.1.2.</span> <span class="nav-text">D.3.2.1.2. cudaGetParameterBuffer</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3-2-2-Parameter-Buffer-Layout"><span class="nav-number">9.3.2.2.</span> <span class="nav-text">D.3.2.2. Parameter Buffer Layout</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#D-3-3-Toolkit-Support-for-Dynamic-Parallelism"><span class="nav-number">9.3.3.</span> <span class="nav-text">D.3.3. Toolkit Support for Dynamic Parallelism</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3-3-1-Including-Device-Runtime-API-in-CUDA-Code"><span class="nav-number">9.3.3.1.</span> <span class="nav-text">D.3.3.1. Including Device Runtime API in CUDA Code</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-3-3-2-Compiling-and-Linking"><span class="nav-number">9.3.3.2.</span> <span class="nav-text">D.3.3.2. Compiling and Linking</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#D-4-Programming-Guidelines"><span class="nav-number">9.4.</span> <span class="nav-text">D.4. Programming Guidelines</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#D-4-1-Basics"><span class="nav-number">9.4.1.</span> <span class="nav-text">D.4.1. Basics</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#D-4-2-Performance"><span class="nav-number">9.4.2.</span> <span class="nav-text">D.4.2. Performance</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#D-4-2-1-Synchronization"><span class="nav-number">9.4.2.1.</span> <span class="nav-text">D.4.2.1. Synchronization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#D-4-2-2-Dynamic-parallelism-enabled-Kernel-Overhead"><span class="nav-number">9.4.2.2.</span> <span class="nav-text">D.4.2.2. Dynamic-parallelism-enabled Kernel Overhead</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#D-4-3-Implementation-Restrictions-and-Limitations"><span class="nav-number">9.4.3.</span> <span class="nav-text">D.4.3. Implementation Restrictions and Limitations</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#D-4-3-1-Runtime"><span class="nav-number">9.4.3.1.</span> <span class="nav-text">D.4.3.1. Runtime</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#D-4-3-1-1-Memory-Footprint"><span class="nav-number">9.4.3.1.1.</span> <span class="nav-text">D.4.3.1.1. Memory Footprint</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#D-4-3-1-2-Nesting-and-Synchronization-Depth"><span class="nav-number">9.4.3.1.2.</span> <span class="nav-text">D.4.3.1.2. Nesting and Synchronization Depth</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#D-4-3-1-3-Pending-Kernel-Launches"><span class="nav-number">9.4.3.1.3.</span> <span class="nav-text">D.4.3.1.3. Pending Kernel Launches</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#D-4-3-1-4-Configuration-Options"><span class="nav-number">9.4.3.1.4.</span> <span class="nav-text">D.4.3.1.4. Configuration Options</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#D-4-3-1-5-Memory-Allocation-and-Lifetime"><span class="nav-number">9.4.3.1.5.</span> <span class="nav-text">D.4.3.1.5. Memory Allocation and Lifetime</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#D-4-3-1-6-SM-Id-and-Warp-Id"><span class="nav-number">9.4.3.1.6.</span> <span class="nav-text">D.4.3.1.6. SM Id and Warp Id</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#D-4-3-1-7-ECC-Errors"><span class="nav-number">9.4.3.1.7.</span> <span class="nav-text">D.4.3.1.7. ECC Errors</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95E%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="nav-number">10.</span> <span class="nav-text">附录E虚拟内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#E-1-Introduction"><span class="nav-number">10.1.</span> <span class="nav-text">E.1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#E-2-Query-for-support"><span class="nav-number">10.2.</span> <span class="nav-text">E.2. Query for support</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#E-3-Allocating-Physical-Memory"><span class="nav-number">10.3.</span> <span class="nav-text">E.3. Allocating Physical Memory</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#E-3-1-Shareable-Memory-Allocations"><span class="nav-number">10.3.1.</span> <span class="nav-text">E.3.1. Shareable Memory Allocations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#E-3-2-Memory-Type"><span class="nav-number">10.3.2.</span> <span class="nav-text">E.3.2. Memory Type</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#E-3-2-1-Compressible-Memory"><span class="nav-number">10.3.2.1.</span> <span class="nav-text">E.3.2.1. Compressible Memory</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#E-4-Reserving-a-Virtual-Address-Range"><span class="nav-number">10.4.</span> <span class="nav-text">E.4. Reserving a Virtual Address Range</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#E-5-Virtual-Aliasing-Support"><span class="nav-number">10.5.</span> <span class="nav-text">E.5. Virtual Aliasing Support</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#E-6-Mapping-Memory"><span class="nav-number">10.6.</span> <span class="nav-text">E.6. Mapping Memory</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#E-7-Control-Access-Rights"><span class="nav-number">10.7.</span> <span class="nav-text">E.7. Control Access Rights</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95F-%E6%B5%81%E5%BA%8F%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D"><span class="nav-number">11.</span> <span class="nav-text">附录F 流序内存分配</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#F-1-Introduction"><span class="nav-number">11.1.</span> <span class="nav-text">F.1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#F-2-Query-for-Support"><span class="nav-number">11.2.</span> <span class="nav-text">F.2. Query for Support</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#F-3-API-Fundamentals-cudaMallocAsync-and-cudaFreeAsync"><span class="nav-number">11.3.</span> <span class="nav-text">F.3. API Fundamentals (cudaMallocAsync and cudaFreeAsync)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#F-5-Default-Impicit-Pools"><span class="nav-number">11.4.</span> <span class="nav-text">F.5. Default&#x2F;Impicit Pools</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#F-6-Explicit-Pools"><span class="nav-number">11.5.</span> <span class="nav-text">F.6. Explicit Pools</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#F-7-Physical-Page-Caching-Behavior"><span class="nav-number">11.6.</span> <span class="nav-text">F.7. Physical Page Caching Behavior</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#F-8-Resource-Usage-Statistics"><span class="nav-number">11.7.</span> <span class="nav-text">F.8. Resource Usage Statistics</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#F-9-Memory-Reuse-Policies"><span class="nav-number">11.8.</span> <span class="nav-text">F.9. Memory Reuse Policies</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#F-9-1-cudaMemPoolReuseFollowEventDependencies"><span class="nav-number">11.8.1.</span> <span class="nav-text">F.9.1. cudaMemPoolReuseFollowEventDependencies</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#F-9-2-cudaMemPoolReuseAllowOpportunistic"><span class="nav-number">11.8.2.</span> <span class="nav-text">F.9.2. cudaMemPoolReuseAllowOpportunistic</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#F-10-Device-Accessibility-for-Multi-GPU-Support"><span class="nav-number">11.9.</span> <span class="nav-text">F.10. Device Accessibility for Multi-GPU Support</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#F-11-IPC-Memory-Pools"><span class="nav-number">11.10.</span> <span class="nav-text">F.11. IPC Memory Pools</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#F-11-1-Creating-and-Sharing-IPC-Memory-Pools"><span class="nav-number">11.10.1.</span> <span class="nav-text">F.11.1. Creating and Sharing IPC Memory Pools</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#F-11-2-Set-Access-in-the-Importing-Process"><span class="nav-number">11.10.2.</span> <span class="nav-text">F.11.2. Set Access in the Importing Process</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#F-11-3-Creating-and-Sharing-Allocations-from-an-Exported-Pool"><span class="nav-number">11.10.3.</span> <span class="nav-text">F.11.3. Creating and Sharing Allocations from an Exported Pool</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#F-11-4-IPC-Export-Pool-Limitations"><span class="nav-number">11.10.4.</span> <span class="nav-text">F.11.4. IPC Export Pool Limitations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#F-11-5-IPC-Import-Pool-Limitations"><span class="nav-number">11.10.5.</span> <span class="nav-text">F.11.5. IPC Import Pool Limitations</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#F-12-Synchronization-API-Actions"><span class="nav-number">11.11.</span> <span class="nav-text">F.12. Synchronization API Actions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#F-13-Addendums"><span class="nav-number">11.12.</span> <span class="nav-text">F.13. Addendums</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#F-13-1-cudaMemcpyAsync-Current-Context-Device-Sensitivity"><span class="nav-number">11.12.1.</span> <span class="nav-text">F.13.1. cudaMemcpyAsync Current Context&#x2F;Device Sensitivity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#F-13-2-cuPointerGetAttribute-Query"><span class="nav-number">11.12.2.</span> <span class="nav-text">F.13.2. cuPointerGetAttribute Query</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#F-13-3-cuGraphAddMemsetNode"><span class="nav-number">11.12.3.</span> <span class="nav-text">F.13.3. cuGraphAddMemsetNode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#F-13-4-Pointer-Attributes"><span class="nav-number">11.12.4.</span> <span class="nav-text">F.13.4. Pointer Attributes</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95G-%E5%9B%BE%E5%86%85%E5%AD%98%E8%8A%82%E7%82%B9"><span class="nav-number">12.</span> <span class="nav-text">附录G 图内存节点</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#G-1-Introduction"><span class="nav-number">12.1.</span> <span class="nav-text">G.1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#G-2-Support-and-Compatibility"><span class="nav-number">12.2.</span> <span class="nav-text">G.2. Support and Compatibility</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#G-3-API-Fundamentals"><span class="nav-number">12.3.</span> <span class="nav-text">G.3. API Fundamentals</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#G-3-1-Graph-Node-APIs"><span class="nav-number">12.3.1.</span> <span class="nav-text">G.3.1. Graph Node APIs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#G-3-2-Stream-Capture"><span class="nav-number">12.3.2.</span> <span class="nav-text">G.3.2. Stream Capture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#G-3-3-Accessing-and-Freeing-Graph-Memory-Outside-of-the-Allocating-Graph"><span class="nav-number">12.3.3.</span> <span class="nav-text">G.3.3. Accessing and Freeing Graph Memory Outside of the Allocating Graph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#G-3-4-cudaGraphInstantiateFlagAutoFreeOnLaunch"><span class="nav-number">12.3.4.</span> <span class="nav-text">G.3.4. cudaGraphInstantiateFlagAutoFreeOnLaunch</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#G-4-Optimized-Memory-Reuse"><span class="nav-number">12.4.</span> <span class="nav-text">G.4. Optimized Memory Reuse</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#G-4-1-Address-Reuse-within-a-Graph"><span class="nav-number">12.4.1.</span> <span class="nav-text">G.4.1. Address Reuse within a Graph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#G-4-2-Physical-Memory-Management-and-Sharing"><span class="nav-number">12.4.2.</span> <span class="nav-text">G.4.2. Physical Memory Management and Sharing</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#G-5-Performance-Considerations"><span class="nav-number">12.5.</span> <span class="nav-text">G.5. Performance Considerations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#G-5-1-First-Launch-cudaGraphUpload"><span class="nav-number">12.5.1.</span> <span class="nav-text">G.5.1. First Launch &#x2F; cudaGraphUpload</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#G-6-Physical-Memory-Footprint"><span class="nav-number">12.6.</span> <span class="nav-text">G.6. Physical Memory Footprint</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#G-7-Peer-Access"><span class="nav-number">12.7.</span> <span class="nav-text">G.7. Peer Access</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#G-7-1-Peer-Access-with-Graph-Node-APIs"><span class="nav-number">12.7.1.</span> <span class="nav-text">G.7.1. Peer Access with Graph Node APIs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#G-7-2-Peer-Access-with-Stream-Capture"><span class="nav-number">12.7.2.</span> <span class="nav-text">G.7.2. Peer Access with Stream Capture</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95H-%E6%95%B0%E5%AD%A6%E6%96%B9%E6%B3%95"><span class="nav-number">13.</span> <span class="nav-text">附录H 数学方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#H-1-Standard-Functions"><span class="nav-number">13.1.</span> <span class="nav-text">H.1. Standard Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Single-Precision-Floating-Point-Functions"><span class="nav-number">13.1.1.</span> <span class="nav-text">Single-Precision Floating-Point Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Double-Precision-Floating-Point-Functions"><span class="nav-number">13.1.2.</span> <span class="nav-text">Double-Precision Floating-Point Functions</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#H-2-Intrinsic-Functions"><span class="nav-number">13.2.</span> <span class="nav-text">H.2. Intrinsic Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Single-Precision-Floating-Point-Functions-1"><span class="nav-number">13.2.1.</span> <span class="nav-text">Single-Precision Floating-Point Functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Double-Precision-Floating-Point-Functions-1"><span class="nav-number">13.2.2.</span> <span class="nav-text">Double-Precision Floating-Point Functions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95I-C-%E8%AF%AD%E8%A8%80%E6%94%AF%E6%8C%81"><span class="nav-number">14.</span> <span class="nav-text">附录I C++ 语言支持</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#I-1-C-11-Language-Features"><span class="nav-number">14.1.</span> <span class="nav-text">I.1. C++11 Language Features</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#I-2-C-14-Language-Features"><span class="nav-number">14.2.</span> <span class="nav-text">I.2. C++14 Language Features</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#I-3-C-17-Language-Features"><span class="nav-number">14.3.</span> <span class="nav-text">I.3. C++17 Language Features</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#I-4-Restrictions"><span class="nav-number">14.4.</span> <span class="nav-text">I.4. Restrictions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-1-Host-Compiler-Extensions"><span class="nav-number">14.4.1.</span> <span class="nav-text">I.4.1. Host Compiler Extensions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-2-Preprocessor-Symbols"><span class="nav-number">14.4.2.</span> <span class="nav-text">I.4.2. Preprocessor Symbols</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-2-1-CUDA-ARCH"><span class="nav-number">14.4.2.1.</span> <span class="nav-text">I.4.2.1. CUDA_ARCH</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-3-Qualifiers"><span class="nav-number">14.4.3.</span> <span class="nav-text">I.4.3. Qualifiers</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-3-1-Device-Memory-Space-Specifiers"><span class="nav-number">14.4.3.1.</span> <span class="nav-text">I.4.3.1. Device Memory Space Specifiers</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-3-2-managed-Memory-Space-Specifier"><span class="nav-number">14.4.3.2.</span> <span class="nav-text">I.4.3.2. __managed__ Memory Space Specifier</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-3-3-Volatile-Qualifier"><span class="nav-number">14.4.3.3.</span> <span class="nav-text">I.4.3.3. Volatile Qualifier</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-4-Pointers"><span class="nav-number">14.4.4.</span> <span class="nav-text">I.4.4. Pointers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-5-Operators"><span class="nav-number">14.4.5.</span> <span class="nav-text">I.4.5. Operators</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-5-1-Assignment-Operator"><span class="nav-number">14.4.5.1.</span> <span class="nav-text">I.4.5.1. Assignment Operator</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-5-2-Address-Operator"><span class="nav-number">14.4.5.2.</span> <span class="nav-text">I.4.5.2. Address Operator</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-6-Run-Time-Type-Information-RTTI"><span class="nav-number">14.4.6.</span> <span class="nav-text">I.4.6. Run Time Type Information (RTTI)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-7-Exception-Handling"><span class="nav-number">14.4.7.</span> <span class="nav-text">I.4.7. Exception Handling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-8-Standard-Library"><span class="nav-number">14.4.8.</span> <span class="nav-text">I.4.8. Standard Library</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-9-Functions"><span class="nav-number">14.4.9.</span> <span class="nav-text">I.4.9. Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-9-1-External-Linkage"><span class="nav-number">14.4.9.1.</span> <span class="nav-text">I.4.9.1. External Linkage</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-9-2-Implicitly-declared-and-explicitly-defaulted-functions"><span class="nav-number">14.4.9.2.</span> <span class="nav-text">I.4.9.2. Implicitly-declared and explicitly-defaulted functions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-9-3-Function-Parameters"><span class="nav-number">14.4.9.3.</span> <span class="nav-text">I.4.9.3. Function Parameters</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#I-4-9-3-1-global-Function-Argument-Processing"><span class="nav-number">14.4.9.3.1.</span> <span class="nav-text">I.4.9.3.1. global Function Argument Processing</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-9-4-Static-Variables-within-Function"><span class="nav-number">14.4.9.4.</span> <span class="nav-text">I.4.9.4. Static Variables within Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-9-5-Function-Pointers"><span class="nav-number">14.4.9.5.</span> <span class="nav-text">I.4.9.5. Function Pointers</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-9-6-Function-Recursion"><span class="nav-number">14.4.9.6.</span> <span class="nav-text">I.4.9.6. Function Recursion</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-9-7-Friend-Functions"><span class="nav-number">14.4.9.7.</span> <span class="nav-text">I.4.9.7. Friend Functions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-9-8-Operator-Function"><span class="nav-number">14.4.9.8.</span> <span class="nav-text">I.4.9.8. Operator Function</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-10-Classes"><span class="nav-number">14.4.10.</span> <span class="nav-text">I.4.10. Classes</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-10-2-%E5%87%BD%E6%95%B0%E6%88%90%E5%91%98"><span class="nav-number">14.4.10.1.</span> <span class="nav-text">I.4.10.2. 函数成员</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-10-3-%E8%99%9A%E5%87%BD%E6%95%B0"><span class="nav-number">14.4.10.2.</span> <span class="nav-text">I.4.10.3. 虚函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-10-4-Virtual-Base-Classes"><span class="nav-number">14.4.10.3.</span> <span class="nav-text">I.4.10.4.  Virtual Base Classes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-10-5-Anonymous-Unions"><span class="nav-number">14.4.10.4.</span> <span class="nav-text">I.4.10.5. Anonymous Unions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-10-6-%E7%89%B9%E5%AE%9A%E4%BA%8E-Windows-%E7%9A%84"><span class="nav-number">14.4.10.5.</span> <span class="nav-text">I.4.10.6. 特定于 Windows 的</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-11-Templates"><span class="nav-number">14.4.11.</span> <span class="nav-text">I.4.11. Templates</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-12-Trigraphs-and-Digraphs"><span class="nav-number">14.4.12.</span> <span class="nav-text">I.4.12. Trigraphs and Digraphs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-13-Const-qualified-variables"><span class="nav-number">14.4.13.</span> <span class="nav-text">I.4.13. Const-qualified variables</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-14-Long-Double"><span class="nav-number">14.4.14.</span> <span class="nav-text">I.4.14. Long Double</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-15-Deprecation-Annotation"><span class="nav-number">14.4.15.</span> <span class="nav-text">I.4.15. Deprecation Annotation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-16-Noreturn-Annotation"><span class="nav-number">14.4.16.</span> <span class="nav-text">I.4.16. Noreturn Annotation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-17-likely-unlikely-Standard-Attributes"><span class="nav-number">14.4.17.</span> <span class="nav-text">I.4.17. [[likely]] &#x2F; [[unlikely]] Standard Attributes</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#I-4-18-const-and-pure-GNU-Attributes"><span class="nav-number">15.</span> <span class="nav-text">I.4.18. const and pure GNU Attributes</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-19-Intel-Host-Compiler-Specific"><span class="nav-number">15.0.1.</span> <span class="nav-text">I.4.19. Intel Host Compiler Specific</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-20-C-11-Features"><span class="nav-number">15.0.2.</span> <span class="nav-text">I.4.20. C++11 Features</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-20-1-Lambda-Expressions"><span class="nav-number">15.0.2.1.</span> <span class="nav-text">I.4.20.1. Lambda Expressions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-20-2-std-initializer-list"><span class="nav-number">15.0.2.2.</span> <span class="nav-text">I.4.20.2. std::initializer_list</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-20-3-Rvalue-references"><span class="nav-number">15.0.2.3.</span> <span class="nav-text">I.4.20.3. Rvalue references</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-20-4-Constexpr-functions-and-function-templates"><span class="nav-number">15.0.2.4.</span> <span class="nav-text">I.4.20.4. Constexpr functions and function templates</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-20-5-Constexpr-variables"><span class="nav-number">15.0.2.5.</span> <span class="nav-text">I.4.20.5. Constexpr variables</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-20-6-Inline-namespaces"><span class="nav-number">15.0.2.6.</span> <span class="nav-text">I.4.20.6. Inline namespaces</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#I-4-20-6-1-Inline-unnamed-namespaces"><span class="nav-number">15.0.2.6.1.</span> <span class="nav-text">I.4.20.6.1. Inline unnamed namespaces</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-20-7-thread-local"><span class="nav-number">15.0.2.7.</span> <span class="nav-text">I.4.20.7. thread_local</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-20-8-global-functions-and-function-templates"><span class="nav-number">15.0.2.8.</span> <span class="nav-text">I.4.20.8. global functions and function templates</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-20-9-managed-and-shared-variables"><span class="nav-number">15.0.2.9.</span> <span class="nav-text">I.4.20.9. managed and shared variables</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-20-10-Defaulted-functions"><span class="nav-number">15.0.2.10.</span> <span class="nav-text">I.4.20.10. Defaulted functions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-21-C-14-Features"><span class="nav-number">15.0.3.</span> <span class="nav-text">I.4.21. C++14 Features</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-21-1-Functions-with-deduced-return-type"><span class="nav-number">15.0.3.1.</span> <span class="nav-text">I.4.21.1. Functions with deduced return type</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-21-2-Variable-templates"><span class="nav-number">15.0.3.2.</span> <span class="nav-text">I.4.21.2. Variable templates</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-4-22-C-17-Features"><span class="nav-number">15.0.4.</span> <span class="nav-text">I.4.22. C++17 Features</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-22-1-Inline-Variable"><span class="nav-number">15.0.4.1.</span> <span class="nav-text">I.4.22.1. Inline Variable</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#I-4-22-2-Structured-Binding"><span class="nav-number">15.0.4.2.</span> <span class="nav-text">I.4.22.2. Structured Binding</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#I-5-Polymorphic-Function-Wrappers"><span class="nav-number">15.1.</span> <span class="nav-text">I.5. Polymorphic Function Wrappers</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#I-6-Extended-Lambdas"><span class="nav-number">15.2.</span> <span class="nav-text">I.6. Extended Lambdas</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#I-6-1-Extended-Lambda-Type-Traits"><span class="nav-number">15.2.1.</span> <span class="nav-text">I.6.1. Extended Lambda Type Traits</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-6-2-Extended-Lambda-Restrictions"><span class="nav-number">15.2.2.</span> <span class="nav-text">I.6.2. Extended Lambda Restrictions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-6-3-Notes-on-host-device-lambdas"><span class="nav-number">15.2.3.</span> <span class="nav-text">I.6.3. Notes on host device lambdas</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-6-4-this-Capture-By-Value"><span class="nav-number">15.2.4.</span> <span class="nav-text">I.6.4. *this Capture By Value</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-6-5-Additional-Notes"><span class="nav-number">15.2.5.</span> <span class="nav-text">I.6.5. Additional Notes</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#I-7-Code-Samples"><span class="nav-number">15.3.</span> <span class="nav-text">I.7. Code Samples</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#I-7-1-Data-Aggregation-Class"><span class="nav-number">15.3.1.</span> <span class="nav-text">I.7.1. Data Aggregation Class</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-7-2-Derived-Class"><span class="nav-number">15.3.2.</span> <span class="nav-text">I.7.2. Derived Class</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-7-3-Class-Template"><span class="nav-number">15.3.3.</span> <span class="nav-text">I.7.3. Class Template</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-7-4-Function-Template"><span class="nav-number">15.3.4.</span> <span class="nav-text">I.7.4. Function Template</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#I-7-5-Functor-Class"><span class="nav-number">15.3.5.</span> <span class="nav-text">I.7.5. Functor Class</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95J-%E7%BA%B9%E7%90%86%E8%8E%B7%E5%8F%96"><span class="nav-number">16.</span> <span class="nav-text">附录J 纹理获取</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#J-1-Nearest-Point-Sampling"><span class="nav-number">16.1.</span> <span class="nav-text">J.1. Nearest-Point Sampling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#J-2-Linear-Filtering"><span class="nav-number">16.2.</span> <span class="nav-text">J.2. Linear Filtering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#J-3-Table-Lookup"><span class="nav-number">16.3.</span> <span class="nav-text">J.3. Table Lookup</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95K-CUDA%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B"><span class="nav-number">17.</span> <span class="nav-text">附录K CUDA计算能力</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#K-1-Features-and-Technical-Specifications"><span class="nav-number">17.1.</span> <span class="nav-text">K.1. Features and Technical Specifications</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K-2-Floating-Point-Standard"><span class="nav-number">17.2.</span> <span class="nav-text">K.2. Floating-Point Standard</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K-3-Compute-Capability-3-x"><span class="nav-number">17.3.</span> <span class="nav-text">K.3. Compute Capability 3.x</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#K-3-1-Architecture"><span class="nav-number">17.3.1.</span> <span class="nav-text">K.3.1. Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-3-2-Global-Memory"><span class="nav-number">17.3.2.</span> <span class="nav-text">K.3.2. Global Memory</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-3-3-Shared-Memory"><span class="nav-number">17.3.3.</span> <span class="nav-text">K.3.3. Shared Memory</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K-4-Compute-Capability-5-x"><span class="nav-number">17.4.</span> <span class="nav-text">K.4. Compute Capability 5.x</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#K-4-1-Architecture"><span class="nav-number">17.4.1.</span> <span class="nav-text">K.4.1. Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-4-2-Global-Memory"><span class="nav-number">17.4.2.</span> <span class="nav-text">K.4.2. Global Memory</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-4-3-Shared-Memory"><span class="nav-number">17.4.3.</span> <span class="nav-text">K.4.3. Shared Memory</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K-5-Compute-Capability-6-x"><span class="nav-number">17.5.</span> <span class="nav-text">K.5. Compute Capability 6.x</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#K-5-1-Architecture"><span class="nav-number">17.5.1.</span> <span class="nav-text">K.5.1. Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-5-2-Global-Memory"><span class="nav-number">17.5.2.</span> <span class="nav-text">K.5.2. Global Memory</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-5-3-Shared-Memory"><span class="nav-number">17.5.3.</span> <span class="nav-text">K.5.3. Shared Memory</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K-6-Compute-Capability-7-x"><span class="nav-number">17.6.</span> <span class="nav-text">K.6. Compute Capability 7.x</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#K-6-2-Independent-Thread-Scheduling"><span class="nav-number">17.6.1.</span> <span class="nav-text">K.6.2. Independent Thread Scheduling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-6-3-Global-Memory"><span class="nav-number">17.6.2.</span> <span class="nav-text">K.6.3. Global Memory</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-6-4-Shared-Memory"><span class="nav-number">17.6.3.</span> <span class="nav-text">K.6.4. Shared Memory</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K-7-Compute-Capability-8-x"><span class="nav-number">17.7.</span> <span class="nav-text">K.7. Compute Capability 8.x</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#K-7-1-Architecture"><span class="nav-number">17.7.1.</span> <span class="nav-text">K.7.1. Architecture</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-7-2-Global-Memory"><span class="nav-number">17.7.2.</span> <span class="nav-text">K.7.2. Global Memory</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K-7-3-Shared-Memory"><span class="nav-number">17.7.3.</span> <span class="nav-text">K.7.3. Shared Memory</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95L-CUDA%E5%BA%95%E5%B1%82%E9%A9%B1%E5%8A%A8API"><span class="nav-number">18.</span> <span class="nav-text">附录L CUDA底层驱动API</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#L-1-Context"><span class="nav-number">18.1.</span> <span class="nav-text">L.1. Context</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L-2-Module"><span class="nav-number">18.2.</span> <span class="nav-text">L.2. Module</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L-3-Kernel-Execution"><span class="nav-number">18.3.</span> <span class="nav-text">L.3. Kernel Execution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L-4-Interoperability-between-Runtime-and-Driver-APIs"><span class="nav-number">18.4.</span> <span class="nav-text">L.4. Interoperability between Runtime and Driver APIs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L-5-Driver-Entry-Point-Access"><span class="nav-number">18.5.</span> <span class="nav-text">L.5. Driver Entry Point Access</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#L-5-1-Introduction"><span class="nav-number">18.5.1.</span> <span class="nav-text">L.5.1. Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L-5-2-Driver-Function-Typedefs"><span class="nav-number">18.5.2.</span> <span class="nav-text">L.5.2. Driver Function Typedefs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L-5-3-Driver-Function-Retrieval"><span class="nav-number">18.5.3.</span> <span class="nav-text">L.5.3. Driver Function Retrieval</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#L-5-3-1-Using-the-driver-API"><span class="nav-number">18.5.3.1.</span> <span class="nav-text">L.5.3.1. Using the driver API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L-5-3-2-Using-the-runtime-API"><span class="nav-number">18.5.3.2.</span> <span class="nav-text">L.5.3.2. Using the runtime API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L-5-3-3-Retrieve-per-thread-default-stream-versions"><span class="nav-number">18.5.3.3.</span> <span class="nav-text">L.5.3.3. Retrieve per-thread default stream versions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L-5-3-4-Access-new-CUDA-features"><span class="nav-number">18.5.3.4.</span> <span class="nav-text">L.5.3.4. Access new CUDA features</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95N-CUDA%E7%9A%84%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98"><span class="nav-number">19.</span> <span class="nav-text">附录N CUDA的统一内存</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#N-1-Unified-Memory-Introduction"><span class="nav-number">19.1.</span> <span class="nav-text">N.1. Unified Memory Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#N-1-1-System-Requirements"><span class="nav-number">19.1.1.</span> <span class="nav-text">N.1.1. System Requirements</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-1-2-Simplifying-GPU-Programming"><span class="nav-number">19.1.2.</span> <span class="nav-text">N.1.2. Simplifying GPU Programming</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-1-3-Data-Migration-and-Coherency"><span class="nav-number">19.1.3.</span> <span class="nav-text">N.1.3. Data Migration and Coherency</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-1-4-GPU-Memory-Oversubscription"><span class="nav-number">19.1.4.</span> <span class="nav-text">N.1.4. GPU Memory Oversubscription</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-1-5-Multi-GPU"><span class="nav-number">19.1.5.</span> <span class="nav-text">N.1.5. Multi-GPU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-1-6-System-Allocator"><span class="nav-number">19.1.6.</span> <span class="nav-text">N.1.6. System Allocator</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-1-7-Hardware-Coherency"><span class="nav-number">19.1.7.</span> <span class="nav-text">N.1.7. Hardware Coherency</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-1-8-Access-Counters"><span class="nav-number">19.1.8.</span> <span class="nav-text">N.1.8. Access Counters</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#N-2-Programming-Model"><span class="nav-number">19.2.</span> <span class="nav-text">N.2. Programming Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#N-2-1-Managed-Memory-Opt-In"><span class="nav-number">19.2.1.</span> <span class="nav-text">N.2.1. Managed Memory Opt In</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#N-2-1-1-Explicit-Allocation-Using-cudaMallocManaged"><span class="nav-number">19.2.1.1.</span> <span class="nav-text">N.2.1.1. Explicit Allocation Using cudaMallocManaged()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#N-2-1-2-Global-Scope-Managed-Variables-Using-managed"><span class="nav-number">19.2.1.2.</span> <span class="nav-text">N.2.1.2. Global-Scope Managed Variables Using managed</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-2-2-Coherency-and-Concurrency"><span class="nav-number">19.2.2.</span> <span class="nav-text">N.2.2. Coherency and Concurrency</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#N-2-2-1-GPU-Exclusive-Access-To-Managed-Memory"><span class="nav-number">19.2.2.1.</span> <span class="nav-text">N.2.2.1. GPU Exclusive Access To Managed Memory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#N-2-2-2-Explicit-Synchronization-and-Logical-GPU-Activity"><span class="nav-number">19.2.2.2.</span> <span class="nav-text">N.2.2.2. Explicit Synchronization and Logical GPU Activity</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#N-2-2-3-Managing-Data-Visibility-and-Concurrent-CPU-GPU-Access-with-Streams"><span class="nav-number">19.2.2.3.</span> <span class="nav-text">N.2.2.3. Managing Data Visibility and Concurrent CPU + GPU Access with Streams</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#N-2-2-4-Stream-Association-Examples"><span class="nav-number">19.2.2.4.</span> <span class="nav-text">N.2.2.4. Stream Association Examples</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#N-2-2-5-Stream-Attach-With-Multithreaded-Host-Programs"><span class="nav-number">19.2.2.5.</span> <span class="nav-text">N.2.2.5. Stream Attach With Multithreaded Host Programs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#N-2-2-6-Advanced-Topic-Modular-Programs-and-Data-Access-Constraints"><span class="nav-number">19.2.2.6.</span> <span class="nav-text">N.2.2.6. Advanced Topic: Modular Programs and Data Access Constraints</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#N-2-2-7-Memcpy-Memset-Behavior-With-Managed-Memory"><span class="nav-number">19.2.2.7.</span> <span class="nav-text">N.2.2.7. Memcpy()&#x2F;Memset() Behavior With Managed Memory</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-2-3-Language-Integration"><span class="nav-number">19.2.3.</span> <span class="nav-text">N.2.3. Language Integration</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#N-2-3-1-Host-Program-Errors-with-managed-Variables"><span class="nav-number">19.2.3.1.</span> <span class="nav-text">N.2.3.1. Host Program Errors with managed Variables</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-2-4-Querying-Unified-Memory-Support"><span class="nav-number">19.2.4.</span> <span class="nav-text">N.2.4. Querying Unified Memory Support</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#N-2-4-1-Device-Properties"><span class="nav-number">19.2.4.1.</span> <span class="nav-text">N.2.4.1. Device Properties</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-2-5-Advanced-Topics"><span class="nav-number">19.2.5.</span> <span class="nav-text">N.2.5. Advanced Topics</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#N-2-5-1-Managed-Memory-with-Multi-GPU-Programs-on-pre-6-x-Architectures"><span class="nav-number">19.2.5.1.</span> <span class="nav-text">N.2.5.1. Managed Memory with Multi-GPU Programs on pre-6.x Architectures</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#N-2-5-2-Using-fork-with-Managed-Memory"><span class="nav-number">19.2.5.2.</span> <span class="nav-text">N.2.5.2. Using fork() with Managed Memory</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#N-3-Performance-Tuning"><span class="nav-number">19.3.</span> <span class="nav-text">N.3. Performance Tuning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#N-3-1-Data-Prefetching"><span class="nav-number">19.3.1.</span> <span class="nav-text">N.3.1. Data Prefetching</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-3-2-Data-Usage-Hints"><span class="nav-number">19.3.2.</span> <span class="nav-text">N.3.2. Data Usage Hints</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-3-3-Querying-Usage-Attributes"><span class="nav-number">19.3.3.</span> <span class="nav-text">N.3.3. Querying Usage Attributes</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Hao Yu</p>
  <div class="site-description" itemprop="description">Introduce something interesting and recode learning process, some articles are written by others, the original link has been given as much as possible, thanks to the original author</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">128</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yuhao0102" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yuhao0102" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yuhhpc0203@gmail.com" title="E-Mail → mailto:yuhhpc0203@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hao Yu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
